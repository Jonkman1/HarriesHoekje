[
  {
    "path": "posts/2021-03-10-machine-learning/",
    "title": "Machine Learning met Tidymodels",
    "description": "Enkele blogs zal ik aan Machine Learning besteden. Ik zal enkele tutorials bewerken die mij veel geleerd hebben. Lisa Lendway, van wie ik al twee keer eerder materiaal gebruikte, schreef een goede blog over tidymodels. Zie hieronder.",
    "author": [
      {
        "name": "Lisa Lendway, vertaling Harrie Jonkman",
        "url": {}
      }
    ],
    "date": "2021-03-11",
    "categories": [],
    "contents": "\nLaad tidyverse, tidymodels en enkele andere pakketten en zet theme (optioneel) voor een bepaalde vormgeving van de figuren.\n\n\n\nLees de King County Housing-data in en kijk eens naar de eerste vijf rijen.\n\n# A tibble: 5 x 21\n  id         date        price bedrooms bathrooms sqft_living sqft_lot\n  <chr>      <date>      <dbl>    <int>     <dbl>       <int>    <int>\n1 7129300520 2014-10-13 221900        3      1           1180     5650\n2 6414100192 2014-12-09 538000        3      2.25        2570     7242\n3 5631500400 2015-02-25 180000        2      1            770    10000\n4 2487200875 2014-12-09 604000        4      3           1960     5000\n5 1954400510 2015-02-18 510000        3      2           1680     8080\n# … with 14 more variables: floors <dbl>, waterfront <lgl>,\n#   view <int>, condition <fct>, grade <fct>, sqft_above <int>,\n#   sqft_basement <int>, yr_built <int>, yr_renovated <int>,\n#   zipcode <fct>, lat <dbl>, long <dbl>, sqft_living15 <int>,\n#   sqft_lot15 <int>\n\nOver de house_prices data lezen we het volgende:\n\n“Deze dataset bevat huizenverkoopprijzen voor King County, waar Seattle deel van uitmaakt. Het omvat huizen verkocht tussen mei 2014 en mei 2015. Deze dataset is verkregen via Kaggle.com.” De beschrijving van de variabelen in de dataset in de documentatie lijkt niet helemaal te kloppen. Een meer accurate beschrijving is hieronder te vinden. In ieder geval willen we hier de prijs van woningen modelleren.\n\n\nExploratie\nKijk eerst eens naar de verdelingen van alle variabelen om te zien of er iets onregelmatigs aan de hand is.\nKwantitatieve variabelen:\n\n\n\nDingen die opvielen en eerste gedachten over het opstartproces: * ‘Right-skewness’ in de variabele price en alle variabelen betreffende vierkante meters –> log transformeren indien lineaire regressie wordt gebruikt. * Veel 0’s in sqft_basement, view, en yr_renovated –> maak indicator variabelen van het hebben van dat kenmerk vs. niet hebben , dat wil zeggen een variabele genaamd basement waar een 0 aangeeft geen kelder (sqft_basement = 0) en wel een kelder bij (sqft_basement > 0).\n* Leeftijd van huis is misschien een betere, interpreteerbare variabele dan bouwjaar –> age_at_sale = year(date) - yr_built.\n\n\n\nData splitsen (in train- en test-sets)\nEerst splitsen we de gegevens op in training- en test-datasets. We gebruiken de trainingsgegevens om verschillende soorten modellen te proberen en de parameters van die modellen zo nodig aan te passen. De testdataset wordt bewaard voor het allerlaatst om een kleine subset van modellen te vergelijken. De initial_split() functie uit de rsample bibliotheek (onderdeel van tidymodels) wordt gebruikt om tot deze splitsing te komen. We splitsen deze dataset random, maar er zijn andere mogelijkheden om tot gestratificeerde steekproeven te komen. Daarna gebruiken we training() en testing() om de twee datasets, house_training en house_testing, te extraheren.\n\n<Analysis/Assess/Total>\n<16210/5403/21613>\n\nLater zullen we 5-voudige cross-validatie gebruiken om het model te evalueren en de modelparameters aan te passen. We zetten de vijfvoud van de trainingsdata op met de vfold_cv() functie. We zullen dit later in meer detail uitleggen.\n\n\n\nData voorspel: recipe() en step_xxx()\nWe gebruiken de recipe()-functie om de uitkomstvariabele en de predictoren te definiëren.\nEen verscheidenheid van step_xxx() functies kan worden gebruikt om data te bewerken/transformeren. Vind ze allemaal hier. Ik heb er een paar gebruikt, met korte beschrijvingen in de code. Ik heb ook een aantal selectiefuncties gebruikt, zoals all_predictors() en all_nominal() om de juiste variabelen te selecteren.\nWe gebruiken ook update_roles() om de rollen van sommige variabelen te veranderen. Voor ons zijn dit variabelen die we misschien willen meenemen voor evaluatiedoeleinden, maar die niet gebruikt zullen worden bij het bouwen van het model. Ik heb gekozen voor de rol evaluative, maar je kunt die rol elke naam geven die je maar wilt, bijvoorbeeld id, extra, junk (misschien een slecht idee?).\n\n\n\nPas het toe op de trainings-dataset, gewoon om te zien wat er gebeurt. Let op de namen van de variabelen.\n\n# A tibble: 16,210 x 36\n   id        date       bedrooms bathrooms sqft_living sqft_lot floors\n   <fct>     <date>        <int>     <dbl>       <dbl>    <dbl>  <dbl>\n 1 71293005… 2014-10-13        3      1           3.07     3.75      1\n 2 64141001… 2014-12-09        3      2.25        3.41     3.86      2\n 3 56315004… 2015-02-25        2      1           2.89     4         1\n 4 24872008… 2014-12-09        4      3           3.29     3.70      1\n 5 19544005… 2015-02-18        3      2           3.23     3.91      1\n 6 72375503… 2014-05-12        4      4.5         3.73     5.01      1\n 7 13214000… 2014-06-27        3      2.25        3.23     3.83      2\n 8 20080002… 2015-01-15        3      1.5         3.03     3.99      1\n 9 24146001… 2015-04-15        3      1           3.25     3.87      1\n10 37935001… 2015-03-12        3      2.5         3.28     3.82      2\n# … with 16,200 more rows, and 29 more variables: waterfront <dbl>,\n#   view <dbl>, sqft_above <dbl>, zipcode <fct>, lat <dbl>,\n#   long <dbl>, price <dbl>, basement <dbl>, renovated <dbl>,\n#   age_at_sale <dbl>, condition_X2 <dbl>, condition_X3 <dbl>,\n#   condition_X4 <dbl>, condition_X5 <dbl>, grade_X7 <dbl>,\n#   grade_X8 <dbl>, grade_X9 <dbl>, grade_high <dbl>,\n#   date_month_Feb <dbl>, date_month_Mar <dbl>, date_month_Apr <dbl>,\n#   date_month_May <dbl>, date_month_Jun <dbl>, date_month_Jul <dbl>,\n#   date_month_Aug <dbl>, date_month_Sep <dbl>, date_month_Oct <dbl>,\n#   date_month_Nov <dbl>, date_month_Dec <dbl>\n\nHet model definiëren en workflows creëren\nNu we de gegevens hebben opgesplitst en voorbewerkt, zijn we klaar om te modelleren! Eerst zullen we price (die nu eigenlijk log(price) is) modelleren met eenvoudige lineaire regressie.\nWe zullen dit doen met behulp van enkele modelleringsfuncties uit het parsnip pakket. Vind alle beschikbare functies hier. Hier is lineaire regressie meer in detail.\nOm ons model te definiëren, moeten we de volgende stappen zetten:\nBepaal het modeltype, dat is het algemene moeltype dat u wilt draaien.\nStel de motor in, die het pakket/de functie bepaalt die zal worden gebruikt om het model te draaien.\nStel de modus in, die ofwel “regressie” is voor continue uitkomstvariabelen of “classificatie” voor binaire/categorische uitkomstvariabelen. (Merk op dat voor lineaire regressie, het alleen “regressie” kan zijn, dus we hebben deze stap in dit geval niet NODIG).\n(OPTIONEEL) Stel argumenten in om af te stemmen (‘tunen’). We zullen hier later een voorbeeld van zien.\n\n\n\nDit is slechts het opzetten van het proces. We hebben het model nog niet aan de gegevens aangepast en er is nog één stap voordat we dat doen - een workflow maken! Hier wordt de voorbewerking en de stappen in het model gecombineerd.\n\n══ Workflow ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ──────────────────────────────────────────────────────\n6 Recipe Steps\n\n● step_rm()\n● step_log()\n● step_mutate()\n● step_rm()\n● step_date()\n● step_dummy()\n\n── Model ─────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\nModelleren en evalueren\nNu zijn we eindelijk klaar om het model te draaien! Na al dat werk, lijkt dit deel eenvoudig. We gebruiken eerst de fit() functie om het model te fitten, door te vertellen op welke dataset we het model willen draaien. Daarna gebruiken we enkele andere functies om de resultaten mooi weer te geven.\n\n# A tibble: 31 x 5\n   term        estimate std.error statistic p.value\n   <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n 1 (Intercept)    4.01      0.048     83.4        0\n 2 bedrooms      -0.018     0.002    -11.2        0\n 3 bathrooms      0.036     0.003     14.3        0\n 4 sqft_living    0.294     0.025     11.7        0\n 5 sqft_lot      -0.038     0.003    -11.0        0\n 6 floors         0.021     0.003      6.87       0\n 7 waterfront     0.194     0.013     15.0        0\n 8 view          -0.061     0.004    -15.6        0\n 9 sqft_above     0.149     0.025      5.99       0\n10 basement      -0.042     0.005     -9.14       0\n# … with 21 more rows\n\nOm het model te evalueren, gebruiken we crossvalidatie (CV), specifiek de 5-voudige CV. (Ik veronderstel dat we niet zowel de vorige stap van het passen van een model op de trainingsgegevens EN deze stap moeten doen, maar ik kon er niet achter komen hoe we het uiteindelijke model uit de CV-gegevens kunnen halen … dus dit was mijn oplossing voor nu). We passen het model dus aan met de 5-voudige dataset die we in het begin hebben gemaakt. Voor een diepere discussie over crossvalidatie, raad ik Bradley Boehmke’s Resampling sectie van Hands on Machine Learning with R aan.\n\n# A tibble: 10 x 5\n   id    .metric .estimator .estimate .config             \n   <chr> <chr>   <chr>          <dbl> <chr>               \n 1 Fold1 rmse    standard       0.135 Preprocessor1_Model1\n 2 Fold1 rsq     standard       0.662 Preprocessor1_Model1\n 3 Fold2 rmse    standard       0.137 Preprocessor1_Model1\n 4 Fold2 rsq     standard       0.644 Preprocessor1_Model1\n 5 Fold3 rmse    standard       0.137 Preprocessor1_Model1\n 6 Fold3 rsq     standard       0.638 Preprocessor1_Model1\n 7 Fold4 rmse    standard       0.133 Preprocessor1_Model1\n 8 Fold4 rsq     standard       0.655 Preprocessor1_Model1\n 9 Fold5 rmse    standard       0.135 Preprocessor1_Model1\n10 Fold5 rsq     standard       0.642 Preprocessor1_Model1\n# A tibble: 2 x 6\n  .metric .estimator  mean     n  std_err .config             \n  <chr>   <chr>      <dbl> <int>    <dbl> <chr>               \n1 rmse    standard   0.135     5 0.000668 Preprocessor1_Model1\n2 rsq     standard   0.648     5 0.00437  Preprocessor1_Model1\n# A tibble: 2 x 5\n# Groups:   .metric [2]\n  .metric .estimator  mean     n  std_err\n  <chr>   <chr>      <dbl> <int>    <dbl>\n1 rmse    standard   0.135     5 0.000668\n2 rsq     standard   0.648     5 0.00437 \n\nVoorspellen en evalueren van testgegevens\nIn dit eenvoudige scenario zijn we wellicht geïnteresseerd in hoe het model presteert op de testgegevens die werden weggelaten. De onderstaande code past het model toe op de trainingsgegevens en past het toe op de testgegevens. Er zijn andere manieren waarop we dit hadden kunnen doen, maar de manier waarop we het hier doen zal nuttig zijn wanneer we complexere modellen gaan gebruiken waarbij we de modelparameters moeten afstellen.\nNadat het model is aangepast en toegepast, verzamelen we de prestatiecijfers en geven we ze weer en tonen we de voorspellingen van de testgegevens.\n\n# A tibble: 2 x 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.135 Preprocessor1_Model1\n2 rsq     standard       0.655 Preprocessor1_Model1\n# A tibble: 5,403 x 5\n   id               .pred  .row price .config             \n   <chr>            <dbl> <int> <dbl> <chr>               \n 1 train/test split  5.58    12  5.67 Preprocessor1_Model1\n 2 train/test split  5.53    17  5.60 Preprocessor1_Model1\n 3 train/test split  5.90    27  5.97 Preprocessor1_Model1\n 4 train/test split  5.58    29  5.64 Preprocessor1_Model1\n 5 train/test split  5.67    31  5.76 Preprocessor1_Model1\n 6 train/test split  5.88    38  5.81 Preprocessor1_Model1\n 7 train/test split  5.69    40  5.78 Preprocessor1_Model1\n 8 train/test split  5.79    41  5.80 Preprocessor1_Model1\n 9 train/test split  5.77    42  5.89 Preprocessor1_Model1\n10 train/test split  5.66    44  5.84 Preprocessor1_Model1\n# … with 5,393 more rows\n\nDe onderstaande code maakt een eenvoudige plot om de voorspelde vs. de werkelijke prijs van de huisgegevens te onderzoeken.\n\n\n\n\n\n\nHoe zal het model worden gebruikt?\nWanneer we modellen creëren is het belangrijk na te denken over hoe het model zal worden gebruikt en met name hoe het model schade zou kunnen berokkenen. Wat opvalt in de bovenstaande grafieken is dat de prijs van woningen met een lagere prijs gemiddeld wordt overschat, terwijl de prijs van woningen met een hogere prijs gemiddeld wordt onderschat.\nWat als dit model werd gebruikt om de prijs van woningen te bepalen voor de onroerendgoedbelasting? Dan zouden lager geprijsde huizen te zwaar worden belast en hoger geprijsde huizen te weinig.\nMer complexe modellen met tuning parameters\nNu gaan we de Least Absolute Shrinkage and Selection Operator (LASSO) regressie proberen. Deze methode krimpt sommige coëfficiënten tot 0 op basis van een strafterm. We zullen crossvalidatie gebruiken om ons te helpen de beste strafterm te vinden.\nHet model opzetten\nWe zetten het model op zoals we het lineaire model hebben opgezet, maar voegen nu een set_args() functie toe. We vertellen het model dat we de penalty parameter later gaan aanpassen.\n\n\n\nDe workflow updaten\nEn dan creëren we een LASSO workflow.\n\n══ Workflow ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ──────────────────────────────────────────────────────\n6 Recipe Steps\n\n● step_rm()\n● step_log()\n● step_mutate()\n● step_rm()\n● step_date()\n● step_dummy()\n\n── Model ─────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n\nAfstemmen van de strafparameter\nWe gebruiken de grid_regular() functie uit de dials bibliotheek om een aantal waarden van de penalty parameter voor ons te kiezen. Als alternatief kunnen we ook een vector van waarden opgeven die we willen proberen.\n\n# A tibble: 20 x 1\n    penalty\n      <dbl>\n 1 1.00e-10\n 2 3.36e-10\n 3 1.13e- 9\n 4 3.79e- 9\n 5 1.27e- 8\n 6 4.28e- 8\n 7 1.44e- 7\n 8 4.83e- 7\n 9 1.62e- 6\n10 5.46e- 6\n11 1.83e- 5\n12 6.16e- 5\n13 2.07e- 4\n14 6.95e- 4\n15 2.34e- 3\n16 7.85e- 3\n17 2.64e- 2\n18 8.86e- 2\n19 2.98e- 1\n20 1.00e+ 0\n\nGebruik de tune_grid() functie om het model te draaien met behulp van crossvalidatie voor alle penalty_grid waarden en evalueer op alle vouwen.\n\n# Tuning results\n# 5-fold cross-validation \n# A tibble: 5 x 4\n  splits                id    .metrics          .notes          \n  <list>                <chr> <list>            <list>          \n1 <rsplit [12968/3242]> Fold1 <tibble [40 × 5]> <tibble [2 × 1]>\n2 <rsplit [12968/3242]> Fold2 <tibble [40 × 5]> <tibble [2 × 1]>\n3 <rsplit [12968/3242]> Fold3 <tibble [40 × 5]> <tibble [2 × 1]>\n4 <rsplit [12968/3242]> Fold4 <tibble [40 × 5]> <tibble [2 × 1]>\n5 <rsplit [12968/3242]> Fold5 <tibble [40 × 5]> <tibble [2 × 1]>\n\nBekijk de resultaten van de cross-validatie.\n\n# A tibble: 100 x 6\n   id     penalty .metric .estimator .estimate .config              \n   <chr>    <dbl> <chr>   <chr>          <dbl> <chr>                \n 1 Fold1 1.00e-10 rmse    standard       0.135 Preprocessor1_Model01\n 2 Fold1 3.36e-10 rmse    standard       0.135 Preprocessor1_Model02\n 3 Fold1 1.13e- 9 rmse    standard       0.135 Preprocessor1_Model03\n 4 Fold1 3.79e- 9 rmse    standard       0.135 Preprocessor1_Model04\n 5 Fold1 1.27e- 8 rmse    standard       0.135 Preprocessor1_Model05\n 6 Fold1 4.28e- 8 rmse    standard       0.135 Preprocessor1_Model06\n 7 Fold1 1.44e- 7 rmse    standard       0.135 Preprocessor1_Model07\n 8 Fold1 4.83e- 7 rmse    standard       0.135 Preprocessor1_Model08\n 9 Fold1 1.62e- 6 rmse    standard       0.135 Preprocessor1_Model09\n10 Fold1 5.46e- 6 rmse    standard       0.135 Preprocessor1_Model10\n# … with 90 more rows\n# A tibble: 20 x 7\n    penalty .metric .estimator  mean     n  std_err .config           \n      <dbl> <chr>   <chr>      <dbl> <int>    <dbl> <chr>             \n 1 1.00e-10 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\n 2 3.36e-10 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\n 3 1.13e- 9 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\n 4 3.79e- 9 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\n 5 1.27e- 8 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\n 6 4.28e- 8 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\n 7 1.44e- 7 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\n 8 4.83e- 7 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\n 9 1.62e- 6 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\n10 5.46e- 6 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\n11 1.83e- 5 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\n12 6.16e- 5 rmse    standard   0.135     5 0.000640 Preprocessor1_Mod…\n13 2.07e- 4 rmse    standard   0.135     5 0.000623 Preprocessor1_Mod…\n14 6.95e- 4 rmse    standard   0.135     5 0.000591 Preprocessor1_Mod…\n15 2.34e- 3 rmse    standard   0.136     5 0.000522 Preprocessor1_Mod…\n16 7.85e- 3 rmse    standard   0.142     5 0.000513 Preprocessor1_Mod…\n17 2.64e- 2 rmse    standard   0.161     5 0.000515 Preprocessor1_Mod…\n18 8.86e- 2 rmse    standard   0.191     5 0.000896 Preprocessor1_Mod…\n19 2.98e- 1 rmse    standard   0.228     5 0.000960 Preprocessor1_Mod…\n20 1.00e+ 0 rmse    standard   0.228     5 0.000960 Preprocessor1_Mod…\n\n# A tibble: 1 x 2\n   penalty .config              \n     <dbl> <chr>                \n1 0.000207 Preprocessor1_Model13\n\nUpdate de workflow voor best afgestemde parameter\nPas de workflow aan om de beste afstemparameter (kleinste rmse, met select_best() in vorige stap) in het model op te nemen. Er zijn andere manieren om modellen te selecteren, zoals select_by_one_std_error() die “het meest eenvoudige model selecteert dat binnen één standaardfout van de numeriek optimale resultaten ligt”.\n\n══ Workflow ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ──────────────────────────────────────────────────────\n6 Recipe Steps\n\n● step_rm()\n● step_log()\n● step_mutate()\n● step_rm()\n● step_date()\n● step_dummy()\n\n── Model ─────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 0.000206913808111479\n  mixture = 1\n\nComputational engine: glmnet \n\nPas de beste afstelling toe op de trainingsgegevens\nNu kunnen we dit toepassen op de trainingsgegevens en het resulterende model bekijken. De uitvoer van het model was niet wat ik verwachtte. Volgens Julia Silge’s antwoord op mijn vraag hier, zou dit verholpen moeten zijn als je parsnip installeert vanaf GitHub] met devtools::install_github(\"tidymodels/parsnip\") van de devtools bibliotheek.\n\n══ Workflow [trained] ════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ──────────────────────────────────────────────────────\n6 Recipe Steps\n\n● step_rm()\n● step_log()\n● step_mutate()\n● step_rm()\n● step_date()\n● step_dummy()\n\n── Model ─────────────────────────────────────────────────────────────\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~1) \n\n   Df  %Dev   Lambda\n1   0  0.00 0.153800\n2   1  7.71 0.140200\n3   1 14.11 0.127700\n4   1 19.42 0.116400\n5   1 23.83 0.106000\n6   1 27.49 0.096610\n7   1 30.53 0.088030\n8   1 33.06 0.080210\n9   1 35.15 0.073080\n10  2 37.42 0.066590\n11  2 39.41 0.060670\n12  2 41.06 0.055280\n13  2 42.43 0.050370\n14  3 43.93 0.045900\n15  3 45.24 0.041820\n16  3 46.32 0.038100\n17  4 47.48 0.034720\n18  5 48.59 0.031640\n19  6 49.53 0.028830\n20  7 50.48 0.026260\n21  7 51.84 0.023930\n22  8 52.98 0.021810\n23  8 54.00 0.019870\n24  9 54.93 0.018100\n25 11 56.10 0.016490\n26 10 57.40 0.015030\n27 11 58.29 0.013690\n28 11 59.06 0.012480\n29 12 59.70 0.011370\n30 12 60.25 0.010360\n31 13 60.71 0.009439\n32 13 61.10 0.008600\n33 13 61.43 0.007836\n34 13 61.71 0.007140\n35 14 61.95 0.006506\n36 15 62.18 0.005928\n37 15 62.40 0.005401\n38 16 62.57 0.004921\n39 17 62.96 0.004484\n40 18 63.28 0.004086\n41 18 63.56 0.003723\n42 20 63.80 0.003392\n43 20 64.00 0.003091\n44 20 64.16 0.002816\n45 20 64.30 0.002566\n46 21 64.42 0.002338\n\n...\nand 32 more lines.\n# A tibble: 31 x 3\n   term        estimate  penalty\n   <chr>          <dbl>    <dbl>\n 1 (Intercept)   4.12   0.000207\n 2 bedrooms     -0.0174 0.000207\n 3 bathrooms     0.0355 0.000207\n 4 sqft_living   0.307  0.000207\n 5 sqft_lot     -0.0376 0.000207\n 6 floors        0.0210 0.000207\n 7 waterfront    0.191  0.000207\n 8 view         -0.0615 0.000207\n 9 sqft_above    0.139  0.000207\n10 basement     -0.0405 0.000207\n# … with 21 more rows\n\nWe kunnen het belang van de variabele visualiseren\n\n\n\nEvalueren op testgegevens\nTen slotte passen we het model toe op de testgegevens en onderzoeken we enkele definitieve metrieken. We tonen ook de metriek van het gewone lineaire model. Het lijkt erop dat de prestaties van het LASSO-model iets beter zijn, maar het scheelt niet veel.\n\n# A tibble: 2 x 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.135 Preprocessor1_Model1\n2 rsq     standard       0.655 Preprocessor1_Model1\n# A tibble: 2 x 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.135 Preprocessor1_Model1\n2 rsq     standard       0.655 Preprocessor1_Model1\n\nBronnen\nDank gaat uit naar verschillende mensen voor het delen van materiaal over tidymodels, waaronder\n En natuurlijk Lisa zelf, haar voeg ik hier zelf aan toe\n\nDit zijn de bronnen die bij deze blog ondersteuning boden:\nRebecca Barter’s blog\ntidymodels website (Alison Hill, Max Kuhn, Desirée De Leon, Julia Silge)\nJulia Silge’s tidymodels example\nUiteraard vooral Lisa Lendway via:\nLisa Lendway/2020_north_tidymodels\n\n\n\n",
    "preview": "posts/2021-03-10-machine-learning/images/house_prices_variables.png",
    "last_modified": "2021-04-20T21:10:36+02:00",
    "input_file": "machine-learning.utf8.md"
  },
  {
    "path": "posts/2021-03-10-github/",
    "title": "GitHub voor samenwerking",
    "description": "Lisa Lendway heeft een aantal interessante repositories op haar GitHub account staan, [zie hier](https://github.com/llendway). Ze zijn vaak kort, maar helder en concreet. Haar stijl en de consistentie daarin bevallen mij zeer. Van haar manier van doen leer ik veel. Zij maakt haar stukken vaak voor haar statistieklessen en deelt zo haar kennis met haar studenten en anderen buiten haar klas. Ik heb mij voorgenomen om er een aantal goed te lezen, te vertalen en te bewerken waar nodig, en deze op mijn website over te nemen. Vorige maand deed ik dat al met een blof over Distill en nu een over GitHub.",
    "author": [
      {
        "name": "Lisa Lendway, vertaling Harrie Jonkman",
        "url": {}
      }
    ],
    "date": "2021-03-10",
    "categories": [],
    "contents": "\nGitHub\nIk was al vaker van plan hier een stukje over te schrijven. Lisa Lendways tekst hierover vind ik heel duidelijk. Lisa, ik hoop dat je het goed vindt dat ik mij aan jou op trek zo. Dank je wel.\nMijn eigen GitHub accountLisa Lendway heeft veel van haar materiaal weghaald uit Happy git with R by Jenny Bryan. Dat is inderdaad een uitstekende bron, maar bevat ook veel informatie die we niet altijd nodig hebben. Als je Git en GitHub op meer geavanceerde manieren wilt gebruiken of als dit stuk onduidelijk is voor je, dan moet je het zeker bekijken. Het is trouwens überhaupt een goede bron.\nVideo uitleg\n\n\nVoicethread tutorial\nGit en GitHub\nGit is een versie controle systeem. Het is net zoiets als Googledocs, maar het biedt ruimte aan veel soorten bestanden, ook bestanden waar Google docs niets mee kan … zoals .rmd bestanden! GitHub is een online interface om met Git te werken.\nWaarom leren we deze dingen?!\nGitHub is goed geïntegreerd met R Studio. Dus, we zullen geen command-line functies hoeven te gebruiken, tenminste niet nadat we alles hebben ingesteld.\nJe bent verplicht om R te gebruiken voor je eindproject. De presentatie of paper moet worden opgeslagen als een .rmd document dat kan worden ‘geknit’ tot een html document. Door GitHub te gebruiken, kun je gemakkelijk met je groep samenwerken, ook als je niet bij elkaar bent.\nGitHub leert je een aantal goede gewoontes aan. Je wordt gedwongen om na te denken over wanneer je ieets opslaat en om notities te maken over welke wijzigingen je hebt gemaakt, bijvoorbeeld.\nMaak eerst een GitHub account aan\nGa naar http://github.com\nGebruik een username … zie Jenny Bryan’s tips. Incorporeer jouw eigen naam, maar gebruik een andere usernaam die je verder gebrukkt, neem iets waar jouw toekomstige baas zich prettig bij voelt. De username van de universiteit is misschien een goede optie.\nInstalleer Git\n1. Controleer of je Git al geïnstalleerd hebt. Dit zal alleen het geval zijn als je het ergens anders gebruikt hebt. Om dit te doen, open je de commandoregel of, in R Studio, vouw je de Console uit. Er zou een tabblad moeten zijn dat Terminal zegt. In dat gebied type je\nwhich git\nHet geeft iets terug als\n/usr/bin/git\ndan ben je klaar en hoef je Git niet meer te installeren. Op een Windows machine kun je misschien niet eens het which git commando succesvol intypen. Dit zou verholpen moeten zijn door git te installeren. Of je zult de shell moeten gebruiken.\nAls je Git niet geïnstalleerd hebt, moet je het installeren. De instructies zijn iets anders voor Windows en Macs.\nVoor een Windows machine:\nInstalleer Git for Windows. Als er gevraagd wordt naar “Aanpassen van uw PATH omgeving”, zorg er dan voor dat u “Git vanaf de commandoregel en ook van software van derden” selecteert. Anders denken we dat het goed is om de standaardinstellingen te accepteren.\nR Studio voor Windows geeft er de voorkeur aan dat Git geïnstalleerd wordt onder C:/Program Files en dit lijkt de standaard te zijn. Dit houdt bijvoorbeeld in dat de Git executable op mijn Windows systeem te vinden is in C:/Program Files/Git/bin/git.exe. Tenzij je specifieke redenen hebt om anders te doen, volg deze conventie.\nVoor een Mac machine:\nGa naar jouw shell/terminal en voer één van deze commando’s in om een aanbod te krijgen om developer command line tools te installeren. Accepteer het aanbod … klik op installeren.\ngit --version\ngit config\nSommigen van jullie die op een Mac werken moeten misschien eerst het volgende doen in de terminal als je een project zonder succes probeert te openen.\nxcode-select --install\nJe komt er zo achter of dit het geval is.\nGa nu terug naar de Console in R Studio en installeer het usethis pakket in R Studio. Sluit vervolgens R Studio en open het opnieuw.\nLaad de usethis bibliotheek door het volgende stukje code in de console uit te voeren:\nlibrary(usethis)\nVoer de volgende code uit in de console met enkele kleine wijzigingen. De user.name is je Git gebruikersnaam. Dit kan anders zijn dan je GitHub gebruikersnaam, hoewel het misschien een goed idee is om het gewoon hetzelfde te houden. De user.email MOET hetzelfde zijn als je GitHub gebruikers email.\nuse_git_config(user.name = \"Jane Doe\", user.email =       \"jane@example.org\")\nMaak een eerste repo (repository) en gebruik RStudio daarbij\nHet woord “repo” is een afkorting van “repository”, en dat is precies wat het is: een plaats waar dingen (onze bestanden, in dit geval) worden opgeslagen. Het is als de map die je gemaakt hebt om al je werk voor deze les in op te slaan.\nLaten we naar GitHub gaan en inloggen. Nadat je ingelogd bent, zou je een klein icoontje in de rechter bovenhoek moeten zien. Het mijne is een afbeelding van mij. Als ik daar op klik verschijnt er een drop-down en kan ik “Your repositories” kiezen. Doe dat. Je zou nu zoiets als dit moeten zien:\n\nKlik op de “New” knop. Geef jouw repository een naam, bv NAME_test_repo, waar NAME eigenlijk jouw naam is. Kies Public en klik de README file aan. Klik dan op Create repository.\n\nEr zijn dingen die je direct binnen GitHub kunt doen, maar we zullen ons richten op de integratie met R Studio.\nKlonen van een repo\nDenk aan het klonen van een repo als het “kopiëren” van de repository naar je computer. Maar als met het kopiëren doet, houdt het de verbinding met de online repo.\nLaten we dit doen. Op je mijn_test_repo pagina, kies je de groene knop met Code en kopieer je het pad door het icoontje met een pijl erop te selecteren en erop te klikken.\nGa nu naar R Studio. Klik op Bestand –> Nieuw Project … Je zou nu een venster moeten zien dat er als volgt uitziet:\n\nKies Version Control. Dan zie je een scherm dat er zo’n beetje zo uitziet:\n\nKies Git. Dan zou je een scherm moeten zien dat er uitziet als dit, zonder alle details ingevuld. De Repository URL is waar je de repo URL moet plakken die je gekloond hebt van github. Het zal ook de Project mapnaam invullen. Laat die gewoon staan. Let op waar de project directory zich bevindt en verander het naar een betere directory indien nodig. Klik op Create Project.\n\nAls je in de Bestanden tab kijkt in het rechter ondervenster van R Studio, dan zou je het .gitignore bestand moeten zien, het project bestand (eindigt op .Rproj), en het README.md bestand. Je zou ook een Git tab moeten zien in het rechter bovenvenster van R Studio. Als je nu op de Git tab klikt, zul je daar niets zien.\nMet de Git tab open, laten we het README.md bestand in R Studio openen. Maak een kleine wijziging in het bestand door de zin “Ik verander iets in dit bestand.” toe te voegen. Klik dan op het save icoon. Als je dit doet, zul je README.md zien verschijnen in de Git tab.\nKlik nu op de Commit knop in de Git tab. Zet een vinkje in het vakje naast het README.md bestand onder het woord Staged (in de toekomst kun je meerdere bestanden tegelijk stagen door de vakjes naast meerdere bestanden aan te vinken) en voeg een commentaar toe aan het commit vakje.\nHet zou er ongeveer zo uit moeten zien:\n\nKlik tenslotte op commit. Je krijgt dan een bericht dat het voltooid is. Het bericht kan cryptisch overkomen als je er niet aan gewend bent. Het ziet er ongeveer zo uit:\n\nDe wijziging die je hebt gemaakt is nu gecommit in het lokale geheugen. Het gewijzigde bestand is alleen gewijzigd op je computer, NIET online als je op GitHub kijkt … ga maar eens kijken. Klik op de Diff knop in de Git tab en je kunt de geschiedenis van je commits zien.\nVervolgens gaan we die wijzigingen naar GitHub pushen door op de groene pijl omhoog in de Git tab te klikken. Dit zal je een bericht geven dat er ongeveer zo uitziet:\n\nWerd je gevraagd om een gebruikersnaam en wachtwoord? Probeer een andere wijziging te maken, vast te leggen en te pushen. Wordt er nog steeds om een gebruikersnaam en wachtwoord gevraagd? Zo ja, dan kun je hier zien hoe je dat doet Jenny Bryans bron.\nPROBEER HET EENS!!\nVoeg een .rmd bestand toe aan je project. Doe dit door te gaan naar Bestand –> Nieuw bestand –> R Markdown … Voeg wat woorden en een R code chunk toe aan het .rmd bestand. Sla het op, commit het (vergeet het bericht niet!), en push het. Controleer GitHub online om er zeker van te zijn dat je het .rmd bestand daar ziet.\nNu, knit je het bestand lokaal. Commit de wijzigingen (zorg ervoor dat je een vinkje zet naast alles wat je ge-staged wilt hebben - .rmd, .html, etc.) en push ze naar GitHub. Controleer GitHub online om er zeker van te zijn dat je alles ziet wat je verwacht.\nPartners toevoegen\nTot nu toe hebben we eigenlijk alleen technieken geleerd om GitHub te gebruiken om onze eigen bestanden te beheren, maar het coolste eraan zijn de samenwerkingsmogelijkheden. De manier waarop we dit gaan leren is door medewerkers aan de repo toe te voegen.\nZoek iemand om mee samen te werken. Als er een oneven aantal is, maak dan een groepje van drie. In je groepje, voeg elkaar toe als medewerkers aan je project. In GitHub, op de repo pagina, ga naar Instellingen. Een van de opties aan de linkerkant is Collaborators. Klik daarop en doe wat er staat.\nDe persoon die is uitgenodigd om samen te werken zal een email ontvangen en zou ook in staat moeten zijn om de uitnodiging op GitHub te zien. Ze moeten deze accepteren. Eenmaal geaccepteerd, zouden jullie beiden (of alle drie) toegang moeten hebben om wijzigingen in het bestand vast te leggen.\nCommit –> Push –> Pull –> … (en Communicatie)\nZodra je medewerkers hebt toegevoegd, kunnen alle medewerkers committen en pushen. Maar, wat gebeurt er als iemand iets commit en terugzet en jij gaat er dan aan werken op je computer… hoe krijg je dan die wijzigingen? … PULL!\nProbeer in jullie groepen het volgende. Jullie moeten allemaal meewerken aan elkaars projecten, dus je kunt van rol wisselen nadat je het één keer gedaan hebt.\nDe medewerker moet eerst de repo klonen waaraan hij gevraagd is om mee te werken. Als ze een ander project open hebben, sla dan op, commit, en push alle wijzigingen. Sluit dan dat project en open het project waar ze om gevraagd is om aan mee te werken door de GitHub repo te klonen. De medewerker moet het project open hebben in R Studio.\nDe medewerker moet proberen te trekken (pullen) door op de aqua pijl omlaag te klikken in de Git tab. Je zou een bericht moeten krijgen dat er als volgt uitziet:\n\nDe persoon die de repo heeft aangemaakt, maakt een wijziging in zijn .rmd bestand. Het kan een kleine wijziging zijn, zoals het toevoegen van een zin. Diezelfde persoon slaat het bestand op, commit (staged en schrijft een commit boodschap), en pushed het naar GitHub. Controleer online om er zeker van te zijn dat de meest recente wijzigingen zijn gepushed.\nDe medewerker haalt nu die wijzigingen naar zijn lokale map (naar zijn computer). Klik op het pull icoon. Je zou een bericht moeten zien dat er ongeveer zo uitziet:\n\nEn controleer het bestand waarin een wijziging is aangebracht om er zeker van te zijn dat de wijziging wordt weerspiegeld in het bestand op uw computer.\nGa nog een paar keer heen en weer en breng kleine wijzigingen aan. Degene die eigenaar is van de repo zou de wijziging moeten maken en de medewerker zou het moeten binnenhalen. Wissel dan van rol. Als je wisselt, wees er dan zeker van dat je aan het juiste project werkt.\nConflicten samenvoegen\nAls je samen aan een project werkt, is de kans groot dat je tegen een moment aanloopt waarop twee van jullie hetzelfde bestand tegelijkertijd aan het bewerken zijn. Soms, als je allebei je wijzigingen probeert te pushen, zul je wat genoemd wordt een “merge conflict” krijgen. GitHub zal niet weten welke te gebruiken. Dus, zal het je dwingen om te beslissen.\nAls je probeert je wijzigingen naar GitHub te pushen en iemand anders heeft zijn wijzigingen met betrekking tot hetzelfde bestand al gepushed, dan zul je een bericht als dit krijgen:\n\nDan, wanneer je de wijzigingen binnenhaalt, krijg je een bericht zoals dit:\n\nMerk op dat het je vertelt in welk bestand het samenvoegconflict zich voordeed. Je moet dat bestand openen en beslissen hoe de conflicterende informatie samengevoegd moet worden. In het begin zal het er ongeveer zo uitzien:\n\nHet deel na het woord HEAD is wat in je lokale bestand staat. Alles na de ====== is wat in het bestand op afstand staat (d.w.z. de wijzigingen die je medewerker heeft gemaakt). Je kunt besluiten om dit op te lossen op elke manier die je wilt: combineer de twee ideeën, verwijder ze allebei, houd er maar een over, etc. Als je klaar bent, zorg er dan voor dat je de <<<<<<< HEAD en >>>>>>> verwijdert, gevolgd door de alfanumerieke string, plus alle andere vreemde tekens.\nSla het bestand dan op en doe de gebruikelijke commit en push. Je zou de wijzigingen naar GitHub gepushed moeten zien worden.\nLaten we dit eens proberen!\nIn groepjes van 3-4, oefen je GitHub vaardigheden.\nKies iemand om een nieuwe repo aan te maken op GitHub genaamd our_collaborative_graph.\nDe maker voegt de anderen toe als collaborators.\nDe medewerkers moeten hun e-mail controleren en accepteren dat ze medewerkers zijn.\nDe maker en de medewerkers klonen de repo lokaal.\nEen medewerker voegt lokaal een .rmd bestand aan het project toe. De titel moet zijn “Onze grafiek” en voeg alle groepsleden toe als auteurs. Voeg een R code stuk toe dat de tidyverse bibliotheek laadt. Sla het bestand op, commit met bericht, en push naar GitHub. Controleer online om er zeker van te zijn dat het goed gepushed is.\nAlle andere groepsleden halen de wijzigingen lokaal op.\nEen andere medewerker voegt een ander R code stuk toe. Maak met de mpg dataset een scatterplot met hwy op de y-as, displ op de x-as en kleur de punten met drv. Sla de wijzigingen op. Brei het bestand. commit dan met bericht en push naar GitHub. Zorg ervoor dat je alle bestanden in de commit staged. Controleer online om er zeker van te zijn dat je de wijzigingen ziet.\nAlle andere groepsleden halen de wijzigingen lokaal op.\nEen ander groepslid (medewerker of maker als je maar 3 groepsleden hebt) wijzigt de R code chunk die de grafiek maakt, door mooie x en y labels toe te voegen en te veranderen naar theme_minimal(). Sla de wijzigingen op. Brei het bestand. commit dan met bericht en push naar GitHub. Zorg ervoor dat je alle bestanden in de commit staged. Controleer online om er zeker van te zijn dat je de veranderingen ziet.\nAlle grop leden trekken. Degene die net gepushed heeft zou moeten zien dat ze al up to date zijn. Alle anderen zouden de wijzigingen lokaal terug moeten zien.\nNu moeten alle groepsleden iets toevoegen aan het .rmd bestand. Vertel elkaar niet wat je toevoegt. Als je klaar bent, sla op, brei, commit, en push naar GitHub. Ten minste één van jullie zal een samenvoeg conflict krijgen, dus zal het je vragen om wijzigingen van GitHub binnen te halen en het conflict op te lossen. Doe dat. Deze keer zul je het .rmd bestand moeten aanpassen in plaats van de README zoals ik eerder liet zien.\nAls je klaar bent, moeten 112 leerlingen een project opzetten met hun eigenlijke groepsprojectleden. Neem een .rmd-bestand op met de naam “ideas.rmd” waarin je ideeën kunt uitwisselen, inclusief onderwerpen en gegevens die je misschien zou willen analyseren. 155 leerlingen moeten de enquête over het groepsproject op de Moodle-pagina invullen.\nBron\nHappy git with R door Jenny Bryan. Leer meer over Distill via https://rstudio.github.io/distill.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-04-20T21:09:01+02:00",
    "input_file": "github.utf8.md"
  },
  {
    "path": "posts/2021-02-19-website-met-distill/",
    "title": "Website met distill",
    "description": "Website met blog maken",
    "author": [
      {
        "name": "Lisa Lendway, vertaling Harrie Jonkman",
        "url": {}
      }
    ],
    "date": "2021-02-19",
    "categories": [],
    "contents": "\nDistill\nMijn eigen Harrie’s Hoekje blog, over een aantal ontwikkelingen in de dataanalyse, maak ik ook met het gebruik van het pakket Distill. Met Distill kun je wetenschappelijke websites maken, een blog en artikelen schrijven. Over hoe je dat doet schreef Lisa Lendway een kort en krachtige blog. Dat kan ik niet beter. Dank je, Lisa, hiervoor. Haar blog staat hier\nWaarom een website?\nNou, eindelijk heb ik het gedaan!, zo begint zij haar blog dat ik (Harrie) hier verder volg.\nIk (Lisa) heb een website gemaakt. En om dat te vieren, ga ik met jullie delen hoe ik het gedaan heb. En waarom heb ik dat gedaan? Twee belangrijke redenen zijn er: 1. om materiaal te delen dat nuttig kan zijn voor anderen, 2. om wat dingen voor mezelf te documenteren, allemaal op één plek.\nIk koos voor een {distill} site omdat het me genoeg vrijheid leek te geven om mijn site aan te passen en ook weer niet zo veel vrijheid dat ik zou verzanden in details (bv. kleuren kiezen … oeps, daar heb ik toch nog veel tijd aan besteed).\nBronnen\nVoordat ik begin, zal ik wat bronnen delen die ik heb gebruikt.\nAlison Hill en Desirée De Leon’s webinar over Sharing on Short Notice. KIJK HIERNAAR voordat je verder gaat. Hier werd ik voor het eerst geïntroduceerd op netlify en toen zag ik pas hoe makkelijk het is om html-files tot een website om te vormen website. Je zou daar zelfs eerst mee kunnen beginnen voordat jij je op een website springt. Misschien spreken sommige van de andere opties die ze bespreken jou meer aan dan {distill}.\nDe distill documentatie, ook al in de vorm van een … distill website!\nDistill websites van anderen: Ijeamaka Anyene, Shannon Pileggi(aka Piping Hot Data), Miles McBain, Tom Mock, en meer!\nAlison Hill’s website voor hoog niveau inhoud en design inspiratie. Iedere keer vind ik wel een nieuw bron als ik haar website bezoek. Bijvoorbeeld, bekijk eens haar praatje op ‘Recent updates in the R markdown family’.\nEn meer! Ik zal proberen in dit blog op enkele bronnen terug te komen.\nKijk ook naar de video die Lisa maakte en die je hier vindt.\nBouwen van de site\nLaten we nu verder gaan met het maken van de site. Onderweg kom ik terug op de YouTube-video. Ik kom steeds terug op dezelfde YouTube-video, maar ik zal ze daar zetten waar ik het op dat moment over heb. Zo is het makkelijker voor je om delen over te slaan als je dat wilt.\nEen GitHub repo opzetten & het project starten\nKijk naar Tom Mock’s post hier. Ik denk dat zijn manier om dit te doen logischer is dan de mijne. Helaas, zag ik het toen ik mijn ding had gedaan :(\nIk probeer er een gewoonte van te maken om al mijn projecten met een GitHub repo te beginnen. Dus, dat is wat ik hier ook heb gedaan. Hier zijn alle stappen:\nMaak een repo\nCreëer project in R Studio door de repo te klonen * Laad de {distill} bibliotheek\nMaak een “starter” site met de create_website() functie. Ik heb dit gebruikt in plaats van create_blog() omdat ik van mijn hoofdpagina een About pagina wilde maken in plaats van een blog. Ik zal het blog gedeelte later toevoegen. Lees de {distill} documentatie om je te helpen bij het beslissingsproces. Omdat ik eerst mijn GitHub repo heb gemaakt, moest ik wat rare dingen doen om de mappenstructuur te fixen. Het werkt, maar het is een beetje lelijk.\nVerplaats alle bestanden behalve het .Rproj bestand van de zojuist gemaakte map naar de hoofdmap van het archief.\nVerwijder de website map (zou leeg moeten zijn, behalve het .Rproj bestand).\nVerwijder het README.md bestand in de hoofdmap van het archief (als ik dat niet deed, bouwde de site later niet).\n\nOf kijk naar dit deel van de video (tot minuut 8:04):\n\n\nDe site voor de eerste keer bouwen\nVervolgens willen we de site bouwen. Om dit op een eenvoudige manier te doen, sla je jouw bestanden op, sluit je RStudio en open je het opnieuw, waarbij jij ervoor zorgt dat je je in het project van jouw distill-site bevindt. Wanneer je dit doet, zou er een Build tab moeten verschijnen in jouw paneel aan de rechterbovenhoek (of waar u gewoonlijk jouw Environment, History, enz. hebt). Klik op het Build Website-icoon en je zou je site moeten zien! (8:25 in de video, als je het mij wilt zien doen).\nOp dit punt zijn er veel verschillende richtingen die je op kunt gaan. Ik zal je vertellen wat ik gedaan heb. Als je niet veel meer wilt aanpassen, kun je naar ?? gaan om een eenvoudige manier te vinden om je website te publiceren.\nAanpassen van de home page\nIk wilde dat mijn “Home” pagina mijn “About” pagina zou worden. Om dit te doen, heb ik eerst wat veranderingen aangebracht in het _site.yml bestand, het “About” gedeelte van de navigatiebalk verwijderd en de tekst voor de homepage hernoemd naar “About”.\nDan, om te beginnen met het aanpassen van mijn “About” pagina, voeg ik een foto van mezelf toe aan het index.Rmd bestand en plaats ik wat plaatshouders voor plaatsen waar ik wat informatie zal schrijven.\nBekijk dit in de video (tot minuut 17:35):\n\n\nVoeg het blog toe en maak jouw eerste post\nAls je de blog route vanaf het begin hebt gevolgd, hoef je dit deel niet te doen. Merk op dat ik in de video de dingen in de verkeerde volgorde deed\nVoeg een post toe met create_post(\"mijnpost\"). Dit genereert een R Markdown bestand met de naam mypost.Rmd (tenzij je de slug verandert), een _posts map, en een map die de datum en de naam van het bericht heeft. Door te beginnen met de datum, houdt het je berichten in een mooie volgorde :)\nBewerk jouuw blog post-Rmarkdownbestand naar believen. Zorg ervoor dat je dit bestand knit zodat het op de blog verschijnt. Deze bestanden worden niet automatisch gebreid. Dat is met opzet.\nMaak een nieuw R Markdown bestand met ALLEEN een yaml kop met een titel en listing. Sla het op in de hoofd repository.\nWijzig het _site.yml bestand om de listing pagina te linken. De tekst kan zijn wat je maar wilt - dit is wat er op de navigatiebalk komt te staan. De href waarde is de .html van het listing .Rmd bestand.\nVoeg een aangepaste blog preview afbeelding toe. Zet de afbeelding die je hiervoor wilt gebruiken in de map voor de blog post. In de yaml kop van het R Markdown bestand van uw blog, voeg je preview: image.png toe, waar image.png de naam van jouw afbeelding is. Standaard zal de preview de eerste plot zijn die gegenereerd wordt in uw R code.\nBekijk dit in de video (tot minuut 33:27):\n\n\nPas_site.yml aan\nIn dit deel voeg ik enkele aangepaste iconen toe aan de bovenste navigatiebalk van de site. Deze bevatten een persoonlijke favicon aan de linkerkant (die ik uiteindelijk toch weer weghaal) en links naar mijn GitHub, LinkedIn en Twitter pagina’s (en later voeg ik er een toe aan mijn YouTube kanaal).\nVoeg het volgende toe aan het _site.yml bestand na de navbar koptekst. Wees voorzichtig met inspringen. Je kunt mijn bestand hier bekijken (ik heb meer bewerkt sinds het maken van de video, dat wel).\n- icon: fa fa-github\n  href: https://github.com/YOUR_USERNAME\n- icon: fa fa-linkedin\n  href: https://www.linkedin.com/in/YOUR_LINKEDIN/\n- icon: fa fa-twitter\n  href: https://twitter.com/YOUR_TWITTER\nOm een gepersonaliseerde favicon toe te voegen, voeg het volgende toe na navbar:, waar ll.png de persoonlijke favicon is. Je kunt ook een link naar een website toevoegen waar hij naartoe gaat als je er op klikt. Nogmaals, wees voorzichtig met inspringen.\n  logo:\n    image: ll.png\nVolg de video hieronder (tot minuut 44:22). Toen ik dit de eerste keer deed, maakte ik wat fouten, dus ik laat je dat deel van de video overslaan.\n\n\nPubliseer de site via netlify\nNu je een website hebt, kun je die gemakkelijk publiceren via netlify. Ik zal je laten zien hoe je deze aan je GitHub repo kunt koppelen, zodat iedere keer dat je wijzigingen naar GitHub stuurt, je website die wijzigingen zal weergeven. Ik raad aan om eerst een account op netlify aan te maken.\nBekijk de video om te zien hoe ik het doe (tot minuut 48:22):\n\n\nMaak het je eigen!\nHet laatste stuk is om wat aanpassingen te doen. Dankzij de geweldige {distill} auteurs, kunnen we de create_theme() functie gebruiken om ons door het aanpassen van wat css te leiden. Ik ben een echte beginner als het op css aankomt, dus er is van een makkelijkere manier. Ik raad ten zeerste aan om de documentatie over theming en de recente updates door te lezen. En lees grondig de tekst van de website (misschien heb ik dat de eerste keer niet gedaan)!\nVoegtheme: \"my_theme.css\" aan de bodem van de _site.yml file toe.\nJe kunt de video tot het einde bekijken:\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-04-20T21:04:31+02:00",
    "input_file": "website-met-distill.utf8.md"
  },
  {
    "path": "posts/2020-11-17-testen-met-bayes/",
    "title": "Testen met Bayes",
    "description": "Resultaten testen met Bayesiaanse onderzoekstechnieken.",
    "author": [
      {
        "name": "Makowski en anderen, vertaling Harrie Jonkman",
        "url": "https://easystats.github.io/bayestestR/"
      }
    ],
    "date": "2021-02-14",
    "categories": [],
    "contents": "\nKorte inleiding\nDe laatste weken lees ik weer regelmatig over de achtergronden, de principes en de voordelen van bayesiaanse onderzoekstechnieken. De update van Statistical Rethinking. A Bayesian Course with Examples in R and Stan (McElreath, 2020) en het nieuwe boek Regression and other stories (Gelman, Hill & Vehtari, 2020) geven veel inspiratie. Daarover later meer. Ondertussen verscheen vorig jaar het R-pakket bayestestR met een hele duidelijke bijbehorende website waarin een aantal uitgangspunten heel duidelijk worden uitgelegd en de voordelen van deze manier van onderzoek doen worden vergeleken met de klassieke onderzoekstechniek. Ik kon het niet laten om een aantal lessen te vertalen om dit goed in mijn vingers te krijgen. Mogelijk dat ik hier later nog een keer aandacht aan besteed. De website is gebaseerd op twee artikelen waar de wetenschappers naar refereren. Natuurlijk moet ik deze artikelen hier aan het begin noemen.\nMakowski, D., Ben-Shachar, M. S., & Lüdecke, D. (2019). bayestestR: Describing Effects and their Uncertainty, Existence and Significance within the Bayesian Framework. Journal of Open Source Software, 4(40), 1541. 10.21105/joss.01541\nMakowski, D., Ben-Shachar, M. S., Chen, S. H. A., & Lüdecke, D. (2019). Indices of Effect Existence and Significance in the Bayesian Framework. Frontiers in Psychology 2019;10:2767. 10.3389/fpsyg.2019.02767\nWaarom zou je het Bayesiaanse kader gebruiken?\nHet Bayesiaanse statistische raamwerk wint snel aan populariteit onder wetenschappers, wat samenhangt met de algemene verschuiving naar open en eerlijke wetenschap. Redenen om de voorkeur te geven aan deze aanpak zijn betrouwbaarheid, nauwkeurigheid (in rommelige data en kleine steekproeven), de mogelijkheid om prior kennis in de analyse te introduceren en, kritisch gezien, de intuïtiviteit van de resultaten en hun rechtstreekse interpretatie (Andrews & Baguley, 2013; Etz & Vandekerckhove, 2016; Kruschke, 2010; Kruschke, Aguinis, & Joo, 2012; Wagenmakers et al., 2018).\nIn het algemeen wordt de frequentisttische aanpak geassocieerd met de focus op null hypothesetests, en het misbruik van p-waarden blijkt kritisch bij te dragen aan de reproduceerbaarheidscrisis van psychologische wetenschap (Chambers, Feredoes, Muthukumaraswamy, & Etchells, 2014; Szucs & Ioannidis, 2016). Men is het er algemeen over eens dat de veralgemening van de Bayesiaanse aanpak een manier is om deze problemen te overwinnen (Benjamin et al., 2018; Etz & Vandekerckhove, 2016).\nAls we het er eenmaal over eens zijn dat het Bayesiaanse raamwerk de juiste weg is, kun je je vervolgens afvragen wat het Bayesiaanse raamwerk is.\nWaar gaat al dat gedoe over?\nWat is het Bayesiaanse kader?\nHet aannemen van het Bayesiaanse raamwerk is meer een verschuiving in het paradigma dan een verandering in de methodologie. Inderdaad, alle gemeenschappelijke statistische procedures (t-tests, correlaties, ANOVA’s, regressies, …) kunnen nog steeds worden uitgevoerd met behulp van het Bayesiaanse raamwerk. Een van de kernverschillen is dat in het frequentische perspectief (de “klassieke” statistiek, met p- en t-waarden, evenals met die rare vrijheidsgraden), de effecten vastliggen (maar onbekend zijn) en data random zijn. Aan de andere kant wordt in het Bayesiaanse inferentieproces, in plaats van schattingen van het “ware effect”, de waarschijnlijkheid van verschillende effecten berekend gegeven de waargenomen gegevens. Dat resulteert in een verdeling van mogelijke waarden voor de parameters, de zogenaamde posterior-distributie.\nDe onzekerheid in de Bayesiaanse inferentie kan bijvoorbeeld worden samengevat door de mediaan van de verdeling, evenals een reeks waarden van de posterior distributie die de 95% meest waarschijnlijke waarden omvat (het 95% waarschijnlijke interval). Cum grano salis, deze worden beschouwd als de tegenhangers van de punt-schatting en het betrouwbaarheidsinterval in een frequentistisch kader. Om het verschil in interpretatie te illustreren, laat het Bayesiaanse raamwerk toe om te zeggen “gezien de geobserveerde gegevens, heeft het effect een 95% kans om binnen dit bereik te vallen”, terwijl het minder eenvoudige alternatief voor de frequentist zou zijn “wanneer herhaaldelijk betrouwbaarheidsintervallen uit deze reeks gegevens worden berekend, is er een 95% kans dat het effect binnen een bepaald bereik valt”. In wezen geven de Bayesiaanse sampling algoritmen (zoals MCMC-bemonstering) een waarschijnlijkheidsverdeling (de posterior) van een effect dat compatibel is met de waargenomen gegevens. Zo kan een effect worden beschreven door de posterior verdeling te karakteriseren in relatie tot de centraliteit (punt-schattingen), de onzekerheid, en het bestaan en de betekenis ervan.\nMet andere woorden, als we de wiskunde achterwege laten, kunnen we dat zeggen:\nDe frequentist probeert “het reële effect” in te schatten, bijvoorbeeld, de “echte” waarde van de correlatie tussen x en y. Vandaar dat de modellen van frequentisten een “punt-schatting” opleveren. (d.w.z. één enkele waarde) van de “echte” correlatie (bv. r = 0,42) die wordt geschat op basis van een aantal onduidelijke veronderstellingen (minimaal, aangezien de gegevens willekeurig worden onttrokken van een “ouder”, meestal een normale verdeling).\nDe Bayesiaan gaat niet van zoiets uit. De gegevens zijn wat ze zijn. Op basis van deze geobserveerde gegevens (en een eerdere overtuiging over het resultaat) geeft het Bayesiaanse samplingsalgoritme (soms ook wel MCMC sampling genoemd) een waarschijnlijkheidsverdeling (de zogenaamde posterior) van het effect dat compatibel is met de geobserveerde gegevens. Voor de correlatie tussen x en y geeft het een verdeling, die bijvoorbeeld zegt: “het meest waarschijnlijke effect is 0,42, maar deze gegevens zijn ook compatibel met correlaties tussen 0,12 en 0,74”.\nOm onze effecten te karakteriseren is geen behoefte aan p-waarden of andere cryptische indices. We beschrijven gewoon de posterior verdeling van het effect. We kunnen bijvoorbeeld de mediaan, de 89% Credible Interval of andere indices rapporteren.\nMet andere woorden, als we de wiskunde even achterwege laten, kunnen we zeggen dat:\n\nHoewel het doel van dit pakket is het gebruik van Bayesiaanse statistieken te verdedigen, zijn er serieuze argumenten die de frequentie-indexen ondersteunen (zie bijvoorbeeld hier). Zoals altijd is de wereld niet zwart-wit (p < .001).\n\nNou… hoe werkt het?\nEen eenvoudig voorbeeld\nInstallatie van BayestestR\nU kunt bayestestR samen met de hele easystats suite installeren (of alleen bayestestR omdat de suite installeren bij mij niet werkte) door het volgende uit te voeren: ## A simple example\n\n\n\nLaten we ook het pakket rstanarm installeren en laden, die het mogelijk maakt om de Bayesiaanse modellen, evenals de bayestestR, te werken.\n\n\n\nTraditionele lineaire regressie\nLaten we beginnen met het aanbrengen van een eenvoudige frequentistische lineaire regressie (de lm() functie staat voor lineair model) tussen twee numerieke variabelen, Sepal.Length en Petal.Length uit de beroemde iris-dataset, standaard opgenomen in R.\n\n\nCall:\nlm(formula = Sepal.Length ~ Petal.Length, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.24675 -0.29657 -0.01515  0.27676  1.00269 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   4.30660    0.07839   54.94   <2e-16 ***\nPetal.Length  0.40892    0.01889   21.65   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4071 on 148 degrees of freedom\nMultiple R-squared:   0.76, Adjusted R-squared:  0.7583 \nF-statistic: 468.6 on 1 and 148 DF,  p-value: < 2.2e-16\n\nDeze analyse suggereert dat er een significante (wat dat ook moge betekenen) en een *positieve** (met een coëfficiënt van 0,41) lineaire relatie bestaat tussen de twee variabelen.\nHet aanpassen en interpreteren van frequentiemodellen is zo eenvoudig dat het duidelijk is dat mensen het gebruiken in plaats van het Bayesiaanse kader… toch?\nNiet meer.\nBayesiaanse lineaire regressie\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000416 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 4.16 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.042514 seconds (Warm-up)\nChain 1:                0.06525 seconds (Sampling)\nChain 1:                0.107764 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2.1e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.21 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.045785 seconds (Warm-up)\nChain 2:                0.065693 seconds (Sampling)\nChain 2:                0.111478 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.9e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.051297 seconds (Warm-up)\nChain 3:                0.068238 seconds (Sampling)\nChain 3:                0.119535 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.6e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.045173 seconds (Warm-up)\nChain 4:                0.059105 seconds (Sampling)\nChain 4:                0.104278 seconds (Total)\nChain 4: \n# Description of Posterior Distributions\n\nParameter    | Median |         89% CI |      pd |        89% ROPE | % in ROPE |  Rhat |      ESS\n-------------------------------------------------------------------------------------------------\n(Intercept)  |  4.306 | [4.183, 4.424] | 100.00% | [-0.083, 0.083] |         0 | 1.000 | 4051.872\nPetal.Length |  0.409 | [0.379, 0.437] | 100.00% | [-0.083, 0.083] |         0 | 1.000 | 3934.690\n\nDat is het! Je hebt een Bayesiaanse versie van het model gedraaid door eenvoudigweg stan_glm() te gebruiken in plaats van lm() en hebt de posterior distributie van de parameters beschreven. De conclusie die we kunnen trekken, voor dit voorbeeld, zijn zeer vergelijkbaar. Het effect (de mediaan van de posterior verdeling van het effect) is ongeveer 0,41, en het kan ook als significant worden beschouwd in de Bayesiaanse zin (meer daarover later).\nDus, klaar om meer te leren?\n1. Initiatie tot Bayesiaanse modellen\nNu je de beginsectie hebt gelezen, laten we een duik nemen in de subtiliteiten van Bayesiaanse modellering met behulp van R.\nLaden van pakketten\nAls je een keer de benodigde pakketten hebt geïnstalleerd, kun je rstanarm laden (om de modellen te draaien) en ook bayestestR (om bruikbare indices te berekenen) en insight (om toegang te krijgen tot de parameters).\n\n\n\nEenvoudig lineair model (ook wel regressie genoemd)\nWe beginnen met het uitvoeren van een eenvoudige lineaire regressie om het verband tussen Petal.Length (onze voorspeller, of onafhankelijke, variabele) en Sepal.Length (onze respons-, of afhankelijke-variabele) te testen vanuit de irisdataset die standaard is opgenomen in R.\nPassend bij het model\nLaten we beginnen met het draaien van de frequentistische versie van het model, gewoon om een referentiepunt te hebben:\n\n\nCall:\nlm(formula = Sepal.Length ~ Petal.Length, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.24675 -0.29657 -0.01515  0.27676  1.00269 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   4.30660    0.07839   54.94   <2e-16 ***\nPetal.Length  0.40892    0.01889   21.65   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4071 on 148 degrees of freedom\nMultiple R-squared:   0.76, Adjusted R-squared:  0.7583 \nF-statistic: 468.6 on 1 and 148 DF,  p-value: < 2.2e-16\n\nIn dit model is de lineaire relatie tussen Petal.Length en Sepal.Length positief en significant (beta = 0,41, t(148) = 21,6, p < .001). Dit betekent dat je voor elke toename van Petal.Length (de voorspeller) met één eenheid kunt verwachten dat de Sepal.Length (het antwoord) met 0,41 zal toenemen. Dit effect kan worden gevisualiseerd door de voorspellingswaarden op de x-as en de responswaarden als y te plotten met behulp van het ggplot2 pakket:\n\n\n\nLaten we nu een Bayesiaanse versie van het model draaien door gebruik te maken van de stan_glm-functie dat in het rstanarmpakket zit:\n\n\n\nJe ziet dat het samplingsalgoritme draait.\nDe posterior eruit halen\nLaten we, als het bovenstaande eenmaal gedaan is, de parameters (d.w.z. de coëfficiënten) van het model extraheren.\n\n  (Intercept) Petal.Length\n1    4.350261    0.3945665\n2    4.217188    0.4308909\n3    4.270616    0.4095938\n4    4.341964    0.3950089\n5    4.276013    0.4220744\n6    4.347153    0.3964028\n\nZoals we kunnen zien, hebben de parameters de vorm van een lange dataframe met twee kolommen, die overeenkomen met de intercept en het effect van Petal.Length. Deze kolommen bevatten de posterior distributies van deze twee parameters. Eenvoudig gezegd is de posterior distributie een set van verschillende plausibele waarden voor elke parameter.\nOver de posterior trekkingen\nLaten we eerst eens kijken naar de lengtes van de posteriors.\n\n[1] 4000\n\n\nWaarom zijn dit er 4000, en niet meer of minder?\n\nIn de eerste plaats worden deze waarnemingen (de rijen) meestal aangeduid als posterior ‘draws’ (trekkingen). De achterliggende gedachte is dat het Bayesiaanse samplingsalgoritme (b.v. Monte Carlo Markov Chains - MCMC) zal putten uit de verborgen ware posterior distributie . Het is dus door middel van deze ‘posterior draws’ dat we de onderliggende ware posterior distribution kunnen inschatten. Hoe meer trekkingen je hebt, hoe beter je de posterior distriubtion kunt inschatten. Meer trekkingen betekent echter ook een langere rekentijd.\nAls we kijken naar de documentatie (?sampling) voor het rstanarm“sampling”-algoritme dat standaard in het bovenstaande model wordt gebruikt, kunnen we verschillende parameters zien die het aantal posterior draws beïnvloeden. Standaard zijn er 4 ketens (je kunt het zien als aparte sampling runs), die elk 2000 iter (trekkingen, iteraties) aanmaken. Echter, slechts de helft van deze iteraties wordt behouden, aangezien de helft wordt gebruikt voor de opwarming (het convergeren van het algoritme). Het totaal is dus 4 ketens * (2000 iteraties - 1000 warming-up) = 4000 posterior trekkingen. Dat kunnen we aanpassen, bijvoorbeeld:\n\n\n\nIn dit geval hebben we, zoals verwacht, 2 ketens * (1000 iteraties - 250 warming-up) = 1500 posterior trekkingen. Maar laten we ons eerste model de standaard instelling aanhouden (omdat het meer trekkingen heeft).\nHet visualiseren van de posterieure verdeling\nNu we hebben begrepen waar deze waarden vandaan komen, laten we er eens naar kijken. We zullen beginnen met het visualiseren van de posterieure distributie van de parameter waarin we geïnteresseerd zijn, het effect van Petal.Length.\n\n\n\nDeze verdeling vertegenwoordigt de waarschijnlijkheid (de y-as) van verschillende effecten (de x-as). De centrale waarden zijn waarschijnlijker dan de extreme waarden. Zoals u ziet varieert deze verdeling van ongeveer 0,35 tot 0,50, waarbij het grootste deel rond 0,41 ligt.\n\nGefeliciteerd! Je hebt zojuist je posterior distribution beschreven.\n\nEn dit is het hart van de Bayesiaanse analyse. We hebben geen p-waarden, t-waarden of vrijheidsgraden nodig: Alles is aanwezig, binnen deze posterior verdeling.\nOnze beschrijving hierboven is consistent met de waarden verkregen uit de frequentistische regressie (die resulteerde in een bèta van 0,41). Dit is geruststellend! Inderdaad, in de meeste gevallen verandert een Bayesiaanse analyse de resultaten niet drastisch of hun interpretatie. Het maakt de resultaten wel beter interpreteerbaar en intuïtief, en uiteindelijk gemakkelijker te begrijpen en te beschrijven.\nWe kunnen nu doorgaan en deze posterior verdeling nauwkeurig karakteriseren.\nDe Posterior beschrijven\nHelaas, het is vaak niet praktisch om de hele posterieure verdelingen als grafiek te rapporteren. We moeten een beknopte manier vinden om het samen te vatten. We raden aan om de posterior verdeling te beschrijven op basis van 3 elementen:\nEen puntschatting die een samenvatting is van één waarde (vergelijkbaar met de bèta in frequente regressies).\nEen credible interval die de bijbehorende onzekerheid weergeeft.\nSommige indices van betekenis, die informatie geven over het relatieve belang van dit effect.\nPuntschatting\nWelke ene waarde kan het beste mijn posterior distributie representeren?\nCentrum indices, zoals het gemiddelde, de mediaan of de modus worden meestal gebruikt als puntschatting - maar wat is het verschil tussen het frequentische en Bayesiaanse raamwerk? Laten we dit beantwoorden door eerst het gemiddelde te inspecteren:\n\n[1] 0.4085763\n\nDit ligt dicht bij de frequentistische beta. Maar zoals we weten, is het gemiddelde vrij gevoelig voor uitschieters of extremen. Misschien is de mediaan robuuster?\n\n[1] 0.408726\n\nNou, dit is zeer dicht bij het gemiddelde (en identiek als de waarden worden afgerond). Misschien kunnen we de modus nemen, dat wil zeggen, de piek van de posterior verdeling? In het Bayesiaanse kader wordt deze waarde de Maximum A Posteriori (MAP) genoemd. Laten we daar eens kijken:\n\nMAP = 0.41\n\nZe zitten allemaal heel dichtbij elkaar! Laten we deze waarden visualiseren op de posterior distributie:\n\n\n\nNou, al deze waarden geven zeer gelijkaardige resultaten. Dus we zullen de mediaan kiezen, omdat deze waarde een directe betekenis heeft vanuit een probabilistisch perspectief: er is 50% kans dat het werkelijke effect hoger is en 50% kans dat het effect lager is (omdat het de verdeling in twee gelijke delen verdeelt).\nOnzekerheid\nNu we een puntschatting hebben, moeten we de onzekerheid beschrijven. We zouden het bereik kunnen berekenen:\n\n[1] 0.3420445 0.4783965\n\nMaar heeft het zin om al deze extreme waarden op te nemen? Waarschijnlijk niet. Dus, we zullen een credible interval berekenen. Lang verhaal kort, het lijkt een beetje op een frequentistische confidence interval, maar is makkelijker te interpreteren en gemakkelijker te berekenen - en het is logischer.\nWe zullen dit credible interval berekenen op basis van het Highest Density Interval (HDI). Het geeft ons het bereik dat de 89% meest waarschijnlijke effectwaarden bevat. We zullen 89% CIs gebruiken in plaats van 95% CIs (zoals in het frequentistische kader), omdat het 89%-niveau stabielere resultaten geeft (Kruschke, 2014) en ons herinnert aan de willekeur van dergelijke conventies (McElreath, 2020).\n\n# Highest Density Interval\n\n89% HDI     \n------------\n[0.38, 0.44]\n\nMooi, dus we kunnen concluderen dat het effect 89% kans heeft om binnen het [0,38, 0,44] bereik te vallen. We hebben zojuist de twee belangrijkste stukken informatie berekend om onze effecten te beschrijven.\nEffect significantie\nOp veel wetenschappelijke gebieden is het echter niet voldoende om alleen de effecten te beschrijven. Wetenschappers willen ook weten of dit effect betekenis heeft in praktische of statistische termen. Of, om het met andere woorden te zeggen, of het effect belangrijk is. Wijkt het effect af van 0? Dus hoe berekenen we de significantie van een effect. Hoe kunnen we dit doen?\nWel, in dit specifieke geval is het zeer welsprekend: Alle mogelijke effectwaarden (d.w.z. de hele posterior distributie) zijn positief en meer dan 0,35, wat al een substantieel bewijs is dat het effect niet nul is.\nMaar toch willen we een objectief beslissingscriterium, om te zeggen of het effect ja of nee ‘significant’ is. Een benadering, vergelijkbaar met het frequentistisch kader, zou zijn om te kijken of het Credible Interval een 0 bevat. Als dat niet het geval is, zou dat betekenen dat ons effect ‘significant’ is.\nMaar deze index is toch niet erg fijnmazig? Kunnen we het beter doen? Ja.\nEen lineair model met een categorische voorspeller\nStel je voor dat je geïnteresseerd bent in hoe het gewicht van de kippen varieert, afhankelijk van twee verschillende voedersoorten. Voor dit examen zullen we beginnen met het selecteren van twee voor ons interessante voersoorten uit de chickwts-dataset (zit ook in basis R) (we hebben wel bijzondere interesses): vleesmaaltijden (‘meat meals’) en zonnebloemen (‘sunflowers’).\nData voorbereiden en model draaien\n\n\n\nLaten we nog een Bayesiaanse regressie uitvoeren om het gewicht te voorspellen met de twee voertypesoorten.\n\n\n\nPosterior beschrijving\n\n\n\nDit representeert de posterior distributie van het verschil tussen ‘meatmeal’ en ‘sunflowers’. Het lijkt erop dat het verschil eerder positief is (de waarden lijken geconcentreerd aan de rechterkant van 0)… Het eten van zonnebloemen maakt je dikker (tenminste, als je een kip bent). Maar, door hoeveel?  Laten we de mediaan en de CI berekenen:\n\n[1] 51.56878\n\n\n# Highest Density Interval\n\n89% HDI       \n--------------\n[11.09, 90.35]\n\nHet maakt je met ongeveer 51 gram (de mediaan) dikker. De onzekerheid is echter vrij groot: er is 89% kans dat het verschil tussen de twee voersoorten tussen 14 en 91 ligt.\n\nVerschilt dit effect van 0?\n\nROPE Percentage\nTesten of deze verdeling anders is dan 0 heeft geen zin, omdat 0 een enkele waarde is (en de kans dat een verdeling anders is dan een enkele waarde is oneindig).\nEen manier om significantie te beoordelen kan echter zijn om een gebied rond 0 te definiëren, wat als praktisch equivalent van nul zal worden beschouwd (d.w.z. afwezigheid van, of verwaarloosbaar, effect). Dit wordt de ‘Region of Practical Equivalence’ (ROPE) genoemd en is een manier om de betekenis van de parameters te testen.\nHoe definiëren we dit gebied?\n\nTringgg Tringgg\n\n– U spreekt met het easystatsteam. Hoe kunnen we u helpen?\n– Ja met Prof. Sanders. Ik ben kippenexpert. Ik bel u vanwegen mijn expertkennis. Een effect tussen -20 en 20 is verwaarloosbaar. Tot ziens.\nNou, dat komt goed uit. Nu weten we dat we de ROPE kunnen definiëren als het [-20, 20] bereik. Alle effecten binnen dit bereik worden als nihil (te verwaarlozen) beschouwd. We kunnen nu het aandeel van de 89% meest waarschijnlijke waarden (de 89% CI) berekenen die niet nul zijn, d.w.z., die buiten dit bereik liggen.\n\n# Proportion of samples inside the ROPE [-20.00, 20.00]:\n\ninside ROPE\n-----------\n4.80 %     \n\n5% van de 89% CI kan als nihil worden beschouwd. Is dat veel? Gebaseerd op onze richtlijnen, ja, het is te veel. Op basis van deze specifieke definitie van ROPE concluderen we dat dit effect niet significant is (de kans dat het verwaarloosbaar is, is te groot).\nHoewel, om eerlijk te zijn, heb ik een aantal twijfels over deze Prof. Sanders. Ik vertrouw zijn definitie van ROPE** niet echt. Is er een meer objectieve manier om het te definiëren?\nJa. Een betrouwbare manier is bijvoorbeeld het gebruik van een tiende (1/10 = 0,1) van de standaardafwijking (SD) van de responsvariabele, die als een “verwaarloosbare” effectomvang kan worden beschouwd (Cohen, 1988).\n\n[1] -6.17469  6.17469\n\nLaten we onze ROPE opnieuw definiëren als de regio binnen het [-6.2, 6.2] bereik. Merk op dat dit direct kan worden verkregen met de rope_range functie :)\n\n[1] -6.17469  6.17469\n\nLaten we nu het percentage in ROPE opnieuw berekenen:\n\n# Proportion of samples inside the ROPE [-6.17, 6.17]:\n\ninside ROPE\n-----------\n0.00 %     \n\nMet deze redelijke definitie van ROPE stellen we vast dat de 89% van de posterior distributie van het effect niet overlapt met de ROPE. We kunnen dus concluderen dat het effect significant is (in de zin van belangrijk genoeg om op te merken).\nWaarschijnlijkheid van Richting (Probability of Direction (pd))\nMisschien zijn we niet geïnteresseerd in de vraag of het effect niet te verwaarlozen is. Misschien willen we alleen weten of dit effect positief of negatief is. In dit geval kunnen we eenvoudigweg berekenen welk deel van de posterior distributie positief is, ongeacht de “grootte” van het effect.\n\n[1] 98.05\n\nWe kunnen concluderen dat het effect positief is met een waarschijnlijkheid van 98%. We noemen deze index de Waarschijnlijkheid van Richting (pd). Het kan in feite gemakkelijker worden berekend met het volgende:\n\npd = 98.05%\n\nInteressant is dat deze index meestal sterk gecorreleerd is met de meest frequente p-waarde. We kunnen de overeenkomstige p-waarde bijna ruwweg afleiden met een eenvoudige transformatie:\nInterestingly, it so happens that this index is usually highly correlated with the frequentist p-value. We could almost roughly infer the corresponding p-value with a simple transformation:\n\n[1] 0.0436\n\nAls we ons model in het frequentistisch kader hebben uitgevoerd, zouden we ongeveer een effect moeten waarnemen met een p-waarde van 0.04. Is dat waar?\nVergelijking met frequentisten\n\n\nCall:\nlm(formula = weight ~ feed, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-123.909  -25.913   -6.917   32.091  103.091 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     276.91      17.20  16.097 2.74e-13 ***\nfeedsunflower    52.01      23.82   2.184   0.0405 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 57.05 on 21 degrees of freedom\nMultiple R-squared:  0.1851,    Adjusted R-squared:  0.1463 \nF-statistic: 4.769 on 1 and 21 DF,  p-value: 0.04047\n\nHet frequentistische model vertelt ons dat het verschil positief en significant (beta = 52, p = 0.04) is.\nAlhoewel we tot een gelijkaardige conclusie kwamen, liet het Bayesiaanse kader ons toe om een meer diepgaand en intuïtief begrip te ontwikkelen van ons effect en van de onzekerheid van de inschatting ervan.\nAlles met één functie\nEn toch, ik ben het ermee eens, het was een beetje omslachtig om alle indices eruit te halen en te berekenen. Maar wat als ik je vertel dat we dit allemaal kunnen doen, en meer, met slechts één functie?\n\nZie, beschrijf_posterior!\n\nDeze functie berekent alle genoemde indexen, en kan direct op het model worden uitgevoerd:\n\n# Description of Posterior Distributions\n\nParameter     |  Median |             89% CI |      pd |        89% ROPE | % in ROPE |        BF |  Rhat |      ESS\n-------------------------------------------------------------------------------------------------------------------\n(Intercept)   | 277.410 | [249.059, 305.796] | 100.00% | [-6.175, 6.175] |         0 | 5.121e+11 | 1.000 | 3071.333\nfeedsunflower |  51.569 | [ 11.086,  90.349] |  98.05% | [-6.175, 6.175] |         0 |     0.764 | 1.000 | 3260.635\n\nTada! Daar hebben we het! De mediaan, de CI, de pd en het ROPE percentage!\nHet begrijpen en beschrijven van posterior distributies is slechts één aspect van Bayesiaanse modellering… Ben je klaar voor meer? \nBevestiging van Bayesiaanse vaardigheden\nNu het beschrijven en begrijpen van posterior distributies van lineaire regressies voor jou geen geheimen meer heeft, zullen we een stap terug doen en wat eenvoudigere modellen bestuderen: correlaties en t-testen.\nMaar laten we eerst even stilstaan bij het feit dat alle statistische basisprocedures zoals correlaties, t-testen, ANOVA’s of Chisquare-testen ** lineaire regressies** zijn (we raden deze uitstekende demonstratie ten zeerste aan). Op basis van deze eenvoudige modellen introduceren we een complexere index, zoals de Bayes-factor.\nCorrelaties\nFrequentistische versie\nLaten we opnieuw beginnen met een frequentistische correlatie tussen twee continue variabelen, de breedte en de lengte van de kelkbladen van sommige bloemen (‘sepals’). De gegevens zijn beschikbaar in R als de iris dataset (dezelfde die we hierboven hebben gebruikt).\nWe zullen een Pearson’s correlatietest berekenen, de resultaten opslaan in een object met de naam resultaat en vervolgens deze resultaten weergeven:\n\n\n    Pearson's product-moment correlation\n\ndata:  iris$Sepal.Width and iris$Sepal.Length\nt = -1.4403, df = 148, p-value = 0.1519\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.27269325  0.04351158\nsample estimates:\n       cor \n-0.1175698 \n\nZoals je in de output kunt zien, heeft de test die we hebben gedaan eigenlijk twee hypothesen vergeleken: de nul-hypothese (h0; geen correlatie) met de alternatieve hypothese (h1; een niet-nul-correlatie). Op basis van de p-waarde kan de nulhypothese niet worden verworpen: de correlatie tussen de twee variabelen is negatief maar niet significant (r = -.12, p > .05).\nBayesiaanse correlatie\nOm een Bayesiaanse correlatietest te berekenen, hebben we het BayesFactor-pakket nodig (u kunt het installeren door install.packages (“BayesFactor”) uit te voeren). We kunnen dan dit pakket laden, de correlatie berekenen met behulp van de correlatieBF() functie en de resultaten op een vergelijkbare manier opslaan.\n\n\n\nLaten we nu eens onze describe_posterior()-functie hierop los:\n\n  Parameter     Median CI     CI_low    CI_high    pd ROPE_CI\n1       rho -0.1102375 89 -0.2294536 0.02167853 0.922      89\n  ROPE_low ROPE_high ROPE_Percentage        BF Prior_Distribution\n1     -0.1       0.1       0.4509969 0.5090175             cauchy\n  Prior_Location Prior_Scale\n1              0   0.3333333\n\nWe zien hier weer veel dingen, maar de belangrijke indices voor nu zijn de mediaan van de posterior distributie, -.11. Dit komt (weer) dicht in de buurt van de frequentistische correlatie. We zouden, zoals eerder, het credible interval, de pd of het ROPE-percentage kunnen beschrijven, maar we zullen ons hier richten op een andere index die door het Bayesiaanse kader wordt geboden, de Bayes-factor (BF).\nBayes-factor (BF)\nWe zeiden eerder dat een correlatietest eigenlijk twee hypothesen vergelijkt, een nul (afwezigheid van effect) met een alarmerende (aanwezigheid van een effect). De Bayes-factor (BF) laat dezelfde vergelijking toe en bepaalt onder welke van twee modellen de geobserveerde gegevens waarschijnlijker zijn: een model met het effect waarin we geinteresseerd zijn, en een nulmodel zonder het effect daarvan. We kunnen de bayes-factor() gebruiken om de Bayes-factor specifiek te berekenen bij het vergelijken van die modellen:\n\n# Bayes Factors for Model Comparison\n\n  Model             BF\n  [2] (rho != 0) 0.509\n\n* Against Denominator: [1] (rho = 0)\n*   Bayes Factor Type: JZS (BayesFactor)\n\nWe hebben een BF van 0,51. Wat betekent dat?\nBayes-factoren zijn continue metingen van het relatieve bewijs, waarbij een Bayes-factor groter dan 1 bewijs geeft ten gunste van één van de modellen (vaak de teller genoemd), en een Bayes-factor kleiner dan 1 die bewijs geeft ten gunste van het andere model (de noemer).\n\nJa, je hebt het goed gehoord, bewijs ten gunste van de nul!\n\nDat is een van de redenen waarom het Bayesiaanse kader soms als superieur wordt beschouwd aan het frequentistische kader. Onthoud uit je statistiekenlessen, dat de p waarde alleen gebruikt kan worden om h0 af te wijzen, maar niet om het te accepteren. Met de Bayes-factor kunt je -evidentie meten tegen - en ook ten gunste van - de nul.\nBF’s die het bewijs voor het alternatief tegen de null vertegenwoordigen kunnen worden teruggedraaid met 𝐵𝐹01=1/𝐵𝐹10 (de 01 en 10 komen respectievelijk overeen met h0 tegen h1 en h1 tegen h0) om het bewijs voor de null weer te geven. Dit verbetert de leesbaarheid in gevallen waarin het BF van het alternatief tegen de nul kleiner is dan 1 (d.w.z. ter ondersteuning van de nul).\nIn ons geval, BF = 1/0,51 = 2, geeft aan dat de gegevens 2 keer meer waarschijnlijk zijn onder de null in vergelijking met de alternatieve hypothese. Die weliswaar de voorkeur geeft aan de nul-hypothese, maar slechts als anekdotisch bewijs moet wordt beschouwd.\nWe kunnen dus concluderen dat er anecdotisch bewijs is ten gunste van de hypothese ‘gebrek aan correlatie tussen de twee variabelen’ (mediaan = 0,11, BF = 0,51), wat veel meer informatie geeft dan wat we kunnen doen met de frequentistische statistiek.\nEn dat is nog niet alles!\nVisualiseren van de Bayes-factor\nIn het algemeen zijn taartgrafieken een absolute ‘no-go’ in datavisualisatie, omdat het waarnemingssysteem van onze hersenen de gepresenteerde informatie op deze manier sterk vervormt. Toch is er één uitzondering: pizzagrafieken.\nHet is een intuïtieve manier om de bewijskracht van BFs te interpreteren als een soort verrassing\nDergelijke “pizzapercelen” kunnen direct worden aangemaakt via het zie visualisatiepakket voor easystats (u kunt het installeren door het uitvoeren van\nDergelijke ‘pizzagrafieken’ kunnen direct worden aangemaakt met het visualisatiepakket voor easystats (u kunt het installeren door install.packages(\"see\")) uit te voeren):\n\n\n\nDus, na het zien van deze pizza, ben je dan nog verrast door de uitkomst?\nt-testen\n\n“Ik weet dat ik niets weet, en vooral niet als versicolor en virginica verschillen in termen van Sepal.Width”, zei de beroemde Socrates.\n\nTijd om eindelijk een antwoord te geven op deze cruciale vraag!\nVersicolor vs. virginica\nBayesiaanse t-testen kunnen worden uitgevoerd op een zeer vergelijkbare manier als correlaties. We zijn met name geïnteresseerd in twee niveaus van de Specie factor, versicolor en virginica. We zullen beginnen met het uit iris uitfilteren van de niet-relevante waarnemingen die overeenkomen met de setosa specie, en we zullen dan de waarnemingen en de distributie van de Sepal.Width variabele visualiseren.\n\n\n\nBereken de Bayesiaanse t-test\nHet lijkt er (visueel) op dat virgnica bloemen gemiddeld een iets grotere kelkbladbreedte hebben. Laten we dit verschil statistisch beoordelen met behulp van de ttestBF in het BayesFactor pakket.\n\n   Parameter    Median CI    CI_low   CI_high    pd ROPE_CI\n1 Difference 0.1871487 89 0.0946914 0.2936964 0.998      89\n    ROPE_low ROPE_high ROPE_Percentage       BF Prior_Distribution\n1 -0.0332751 0.0332751               0 17.71872             cauchy\n  Prior_Location Prior_Scale\n1              0   0.7071068\n\nOp basis van de indexen kunnen we zeggen dat het verschil tussen virginica en versicolor (van Sepal.Width) een kans heeft van 100% om negatief te zijn [van de pd en het teken van de mediaan] (mediaan = -0,19, 89% CI [-0,29, -0,092]). De gegevens leveren een sterk bewijs tegen de nulhypothese (BF = 18).\nHoud dat in gedachten, want we zullen een andere manier zien om deze vraag te onderzoeken.\nLogistisch Model\nEen hypothese waarvoor men een t-test gebruikt, kan ook getest worden met een binomiaal model (bv. een logistisch model). Het is inderdaad mogelijk om de volgende hypothese te herformuleren, “er is een belangrijk verschil in deze variabele tussen de twee groepen” door “deze variabele in staat te stellen om te discrimineren tussen (of te classificeren in) de twee groepen”. Deze modellen zijn echter veel krachtiger dan een gewone t-test.\nIn het geval van het verschil van Sepal.Width tussen virginica en versicolor wordt de vraag, hoe goed kunnen we de twee soorten classificeren met alleen Sepal.Width.\nHet model fitten\n\n\n\nPrestatie en parameters\nEerst prestatie van het model in kaart brengen.\n\nCan't calculate log-loss.\n# Indices of model performance\n\nELPD   | ELPD_SE |  LOOIC | LOOIC_SE |   WAIC |   R2 | RMSE | Sigma | Score_log | Score_spherical\n-------------------------------------------------------------------------------------------------\n-66.25 |    3.04 | 132.51 |     6.08 | 132.50 | 0.10 | 1.11 |  1.00 |   -105.55 |            0.01\n\nVervolgens de resultaten van enkele indices presenteren.\n\n# Description of Posterior Distributions\n\nParameter   | Median |           89% CI |      pd |        89% ROPE | % in ROPE |     BF |  Rhat |      ESS\n-----------------------------------------------------------------------------------------------------------\n(Intercept) | -6.081 | [-9.268, -2.676] | 100.00% | [-0.181, 0.181] |         0 | 12.135 | 1.000 | 2931.465\nSepal.Width |  2.118 | [ 0.916,  3.198] | 100.00% | [-0.181, 0.181] |         0 | 17.399 | 1.001 | 2912.719\n\nReferenties\nAndrews, M., & Baguley, T. (2013). Prior approval: The growth of bayesian methods in psychology. British Journal of Mathematical and Statistical Psychology, 66(1), 1–7.\nBenjamin, D. J., Berger, J. O., Johannesson, M., Nosek, B. A., Wagenmakers, E.-J., Berk, R., … others. (2018). Redefine statistical significance. Nature Human Behaviour, 2(1), 6.\nChambers, C. D., Feredoes, E., Muthukumaraswamy, S. D., & Etchells, P. (2014). Instead of ’playing the game’ it is time to change the rules: Registered reports at aims neuroscience and beyond. AIMS Neuroscience, 1(1), 4–17.\nEtz, A., & Vandekerckhove, J. (2016). A bayesian perspective on the reproducibility project: Psychology. PloS One, 11(2), e0149794.\nKruschke, J. K. (2010). What to believe: Bayesian methods for data analysis. Trends in Cognitive Sciences, 14(7), 293–300.\nKruschke, J. K., Aguinis, H., & Joo, H. (2012). The time has come: Bayesian methods for data analysis in the organizational sciences. Organizational Research Methods, 15(4), 722–752.\nSzucs, D., & Ioannidis, J. P. (2016). Empirical assessment of published effect sizes and power in the recent cognitive neuroscience and psychology literature. BioRxiv, 071530.\nWagenmakers, E.-J., Marsman, M., Jamil, T., Ly, A., Verhagen, J., Love, J., … others. (2018). Bayesian inference for psychology. Part i: Theoretical advantages and practical ramifications. Psychonomic Bulletin & Review, 25(1), 35–57.\n\n\n\n",
    "preview": "posts/2020-11-17-testen-met-bayes/testen-met-bayes_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2021-02-14T12:26:03+01:00",
    "input_file": "testen-met-bayes.utf8.md"
  },
  {
    "path": "posts/2020-11-01-nogmaals-grafieken/",
    "title": "Het Goede, het Slechte en het Lelijke: Data effectief visualiseren en communiceren",
    "description": "Shirin Elsinghorst schreef deze blog onlangs op [Codecentric](https://blog.codecentric.de/2020/10/goodbadugly/). Omdat ze op mijn werk de vormgeving van de uitgaven hebben aangepast, wilde ik het maken van figuren aan de nieuwe kleursetting van mijn werk aanpassen. Shirin's blog was een mooie oefenplaats voor mij. Tegelijk is het een mooie introductie op datavisualisatie en daarom de moeite waard het in het Nederlands te bewerken.    [Hier](https://docs.google.com/presentation/d/e/2PACX-1vR4pD2EmW9Gzxr1Q3qwgjEYkU64o2-ThlX1mXqfNQ2EKteVUVt6Qg2ImEKKi9XLv-Iutb3lD8esLyU7/pub?start=false&loop=false&delayms=3000&slide=id.g58b36409ef_0_0) vind je de presentatie die zij zelf hierover op 20 oktober 2020 in Duitsland gaf.",
    "author": [
      {
        "name": "Shirin Elsinghorst, vertaling Harrie Jonkman",
        "url": "https://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/"
      }
    ],
    "date": "2020-11-01",
    "categories": [],
    "contents": "\nEnkele handelingen vooraf\nEerst maar eens de pakketten laden die gebruikt worden:\n\n\n\nVervolgens de kleuren instellen.\nDe dataset\n\n# A tibble: 6 x 8\n  species island bill_length_mm bill_depth_mm flipper_length_…\n  <fct>   <fct>           <dbl>         <dbl>            <int>\n1 Adelie  Torge…           39.1          18.7              181\n2 Adelie  Torge…           39.5          17.4              186\n3 Adelie  Torge…           40.3          18                195\n4 Adelie  Torge…           NA            NA                 NA\n5 Adelie  Torge…           36.7          19.3              193\n6 Adelie  Torge…           39.3          20.6              190\n# … with 3 more variables: body_mass_g <int>, sex <fct>, year <int>\n\n\n\n[1] 0.3926991 1.1780972 1.9634954 2.7488936 3.5342917 4.3196899\n[7] 5.1050881 5.8904862\n\n\n\n\n\n\n\n\n\n\n0. Datavisualisatie, cruciaal voor begrip en communicatie\nDatavisualisatie is een cruciaal onderdeel van elke analyse. Of het nu “voor jezelf” is om verbanden en resultaten beter te begrijpen of om resultaten te presenteren en te “verkopen” aan anderen. Omdat goede grafieken de gegevens intuïtief toegankelijk maken, vertellen ze een verhaal en laten ze duidelijk patronen, trends of uitschieters zien. Daarom is Explorative Data Analyse (EDA) meestal de eerste stap van elke gegevensanalyse en -modellering. Alleen als we onze gegevens begrijpen, kunnen we de juiste voorbewerkingsstappen en analysemethoden, statistieken of ’deep learning’technieken toepassen. En aangezien mensen veel beter zijn in het visueel begrijpen van getallen in een grafiek dan in tabellen, moeten we de kracht van datavisualisatie gebruiken! Vooral bij het maken van visualisaties voor rapporten of publicaties is het cruciaal dat deze zowel feitelijk correct als visueel aantrekkelijk zijn.\n\nDatavisualisatie is deels kunst en deels wetenschap. De uitdaging is om de kunst goed te krijgen zonder dat de wetenschap het bij het verkeerde eind heeft en vice versa. (Wilke 2019)\n\nHet doel van een goede illustratie is dat het in één oogopslag begrijpelijk is en een duidelijke uitspraak doet. Slechte grafieken variëren van eenvoudigweg lelijk tot (opzettelijk?) misleidend of zelfs verkeerd. In dit artikel leg ik uit wat goede grafieken zijn (en wat niet) en hoe we ze kunnen maken met behulp van de Grammatica van Grafieken. En ik introduceer enkele van de meest gebruikte soorten grafieken, samen met negatieve voorbeelden “uit de vrije natuur”.\nWat maakt een goede grafiek?\n1. Data\nHet belangrijkste aspect en de basis van elke grafiek zijn de gebruikte gegevens! Ze moeten correct zijn en we moeten altijd controleren op mogelijke (meet)fouten. Zoals bijvoorbeeld in deze afbeelding:\n\nHet is duidelijk dat er een fout in de voorspelling zit, want temperaturen van -100°C, zelfs op een hele speciale dag, zijn echt heel onwaarschijnlijk! En niet alleen toont deze grafiek duidelijk verkeerde gegevens; deze uitschietwaarde comprimeert de rest van de gegevens in de grafiek zodanig dat de curven niet meer duidelijk zichtbaar zijn en de waarden van verschillende dagen moeilijk te vergelijken zijn.\nTot goede praktijk van datavisualisatie behoort tevens het specificeren van de gegevensbron.\n2. Overzichtelijkheid & kleuren\nEen goede grafiek is juist zo complex omdat ze haar boodschap in één oogopslag moet overbrengen; ze moet duidelijk zijn en geen “chart junk” bevatten. Edward Tufte beschouwde alle visuele elementen in een grafiek die ofwel niet nodig zijn om de grafiek te begrijpen, ofwel zelfs afleiden van de centrale informatie als grafiektroep (Tufte 1983). Drie voorbeelden daarvan zijn in deze figuur te zien:\n\nIedereen die regelmatig kranten leest zal merken dat “chart junk” extreem vaak voorkomt en vooral populair is bij populaire media om een chart te kruiden. In veel gevallen kan een onschuldige chartjunk worden afgedaan als “artistieke vrijheid”, maar hoe ernstiger jouw grafiek wilt maken, hoe meer je die moet vermijden.\nDuidelijkheid omvat ook de keuze van geschikte kleuren en duidelijke contrasten. Te veel, felle of grillige kleuren maken een plot visueel weerzinwekkend en zorgen ervoor dat het er overbeladen en verwarrend uitziet. Daarnaast moet je er bij de keuze van de kleuren ook op letten dat het voor kleurenblinden mogelijk is om de grafiek te lezen. Dit moet worden gedaan met behulp van een geschikt kleurenpalet, evenals redundante functies zoals verschillende vormen, patronen of lijntypes die het mogelijk maken om de grafiek te lezen, zelfs zonder kleurinformatie.\nKleuren\n3. etikettering & assen\nEen goede grafiek is in één oogopslag te begrijpen, wat betekent dat ze voor zichzelf spreken. Essentieel hiervoor is de juiste etikettering met titel, aslabels (met eenheden!), legenda’s en bijschriften! Ook het aantal assentikken moet op de juiste manier worden gekozen. En vooral: de afstanden tussen de assen moeten voor de numerieke waarden regelmatig zijn, d.w.z. dat de afstand tussen de teken in geen geval mag variëren, zoals te zien is op de y-as in deze vreselijke illustratie:\n\nDe grammatica van de grafiek\nWe kennen nu de belangrijkste aspecten van goede grafieken. Maar wat is de beste manier om goede grafieken te maken? Er zijn vele manieren om grafieken te genereren: met de hand tekenen, met Excel of met verschillende programmeertalen zoals R, Python, Java, enz. De beste manier, hoewel met een hogere instapdrempel, is het gebruik van programmeertalen. Dit is de eenvoudigste manier om ervoor te zorgen dat de gegevens schoon zijn en op een traceerbare manier kunnen worden verwerkt. Excel kan ook gebruikt worden om grafieken te maken, maar het programma heeft een paar valkuilen: Excel-format kan makkelijk leiden tot fouten in de gegevens. En het maakt het erg moeilijk om grafieken te reproduceren, omdat het niet documenteert welke stappen handmatig werden uitgevoerd en in welke volgorde.\nR en Python zijn bijzonder geschikt omdat ze de meest gebruikte programmeertalen zijn voor het genereren van grafieken en omdat ze pakketten aanbieden die het analyseren van gegevens en het maken van grafieken zeer efficiënt maken. Hier presenteer ik de (naar mijn mening) beste manier om op een gestructureerde manier grafieken te genereren: met de pakketten ggplot2 voor R of plotnine voor Python (gebaseerd op ggplot2).\nMet ggplot2 heeft Hadley Wickham een implementatie gemaakt van de 1999 Grammatica van Graphics voor de door Leland Wilkinson beschreven R-programmeertaal, die ik hieronder introduceer met codevoorbeelden (Wilkinson et al. 1999; Wickham 2010). Deze Grammatica van Graphics beschrijft een raamwerk voor het gestructureerd maken van grafieken die bestaan uit lagen die op elkaar voortbouwen (Wickham en Grolemund 2017). Hieronder laat ik een paar voorbeelden zien. Een overzicht van alle mogelijke opties is te vinden in het ggplot2-cheatsheet.\n1. Data\nOok voor de Grammatica van Graphics zijn data het belangrijkste en meest fundamentele element. Hier gebruik ik een voorbeelddataset met verschillende groottes van drie pinguïnsoorten (Gorman, 2014). De centrale functie van het ggplot2-pakket wordt ggplot() genoemd en neemt een dataset als invoer. Deze functie creëert eerst een leeg coördinatensysteem waarop we met de volgende lagen kunnen bouwen en zo onze grafiek stap voor stap kunnen creëren, aanpassen en uitbreiden.\n2. Esthetiek\nHet tweede argument dat we definiëren in de ggplot() functie is de esthetiek (aes()). Esthetiek beschrijft grafische elementen zoals X- en Y-waarden, grootte, kleuren, vormen, enz. Voor een eenvoudige scatterplot moeten we ten minste de X- en Y-posities specificeren. Hiervoor moeten we eerst beslissen welke gegevens (variabelen) we in kaart willen brengen. Bijvoorbeeld hier de snavellengte van de pinguïn op de X-as tegen de vinlengte op de Y-as:\n\n\n\nZelfs met een bepaalde esthetiek krijgen we nog steeds geen echte grafiek te zien, maar we hebben de volgende laag nodig, de zogenaamde geometrie. Omdat esthetiek en geometrie zeer nauw met elkaar verbonden zijn en deels van elkaar afhankelijk zijn, laat ik hieronder extra esthetiek zien.\n3. Geometriek\nGeometrische objecten of geometrieën beschrijven hoe de gegevens die we in de esthetiek hebben gedefinieerd, moeten worden weergegeven. Dit kan bijvoorbeeld een puntgrafiek (geom_point()) of een lijngrafiek (geom_line()) zijn, die nu ook als grafiek in deze laag wordt weergegeven:\n\n\n\nEen lijngrafiek is echter niet nuttig voor de gegevens hier; meer hierover in de sectie Grafiektypen - Lijngrafieken. Andere geometrieën zijn staafdiagrammen (geom_bar()) of boxplots (geom_boxplot()). Geometrie en esthetiek zijn onderling afhankelijk in die zin dat het gegevenstype van de esthetische variabelen alleen bepaalde geometrieën toelaat of zinvol is. Zo zijn strooi- en lijndiagrammen geschikt voor doorlopende X- en Y-assen (rationele getallen, tijden of datum). Voor staafdiagrammen moeten de X-as gegevens categorisch zijn. Voordat ik in de loop van latere lagen meer in detail zal ingaan op geometrieën en esthetiek, zal ik eerst facetten introduceren.\n4. Facetten\nFacetten betekent het splitsen van een grafiek in verschillende subplots. In onze voorbeelddataset worden de meetwaarden van drie verschillende pinguïnsoorten verzameld. Het bovenstaande strooiplot laat echter niet toe om een onderscheid te maken tussen de drie soorten, wat natuurlijk een belangrijke bijkomende informatie is in de gegevens. Daarom moeten we dit in onze grafiek weergeven. Een manier om dit te doen is door gebruik te maken van facetten:\n\n\n\nNu zien we de punten voor elke soort pinguïn in een aparte subplot. Facetten kunnen worden gecreëerd voor één of meer categorische variabelen, maar meer dan twee facetten zullen in het algemeen verwarrend zijn. Standaard gebruikt ggplot2 dezelfde X- en Y-asafmetingen om de subplots vergelijkbaar te maken. Met facetten is het in dit geval echter niet zo eenvoudig om de drie typen te vergelijken. Als alternatief kunnen we de drie pinguïnsoorten zichtbaar maken met behulp van verschillende kleuren. Deze mogelijkheid valt onder schaalvergroting.\n5. Schaalvergroting\nMet schaalvergroting kunnen we naast de twee X- en Y-dimensies nog extra dimensies tonen, vergelijkbaar met wat we al gedaan hebben voor de pinguïnsoorten met facetten. Zo kunnen we bijvoorbeeld een kleurenschaal kiezen. In ggplot2 worden schalen gegeven door extra variabelen in de esthetiek:\n\n\n\nDe bijbehorende legende wordt automatisch aangemaakt. Andere schalen zijn maatschalen, punt- of lijntypes. Niet alle schalen zijn geschikt voor elk datatype. Terwijl kleuren ook continue getallen kunnen vertegenwoordigen, zijn punt- en lijntypes slechts voor een beperkt aantal categorieën beschikbaar.\n\n\n\nIn principe kan elk aantal dimensies van de gegevens in een grafiek worden weergegeven, ook al kunnen meer dan vier dimensies de grafiek meestal te chaotisch en verwarrend maken.\nEen ander type schaling is de asschaalverdeling. Zo kunnen we bijvoorbeeld de assen omdraaien zodat de waarden niet van links/naar beneden = laag naar rechts/boven = hoog worden weergegeven, maar de hoge waarden wel links/naar beneden worden weergegeven:\n\n\n\n6. Statistische Transformaties\nStats, afkorting voor statistische transformaties, worden gebruikt om statistische waarden of berekeningen toe te voegen aan een plot of om deze te definiëren. Dit kunnen bijvoorbeeld gemiddelde waarden zijn, mediaan, betrouwbaarheidsintervallen, standaardafwijkingen, enz.\nIn deze figuur is een staafdiagram weergegeven met een numerieke waarde:\n\n\n\nOmdat de standaardstatistiek voor staafdiagrammen in ggplot2 „Aantal (count)“ ist, kunnen deze verhoudingen mit de stat „identity“ worden veranderd.\nEen van de meest gebruikte statistieken zijn ‘Smoothed Conditional Means’, om bijvoorbeeld de samenhang van de X- en Y-Variabelen met gladde lijnen en bijbehorende foutmarges aan te tonen:\n\n\n\n7. Coördinatensystemen\nDe laatste laag in de Grammatica van Graphics zijn coördinatensystemen. Coördinatensystemen bepalen hoe de assen van onze grafiek moeten worden gerangschikt. Meestal is de X-as horizontaal en de Y-as verticaal (cartesiaans coördinatenstelsel); maar er zijn ook gevallen waarin we radiale of gebogen assen hebben, bijvoorbeeld in een taartdiagram of kaartweergave. Een taartdiagram is dus niets meer dan een staafdiagram waarin we het coördinatensysteem hebben veranderd:\n\n\n\nDiagramtypen Met deze Grammatica van Graphics kunnen nu alle gangbare diagramtypen eenvoudig en flexibel worden gegenereerd en uitgebreid. De meest gebruikte diagramtypen zijn:\nPuntdiagrammen\n\n\n\nPuntdiagrammen worden vaak gebruikt wanneer we numerieke X-waarden tegen numerieke Y-waarden willen weergeven en dus hun correlatie willen laten zien. Plots kunnen verschillende kleuren, vormen en maten hebben. Meestal zijn puntdiagrammen gemakkelijk te begrijpen, maar ze kunnen ook verwarrend worden als er te veel overlappende punten zijn.\nLijndiagrammen\n\n\n\nLijndiagrammen zijn meestal vergelijkbaar met puntdiagramma, met dat verschil dat de (denkbeeldige) punten met elkaar verbonden zijn door lijnen. Deze verbonden lijnen vertegenwoordigen de denkbeeldige tussenliggende waarden tussen twee meetpunten; punten moeten dus alleen verbonden worden als deze veronderstelling wordt gemaakt! Om deze reden is een lijngrafiek niet bruikbaar voor het bovenstaande voorbeeld, omdat we onafhankelijke metingen van individuele personen laten zien. Lijndiagrammen zijn vooral nuttig voor tijdreeksen.\nStaafdiagrammen\n\n\n\nStaafdiagrammen tonen ofwel het aantal gebeurtenissen of ze tonen een numerieke waarde Y ter vergelijking tussen verschillende categorieën. Vooral bij staafdiagrammen vinden we veel negatieve voorbeelden met misleidende voorstellingen (waarschijnlijk omdat ze zo gemakkelijk te maken zijn met de eenvoudigste tekenprogramma’s zonder enige gegevensbasis). Hier zijn twee zeer opvallende negatieve voorbeelden van de coronavirus situatie: in beide voorbeelden komen de staafhoogtes niet overeen met de waarden op de (niet getoonde) Y-as!\n\n\nEen verzameling van andere veelgebruikte diagramtypen met illustraties en negatieve voorbeelden is te vinden in de dia’s bij deze lezing.\nReferenties\nGorman, Tony D. AND Fraser, Kristen B. AND Williams. 2014. “Ecological Sexual Dimorphism and Environmental Variability Within a Community of Antarctic Penguins (Genus Pygoscelis).” PLOS ONE 9 (3): 1–14. https://doi.org/10.1371/journal.pone.0090081.\nTufte, Edward R. 1983. The Visual Display of Quantitative Information. Graphics Press.\nWickham, Hadley. 2010. “A Layered Grammar of Graphics.” Journal of Computational and Graphical Statistics 19 (1): 3–28. https://doi.org/10.1198/jcgs.2009.07098. Wickham, Hadley, and Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 1st ed. O’Reilly Media, Inc. https://r4ds.had.co.nz/.\nWilke, C. O. 2019. Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures. O’Reilly Media. https://books.google.de/books?id=L3ajtgEACAAJ.\nWilkinson, L., D. Wills, J. Chambers, R. Dubbs, W. Eddy, A. Norton, and W. Haerdie. 1999. The Grammar of Graphics. Statistics and Computing. Springer New York. https://books.google.de/books?id=5boZAQAAIAAJ.\n\n\n\n",
    "preview": "posts/2020-11-01-nogmaals-grafieken/nogmaals-grafieken_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2020-12-06T15:24:27+01:00",
    "input_file": "nogmaals-grafieken.utf8.md"
  },
  {
    "path": "posts/2020-10-17-een-eenvoudige-introductie-op-machine-learning/",
    "title": "Een eenvoudige introductie op `tidymodels`",
    "description": "Edgar Ruiz' eenvoudige introductie op machine learning met de inzet van het pakket `tidymodels`.",
    "author": [
      {
        "name": "Edgar Ruiz, vertaling Harrie Jonkman",
        "url": "https://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/"
      }
    ],
    "date": "2020-10-17",
    "categories": [],
    "contents": "\nIntroductie Harrie\nIk heb me voorgenomen om wat machine learning te leren. Het komt mij allemaal nog wat onbekend voor. Door soms tutorials van anderen over te zetten en te kijken wat er gebeurt, wil ik hierin verder komen. Edgar Ruiz schreef een korte inleiding en zijn tutorial heb ik, brutaal als ik ben, naar het Nederlands overgezet. Edgar Ruiz, ik hoop dat je dit goed vindt. Onderaan vermeld ik waar de lezer jouw oorspronkelijke tutorial kan vinden.\nEen rustige introductie op tidymodels\nThe WorkflowOnlangs had Edgar Ruiz de gelegenheid om tidymodels te laten zien in workshops en gesprekken. Omdat hij zichzelf meer ziet als gebruiker dan ontwikkelaar, zou het wel eens waardevol en interessant kunnen zijn, zo dacht hij, om te delen wat hij tot nu toe had geleerd. Laten we eerst eens bekijken wat tidymodels in onze analyseprojecten kan betekenen, dat was het doel van zijn korte en duidelijke introductie tidymodels.\nHet figuur hierboven is gebaseerd op een figuur uit R voor Data Science boek, het boek van Wickham en Grolemund en wordt heel vaak gebruikt. Alleen wordt hier het onderdeel modeleren met tidymodels uitgevoerd en dat is nieuw. In de introductie laat hij zien welke stappen hier gezet moeten worden. Modeleren kan baat hebben bij een ‘nette’ interface, dat is waar tidymodels een rol speelt.\nHet is belangrijk om te verduidelijken dat de groep van pakketten die deel uitmaken van tidymodels niet zelf statistische modellen implementeren. In plaats daarvan richten ze zich vooral op het makkelijker maken van alle taken die te maken hebben met modeleren. Deze taken omvatten het voorbewerken van gegevens tot en met het valideren van resultaten.\nIn zekere zin kent het modeleren enkele substappen. Voor deze substappen levert tidymodels één of meerdere pakketten. Dit artikel toont functies uit vier tidymodels pakketten, die alle vier in de suite tidymodels zijn opgesloten:\nrsample - Verschillende types van re-samples recipes - Transformaties om data voor te bewerken voor modeleren parnip - Een algemene interface voor modelcreatie yardstick - Meten hoe het model het doet\nHet volgend figuur illustrates elkw modelleerstapt, en laat de tidymodels pakketten zien die we in dit artikel zullen gebruiken:\nThe WorkflowIn een bepaalde analyse kan al dan niet het tidyverse pakket worden gebruikt. Niet alle projecten hoeven te werken met tijdsvariabelen, dus het is niet altijd nodig om functies uit het hms pakket te gebruiken. Hetzelfde idee geldt voor tidymodels. Afhankelijk van wat voor soort modelering er gedaan gaat worden, zullen alleen functies uit sommige van de pakketten gebruikt worden.\nEen voorbeeld\nWe zullen de iris dataset hiervoor gebruiken. De data zijn al binnen gehaald en voldoende opgeschoond om direct te modeleren.\nLaad alleen de tidymodels bibliotheek\nDit is mogelijk het eerste artikel dat hij heeft geschreven waarbij hij slechts één pakket heeft aangeroepen via de bibliotheek(). Naast het laden van de kernpakketten voor het modelleren, laadt tidymodels, ook handig, een aantal tidyverse pakketten, waaronder dplyr en ggplot2. Gedurende deze oefening zullen we enkele functies uit die pakketten gebruiken, maar we hoeven ze dus niet expliciet in onze R-sessie te laden.\n\n\n\nHet voorwerk\nDeze eerstestap richt zich op het geschikt maken van data voor modellering. Daarbij wordt gebruik gemaakt van datatransformaties. Alle transformaties kunnen worden uitgevoerd met dplyr, of andere tidyverse pakketten Overweeg het gebruik van tidymodels pakketten wanneer dit deel zwaarder en complexer is.\nData Sampling\nDe initial_split() functie is vooral gebouwd om de dataset op te splitsen in een trainings en test set. Standaard wordt 3/4 van de data voor training en de rest voor testen gebruikt. Dat kan aangepast worden door het prop functie te gebruiken. Dit genereert een rplit object, geen dataframe. De geprinte output laat het aantal rijen voor testen, trainen en het totaal zien.\n\n<Analysis/Assess/Total>\n<90/60/150>\n\nOm toegang te krijgen tot de observaties van de trainingsset, gebruik je de training() functie. Hetzelfde voor testset waar je toegang toe krijgt via testing().\n\nRows: 90\nColumns: 5\n$ Sepal.Length <dbl> 4.7, 5.4, 5.0, 4.4, 4.9, 4.8, 4.3, 5.8, 5.1, 5…\n$ Sepal.Width  <dbl> 3.2, 3.9, 3.4, 2.9, 3.1, 3.0, 3.0, 4.0, 3.8, 3…\n$ Petal.Length <dbl> 1.3, 1.7, 1.5, 1.4, 1.5, 1.4, 1.1, 1.2, 1.5, 1…\n$ Petal.Width  <dbl> 0.2, 0.4, 0.2, 0.2, 0.1, 0.1, 0.1, 0.2, 0.3, 0…\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa…\n\nDeze samplingfuncties zijn mogelijk met behulp van het rsample pakket, dat deel uitmaakt van tidymodels.\nPre-proces interface\nIn tidymodels biedt het recipes pakket een interface dat gespecialiseerd is in het voorbewerken van gegevens. Binnen het pakket worden de functies die de gegevenstransformaties starten, of uitvoeren, vernoemd naar kookacties. Dat maakt de interface gebruiksvriendelijker. Bijvoorbeeld:\nrecipe() - start een nieuwe set van toe te passen transformaties, vergelijkbaar met het ggplot() commando. Het belangrijkste argument is de formule van het model.\nprep() - Voert de transformaties uit bovenop de geleverde gegevens (meestal de trainingsgegevens).\nElke datatransformatie is een stap. Functies komen overeen met specifieke soorten stappen, die elk een voorvoegsel van step_ hebben. Er zijn verschillende step_ functies; in dit voorbeeld gebruiken we er drie:\nstep_corr() - Verwijdert variabelen die sterk correleren met andere variabelen\nstep_center() - Normaliseert numerieke data die een gemiddelde van nul krijgen\nstep_scale() - Normaliseert numerieke data die een standard deviatie van één krijgen\nEen ander aardig kenmerk van deze step is dat deze kan worden toegepast op een specifieke variabele, groepen variabelen of alle variabelen. De all_outocomes() en all_predictors() functie bieden een hele prettige manier om specifieke groepen variabelen te specificeren. Bijvoorbeeld, als we de step_corr() willen gebruiken om alleen de predictorvariabelen te analyseren, gebruiken we step_corr(all_predictors()). Zo hoeven we niet elke variabele op te sommen.\nIn het volgende voorbeeld, brengen we de recipe(), prep() en step-functies om een recipe object te creëren. De training() functie wordt gebruikt om de dataset uit de eerder aangemaakte gesplitste dataset te halen.\n\n\n\nAls we het iris_recipe object oproepen, zal het details hierover afdrukken. De Operations sectie beschrijft wat er met de gegevens is gedaan. Een van de bewerkingen in het voorbeeld legt uit dat de correlatiestap de Petal.Length variabele heeft verwijderd.\n\nData Recipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          4\n\nTraining data contained 90 data points and no missing data.\n\nOperations:\n\nCorrelation filter removed Petal.Length [trained]\nCentering for Sepal.Length, ... [trained]\nScaling for Sepal.Length, ... [trained]\n\nUitvoeren van het pre-proces\nDe testgegevens kunnen nu worden getransformeerd met behulp van precies dezelfde stappen, gewichten en categorisatie als bij de voorbewerking van de trainingsgegevens. Hiervoor wordt een andere functie met een kookterm gebruikt: bake(). Merk op dat de functie testing() wordt gebruikt om de juiste dataset te extraheren.\nThe testing data can now be transformed using the exact same steps, weights, and categorization used to pre-process the training data. To do this, another function with a cooking term is used: bake(). Notice that the testing() function is used in order to extract the appropriate data set.\n\nRows: 60\nColumns: 4\n$ Sepal.Length <dbl> -0.9090229, -1.1427716, -1.4933947, -1.0258972…\n$ Sepal.Width  <dbl> 1.0457854, -0.1076544, 0.1230336, 1.2764734, 0…\n$ Petal.Width  <dbl> -1.3566929, -1.3566929, -1.3566929, -1.3566929…\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa…\n\nHet uitvoeren van dezelfde operatie over de trainingsgegevens is overbodig, omdat die gegevens al zijn voorgeprogrammeerd. Om de voorbereide trainingsgegevens in een variabele te laden, gebruiken we juice(). Het zal de gegevens uit het iris_recipe object halen.\n\nRows: 90\nColumns: 4\n$ Sepal.Length <dbl> -1.37652034, -0.55839976, -1.02589723, -1.7271…\n$ Sepal.Width  <dbl> 0.3537215, 1.9685372, 0.8150974, -0.3383423, 0…\n$ Petal.Width  <dbl> -1.3566929, -1.0918288, -1.3566929, -1.3566929…\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa…\n\nModel training\nIn R zijn er meerdere pakketten die op hetzelfde type model passen. Het is gebruikelijk dat elk pakket een unieke interface biedt. Met andere woorden, zaken als een argument voor hetzelfde modelattribuut wordt voor elk pakket anders gedefinieerd. Bijvoorbeeld, de ranger en randomForest-pakketten passen bij Random Forest-modellen. In de ranger() functie gebruiken we num.trees om het aantal bomen te definiëren. In randomForest wordt dat argument dan weer ntree genoemd. Het is niet gemakkelijk om te wisselen tussen pakketten om hetzelfde model te draaien.\nIn plaats van het modelleerpakket te vervangen, vervangt tidymodels de interface. Beter gezegd, tidymodels biedt een enkele set functies en argumenten om een model te definiëren. Een specifiek pakket wordt dan aangepast in het model en algemeen gemaakt.\nIn het onderstaande voorbeeld wordt de rand_forest() functie gebruikt om een Random Forest model te initialiseren. Om het aantal bomen te definiëren wordt het treesargument gebruikt. Om de ranger versie van Random Forest te gebruiken, wordt de set_engine() functie gebruikt. Tenslotte wordt de fit() functie gebruikt om het model uit te voeren. De verwachte argumenten zijn de formule en de gegevens. Merk op dat het model boven op de gesausde getrainde gegevens draait.\n\n\n\nAls we nu hetzelfde model niet met ranger maar met randomForest willen draaien, hoeven we alleen maar de waarde in set_engine() te veranderen in randomForest.\n\n\n\nHet is ook het vermelden waard dat het model niet in een enkele, grote functie met veel argumenten is gedefinieerd. De definitie van het model is gescheiden in kleinere functies zoals fit() en set_engine(). zo krijgen we een flexibelere - en gemakkelijker te leren - interface.\nVoorspellingen\nIn plaats van een vector geeft de predictfunctie tibble terug. Standaard wordt de voorspellingsvariabele .pred_class genoemd. Merk op dat in het voorbeeld de ’kook’testgegevens worden gebruikt.\n\n# A tibble: 60 x 1\n   .pred_class\n   <fct>      \n 1 setosa     \n 2 setosa     \n 3 setosa     \n 4 setosa     \n 5 setosa     \n 6 setosa     \n 7 setosa     \n 8 setosa     \n 9 setosa     \n10 setosa     \n# … with 50 more rows\n\nHet is eenvoudig om de voorspellingen toe te voegen aan de ’kook’testgegevens door gebruik te maken van dplyr’s bind_cols() functie.\n\nRows: 60\nColumns: 5\n$ .pred_class  <fct> setosa, setosa, setosa, setosa, setosa, setosa…\n$ Sepal.Length <dbl> -0.9090229, -1.1427716, -1.4933947, -1.0258972…\n$ Sepal.Width  <dbl> 1.0457854, -0.1076544, 0.1230336, 1.2764734, 0…\n$ Petal.Width  <dbl> -1.3566929, -1.3566929, -1.3566929, -1.3566929…\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa…\n\nModel validatie\nGebruik de metrics() functie om de prestaties van het model te meten. Het zal automatisch metrieken kiezen die geschikt zijn voor een bepaald type model. De functie verwacht een tibble dat de werkelijke resultaten bevat (waarheid) en wat het model heeft voorspeld (schatting).\n\n# A tibble: 2 x 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy multiclass     0.983\n2 kap      multiclass     0.975\n\nDoor de consistentie van de nieuwe interface is het meten van de resultaten voor het randomForest-model eenvoudig omdat je alleen maar de modelvariabele aan de bovenkant van de code hoeft te vervangen (nu iris_rf ipv iris_ranger.\n\n# A tibble: 2 x 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy multiclass     0.983\n2 kap      multiclass     0.975\n\nPer classificator metriek\nHet is eenvoudig om de waarschijnlijkheid voor elke mogelijke voorspelde waarde te verkrijgen door het type-argument op prob te zetten. Dat levert een tibble op met zoveel mogelijk variabelen als er mogelijke voorspelde waarden zijn. Hun naam zal standaard op de oorspronkelijke waarde worden gezet, voorafgegaan door .pred_.\n\nRows: 60\nColumns: 7\n$ .pred_setosa     <dbl> 0.99800000, 0.90737302, 0.97193651, 0.9780…\n$ .pred_versicolor <dbl> 0.000000000, 0.062146825, 0.006666667, 0.0…\n$ .pred_virginica  <dbl> 0.002000000, 0.030480159, 0.021396825, 0.0…\n$ Sepal.Length     <dbl> -0.9090229, -1.1427716, -1.4933947, -1.025…\n$ Sepal.Width      <dbl> 1.0457854, -0.1076544, 0.1230336, 1.276473…\n$ Petal.Width      <dbl> -1.3566929, -1.3566929, -1.3566929, -1.356…\n$ Species          <fct> setosa, setosa, setosa, setosa, setosa, se…\n\nOok hier, gebruik bind_cols() om de voorspellingen toe te voegen aan de test dataset die je hiervoor hebt voorbereid.\n\nRows: 148\nColumns: 5\n$ .level          <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"se…\n$ .n              <dbl> 0, 2, 3, 6, 7, 8, 10, 11, 12, 13, 14, 15, 1…\n$ .n_events       <dbl> 0, 2, 3, 6, 7, 8, 10, 11, 12, 13, 14, 15, 1…\n$ .percent_tested <dbl> 0.000000, 3.333333, 5.000000, 10.000000, 11…\n$ .percent_found  <dbl> 0.00000, 9.52381, 14.28571, 28.57143, 33.33…\n\nNu alles in een tibble zit, is het eenvoudig om curve-methoden te berekenen. In dit geval gebruiken we de gain_curve().\n\n\n\nIn de curve-methoden zit ook een autoplot() functie dat makkelijk kan worden omgezet naar een ggplot2 visualizatie.\n\n\n\nDit is een voorbeeld van een roc_curve(). Nogmaals, vanwege de consistentie van de interface, hoeft maar een functienaam te worden omgezet; zelfs de argument waarden blijven hetzelfde.\n\n\n\nOm de gecombineerde enkelvoudige voorspelde waarde en de waarschijnlijkheid van elke mogelijke waarde te meten, combineer je de twee voorspellingsmodi (met en zonder prop type). In dit voorbeeld is met het gebruik van dplyr’s select() de resulterende tibble makkelijker af te lezen.\n\nRows: 60\nColumns: 5\n$ .pred_setosa     <dbl> 0.99800000, 0.90737302, 0.97193651, 0.9780…\n$ .pred_versicolor <dbl> 0.000000000, 0.062146825, 0.006666667, 0.0…\n$ .pred_virginica  <dbl> 0.002000000, 0.030480159, 0.021396825, 0.0…\n$ .pred_class      <fct> setosa, setosa, setosa, setosa, setosa, se…\n$ Species          <fct> setosa, setosa, setosa, setosa, setosa, se…\n\nPipe de resultatentabel in metrics(). In dit geval, specificeer de .pred_class als de schatting.\n\n# A tibble: 4 x 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 accuracy    multiclass     0.983\n2 kap         multiclass     0.975\n3 mn_log_loss multiclass     0.167\n4 roc_auc     hand_till      0.990\n\nSlotopmerkingen van Edgar Ruiz\nDit voorbeeld is bedoeld als een hele voorzichtige kennismaking met tidymodels. Het aantal functies en de mogelijkheden van dergelijke functies zijn voor deze demonstratie tot een minimum beperkt, maar er kan nog veel meer worden gedaan met deze prachtige suite van pakketten. Hopelijk helpt dit artikel jou op weg en moedigt het u misschien zelfs aan om uw kennis verder uit te breiden.\nDank je wel!\nEdgar Ruiz wil graag Max Kuhn en Davis Vaughan, de ontwikkelaars van tidymodels, bedanken. Ze waren hem genadig in het geven van instructie, feedback en begeleiding tijdens zijn reis om tidymodels te leren.\nSlotopmerkingen van Harrie Jonkman\nVoor mij was dit inderdaad een van de eerste kennismakingen met tidymodels. Ik kende het pakket caret en ook het mlr. tidymodels is een modernisering van deze eerste interfaces en moet net zo’n RStudio succes worden als tidyverse. Ondertussen het ik de website van tidymodels gelezen, de interactieve cursus van Julia Silge en de introducties van Alison Hill. Ook het nieuwe boek van Max Kuhn en Julia Silge ben ik op dit moment aan het lezen. Hieronder vind je die literatuur die ik op dit moment wat bij elkaar aan het zoeken ben. Voor mij is er nog een lange weg te gaan maar het artikel van Edgar Ruiz was voor mij een eerste uitstapje. Ik wil hem hartelijk dank voor zijn uitnodiging.\nLiteratuur en verwijzingen\nEdgar Ruiz (A Gentle introduction to tidymodels)[https://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/]\nTidymodels (website)[https://www.tidymodels.org/]\nHefin Rhys (Machine learning with R)[https://education.rstudio.com/blog/2020/02/conf20-intro-ml/]\nJulia Silge (Supervised learning course)[https://juliasilge.com/blog/supervised-ml-course/]\nMax Kuhn and Julia Silge (Tidy modeling with R)[https://www.tmwr.org/]\nAlison Hill (Introduction to machine learning)[https://education.rstudio.com/blog/2020/02/conf20-intro-ml/]\nRebecca Barter (Tidymodels: tidy machine learning in R)[http://www.rebeccabarter.com/blog/2020-03-25_machine_learning/]\n\n\n\n",
    "preview": "posts/2020-10-17-een-eenvoudige-introductie-op-machine-learning/een-eenvoudige-introductie-op-machine-learning_files/figure-html5/unnamed-chunk-16-1.png",
    "last_modified": "2020-11-01T20:31:54+01:00",
    "input_file": "een-eenvoudige-introductie-op-machine-learning.utf8.md"
  },
  {
    "path": "posts/2020-08-21-survival-analyse-met-r/",
    "title": "Survival analyse met R",
    "description": "Dit is een deel van een tutorial die Emily Zabore schreef over survival analyse met R.",
    "author": [
      {
        "name": "Emily Zabore, bewerking Harrie Jonkman",
        "url": "www.harriejonkman.nl"
      }
    ],
    "date": "2020-08-21",
    "categories": [],
    "preview": "posts/2020-08-21-survival-analyse-met-r/img/trial_anatomy.png",
    "last_modified": "2020-08-21T11:07:13+02:00"
  },
  {
    "path": "posts/2020-05-31-grafieken/",
    "title": "Grafieken",
    "description": "Hier een serie grafieken die je kunt maken. Dit is gebaseerd op blog van het Urban Institute in de Verenigde Staten. Zij maken hun grafieken altijd op eenzelfde manier. Hoe ze dat doen kan je hierlezen",
    "author": [
      {
        "name": "Urban Institute, bewerking Harrie Jonkman",
        "url": "www.harriejonkman.nl"
      }
    ],
    "date": "2020-05-31",
    "categories": [],
    "contents": "\nR is een krachtige, open-source programmeertaal en -omgeving. R blinkt uit in databeheer en bewerking, traditionele statistische analyse, machine learning en reproduceerbaar onderzoek. Maar is waarschijnlijk nog het meest bekend om zijn grafieken. In deze blog staan voorbeelden en instructies voor populaire en minder bekende plottechnieken in R. Het bevat ook instructies voor het gebruik van urbnthemes, het R-pakket van het Urban Institute voor het maken van bijna-publicatie-klare plots met ggplot2. Als u vragen heeft, zo staat op hun site, aarzel dan niet om contact op te nemen met Aaron Williams of Kyle Ueyama.\nAchtergrond\nHet R-pakket (library(urbnthemes)) maakt ggplot2 output dat verbonden is met Urban Institute’s Data Visualisatie stijl gids. Tzijn pakket produceert ** geen publicatieklare grafieken**. Visuele stijlen moeten nog steeds worden bewerkt met behulp van de normale bewerkingsworkflow van het project. Door grafieken te exporteren als pdf kunnen ze gemakkelijker worden bewerkt. Zie de sectie Plots opslaan voor meer informatie.\nHet vaste thema dat hier gebruikt wordt is getest met ggplot2 versie 3.0.0. Het zal niet goed functioneren met oudere versies van ggplot2.\nGebruik library(urbnthemes)\nJe moet in ieder geval de volgende code gebruiken om urbnthemes te installeren of te updaten:\n# install.packages(\"devtools\")\n# devtools::install_github(\"UrbanInstitute/urbnthemes\")\nVoer de volgende code bovenaan elk script uit. Als je dit hebt gedaan, kun je aan de slag:\n\n\nlibrary(tidyverse)\nlibrary(urbnthemes)\nlibrary(ggrepel)\nlibrary(extrafont)\n\nset_urbn_defaults(style = \"print\")\n\n\n\nIAls het nog niet is geïnstalleerd, installeer dan het gratis Lato-lettertype van Google-lettertypen. Als je op een Mac werkt sla je Lato op in je font-book. Als je op Windows werkt, moet je eerst Ghostscript installeren. Vertel dan in R waar uw ghostscript-bestand zich bevindt. Bewerk het bestandspad als het uwe zich op een andere plaats bevindt.\nSys.setenv(R_GSCMD=\"C:/Program Files/gs/gs9.05/bin/gswin32c.exe\")\nVoer dit script één keer uit om Lato te importeren en te registreren:\n# install.packages(c(\"ggplot2\", \"ggrepel\", \"extrafont\"))\n# urbnthemes::lato_install()\nHet laden en importeren van dit lettertype kan enkele minuten duren.\nGrammar of Graphics en de conventies\nHadley Wickham’s ggplot2 is gebaseerd op Leland Wilkinsons The Grammar of Graphics en Wickhams A Layered Grammar of Graphics. De gelaagde Grammer of Graphics is een gestructureerde manier van denken over de componenten van een plot, die zich vervolgens lenen voor de eenvoudige structuur van ggplot2.\nData zijn wat in een plot wordt gevisualiseerd en mappings zijn aanwijzingen voor hoe gegevens in een plot in kaart worden gebracht op een manier die door de mens kan worden waargenomen.\nGegevens zijn weergaven van de werkelijke gegevens zoals punten, lijnen en balken.\nStatistieken zijn statistische transformaties die samenvattingen van de gegevens weergeven, zoals histogrammen.\nScales kaartwaarden in de dataruimte naar waarden in de esthetische ruimte. Schalen tekenen legendes en assen.\nCoördinatensystemen beschrijven hoe geomen in het vlak van de grafiek in kaart worden gebracht.\nFacetten splitsen de gegevens op in betekenisvolle deelverzamelingen zoals kleine veelvouden. *Thema’s** controleren de fijnere punten van een plot zoals lettertypes, lettergroottes en achtergrondkleuren.\nMeer informatie vind je hier: ggplot2: Elegant Graphics for Data Analysis\nTips en trucs\nggplot2 verwacht dat de gegevens in dataframes of tibbles zitten. Het heeft de voorkeur dat de dataframes “netjes” zijn met elke variabele als een kolom, elke obseravtion als een rij, en elke observatie-eenheid als een aparte tabel. De dplyr en tidyr bevatten beknopte en effectieve hulpmiddelen voor het “opruimen” van gegevens.\nR staat toe dat functie-argumenten expliciet bij naam en impliciet bij positie worden aangeroepen. De codeervoorbeelden in deze handleiding bevatten alleen benoemde argumenten voor de duidelijkheid.\nGrafieken zullen soms verschillend worden weergeven op verschillende besturingssystemen. Dit zal geen probleem zijn als de afbeeldingen eenmaal zijn opgeslagen.\nDoorlopende x-assen hebben tikken. Discrete x-assen hebben geen teken. Gebruik remove_ticks() om teken te verwijderen.\nStaaf grafieken\nEen kleur\n\n\nmtcars %>%\n  count(cyl) %>%\n  ggplot(mapping = aes(x = factor(cyl), y = n)) +\n  geom_col() +\n  geom_text(mapping = aes(label = n), vjust = -1) +    \n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.1))) +\n  labs(x = \"Cilinders\",\n       y = NULL) +\n  remove_ticks() +\n  remove_axis() \n\n\n\n\nEen kleur (Geroteerd)\nDit introduceert coord_flip() en remove_axis(axis = \"x\", flip = TRUE). remove_axis() komt van library(urbnthemes) en creëert een aangepast thema voor geroteerde staafgrafieken.\n\n\nmtcars %>%\n  count(cyl) %>%\n  ggplot(mapping = aes(x = factor(cyl), y = n)) +\n  geom_col() +\n  geom_text(mapping = aes(label = n), hjust = -1) +  \n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.1))) +\n  labs(x = \"Cilinders\",\n       y = NULL) +  \n  coord_flip() +\n  remove_axis(axis = \"x\", flip = TRUE)\n\n\n\n\nDrie kleuren\nDit is identiek aan de vorige grafiek, behalve dat kleuren en een legenda zijn toegevoegd met fill = cyl. Door x om te zetten in een factor met factor(cyl) worden 5 en 7 op de x-as overgeslagen. Het toevoegen van fill = cyl zonder factor() zou een doorlopend kleurenschema en een legenda hebben gecreëerd.\n\n\nmtcars %>%\n  mutate(cyl = factor(cyl)) %>%\n  count(cyl) %>%\n  ggplot(mapping = aes(x = cyl, y = n, fill = cyl)) +\n  geom_col() +\n  geom_text(mapping = aes(label = n), vjust = -1) +    \n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.1))) +\n  labs(x = \"Cylinders\",\n       y = NULL) +\n  remove_ticks() +\n  remove_axis()\n\n\n\n\nGestapelde staafgrafiek\nEen extra esthetiek kan eenvoudig worden toegevoegd aan de staafgrafiek door fill = categorical variable toe te voegen aan de mapping. Hier toont elk onderdeel een subset van een aantal auto’s met verschillende aantallen cilinders.\n\n\nmtcars %>%\n  mutate(am = factor(am, labels = c(\"Automatic\", \"Manual\")),\n         cyl = factor(cyl)) %>%  \n  group_by(am) %>%\n  count(cyl) %>%\n  group_by(cyl) %>%\n  arrange(desc(am)) %>%\n  mutate(label_height = cumsum(n)) %>%\n  ggplot() +\n  geom_col(mapping = aes(x = cyl, y = n, fill = am)) +\n  geom_text(aes(x = cyl, y = label_height - 0.5, label = n, color = am)) +\n  scale_color_manual(values = c(\"white\", \"black\")) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.1))) +\n  labs(x = \"Cylinders\",\n       y = NULL) +  \n  remove_ticks() +\n  remove_axis() +\n  guides(color = FALSE)\n\n\n\n\nGestapelde staafgrafiek met Position = Fill\nDe vorige voorbeelden gebruiken geom_col(), die een y-waarde voor de staafhoogte neemt. Dit voorbeeld gebruikt geom_bar() die de waarden opsomt en een waarde voor de staafhoogte genereert. In dit voorbeeld verandert position = \"fill\" in geom_bar() de y-as van de telling naar de verhouding van elke staaf.\n\n\nmtcars %>%\n  mutate(am = factor(am, labels = c(\"Automatic\", \"Manual\")),\n         cyl = factor(cyl)) %>%  \n  ggplot() +\n  geom_bar(mapping = aes(x = cyl, fill = am), position = \"fill\") +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.1)), labels = scales::percent) +\n  labs(x = \"Cylinders\",\n       y = NULL) +  \n  remove_ticks() +\n  guides(color = FALSE)\n\n\n\n\nOpgedeelde staafgrafiek\nDelen van de staafgrafiek in ggplot2 worden standaard opgestapeld. position = \"dodge\" in geom_col() breidt het staafdiagram uit zodat de subsets naast elkaar verschijnen.\n\n\nmtcars %>%\n  mutate(am = factor(am, labels = c(\"Automatic\", \"Manual\")),\n         cyl = factor(cyl)) %>%\n  group_by(am) %>%\n  count(cyl) %>%\n  ggplot(mapping = aes(cyl, y = n, fill = factor(am))) +\n  geom_col(position = \"dodge\") +\n  geom_text(aes(label = n), position = position_dodge(width = 0.7), vjust = -1) +    \n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.1))) +\n  labs(x = \"Cylinders\",\n       y = NULL) +  \n  remove_ticks() +\n  remove_axis()\n\n\n\n\nLolly grafiek/Cleveland puntgrafiek\nLolly en Cleveland puntgrafiek zijn minimalistische alternatieven voor staafgrafieken. De sleutel tot beide grafieken is om de gegevens te ordenen op basis van de continue variabele met behulp van arrange() en dan de discrete variabele om te zetten in een factor met de geordende niveaus van de continue variabele met behulp van mutate(). Deze stap “slaat” de volgorde van de gegevens op.\nLollygrafiek\n\n\nmtcars %>%\n  rownames_to_column(\"model\") %>%\n  arrange(mpg) %>%\n  mutate(model = factor(model, levels = .$model)) %>%\n  ggplot(aes(mpg, model)) +\n    geom_segment(aes(x = 0, xend = mpg, y = model, yend = model)) +  \n    geom_point() +\n    scale_x_continuous(expand = expand_scale(mult = c(0, 0)), limits = c(0, 40)) +\n    labs(x = NULL, \n         y = \"Miles Per Gallon\")\n\n\n\n\nCleveland puntgrafiek\n\n\nmtcars %>%\n  rownames_to_column(\"model\") %>%\n  arrange(mpg) %>%\n  mutate(model = factor(model, levels = .$model)) %>%\n  ggplot(aes(mpg, model)) +\n    geom_point() +\n    scale_x_continuous(expand = expand_scale(mult = c(0, 0)), limits = c(0, 40)) +\n    labs(x = NULL, \n         y = \"Miles Per Gallon\")\n\n\n\n\nDumbellgrafieken\nPuntengrafieken\nEen kleur puntengrafiek\nPuntengrafieken zijn nuttig voor het tonen van relaties tussen twee of meer variabelen. Gebruik scatter_grid() van library(urbnthemes) om eenvoudig verticale rasterlijnen toe te voegen aan de puntengrafieken.\n\n\nmtcars %>%\n  ggplot(mapping = aes(x = wt, y = mpg)) +\n  geom_point() +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(0, 6),\n                     breaks = 0:6) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \n                     limits = c(0, 40),\n                     breaks = 0:8 * 5) +\n  labs(x = \"Gewicht (duizenden ponden)\",\n       y = \"MPG\") +\n  scatter_grid()\n\n\n\n\nPuntengrafiek met hoge dichtheid met transparantie\nGrote aantallen waarnemingen maken soms strooiplekken soms moeilijk om te interpreteren omdat punten elkaar overlappen. Het toevoegen van alpha = met een getal tussen 0 en 1 voegt transparantie toe aan punten en helderheid aan grafieken.\n\n\ndiamonds %>%\n  ggplot(mapping = aes(x = carat, y = price)) +\n  geom_point(alpha = 0.05) +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(0, 6),\n                     breaks = 0:6) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \n                     limits = c(0, 20000),\n                     breaks = 0:4 * 5000,\n                     labels = scales::dollar) +\n  labs(x = \"Carat\",\n       y = \"Price\") +\n  scatter_grid()\n\n\n\n\nHexus puntengrafiek\nSoms is transparantie niet genoeg om duidelijkheid te brengen in een verstrooide grafiek met veel waarnemingen. Als n toeneemt in de honderdduizenden en zelfs miljoenen, kan geom_hex een van de beste manieren zijn om relaties tussen twee variabelen weer te geven.\n\n\ndiamonds %>%\n  ggplot(mapping = aes(x = carat, y = price)) +\n  geom_hex(mapping = aes(fill = ..count..)) +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(0, 6),\n                     breaks = 0:6) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \n                     limits = c(0, 20000),\n                     breaks = 0:4 * 5000,\n                     labels = scales::dollar) +\n  scale_fill_gradientn(labels = scales::comma) +  \n  labs(x = \"Carat\",\n       y = \"Price\") +\n  scatter_grid() +\n  theme(legend.position = \"right\",\n        legend.direction = \"vertical\")\n\n\n\n\nPuntengrafiek met random vervuiling\nSoms hebben puntengrafieken veel overlappende punten, maar een redelijk aantal waarnemingen. Geom_jitter voegt een kleine hoeveelheid willekeurige ruis toe zodat punten minder snel overlappen. De breedte en hoogte bepalen de hoeveelheid ruis die wordt toegevoegd. Merk in het volgende voor- en naschrijven op hoeveel punten er nog zichtbaar zijn na het toevoegen van jitter.\nVoor\n\n\nmpg %>%\n  ggplot(mapping = aes(x = displ, y = cty)) +\n  geom_point() +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(0, 8),\n                     breaks = 0:8) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \n                     limits = c(0, 40),\n                     breaks = 0:4 * 10) +\n  labs(x = \"Uitval\",\n       y = \"MPG\") +\n  scatter_grid()\n\n\n\n\nNa\n\n\nset.seed(2017)\nmpg %>%\n  ggplot(mapping = aes(x = displ, y = cty)) +\n  geom_jitter() +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(0, 8),\n                     breaks = 0:8) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \n                     limits = c(0, 40),\n                     breaks = 0:4 * 10) +\n  labs(x = \"Uitval\",\n       y = \"MPG\") +\n  scatter_grid()\n\n\n\n\nPuntengrafiek met gevarieerde puntenomvang\nGewichten en populaties kunnen in puntengrafieken met de grootte van de punten in kaart worden gebracht. Hier wordt het aantal huishoudens in elke staat in kaart gebracht op de grootte van elk punt met behulp van aes(size = hhpop). Opmerking: ggplot2::geom_point() wordt gebruikt in plaats van geom_point().\nWel eerst dit pakket laden (wel installeren als je dat nog niet hebt gedaan).\n\n\nlibrary(urbnmapr)\n\n\n\n\n\nurbnmapr::statedata %>%\n  ggplot(mapping = aes(x = medhhincome, y = horate)) +\n  ggplot2::geom_point(mapping = aes(size = hhpop), alpha = 0.3) +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(30000, 80000),\n                     breaks = 3:8 * 10000,\n                     labels = scales::dollar) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \n                     limits = c(0, 0.8),\n                     breaks = 0:4 * 0.2) +\n  scale_radius(range = c(3, 15),\n               breaks = c(2500000, 7500000, 12500000), \n               labels = scales::comma) +\n  labs(x = \"Huishoud inkomen\",\n       y = \"Ratio huizenbezit\") +\n  scatter_grid() +\n  theme(plot.margin = margin(r = 20))\n\n\n\n\nPuntengrafieken met vulling\nEen derde esthetiek kan worden toegevoegd aan puntengrafieken. Hier betekent kleur het aantal cilinders in elke auto. Voordat ggplot() wordt aangeroepen, worden de cilinders aangemaakt met behulp van library(dplyr) en de functie %>%.\n\n\nmtcars %>%\n  mutate(cyl = paste(cyl, \"cylinders\")) %>%\n  ggplot(aes(x = wt, y = mpg, color = cyl)) +\n  geom_point() +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(0, 6),\n                     breaks = 0:6) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \n                     limits = c(0, 40),\n                     breaks = 0:8 * 5) +\n  labs(x = \"Gewicht (duizenden ponden)\",\n       y = \"MPG\") +\n  scatter_grid()\n\n\n\n\nLijngrafieken\n\n\neconomics %>%\n  ggplot(mapping = aes(x = date, y = unemploy)) +\n  geom_line() +\n  scale_x_date(expand = expand_scale(mult = c(0.002, 0)), \n               breaks = \"10 years\",\n               limits = c(as.Date(\"1961-01-01\"), as.Date(\"2020-01-01\")),\n               date_labels = \"%Y\") +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \n                     breaks = 0:4 * 4000,\n                     limits = c(0, 16000),\n                     labels = scales::comma) +\n  labs(x = \"Jaar\", \n       y = \"Aantal werklozen (1,000den)\")\n\n\n\n\nLijngrafieken met meerdere lijnen\n\n\nlibrary(gapminder)\n\ngapminder %>%\n  filter(country %in% c(\"Australia\", \"Netherlands\", \"New Zealand\")) %>%\n  mutate(country = factor(country, levels = c(\"Netherlands\", \"Australia\", \"New Zealand\"))) %>%\n  ggplot(aes(year, gdpPercap, color = country)) +\n  geom_line() +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     breaks = c(1952 + 0:12 * 5), \n                     limits = c(1952, 2007)) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \n                     breaks = 0:8 * 5000,\n                     labels = scales::dollar, \n                     limits = c(0, 40000)) +\n  labs(x = \"Jaap\",\n       y = \"BNP per hoofd van de bevolking (US dollars)\")\n\n\n\n\nHet plotten van meer dan één variabele kan nuttig zijn om de relatie van variabelen in de tijd te zien, maar het vergt een kleine hoeveelheid databewerking.\nDit komt omdat ggplot2 gegevens in een “lang” formaat wil hebben in plaats van een “breed” formaat voor lijnplots met meerdere lijnen. gather() en spread() uit het tidyr pakket maakt het wisselen tussen “lang” en “breed” pijnloos. In wezen gaan variabele titels naar “key” en variabele waarden naar “value”. Dan verandert ggplot2, de verschillende niveaus van de sleutelvariabele (bevolking, werkloosheid) in kleuren.\n\n\nas_tibble(EuStockMarkets) %>%\n  mutate(date = time(EuStockMarkets)) %>%\n  gather(key = \"key\", value = \"value\", -date) %>%\n  ggplot(mapping = aes(x = date, y = value, color = key)) +\n  geom_line() +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(1991, 1999), \n                     breaks = c(1991, 1993, 1995, 1997, 1999)) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \n                     breaks = 0:4 * 2500,\n                     labels = scales::dollar, \n                     limits = c(0, 10000)) +  \n  labs(x = \"Tijd\",\n       y = \"Waarde\")\n\n\n\n\nTrapgrafiek\ngeom_line() verbindt coördinaten met de kortst mogelijke rechte lijn. Soms zijn trapgrafieken nodig omdat de y-waarden niet veranderen tussen de coördinaten. Zo wordt bijvoorbeeld de bovengrens van de Federal Funds Rate met regelmatige tussenpozen ingesteld en blijft deze constant totdat deze wordt gewijzigd.\n\n\n# downloaded from FRED on 2018-12-06\n\n# https://fred.stlouisfed.org/series/DFEDTARU\n\nfed_fund_rate <- read_csv(\n  \"date, fed_funds_rate\n   2014-01-01,0.0025\n   2015-12-16,0.0050\n   2016-12-14,0.0075\n   2017-03-16,0.0100\n   2017-06-15,0.0125\n   2017-12-14,0.0150\n   2018-03-22,0.0175\n   2018-06-14,0.0200\n   2018-09-27,0.0225\n   2018-12-06,0.0225\")\n\nfed_fund_rate %>%\n  ggplot(mapping = aes(x = date, y = fed_funds_rate)) + \n  geom_step() +\n  scale_x_date(expand = expand_scale(mult = c(0.002, 0)), \n               breaks = \"1 year\",\n               limits = c(as.Date(\"2014-01-01\"), as.Date(\"2019-01-01\")),\n               date_labels = \"%Y\") +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \n                     breaks = c(0, 0.01, 0.02, 0.03),\n                     limits = c(0, 0.03),\n                     labels = scales::percent) +  \n  labs(x = \"Tijd\",\n       y = \"Bovengrens van de Federal Funds Rate\")\n\n\n\n\nPadgrafiek\nDe Beveridge-curve is een macro-economisch plot dat een verband toont tussen de werkloosheidsgraad en de vacaturegraad. Bewegingen op de curve duiden op veranderingen in de bedrijfscultuur en horizontale verschuivingen van de curve duiden op structurele veranderingen op de arbeidsmarkt.\nLijnen in de Beveridge-curve bewegen niet monotoon van links naar rechts. Daarom is het noodzakelijk om geom_pad() te gebruiken.\n\n\nlibrary(ggrepel)\n\nbeveridge <- read_csv(\n  [1336 chars quoted with '\"'])\n\nlabels <- beveridge %>%\n  filter(lubridate::month(quarter) == 1)\n\nbeveridge %>%\n  ggplot() +\n  geom_path(mapping = aes(x = unempoyment_rate, y = vacanacy_rate), alpha = 0.5) +\n  geom_point(data = labels, mapping = aes(x = unempoyment_rate, y = vacanacy_rate)) +\n  geom_text_repel(data = labels, mapping = aes(x = unempoyment_rate, y = vacanacy_rate, label = lubridate::year(quarter))) +  \n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(0.04, 0.1),\n                     labels = scales::percent) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \n                     breaks = c(0, 0.01, 0.02, 0.03, 0.04, 0.05),\n                     limits = c(0, 0.05),\n                     labels = scales::percent) +  \n  labs(x = \"Seizoen gecontroleerd werkloosheidspercentage\",\n       y = \"Seizoen gecontroleerd beschikbare banenpercentage\") +  \n  scatter_grid()\n\n\n\n\nHellingsgrafiek\n\n\n# https://www.bls.gov/lau/\nlibrary(ggrepel)\n\nunemployment <- tibble(\n  time = c(\"October 2009\", \"October 2009\", \"October 2009\", \"August 2017\", \"August 2017\", \"August 2017\"),\n  rate = c(7.4, 7.1, 10.0, 3.9, 3.8, 6.4),\n  state = c(\"Maryland\", \"Virginia\", \"Washington, D.C.\", \"Maryland\", \"Virginia\", \"Washington, D.C.\")\n)\n\nlabel <- tibble(label = c(\"October 2009\", \"August 2017\"))\noctober <- filter(unemployment, time == \"October 2009\")\naugust <- filter(unemployment, time == \"August 2017\")\n\nunemployment %>%\n  mutate(time = factor(time, levels = c(\"October 2009\", \"August 2017\")),\n         state = factor(state, levels = c(\"Washington, D.C.\", \"Maryland\", \"Virginia\"))) %>%\n  ggplot() + \n  geom_line(aes(time, rate, group = state, color = state), show.legend = FALSE) +\n  geom_point(aes(x = time, y = rate, color = state)) +\n  labs(subtitle = \"Werkloosheidspercentage\") +\n  theme(axis.ticks.x = element_blank(),\n        axis.title.x = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.y = element_blank(), \n        axis.text.y = element_blank(),\n        panel.grid.major.y = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        panel.grid.major.x = element_blank(),\n        axis.line = element_blank()) +\n  geom_text_repel(data = october, mapping = aes(x = time, y = rate, label = as.character(rate)), nudge_x = -0.06) + \n  geom_text_repel(data = august, mapping = aes(x = time, y = rate, label = as.character(rate)), nudge_x = 0.06)\n\n\n\n\nUnivariaat\nEr zijn een aantal manieren om de verdeling van univariate data in R te onderzoeken. Sommige methoden, zoals strookdiagrammen, laten alle datapunten zien. Andere methoden, zoals de box- en whiskerplot, tonen geselecteerde datapunten die belangrijke waarden communiceren zoals de mediaan en het 25e percentiel. Tenslotte tonen sommige methoden geen van de onderliggende data, maar berekenen ze dichtheidsschattingen. Elke methode heeft voor- en nadelen, dus het is de moeite waard om de verschillende vormen te begrijpen. Lees voor meer informatie 40 jaar boxplottem van Hadley Wickham en Lisa Stryjewski.\nStripdiagram\nStripdiagrammen, de eenvoudigste univariate plot, tonen de verdeling van de waarden langs één as. Stripdiagrammen werken het beste met variabelen die veel variatie hebben. Zo niet, dan hebben de punten de neiging om op elkaar te clusteren. Zelfs als de variabele veel variatie heeft, is het vaak belangrijk om transparantie toe te voegen aan de punten met alpha = zodat overlappende waarden zichtbaar zijn.\n\n\nmsleep %>%\n  ggplot(aes(x = sleep_total, y = factor(1))) +\n  geom_point(alpha = 0.2, size = 5) +\n  labs(y = NULL) +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(0, 25), \n                     breaks = 0:5 * 5) +\n  scale_y_discrete(labels = NULL) +\n  labs(title = \"Totale slaaptijd van verschillende zoogdieren\",\n       x = \"Totale slaaptijd (uren)\",\n       y = NULL) +\n  theme(axis.ticks.y = element_blank())\n\n\n\n\nStripdiagram met Highlighting\nOmdat strookdiagrammen alle waarden weergeven, zijn ze nuttig om aan te geven waar de geselecteerde punten in de verdeling van een variabele liggen. De duidelijkste manier om dit te doen is door geom_point() tweemaal toe te voegen met filter() in het gegevensargument. Op deze manier worden de geaccentueerde waarden boven op de niet geaccentueerde waarden getoond.\n\n\nggplot() +\n  geom_point(data = filter(msleep, name != \"Red fox\"), \n                    aes(x = sleep_total, \n                        y = factor(1)),\n             alpha = 0.2, \n             size = 5,\n             color = \"grey50\") +\n  geom_point(data = filter(msleep, name == \"Red fox\"),\n             aes(x = sleep_total, \n                 y = factor(1), \n                 color = name),\n             alpha = 0.8,\n             size = 5) +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(0, 25), \n                     breaks = 0:5 * 5) +  \n  scale_y_discrete(labels = NULL) +\n  labs(title = \"Totale slaaptijd van verschillende zoogdieren\",\n       x = \"Totale slaaptijd (uren)\",\n       y = NULL,\n       legend) +\n  guides(color = guide_legend(title = NULL)) +\n  theme(axis.ticks.y = element_blank())\n\n\n\n\nOndergeschikte Strip Chart\nVoeg een y-variabele toe om de verdelingen van de continue variabele in deelverzamelingen van een categorische variabele te zien.\n\n\nlibrary(forcats)\n\nmsleep %>%\n  filter(!is.na(vore)) %>%\n  mutate(vore = fct_recode(vore, \n                            \"Insectivore\" = \"insecti\",\n                            \"Omnivore\" = \"omni\", \n                            \"Herbivore\" = \"herbi\", \n                            \"Carnivore\" = \"carni\"\n                            )) %>%\n  ggplot(aes(x = sleep_total, y = vore)) +\n  geom_point(alpha = 0.2, size = 5) +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(0, 25), \n                     breaks = 0:5 * 5) +  \n  labs(title = \"Totale slaaptijd van verschillende zoogdieren volgens dieet\",\n       x = \"Totale slaaptijd (uren)\",\n       y = NULL) +\n  theme(axis.ticks.y = element_blank())\n\n\n\n\nHistogrammen\nHistogrammen verdelen de verdeling van een variabele in n staven van gelijke grootte en tellen en tonen vervolgens het aantal waarnemingen in elke staaf. Histogrammen zijn gevoelig voor de breedte van de staven. Zoals ?geom_histogram merkt op, “U moet altijd de waarde van [de standaard staafbreedte] overschrijven, door meerdere breedtes te onderzoeken om het beste te vinden om de verhalen in uw gegevens te illustreren”.\n\n\nggplot(data = diamonds, mapping = aes(x = depth)) + \n  geom_histogram(bins = 100) +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(0, 100)) +  \n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.2)), labels = scales::comma) +\n  labs(x = \"Depth\",\n       y = \"Count\")\n\n\n\n\nBoxplots\nBoxplots zijn in de jaren zeventig uitgevonden door John Tukey. In plaats van de onderliggende gegevens te tonen of het aantal blikken van de onderliggende gegevens, richten ze zich op belangrijke waarden zoals het 25e percentiel, de mediaan en het 75e percentiel.\n\n\nInsectSprays %>%\n  ggplot(mapping =  aes(x = spray, y = count)) +\n  geom_boxplot() +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.2))) +\n  labs(x = \"Type of insect spray\",\n       y = \"Number of dead insects\") +\n  remove_ticks()\n\n\n\n\nGladde kernel verdelingsgrafieken\nDoorlopende variabelen met vloeiende verdelingen worden soms beter weergegeven met afgevlakte kerneldichtheidsschattingen dan histogrammen of boxplots. geom_density() berekent en plot een kerneldichtheidsschatting. Let op de klontjes rond gehele en halve getallen in de volgende verdeling vanwege afrondingen.\n\n\ndiamonds %>%\n  ggplot(mapping = aes(carat)) +\n  geom_density(color = NA) +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(0, NA)) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.2))) +\n  labs(x = \"Carat\",\n       y = \"Density\")\n\n\n\n\n\n\ndiamonds %>%\n  mutate(cost = ifelse(price > 5500, \"More than $5,500 +\", \"$0 to $5,500\")) %>%\n  ggplot(mapping = aes(carat, fill = cost)) +\n  geom_density(alpha = 0.25, color = NA) +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(0, NA)) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.1))) +\n  labs(x = \"Carat\",\n       y = \"Density\")\n\n\n\n\n‘Ridgeline’ grafieken\nRidgeline plots zijn gedeeltelijk overlappende afgevlakte kerneldichtheid plots gefacetteerd door een categorische variabele die veel informatie verpakken in één elegante plot. Onderstaande maakt duidelijk wat we hiermee bedoelen.\n\n\nlibrary(ggridges)\n\nggplot(diamonds, mapping = aes(x = price, y = cut)) +\n  geom_density_ridges(fill = \"#1696d2\") +\n  labs(x = \"Price\",\n       y = \"Cut\")\n\n\n\n\nViool grafieken\nVioolplots zijn symmetrische weergaven van gladde kerneldichtheidplots.\n\n\nInsectSprays %>%\n  ggplot(mapping = aes(x = spray, y = count, fill = spray)) +\n  geom_violin(color = NA) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.2))) +\n  labs(x = \"Type of insect spray\",\n       y = \"Number of dead insects\") +\n  remove_ticks()\n\n\n\n\nBonenplot\nIndividuele uitschieters en belangrijke samenvattende waarden zijn niet zichtbaar in vioolplots of afgevlakte kerneldichtheidsplots. Bonenplots, gemaakt door Peter Kampstra in 2008, zijn vioolplots met gegevens weergegeven als kleine lijnen in een eendimensionale stripplot en grotere lijnen voor het gemiddelde.\n\n\nmsleep %>%\n  filter(!is.na(vore)) %>%\n  mutate(vore = fct_recode(vore, \n                            \"Insectivore\" = \"insecti\",\n                            \"Omnivore\" = \"omni\", \n                            \"Herbivore\" = \"herbi\", \n                            \"Carnivore\" = \"carni\"\n                            )) %>%\n  ggplot(aes(x = vore, y = sleep_total, fill = vore)) +\n  stat_summary(fun.y = \"mean\",\n               colour = \"black\", \n               size = 30,\n               shape = 95,\n               geom = \"point\") +\n  geom_violin(color = NA) +\n  geom_jitter(width = 0,\n              height = 0.05,\n              alpha = 0.4,\n              shape = \"-\",\n              size = 10,\n              color = \"grey50\") +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.2))) +  \n    labs(x = NULL,\n         y = \"Total sleep time (hours)\") +\n  theme(legend.position = \"none\") +\n  remove_ticks()\n\n\n\n\nGebiedsplot\nGestapeld gebied\n\n\ntxhousing %>%\n  filter(city %in% c(\"Austin\",\"Houston\",\"Dallas\",\"San Antonio\",\"Fort Worth\")) %>%\n  group_by(city, year) %>%\n  summarize(sales = sum(sales)) %>%\n  ggplot(aes(x = year, y = sales, fill = city)) +\n  geom_area(position = \"stack\") +\n  scale_x_continuous(expand = expand_scale(mult = c(0, 0)),\n                     limits = c(2000, 2015),\n                     breaks = 2000 + 0:15) +  \n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.2)), \n                     labels = scales::comma) +\n  labs(x = \"Year\",\n       y = \"Home sales\")\n\n\n\n\nGevuld gebied\n\n\ntxhousing %>%\n  filter(city %in% c(\"Austin\",\"Houston\",\"Dallas\",\"San Antonio\",\"Fort Worth\")) %>%\n  group_by(city, year) %>%\n  summarize(sales = sum(sales)) %>%\n  ggplot(aes(x = year, y = sales, fill = city)) +\n  geom_area(position = \"fill\") +\n  scale_x_continuous(expand = expand_scale(mult = c(0, 0)),\n                     limits = c(2000, 2015),\n                     breaks = 2000 + 0:15) +  \n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.02)),\n                     breaks = c(0, 0.25, 0.5, 0.75, 1),\n                     labels = scales::percent) +\n  labs(x = \"Year\",\n       y = \"Home sales\")\n\n\n\n\nWafelkaart / Vierkante taartkaart\nHet wafelpakket {CRAN en Github} maakt vierkante taartkaarten. Het kan ook gecombineerd worden met glyphs voor meer elegantere vormen dan vierkanten. Data hiervoor komen hier vandaan: A Vision for an Equitable DC.\nWafelkaarten vereisen een beetje extra knutselen omdat ze worden genoemd vanuit library(waffle) in plaats van library(ggplot2). Het belangrijkste is dat voor wafeldiagrammen theme_urban(text = element_text(family = \"Lato\")) nodig is voor het lettertype Lato.\nEnkele wafelkaart\n\n\nlibrary(waffle)\n\nparts <- c(`Virginia\\nClinics` = (1000 - 208 - 105), `Maryland\\nClinics` = 208, `D.C.\\nClinics` = 105)\nwaffle(parts, rows = 25, size = 1, colors = c(\"#1696d2\", \"#fdbf11\", \"#000000\"), legend_pos = \"bottom\") +\n  labs(title = \"Free Clinics in the D.C.-Maryland-Virginia Area\",\n       subtitle = \"1 Square == 1 Clinic\") +\n  theme(text = element_text(family = \"Lato\"))\n\n\n\n\nMeervoudige wafelkaarten\nlibrary(waffle) allows multiple waffle charts to be ironed together using iron(). maakt het mogelijk om meerdere wafelkaarten in elkaar te strijken met behulp van iron(). Het samen strijken van meerdere wafeldiagrammen vereist wat trial-and-error om de maten en resolutie goed te krijgen, maar de resultaten kunnen de moeite waard zijn. Vergeet niet theme(text = element_text(family = \"Lato\"))!\n\n\nlibrary(waffle)\n\nwhite <- c(`With Degree` = 169300, `Without Degree` = 800)\nblack <- c(`With Degree` = 174900, `Without Degree` = 34700)\nhispanic <- c(`With Degree` = 27700, `Without Degree` = 12400)\n\niron(\n  waffle(white / 83, rows = 40, size = 0.25, colors = c(\"#1696d2\", \"#fdbf11\"), title = \"White\", keep = FALSE, pad = 10) + \n  theme(text = element_text(family = \"Lato\")),\n  waffle(black / 83, rows = 40, size = 0.25, colors = c(\"#1696d2\", \"#fdbf11\"), title = \"Black\", keep = FALSE) + \n  theme(text = element_text(family = \"Lato\")),\n  waffle(hispanic / 83, rows = 40, size = 0.25, colors = c(\"#1696d2\", \"#fdbf11\"), title = \"Hispanic\", keep = FALSE, pad = 59, xlab = \"1 Square == 83 People\") + \n  theme(text = element_text(family = \"Lato\"))\n) \n\n\n\n\nWarmtekaart\n\n\nlibrary(fivethirtyeight)\n\nbad_drivers %>%\n  filter(state %in% c(\"Maine\", \"New Hampshire\", \"Vermont\", \"Massachusetts\", \"Connecticut\", \"New York\")) %>%\n  mutate(`Number of\\nDrivers` = scale(num_drivers),\n         `Percent\\nSpeeding` = scale(perc_speeding),\n         `Percent\\nAlcohol` = scale(perc_alcohol),\n         `Percent Not\\nDistracted` = scale(perc_not_distracted),\n         `Percent No\\nPrevious` = scale(perc_no_previous),\n         state = factor(state, levels = rev(state))\n         ) %>%\n  select(-insurance_premiums, -losses, -(num_drivers:losses)) %>%\n  gather(`Number of\\nDrivers`:`Percent No\\nPrevious`, key = \"variable\", value = \"SD's from Mean\") %>%\n  ggplot(aes(variable, state)) +\n    geom_tile(aes(fill = `SD's from Mean`)) +\n    labs(x = NULL,\n         y = NULL) + \n    scale_fill_gradientn() +\n    theme(legend.position = \"right\",\n          legend.direction = \"vertical\",\n          axis.line.x = element_blank(),\n          panel.grid.major.y = element_blank()) +\n  remove_ticks()\n\n\n\n#https://learnr.wordpress.com/2010/01/26/ggplot2-quick-heatmap-plotting/\n\n\n\nFaceteren en kleine kaartjes\nfacet_wrap()\nR’s faceteersysteem is een krachtige manier om kleinere kaarten te maken.\nSommige bewerkingen aan het thema kunnen nodig zijn, afhankelijk van het aantal rijen en kolommen in de plot.\n\n\ndiamonds %>%\n  ggplot(mapping = aes(x = carat, y = price)) +\n  geom_point(alpha = 0.05) +\n  facet_wrap(~cut, ncol = 5) +\n  scale_x_continuous(expand = expand_scale(mult = c(0, 0)),\n                     limits = c(0, 6)) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0)),\n                     limits = c(0, 20000), \n                     labels = scales::dollar) +\n  labs(x = \"Karaat\",\n       y = \"Prijs\") +\n  scatter_grid()\n\n\n\n\nfacet_grid()\n\n\ndiamonds %>%\n  filter(color %in% c(\"D\", \"E\", \"F\", \"G\")) %>%\n  ggplot(mapping = aes(x = carat, y = price)) +\n  geom_point(alpha = 0.05) +\n  facet_grid(color ~ cut) +\n  scale_x_continuous(expand = expand_scale(mult = c(0, 0)),\n                     limits = c(0, 4)) +  \n  scale_y_continuous(expand = expand_scale(mult = c(0, 0)),\n                     limits = c(0, 20000), \n                     labels = scales::dollar) +\n  labs(x = \"Carat\",\n       y = \"Price\") +\n  theme(panel.spacing = unit(20L, \"pt\")) +\n  scatter_grid()\n\n\n\n\n‘Smoothers’\ngeom_smooth() past en modelleert op gegevens met twee of meer dimensies.\nHet begrijpen en manipuleren van defaults is belangrijker voor geom_smooth() dan andere ‘geoms’ omdat het een aantal aannames bevat. geom_smooth() gebruikt automatisch loess voor datasets met minder dan 1.000 waarnemingen en een algemeen model met formula = y ~ s(x, bs = \"cs\") voor datasets met meer dan 1.000 waarnemingen. Beide zijn standaard ingesteld op een betrouwbaarheidsinterval van 95%.\nModellen worden gekozen met method = en kunnen worden ingesteld op lm(), glm(), gam(), loess(), rlm(), en meer. Formules kunnen worden opgegeven met formule = en y ~ x syntaxis. Het plotten van de standaardfout wordt omgeschakeld met se = TRUE en se = FALSE, en het niveau wordt gespecificeerd met level =. Zoals altijd is er meer informatie te zien in RStudio met ?geom_smooth().\ngeom_point() voegt een scatterplot toe aan geom_smooth(). De volgorde van de functie-aanroepen is belangrijk. De functie die als tweede wordt aangeroepen wordt bovenop de functie die als eerste wordt aangeroepen gelegd.\n\n\ndiamonds %>%\n  ggplot(mapping = aes(x = carat, y = price)) +\n  geom_point(alpha = 0.05) +\n  geom_smooth(color =  \"#ec008b\") +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(0, 5),\n                     breaks = 0:5) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)),\n                     limits = c(0, 20000), \n                     labels = scales::dollar) +  \n  labs(x = \"Carat\",\n       y = \"Price\") +\n  scatter_grid()\n\n\n\n\ngeom_smooth kan worden onderverdeeld in categorische en factorvariabelen. Dit vereist subgroepen met een behoorlijk aantal waarnemingen en een behoorlijke mate van variabiliteit over de x-as. De betrouwbaarheidsintervallen worden vaak groter aan de uiteinden, zodat speciale zorg nodig is om de grafiek zinvol en leesbaar te maken.\nDit voorbeeld gebruikt Loess met MPG = verplaatsing.\n\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = factor(cyl))) +\n  geom_point(alpha = 0.2) +\n  geom_smooth() +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(0, 7),\n                     breaks = 0:7) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)),\n                     limits = c(0, 60)) +  \n  labs(x = \"Engine displacement\",\n       y = \"Highway MPG\") +\n  scatter_grid()\n\n\n\n\nDit voorbeeld gebruikt liniaire regressie met MPG = displacement.\n\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = factor(cyl))) +\n  geom_point(alpha = 0.2) +\n  geom_smooth(method = \"lm\") +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(0, 7),\n                     breaks = 0:7) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)),\n                     limits = c(0, 60)) +  \n  labs(x = \"Engine displacement\",\n       y = \"Highway MPG\") +\n  scatter_grid()\n\n\n\n\nBenadrukken\nlibrary(gghighlight) maakt intuitief benadrukken van ggplot2 plots mogelijk. gghighlight wijzigt bestaande ggplot2-objecten, dus geen enkele andere code mag veranderen. Alle markering wordt afgehandeld door de functie gghighlight(), die alle soorten geomen kan verwerken.\nWaarschuwing:  R zal een fout maken als er te veel kleuren worden gemarkeerd vanwege het ontwerp van urbnthemes. Verlaag gewoon het aantal gemarkeerde geomen om dit probleem op te lossen.\nEr zijn twee belangrijke manieren om de aandacht te vestigen.\nDrempelwaarde\nDe eerste manier om te markeren is met een drempel. Voeg een logische test toe aan gghighlight() om te beschrijven welke lijnen gemarkeerd moeten worden. Hier worden regels met een maximale verandering in het bruto binnenlands product per hoofd van de bevolking van meer dan $35.000 gemarkeerd met gghighlight(max(pcgpd_change) > 35000, use_direct_label = FALSE).\n\n\nlibrary(gghighlight)\nlibrary(gapminder)\n\ndata <- gapminder %>%\n  filter(continent %in% c(\"Europe\")) %>%\n  group_by(country) %>%\n  mutate(pcgpd_change = ifelse(year == 1952, 0, gdpPercap - lag(gdpPercap))) %>%\n  mutate(pcgpd_change = cumsum(pcgpd_change))\n  \ndata %>%\n  ggplot(aes(year, pcgpd_change, group = country, color = country)) +\n  geom_line() +\n  gghighlight(max(pcgpd_change) > 35000, use_direct_label = FALSE) +  \n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)),\n                     breaks = c(seq(1950, 2010, 10)),\n                     limits = c(1950, 2010)) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)),\n                     breaks = 0:8 * 5000,\n                     labels = scales::dollar,\n                     limits = c(0, 40000)) +\n  labs(x = \"Year\",\n       y = \"Change in per-capita GDP (US dollars)\")\n\n\n\n\nRang\nDe tweede manier om te markeren is door middel van een rangorde. Hier worden de landen met de eerste hoogste waarden voor verandering in het bruto binnenlands product per hoofd van de bevolking benadrukt met gghighlight(max(pcgpd_change), max_highlight = 5, use_direct_label = FALSE).\n\n\ndata %>%\n  ggplot(aes(year, pcgpd_change, group = country, color = country)) +\n  geom_line() +\n  gghighlight(max(pcgpd_change), max_highlight = 5, use_direct_label = FALSE) +  \n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)),\n                     breaks = c(seq(1950, 2010, 10)),\n                     limits = c(1950, 2010)) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)),\n                     breaks = 0:8 * 5000,\n                     labels = scales::dollar,\n                     limits = c(0, 40000)) +\n  labs(x = \"Year\",\n       y = \"Change in per-capita GDP (US dollars)\")\n\n\n\n\nFaceteren\ngghighlight() werkt goed met ggplot2’s facetsysteem.\n\n\ndata %>%\n  ggplot(aes(year, pcgpd_change, group = country)) +\n  geom_line() +\n  gghighlight(max(pcgpd_change), max_highlight = 4, use_direct_label = FALSE) +  \n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)),\n                     breaks = c(seq(1950, 2010, 10)),\n                     limits = c(1950, 2010)) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)),\n                     breaks = 0:8 * 5000,\n                     labels = scales::dollar,\n                     limits = c(0, 40000)) +\n  labs(x = \"Year\",\n       y = \"Change in per-capita GDP (US dollars)\") +\n  facet_wrap(~ country) +\n  theme(panel.spacing = unit(20L, \"pt\"))\n\n\n\n\nTekst en annotatie\nVerschillende functies kunnen worden gebruikt om verschillende delen van percelen te annoteren, te labelen en te markeren. geom_text() en geom_text_repel() geven beide variabelen uit dataframes weer. annotate(), die verschillende toepassingen heeft, geeft variabelen en waarden weer die zijn opgenomen in de functie-aanroep.\ngeom_text()\ngeom_text() zet tekstvariabelen in datasets om in geometrische objecten. Dit is nuttig voor het labelen van gegevens in plots. Beide functies hebben x waarden en y waarden nodig om de plaatsing op het coördinatenvlak te bepalen en een tekstvector van labels.\nDit kan gebruikt worden voor het labelen van geom_bar().\n\n\ndiamonds %>%\n  group_by(cut) %>%\n  summarize(price = mean(price)) %>%\n  ggplot(aes(cut, price)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = scales::dollar(price)), vjust = -1) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.2)),\n                     labels = scales::dollar) +\n  labs(title = \"Average Diamond Price by Diamond Cut\",\n       x = \"Cut\",\n       y = \"Price\") +\n  remove_ticks()\n\n\n\n\nHet kan ook gebruikt worden om punten in een scatterplot te labelen.\nHet is zelden nuttig om elk punt in een scatter plot te labelen. Gebruik filter() om een tweede dataset te maken die wordt onderverdeeld en deze door te geven aan de labelfunctie.\n\n\nlabels <- mtcars %>%\n  rownames_to_column(\"model\") %>%\n  filter(model %in% c(\"Toyota Corolla\", \"Merc 240D\", \"Datsun 710\"))\n\nmtcars %>%\n  ggplot() +\n  geom_point(mapping = aes(x = wt, y = mpg)) +\n  geom_text(data = labels, mapping = aes(x = wt, y = mpg, label = model), nudge_x = 0.38) +\n  scale_x_continuous(expand = expand_scale(mult = c(0, 0.002)),\n                     limits = c(0, 6)) + \n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)),\n                     limits = c(0, 40)) +  \n  labs(x = \"Weight (Tons)\",\n       y = \"Miles per gallon (MPG)\") +\n  scatter_grid()\n\n\n\n\nTekst overlapt te vaak met andere tekst of geomen bij gebruik van geom_text(). library(ggrepel) is een library(ggplot2) add-on die automatisch tekst positioneert zodat deze niet overlapt met geomen of andere tekst. Om deze functionaliteit toe te voegen, installeer en laad je library(ggrepel) en gebruikt je vervolgens geom_text_repel() met dezelfde syntaxis als geom_text().\ngeom_text_repel()\n\n\nlibrary(ggrepel)\n\nlabels <- mtcars %>%\n  rownames_to_column(\"model\") %>%\n  top_n(5, mpg)\n\nmtcars %>%\n  ggplot(mapping = aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_text_repel(data = labels, \n                  mapping = aes(label = model), \n                  nudge_x = 0.38) +\n  scale_x_continuous(expand = expand_scale(mult = c(0, 0.002)),\n                     limits = c(0, 6)) + \n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)),\n                     limits = c(0, 40)) +  \n  labs(x = \"Weight (Tons)\",\n       y = \"Miles per gallon (MPG)\") +\n  scatter_grid()\n\n\n\n\nannotate()\nannotate() gebruikt geen dataframes. In plaats daarvan zijn er waarden nodig voor x = en y =. Het kan tekst, rechthoeken, segmenten en puntenreeksen toevoegen.\n\n\nmsleep %>%\n  filter(bodywt <= 1000) %>%\n  ggplot(aes(bodywt, sleep_total)) +\n  geom_point() +\n  scale_x_continuous(expand = expand_scale(mult = c(0, 0.002)),\n                     limits = c(-10, 1000),\n                     labels = scales::comma) + \n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)),\n                     limits = c(0, 25)) +  \n  annotate(\"text\", x = 500, y = 12, label = \"These data suggest that heavy \\n animals sleep less than light animals\") +\n  labs(x = \"Body weight (pounds)\",\n       y = \"Sleep time (hours)\") +\n  scatter_grid()  \n\n\n\n\n\n\nlibrary(AmesHousing)\n\names <- make_ames()\n\names %>%\n  mutate(square_footage = Total_Bsmt_SF - Bsmt_Unf_SF + First_Flr_SF + Second_Flr_SF) %>%\n  mutate(Sale_Price = Sale_Price / 1000) %>%  \n  ggplot(aes(square_footage, Sale_Price)) +\n  geom_point(alpha = 0.2) +\n  scale_x_continuous(expand = expand_scale(mult = c(0, 0.002)),\n                     limits = c(-10, 12000),\n                     labels = scales::comma) + \n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)),\n                     limits = c(0, 800),\n                     labels = scales::dollar) +  \n  annotate(\"rect\", xmin = 6800, xmax = 11500, ymin = 145, ymax = 210, alpha = 0.1) +\n  annotate(\"text\", x = 8750, y = 230, label = \"Unfinished homes\") +\n  labs(x = \"Square footage\", \n       y = \"Sale price (thousands)\") +\n  scatter_grid()   \n\n\n\n\nGelaagde geoms\nGeomen kunnen worden gelaagd in ggplot2. Dit is nuttig voor het ontwerp en de analyse.\nHet is vaak nuttig om punten toe te voegen aan lijngrafieken met een klein aantal waarden over de x-as. Dit voorbeeld uit R voor Data Science laat zien hoe het veranderen van de lijn naar grijs aantrekkelijk kan zijn.\nDesign\nVoor\n\n\ntable1 %>%\n  ggplot(aes(x = year, y = cases)) +\n    geom_line(aes(color = country)) +\n    geom_point(aes(color = country)) +\n    scale_y_continuous(expand = expand_scale(mult = c(0, 0.2)), \n                       labels = scales::comma) +\n    scale_x_continuous(breaks = c(1999, 2000)) +\n    labs(title = \"Changes in Tuberculosis Cases in Three Countries\")\n\n\n\n\nNa\n\n\ntable1 %>%\n  ggplot(aes(year, cases)) +\n    geom_line(aes(group = country), color = \"grey50\") +\n    geom_point(aes(color = country)) +\n    scale_y_continuous(expand = expand_scale(mult = c(0, 0.2)), \n                       labels = scales::comma) +\n    scale_x_continuous(breaks = c(1999, 2000)) +\n    labs(title = \"Changes in Tuberculosis Cases in Three Countries\")\n\n\n\n\nGelaagde geomen zijn ook nuttig voor het toevoegen van trendlijnen en centroïden aan scatterplots.\n\n\n# Simpele lijn\n# Regressie model\n# Centroiden\n\n\n\nCentroiden\n\n\nmpg_summary <- mpg %>%\n  group_by(cyl) %>%\n  summarize(displ = mean(displ), cty = mean(cty))\n\nmpg %>%\n  ggplot() +\n  geom_point(aes(x = displ, y = cty, color = factor(cyl)), alpha = 0.5) +\n  geom_point(data = mpg_summary, aes(x = displ, y = cty), size = 5, color = \"#ec008b\") +\n  geom_text(data = mpg_summary, aes(x = displ, y = cty, label = cyl)) +\n  scale_x_continuous(expand = expand_scale(mult = c(0, 0.002)), \n                     limits = c(0, 8)) +  \n  scale_y_continuous(expand = expand_scale(mult = c(0, 0)), \n                     limits = c(0, 40)) +\n  labs(x = \"Displacement\",\n       y = \"City MPG\") +\n  scatter_grid()\n\n\n\n\nGrafieken opslaan\nggsave() exporteert ggplot2 percelen. De functie kan op twee manieren worden gebruikt. Als plot = niet is gespecificeerd in de functie-aanroep, dan slaat ggsave() automatisch de plot op die het laatst werd weergegeven in het Viewer-venster. Ten tweede, als plot = is gespecificeerd, dan slaat ggsave() het gespecificeerde plot op. ggsave() raadt het type grafische soort dat gebruikt moet worden bij het exporteren (.png, .pdf, .svg, etc.) van de bestandsextensie in de bestandsnaam.\nmtcars %>%\n  ggplot(aes(x = wt, y = mpg)) +\n  geom_point()\n\nggsave(filename = \"cars.png\")\n\nplot2 <- mtcars %>%\n  ggplot(aes(x = wt, y = mpg)) +\n  geom_point()\n\nggsave(filename = \"cars.png\", plot = plot2)\nGeëxporteerde plots zien er zelden identiek uit als de plots die in het Viewervenster in RStudio verschijnen omdat de totale grootte en de beeldverhouding van de Viewer vaak anders is dan de standaardinstellingen voor ggsave(). Specifieke afmetingen, beeldverhoudingen en resoluties kunnen worden gecontroleerd met argumenten in ggsave(). RStudio heeft een nuttig cheatsheet genaamd “How Big is Your Graph?” dat zou moeten helpen bij het kiezen van de beste grootte, beeldverhouding en resolutie.\nLettertypen zijn niet standaard in PDF’s opgenomen. Om lettertypes in te sluiten in PDF’s, neem device = cairo_pdf op in ggsave().\nplot <- mtcars %>%\n  ggplot(aes(x = wt, y = mpg)) +\n  geom_point()\n\nggsave(filename = \"cars.pdf\", plot = plot2, width = 6.5, height = 4, device = cairo_pdf)\nBibliography and Session Information\nNote: Examples present in this document by Aaron Williams were created during personal time.\nBob Rudis and Dave Gandy (2017). waffle: Create Waffle Chart Visualizations in R. R package version 0.7.0. https://CRAN.R-project.org/package=waffle\nChester Ismay and Jennifer Chunn (2017). fivethirtyeight: Data and Code Behind the Stories and Interactives at ‘FiveThirtyEight’. R package version 0.3.0. https://CRAN.R-project.org/package=fivethirtyeight\nHadley Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2009.\nHadley Wickham (2017). tidyverse: Easily Install and Load the ‘Tidyverse’. R package version 1.2.1. https://CRAN.R-project.org/package=tidyverse\nHadley Wickham (2017). forcats: Tools for Working with Categorical Variables (Factors). R package version 0.2.0. https://CRAN.R-project.org/package=forcats\nJennifer Bryan (2017). gapminder: Data from Gapminder. R package version 0.3.0. https://CRAN.R-project.org/package=gapminder\nKamil Slowikowski (2017). ggrepel: Repulsive Text and Label Geoms for ‘ggplot2’. R package version 0.7.0. https://CRAN.R-project.org/package=ggrepel\nMax Kuhn (2017). AmesHousing: The Ames Iowa Housing Data. R package version 0.0.3. https://CRAN.R-project.org/package=AmesHousing\nPeter Kampstra (2008). Beanplot: A Boxplot Alternative for Visual Comparison of Distributions, Journal of Statistical Software, 2008. https://www.jstatsoft.org/article/view/v028c01\nR Core Team (2017). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.\nWinston Chang, (2014). extrafont: Tools for using fonts. R package version 0.17. https://CRAN.R-project.org/package=extrafont\nYihui Xie (2018). knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version 1.19.\n\n\nsessionInfo()\n\n\nR version 4.0.3 (2020-10-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS High Sierra 10.13.6\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] AmesHousing_0.0.4     gghighlight_0.3.0     fivethirtyeight_0.6.1\n [4] waffle_0.7.0          ggridges_0.5.2        gapminder_0.3.0      \n [7] urbnmapr_0.0.0.9002   extrafont_0.17        ggrepel_0.8.2        \n[10] urbnthemes_0.0.1      forcats_0.5.0         stringr_1.4.0        \n[13] dplyr_1.0.2           purrr_0.3.4           readr_1.4.0          \n[16] tidyr_1.1.2           tibble_3.0.4          ggplot2_3.3.2        \n[19] tidyverse_1.3.0       knitr_1.30           \n\nloaded via a namespace (and not attached):\n [1] httr_1.4.2         jsonlite_1.7.1     splines_4.0.3     \n [4] modelr_0.1.8       assertthat_0.2.1   blob_1.2.1        \n [7] cellranger_1.1.0   yaml_2.2.1         Rttf2pt1_1.3.8    \n[10] pillar_1.4.6       backports_1.1.10   lattice_0.20-41   \n[13] glue_1.4.2         extrafontdb_1.0    digest_0.6.27     \n[16] RColorBrewer_1.1-2 rvest_0.3.6        colorspace_1.4-1  \n[19] Matrix_1.2-18      htmltools_0.5.0    plyr_1.8.6        \n[22] pkgconfig_2.0.3    broom_0.7.2        haven_2.3.1       \n[25] scales_1.1.1       distill_1.0        downlit_0.2.0     \n[28] mgcv_1.8-33        generics_0.0.2     farver_2.0.3      \n[31] ellipsis_0.3.1     withr_2.3.0        hexbin_1.28.1     \n[34] cli_2.1.0          magrittr_1.5       crayon_1.3.4      \n[37] readxl_1.3.1       evaluate_0.14      fs_1.5.0          \n[40] fansi_0.4.1        nlme_3.1-149       xml2_1.3.2        \n[43] tools_4.0.3        hms_0.5.3          lifecycle_0.2.0   \n[46] munsell_0.5.0      reprex_0.3.0       compiler_4.0.3    \n[49] rlang_0.4.8        grid_4.0.3         rstudioapi_0.11   \n[52] labeling_0.4.2     rmarkdown_2.5      gtable_0.3.0      \n[55] DBI_1.1.0          R6_2.4.1           gridExtra_2.3     \n[58] lubridate_1.7.9    stringi_1.5.3      Rcpp_1.0.5        \n[61] vctrs_0.3.4        dbplyr_1.4.4       tidyselect_1.1.0  \n[64] xfun_0.18         \n\n\n\n\n",
    "preview": "posts/2020-05-31-grafieken/grafieken_files/figure-html5/staafgrafiek-1.png",
    "last_modified": "2020-11-02T20:56:54+01:00",
    "input_file": "grafieken.utf8.md"
  },
  {
    "path": "posts/2020-05-31-dslabs/",
    "title": "dslabs",
    "description": "Een aantal mooie grafieken uit het goede data-analyseboek van Rafa Irizarri (Harvard University)",
    "author": [
      {
        "name": "Rafa Irizarri en Amy Hill, bewerking Harrie Jonkman",
        "url": "www.harriejonkman.nl"
      }
    ],
    "date": "2020-04-22",
    "categories": [],
    "preview": "posts/2020-05-31-dslabs/dslabs_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2020-08-21T10:38:38+02:00"
  },
  {
    "path": "posts/2019-12-15-beste-boeken-2019/",
    "title": "Beste Boeken 2019",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "www.harriejonkman.nl"
      }
    ],
    "date": "2019-12-15",
    "categories": [],
    "preview": {},
    "last_modified": "2020-03-13T23:28:58+01:00"
  },
  {
    "path": "posts/2019-12-15-r-als-een-gis/",
    "title": "R als een Gis",
    "description": "Over ruimtelijke data en het gebruik van R als een GIS",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "www.harriejonkman.nl"
      }
    ],
    "date": "2019-12-15",
    "categories": [],
    "preview": "posts/2019-12-15-r-als-een-gis/r-als-een-gis_files/figure-html5/unnamed-chunk-19-1.png",
    "last_modified": "2020-03-13T23:28:59+01:00"
  },
  {
    "path": "posts/2019-12-04-bayesbasis/",
    "title": "Bayes'basis",
    "description": "Over statistiek en waarschijnlijkheid op de eenvoudige manier.",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "www.harriejonkman.nl"
      }
    ],
    "date": "2019-12-04",
    "categories": [],
    "preview": "posts/2019-12-04-bayesbasis/bayesbasis_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2020-03-13T23:28:58+01:00"
  },
  {
    "path": "posts/2019-09-25-sf-ggplot-en-tmap/",
    "title": "Sf, ggplot en tmap",
    "description": "Over het maken van kaarten van Nederland met nieuwe pakketten en mogelijkheden.",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "www.harriejonkman.nl"
      }
    ],
    "date": "2019-09-25",
    "categories": [],
    "preview": "posts/2019-09-25-sf-ggplot-en-tmap/sf-ggplot-en-tmap_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2020-03-13T23:28:58+01:00"
  },
  {
    "path": "posts/2019-08-31-geocomputation/",
    "title": "Geocomputation",
    "description": "Bespreking van het fantastische boek Geocomputation with R",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "www.harriejonkman.nl"
      }
    ],
    "date": "2019-08-31",
    "categories": [],
    "preview": "posts/2019-08-31-geocomputation/geocomputation_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2020-03-13T23:28:58+01:00"
  },
  {
    "path": "posts/2019-08-17-xaringan/",
    "title": "Xaringan",
    "description": "Een mooie presentatie geven met het pakket Xaringan",
    "author": [
      {
        "name": "Yuhui Xie, overgezet Harrie Jonkman",
        "url": "www.harriejonkman.nl"
      }
    ],
    "date": "2019-08-17",
    "categories": [],
    "preview": {},
    "last_modified": "2020-03-13T23:28:58+01:00"
  },
  {
    "path": "posts/2019-06-24-bookdown/",
    "title": "Bookdown",
    "description": "Hoe maak je een boek. Bookdown is het pakket van R waar dat mee kan. Hier enkele tips om dat te doen",
    "author": [
      {
        "name": "Jack Dougherty en Ilya Ilyankou, bewerking Harrie Jonkman",
        "url": "www.harriejonkman.nl"
      }
    ],
    "date": "2019-06-24",
    "categories": [],
    "preview": {},
    "last_modified": "2020-03-13T23:28:58+01:00"
  },
  {
    "path": "posts/2019-05-19-website-maken/",
    "title": "Websites maken in R",
    "description": "Met R kun je ook website maken. Maar hoe doe je dat? Emily Zabor schreef hierover een leerzaam blog dat ik hier licht heb bewerkt en aangevuld.",
    "author": [
      {
        "name": "Emily C. Zabor, bewerking Harrie Jonkman",
        "url": "www.harriejonkman.nl"
      }
    ],
    "date": "2019-06-09",
    "categories": [],
    "preview": {},
    "last_modified": "2020-03-13T23:28:58+01:00"
  },
  {
    "path": "posts/2019-04-23-shiny_files/",
    "title": "Shiny lespakket",
    "description": "Met Shiny kun je in R apps maken. Maar hoe doe je dat? Julia Wrobel gaf hierop vorig jaar een interessante inleiding die ik hier licht heb bewerkt.",
    "author": [
      {
        "name": "Julia Wrobel, bewerking Harrie Jonkman",
        "url": "www.harriejonkman.nl"
      }
    ],
    "date": "2019-04-23",
    "categories": [],
    "preview": "posts/2019-04-23-shiny_files/distill-preview.png",
    "last_modified": "2020-03-13T23:28:58+01:00"
  },
  {
    "path": "posts/2019-03-18-interactieve-grafiek/",
    "title": "Interactieve grafiek met plotly",
    "description": "In deze twee maanden wilde ik toch eens kijken naar interactieve mogelijkheden die het programma R/RStudio ons biedt. `Plotly` is zo'n mogelijkheid en daarover gaat dit blog. `Shiny` is de andere mogelijkheid en daar zal ik een volgende keer aandacht aan besteden. `Plotly` heeft een eigen website waar veel informatie over het programma is te vinden [hier adres website](https://plot.ly/). Er is ook een uitgebreide handleiding over `Plotly` geschreven [hier handleiding](https://plotly-r.com/the-plotly-cookbook.html). Onlangs stond er op de blog van RBloggers een goede introductie van Laura Ellis, die mij veel vertelde over het gebruik van `Plotly`. Haar bijdrage [zie hier](https://www.r-bloggers.com/create-interactive-ggplot2-graphs-with-plotly/) heb ik hier naar het Nederlands overgezet en hier en daar iets bewerkt.",
    "author": [
      {
        "name": "Laura Ellis op R-bloggers (13 maart 2019), bewerking Harrie Jonkman.",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2019-04-03",
    "categories": [],
    "preview": {},
    "last_modified": "2020-03-13T23:28:58+01:00"
  },
  {
    "path": "posts/2019-02-20-bbc-en-data-journalisme/",
    "title": "BBC en data-journalisme",
    "description": "Een blog over hoe de BBC omgaat met visualisatie en data-journalisme",
    "author": [
      {
        "name": "R-bloggers, bewerking Harrie Jonkman.",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2019-02-20",
    "categories": [],
    "preview": "posts/2019-02-20-bbc-en-data-journalisme/bbc-en-data-journalisme_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2020-03-13T23:28:58+01:00"
  },
  {
    "path": "posts/2019-01-23-data-visualization-a-practical-introduction/",
    "title": "Data visualisatie. Een practische introductie",
    "description": "Naar aanleiding van het nieuwe boek van Kieran Healey. Data visualization/A Practical Introduction.",
    "author": [
      {
        "name": "Harrie Jonkman.",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2019-01-23",
    "categories": [],
    "preview": "posts/2019-01-23-data-visualization-a-practical-introduction/data-visualization-a-practical-introduction_files/figure-html5/01-first_plot-1.png",
    "last_modified": "2020-03-13T23:28:58+01:00"
  },
  {
    "path": "posts/2018-12-23-statistisch-omdenken/",
    "title": "Statistisch omdenken",
    "description": "Over 'Statistical rethinking' van Richard McElreath (2016).",
    "author": [
      {
        "name": "Harrie Jonkman met dan aan Solomon Kurz.",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2018-12-23",
    "categories": [],
    "preview": "posts/2018-12-23-statistisch-omdenken/statistisch-omdenken_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2020-03-13T23:28:57+01:00"
  },
  {
    "path": "posts/2018-11-26-communiceren-met-rmarkdown/",
    "title": "Communiceren met RMarkdown",
    "description": "RMarkdown is de nieuwe manier om diverse wetenschappelijke producten te delen met anderen. Het kan op verschillende manieren gereproduceerd worden en het kan de opbrengsten aantrekkelijk communiceren naar de buitenwereld. Hier een introductie op de werkwijze en enkele mogelijke producten.",
    "author": [
      {
        "name": "M. Schmidt en bewerkt door Harrie Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2018-11-26",
    "categories": [],
    "preview": "posts/2018-11-26-communiceren-met-rmarkdown/Figures/single-fig-center-1.png",
    "last_modified": "2020-03-13T23:28:57+01:00"
  },
  {
    "path": "posts/2018-11-14-simple-features/",
    "title": "Bewerking geografische data in R: Nieuwe ontwikkelingen",
    "description": "Het nieuwe R sf-pakket, dat sp vervangt om met geografische objecten om te gaan, is  ontworpen om makkelijk met Tidyverse om te gaan. Hier laat ik zien hoe sf-objecten als data-frames worden opgeslagen en jou in staat stelt om met  ggplot2, dplyr en tidyr te werken. Ook het R-pakket tmap biedt veel nieuwe mogelijkheden.",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2018-11-14",
    "categories": [],
    "preview": "posts/2018-11-14-simple-features/simple-features_files/figure-html5/ggplot-1.png",
    "last_modified": "2020-03-13T23:28:57+01:00"
  },
  {
    "path": "posts/2018-11-14-visualisatiegapminder/",
    "title": "Visualisatie met inzet van Gapminder",
    "description": "Een voorbeeld van datavisualisatie: Trends op het gebied van de wereldgezondheid\nen de economie",
    "author": [
      {
        "name": "Rafael A. Irizarry, bewerkt H. Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2018-08-14",
    "categories": [],
    "preview": "posts/2018-11-14-visualisatiegapminder/visualisatiegapminder_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2020-03-13T23:28:57+01:00"
  },
  {
    "path": "posts/2018-11-14-sem/",
    "title": "SEM",
    "description": "In de sociale wetenschappen kunnen sommige constructen, zoals intelligentie, vertrouwen, motivatie, vervreemding of conservatisme, niet direct worden geobserveerd. Het zijn in essentie constructen of concepten waarvoor geen methode bestaat om ze direct te meten. Onderzoekers gebruiken hiervoor geobserveerde maten die indicatoren zijn voor een latente variabele. Structural equation modeling is een onderzoeks-raamwerk dat rekening kan houden met de meetfouten in de geobserveerde variabelen die in het model zitten. SEM is een flexibel en krachtige methode om tegelijkertijd op een goede manier de kwaliteit van het meten in de gaten te houden als om causale relaties tussen de constructen vast te stellen. In de map vind je een korte presentatie over SEM",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2018-07-07",
    "categories": [],
    "preview": {},
    "last_modified": "2020-03-13T23:28:57+01:00"
  },
  {
    "path": "posts/2018-11-14-rrstudiormarkdown/",
    "title": "RRStudioRmarkdown",
    "description": "Hier een klein boekje om jou te laten wennen aan reproduceerbaar onderzoek. Het introduceert het programma R, de RStudio-schil en de programmeertaal RMarkdown.",
    "author": [
      {
        "name": "Chester Ismay, bewerkt Harrie Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2018-06-05",
    "categories": [],
    "preview": {},
    "last_modified": "2020-03-13T23:28:57+01:00"
  },
  {
    "path": "posts/2018-11-14-bayes/",
    "title": "Bayes",
    "description": "Over de geschiedenis van de Bayesiaanse statistiek",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2018-05-14",
    "categories": [],
    "preview": {},
    "last_modified": "2020-03-13T23:28:57+01:00"
  },
  {
    "path": "posts/2018-11-14-reproducable-research/",
    "title": "Dynamische documenten maken met RMarkdown en Knitr",
    "description": "RMarkdown en Knitr zijn pakketten die je in staat stellen om reproduceerbare en dynamische documenten te maken. In deze blog wordt uitgelegd hoe je hiermee kunt werken.",
    "author": [
      {
        "name": "Marian L. Schmidt, bewerkt door Harrie Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2018-03-15",
    "categories": [],
    "preview": "posts/2018-11-14-reproducable-research/reproducable-research_files/figure-html5/single-fig-center-1.png",
    "last_modified": "2020-03-13T23:28:57+01:00"
  },
  {
    "path": "posts/2018-11-14-exploratie/",
    "title": "Data exploratie",
    "description": "Een introductie op data exploratie aan de hand van een boek van Chester Ismay en Albert Y. Kim.",
    "author": [
      {
        "name": "Chester Ismay en Albert Y. Kim, bewerkt door Harrie Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2017-10-15",
    "categories": [],
    "preview": {},
    "last_modified": "2020-03-13T23:28:57+01:00"
  },
  {
    "path": "posts/2018-11-14-psm/",
    "title": "PSM",
    "description": "Om precies het effect van een aanpak of politieke keuze vast te stellen is een ingewikkelde kwestie. Toch is er dat soort onderzoek nodig om de keuze voor programma's te legitimeren. Tegenwoordig is er een heel spectrum van technieken om de impact van programma's vast te stellen. Dit zijn technieken die kunnen worden gebruikt binnen hele verschillende soorten impactstudies. Het is goed daar kennis van te nemen, zeker nu steeds meer mogelijk is omdat er meer data beschikbaar zijn waarop deze evaluaties gebaseerd kunnen worden. Impactstudies worden uitgevoerd om vast te stellen of programma's de effecten opleveren die ze nastreven, om te begrijpen of en waarom deze programma's werken, om vast te stellen in hoeverre veranderingen zijn toe te schrijven aan de inzet van het programma en ook om vast te stellen of de gelden op een goede manier worden besteed. Op dit terrein is er natuurlijk een enorme hoeveelheid literatuur en enkele uitgaven geven ons hiervan een goed en up-to-date overzicht^[Khandker, S.R., Koolwal, G.B. & Samad, H.A. (2010). *Handbook on Impactevaluation. Quantative Methods and Practices*. Washington D.C: The World Bank; Gertler, P.J., Martinez, S., Prenard, P., Rawlings, L.B. & Vermeersch, C.M. (2011). *Impact Evaluation in Practice*. Washington D.C.: The World Bank; Murnane, R.J. & Willet, J.B.(2011). *Methods Matter. Improving Causal Inference in Educational and Social Science Research*. New York: Oxford University]. Experimentele studies kunnen natuurlijk goede impactstudies zijn, met sterke punten en beperkingen. Maar er zijn ook aanvullende methodes die in quasi-experimentele of observationele studies kunnen worden toegepast. Zo zijn er panel datamethodes die gebruikt kunnen worden, regressie discontinu?teit methodes en instrumentele variabelen methodes. Daarnaast zijn er verschillende matchingsmethodes die in impactstudies worden gebruikt. Hier stellen we zo'n matchingsmethode voor die goed gebruikt kan worden in verschillende soorten impactstudies en laten we zien hoe deze uitgevoerd kan worden.",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2017-09-14",
    "categories": [],
    "preview": "posts/2018-11-14-psm/psm_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2020-03-13T23:28:57+01:00"
  },
  {
    "path": "posts/2018-11-14-tufte/",
    "title": "Tufte",
    "description": "De Tufte-stijl is een stijl die Edward Tufte gebruikt in zijn boeken en handouts. Tufte's stijl is bekend vanwege zijn veelvuldig gebruik van opmerkingen aan de zijkant (sidenotes), strakke integratie van zijn grafieken met tekst en zijn duidelijk gezette typografie. Deze stijl is geimplementeerd in repectievelijk LaTeX en HTML/CSS^[Zie Github repositories [tufte-latex](https://github.com/tufte-latex/tufte-latex) en [tufte-css](https://github.com/edwardtufte/tufte-css)], respectively. Beide implementaties zitten nu ook in het [**tufte** pakket](https://github.com/rstudio/tufte). Als je een LaTeX/PDF output wilt, gebruik dan `tufte_handout` format voor handouts en `tufte_book` voor boeken. Voor HTML output, gebruik je `tufte_html`. Deze formatten kunnen worden gespecificeerd in de YAML metadata aan het begin van een R Markdown-document (zie het voorbeeld hieronder), of overgebracht via de `rmarkdown::render()` functie. Zie @R-rmarkdown voor meer informatie over **rmarkdown**.",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2017-09-14",
    "categories": [],
    "preview": {},
    "last_modified": "2020-03-13T23:28:57+01:00"
  },
  {
    "path": "posts/2018-11-14-op-weg-naar-infografieken/",
    "title": "Op weg naar infografieken",
    "description": "Hier gaat het om een korte handleiding voor R_gebruikers die omwille van de leesbaarheid en esthetiek hun figuren in het populaire grafische design programma Illustrator willen 'oppoetsen'. Als het op visualisatie aankomt blijven de meeste R-gebruikers binnen dit programma werken. Dat is natuurlijk prima als het gaat om figuren die de analyse moeten ondersteunen en jij degene bent die er alleen naar moet kijken. Dan hoef je ook niets over de context te vermelden, niets verder uit te leggen of ervoor te zorgen dat het er allemaal mooi uitziet. Het doel dan is vooral snel figuren maken zodat je gevoel bij jouw data krijgt. R biedt je ook heel veel mogelijkheden, ook voor goede visualisatie. Echter, als het gaat om het maken van figuren die voor een breder publiek toegankelijk en leesbaar zijn en die zelf een verhaal moeten vertellen, kan het wel eens bruikbaarder en efficiënter zijn om dit R-figuur als PDF op te slaan en aanpassingen door te voeren in een vector georienteerd programma zoals Adobe Illustrator (https://www.adobe.com/nl/) of zijn open-source alternatief Inkscape (https://www.inkscape.org/nl/). Inkscape is vrij toegankelijk maar hier besteden wij enkel aandacht aan het bewerken in Adobe Illustrator.",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2017-07-14",
    "categories": [],
    "preview": {},
    "last_modified": "2020-03-13T23:28:57+01:00"
  },
  {
    "path": "posts/2018-11-14-latex/",
    "title": "Latex",
    "description": "Introductie op Latex.",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "https://www.harriejonkman.nl"
      }
    ],
    "date": "2017-04-14",
    "categories": [],
    "preview": {},
    "last_modified": "2020-03-13T23:28:57+01:00"
  },
  {
    "path": "posts/2018-11-14-visualisatie/",
    "title": "Visualisatie",
    "description": "Hoe kun je goed werken aan datavisualisatie met ggplot2 binnen R/RStudio",
    "author": [
      {
        "name": "Zev Ross, bewerkt door Harrie Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2017-02-11",
    "categories": [],
    "preview": "posts/2018-11-14-visualisatie/visualisatie_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2020-03-13T23:28:57+01:00"
  }
]
