[
  {
    "path": "posts/2021-07-18-classificeren-van-palmer-penguins/",
    "title": "Classificeren van Palmer penguins",
    "description": "De laatste tijd heeft Julia Silge een aantal videoopnamen gemaakt die laten zien hoe het `tidymodels` raamwerk is te gebruiken.Het zijn opnamen over de eerste stappen in het modelleren tot hoe complexe modellen zijn te evalueren. Deze videoopname is goed voor mensen die net beginnen met `tidymodels`. Ze maakt daarbij gebruik van een #TidyTuesday dataset over pinguïns. Hier gaat het om classificeren.",
    "author": [
      {
        "name": "Julia Silge, bewerking Harrie Jonkman",
        "url": {}
      }
    ],
    "date": "2021-07-18",
    "categories": [],
    "contents": "\nPalmer penguins classificatie\nHier kun je overigen haar opnmame vinden. Julia Silge on youtube\nDe laatste tijd heeft Julia Silge een aantal videoopnamen gemaakt die laten zien hoe het tidymodels raamwerk is te gebruiken.Het zijn opnamen over de eerste stappen in het modelleren tot hoe complexe modellen zijn te evalueren. Deze videoopname is goed voor mensen die net beginnen met tidymodels. Ze maakt daarbij gebruik van een #TidyTuesday dataset over pinguïns. Hier gaat het om classificeren.\nHier kun je haar opnmame vinden. Julia Silge on youtube\nEerst maar eens enkele pakketten laden en het databestand openen.\n\n# A tibble: 344 x 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm\n   <fct>   <fct>              <dbl>         <dbl>             <int>\n 1 Adelie  Torgersen           39.1          18.7               181\n 2 Adelie  Torgersen           39.5          17.4               186\n 3 Adelie  Torgersen           40.3          18                 195\n 4 Adelie  Torgersen           NA            NA                  NA\n 5 Adelie  Torgersen           36.7          19.3               193\n 6 Adelie  Torgersen           39.3          20.6               190\n 7 Adelie  Torgersen           38.9          17.8               181\n 8 Adelie  Torgersen           39.2          19.6               195\n 9 Adelie  Torgersen           34.1          18.1               193\n10 Adelie  Torgersen           42            20.2               190\n# … with 334 more rows, and 3 more variables: body_mass_g <int>,\n#   sex <fct>, year <int>\n\nAls je een classificatiemodel voor soorten pinquins probeert op te stellen, zul je waarschijnlijk een bijna perfecte pasvorm vinden, omdat dit soort waarnemingen in feite de verschillende soorten onderscheiden. sex (geslacht) daarentegen geeft een wat rommeliger beeld, vandaar dat hier deze uitkomstvariabelen op basis van predictoren wordt voorspeld.\n\n\n\nHet ziet er naar uit dat de vrouwelijke pinguïnflippers kleiner zijn met kleinere snavels, maar laten we ons klaarmaken voor het modelleren om meer te weten te komen! De informatie over het eiland of het jaar zullen we niet gebruiken in ons model. Die halen we eruit.\n\n\n\nEen modelopbouwen\nWe zullen ook het tidymodels metapakket laden en vervolgens onze gegevens splitsen in een trainings- en testingssets.\n\n\n\nOmdat het een relatieve kleine dataset betreft (zeker de testset), maken we vervolgens hier gebruik van bootstrap-resamples van de trainingsgegevens, om onze modellen te evalueren.\n\n# Bootstrap sampling \n# A tibble: 25 x 2\n   splits            id         \n   <list>            <chr>      \n 1 <rsplit [250/93]> Bootstrap01\n 2 <rsplit [250/92]> Bootstrap02\n 3 <rsplit [250/90]> Bootstrap03\n 4 <rsplit [250/92]> Bootstrap04\n 5 <rsplit [250/86]> Bootstrap05\n 6 <rsplit [250/88]> Bootstrap06\n 7 <rsplit [250/96]> Bootstrap07\n 8 <rsplit [250/89]> Bootstrap08\n 9 <rsplit [250/96]> Bootstrap09\n10 <rsplit [250/90]> Bootstrap10\n# … with 15 more rows\n\nLaten we eens twee verschillende modellen vergelijken, een logistisch regressiemodel en een random forest model. We beginnen met het maken van de modelspecificaties voor beide modellen.\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm \n\n\nRandom Forest Model Specification (classification)\n\nComputational engine: ranger \n\nLaten we nu beginnen met het samenstellen van een tidymodels workflow(), een object dat helpt om modelleer-pijplijnen te beheren met stukjes die in elkaar passen als Lego-blokjes. Merk op dat er nog geen model is:\n\n══ Workflow ══════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: None\n\n── Preprocessor ──────────────────────────────────────────────────────\nsex ~ .\n\nNu kunnen we een model toevoegen, en de fit voor elk van de resamples. Eerst kunnen we het logistische regressiemodel passen.\n\n# Resampling results\n# Bootstrap sampling \n# A tibble: 25 x 5\n   splits         id        .metrics      .notes       .predictions   \n   <list>         <chr>     <list>        <list>       <list>         \n 1 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [93 × …\n 2 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [92 × …\n 3 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [90 × …\n 4 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [92 × …\n 5 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [86 × …\n 6 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [88 × …\n 7 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [96 × …\n 8 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [89 × …\n 9 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [96 × …\n10 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [90 × …\n# … with 15 more rows\n\nTen tweede kunnen we het random forest model toepassen.\n\n# Resampling results\n# Bootstrap sampling \n# A tibble: 25 x 5\n   splits         id        .metrics      .notes       .predictions   \n   <list>         <chr>     <list>        <list>       <list>         \n 1 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [93 × …\n 2 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [92 × …\n 3 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [90 × …\n 4 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [92 × …\n 5 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [86 × …\n 6 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [88 × …\n 7 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [96 × …\n 8 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [89 × …\n 9 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [96 × …\n10 <rsplit [250/… Bootstra… <tibble [2 ×… <tibble [0 … <tibble [90 × …\n# … with 15 more rows\n\nWij hebben elk van onze kandidaat-modellen aangepast aan onze opnieuw bemonsterde trainingsreeks!\nHet model evalueren.\nLaten we nu eens kijken hoe we het gedaan hebben. Eerst het logistisch regressiemodel.\n\n# A tibble: 2 x 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy binary     0.897    25 0.00631 Preprocessor1_Model1\n2 roc_auc  binary     0.964    25 0.00368 Preprocessor1_Model1\n\nGoed zo! De functie collect_metrics() extraheert en formatteert de .metrics kolom van resampling resultaten zoals hierboven voor het glm-model. Nu het random-forest model.\n\n# A tibble: 2 x 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy binary     0.890    25 0.00595 Preprocessor1_Model1\n2 roc_auc  binary     0.959    25 0.00342 Preprocessor1_Model1\n\nDus… ook geweldig! Als ik in een situatie zit waarin een complexer model (zoals een random forest) hetzelfde presteert als een eenvoudiger model (zoals logistische regressie), dan kies ik het eenvoudiger model. Laten we eens dieper ingaan op hoe het het doet. Bijvoorbeeld, hoe voorspelt het glm-model de twee klassen?\n\n# A tibble: 4 x 3\n  Prediction Truth   Freq\n  <fct>      <fct>  <dbl>\n1 female     female 40.6 \n2 female     male    4.48\n3 male       female  4.92\n4 male       male   41.4 \n\nOngeveer hetzelfde, wat goed is. We kunnen ook een ROC curve maken.\n\n\n\nDeze ROC-curve is grilliger dan andere die u wellicht hebt gezien omdat de dataset klein is.\nHet is eindelijk tijd om terug te keren naar de testset. Merk op dat we de testset tijdens deze hele analyse nog niet hebben gebruikt; de testset is kostbaar en kan alleen worden gebruikt om de prestaties op nieuwe gegevens in te schatten. Laten we nog een keer passen op de trainingsgegevens en evalueren op de testgegevens met behulp van de functie last_fit().\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 x 6\n  splits      id         .metrics    .notes    .predictions  .workflow\n  <list>      <chr>      <list>      <list>    <list>        <list>   \n1 <rsplit [2… train/tes… <tibble [2… <tibble … <tibble [83 … <workflo…\n\nDe metriek en voorspellingen hier zijn op de testgegevens.\n\n# A tibble: 2 x 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.940 Preprocessor1_Model1\n2 roc_auc  binary         0.991 Preprocessor1_Model1\n\n\n          Truth\nPrediction female male\n    female     39    3\n    male        2   39\n\nDe coëfficiënten (die we eruit kunnen halen met tidy()) zijn geschat met behulp van de trainingsdata. Als we exponentiate = TRUE gebruiken, hebben we odds ratio’s.\n\n# A tibble: 7 x 5\n  term              estimate std.error statistic       p.value\n  <chr>                <dbl>     <dbl>     <dbl>         <dbl>\n1 (Intercept)       3.12e-35  13.5         -5.90 0.00000000369\n2 speciesChinstrap  1.34e- 3   1.70        -3.89 0.000101     \n3 speciesGentoo     1.08e- 4   2.89        -3.16 0.00159      \n4 bill_length_mm    1.78e+ 0   0.137        4.20 0.0000268    \n5 bill_depth_mm     3.89e+ 0   0.373        3.64 0.000273     \n6 flipper_length_mm 1.07e+ 0   0.0538       1.31 0.189        \n7 body_mass_g       1.01e+ 0   0.00108      4.70 0.00000260   \n\nDe grootste kansverhouding geldt voor de snaveldiepte, en de op één na grootste voor de snavellengte. Een toename van 1 mm snaveldiepte komt overeen met bijna 4x meer kans om een mannetje te zijn. De kenmerken van de bek van een pinguïn moeten geassocieerd zijn met het geslacht.\nWe hebben geen sterke aanwijzingen dat de lengte van de vleugels verschillend is tussen mannelijke en vrouwelijke pinguïns, als we de andere maten controleren; misschien moeten we dat onderzoeken door de eerste grafiek te veranderen!\n\n\n\nJa, de mannetjes- en vrouwtjespinguïns zijn nu veel meer gescheiden.\n\n\n\n",
    "preview": "posts/2021-07-18-classificeren-van-palmer-penguins/classificeren-van-palmer-penguins_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-07-18T21:55:44+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-18-verzekeringskosten/",
    "title": "Verzekeringskosten",
    "description": "Dit is de blog die [Arta Seyedan op 14 februari 2021 R-bloggers](https://www.r-bloggers.com/2021/02/using-tidymodels-to-predict-health-insurance-cost/) schreef en die ik wat bewerkt en vertaald heb.",
    "author": [
      {
        "name": "Arta Seyadan, bewerking Harrie Jonkman",
        "url": {}
      }
    ],
    "date": "2021-07-18",
    "categories": [],
    "contents": "\nVerzekeringskosten voorspellen met behulp van lineaire regressie\nDit is de blog die Arta Seyedan op 14 februari 2021 R-bloggers schreef en die ik wat bewerkt en vertaald heb.\nRond eind oktober 2020 woonde AS de Open Data Science Conferentie bij, voornamelijk voor de workshops en trainingssessies die daar werden aangeboden. De eerste workshop die hij bijwoonde was een demonstratie door Jared Lander over hoe je machine learning methoden in R kunt implementeren met behulp van een nieuw pakket genaamd tidymodels. Hij ging die training in en wist bijna niets over machine learning en heeft vervolgens uitsluitend gebruik gemaakt van gratis online materiaal om te begrijpen hoe je data analyseert met behulp van dit “meta-pakket”.\ntidymodels is net als tidyverse niet een enkel pakket. Het is eerder een verzameling van data science pakketten (een suite zeg maar) ontworpen volgens principes van tidyverse. Er is overeenkomst tussen tidymodels en tidyverse. Wat tidymodels echter anders maakt dan tidyverse, is dat veel van deze pakketten bedoeld zijn voor voorspellend modelleren. Het biedt een universele standaard interface voor alle verschillende machine learning methoden die beschikbaar zijn in R.\nOm te laten zien hoe het werkt, wordt hier een dataset aangeboden met informatie van ziektekostenverzekering van ~1300 klanten van een ziektekostenverzekeringsmaatschappij. Deze dataset is afkomstig uit een boek getiteld Machine Learning with R van Brett Lantz. Laten we tegelijk enkele pakketten openen.\n\n\n\nDit zijn de zeven variabelen die erin zitten.\n\n[1] \"age\"      \"sex\"      \"bmi\"      \"children\" \"smoker\"   \"region\"  \n[7] \"charges\" \n\nZo zien de variabelen er vervolgens uit.\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  18.00   27.00   39.00   39.21   51.00   64.00 \n\n\n.\nfemale   male \n   662    676 \n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  15.96   26.30   30.40   30.66   34.69   53.13 \n\n\n.\n  no  yes \n1064  274 \n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1122    4740    9382   13270   16640   63770 \n\nHierboven zie je dat je pakketten als parsnip en recipes hebt geladen. Deze pakketten vormen dus, samen met andere pakketten, het meta-pakket tidymodels, dat gebruikt wordt voor modelleren en statistische analyse en machine learning.\nZoals je kunt zien, zijn er 7 verschillende, relatief voor zichzelf sprekende variabelen in deze dataset, waarvan sommige vermoedelijk worden gebruikt door de particuliere ziektekostenverzekeraar in kwestie om te bepalen hoeveel een bepaald individu uiteindelijk in rekening wordt gebracht. age(Leeftijd), sex (geslacht) en region (regio) lijken demografische achtergrondvariabelen te zijn, waarbij de leeftijd niet lager dan 18 en niet hoger dan 64 jaar is, met een gemiddelde van ongeveer 40 jaar. Het aantal mannen en vrouwen is vrijwel hetzelfde.\nErvan uitgaande dat de variabele bmi overeenkomt met Body Mass Index, wordt een BMI van 30 of hoger als klinisch zwaarlijvig beschouwd. In onze huidige gegevensverzameling ligt het gemiddelde net boven de grens van zwaarlijvigheid.\nVervolgens hebben we het aantal rokers versus niet-rokers. Nu kan ik je zeker al vertellen dat het al of nietroker zijn belangrijk zal zijn bij het bepalen van de kosten van een bepaalde ziektekostenverzekeraar.\nTenslotte, hebben we charge (kosten). De gemiddelde jaarlijkse kosten voor een ziektekostenverzekering zijn een bescheiden 13.000 dollar.\nExploratieve Data Analyse\n\nTable 1: Data summary\nName\ninsur_dt\nNumber of rows\n1338\nNumber of columns\n7\nKey\nNULL\n_______________________\n\nColumn type frequency:\n\ncharacter\n3\nnumeric\n4\n________________________\n\nGroup variables\nNone\nVariable type: character\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\nsex\n0\n1\n4\n6\n0\n2\n0\nsmoker\n0\n1\n2\n3\n0\n2\n0\nregion\n0\n1\n9\n9\n0\n4\n0\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nage\n0\n1\n39.21\n14.05\n18.00\n27.00\n39.00\n51.00\n64.00\n▇▅▅▆▆\nbmi\n0\n1\n30.66\n6.10\n15.96\n26.30\n30.40\n34.69\n53.13\n▂▇▇▂▁\nchildren\n0\n1\n1.09\n1.21\n0.00\n0.00\n1.00\n2.00\n5.00\n▇▂▂▁▁\ncharges\n0\n1\n13270.42\n12110.01\n1121.87\n4740.29\n9382.03\n16639.91\n63770.43\n▇▂▁▁▁\n\nDe dataset ziet er vrij schoon uit; je zult waarschijnlijk nooit een dataset als deze zo tegenkomen. Er zijn geen NAs (missende waarden) en, zoals al eerder opgemerkt, geslacht is evenwichtig verdeeld. Laten we eens kijken naar de verdeling van de children (kinderen):\n\n\n  0   1   2   3   4   5 \n574 324 240 157  25  18 \n\nVrij standaard toch; de overgrote meerderheid van de mensen in deze set heeft geen kinderen. Het volgende hoogste aantal is 1, het op een na hoogste 2, enz.\n\n\n\nGGally is een pakket dat het proces van exploratieve data analyse vergemakkelijkt door automatisch ggplots te genereren met de variabelen die in het data frame zitten. Het helpt je om een beter inzicht te krijgen in de relaties die er tussen variabelen zouden kunnen bestaan. De meeste van deze plots zijn gewoon ruis, maar er zijn een paar interessante. Kijk maar eens naar de twee linksonder, die charge vs age en charge vs bmi beoordelen. Verder naar rechts, is er ook charge vs smoker. Laten we een aantal van deze verbanden eens nader bekijken:\n\n\n\nIk wilde zien of er regio’s zijn die op de een of andere manier anders belast worden dan de andere, maar deze plots zien er allemaal hetzelfde uit. Zoals je ziet, zijn er ongeveer twee verschillende blobs die van 0,0 naar het centrum van de plot gaan. We komen daar later op terug.\n\n\n\nHier, wilde ik zien of er een soort van herkenbaar verband was tussen leeftijd en kosten. In de vier regio’s lijken de meeste op een helling bij de X-as te liggen, die licht toeneemt met de leeftijd. Er is echter een patroon dat lijkt te bestaan uit twee niveaus die van die basislijn afkomen. Aangezien we geen variabele hebben voor het soort ziektekostenverzekering dat deze mensen hebben, moeten we voorlopig maar even wachten met een oordeel over wat dit zou kunnen zijn.\nLaten we overgaan tot wat ongetwijfeld het ‘pièce de résistance’ is van de ziektekostenverzekeringsdekking: rokers.\n\n\n\nWow. Wat een groot verschil. Hier zie je dat rokers bijna een hele nieuwe klodder punten creëert los van niet-rokers… en die klodder stijgt sterk na ‘bmi = 30’. Zeg, wat was de officiële cutoff-score van de CDC voor obesitas ook alweer?\n\n\n\nJe kunt zien dat leeftijd een rol speelt bij kosten, maar het is nog steeds gestratificeerd binnen de 3 clusters van punten. Dus zelfs onder de hoge bmi-rokers, betalen jongere mensen nog steeds minder geld dan oudere mensen op een consistente manier, dus het is logisch. Het lijkt er echter niet op dat leeftijd een wisselwerking heeft met bmi of roker, wat betekent dat het onafhankelijk effect heeft op de prijs`.\nTenslotte, kinderen heeft geen significant effect op de lading, zie maar.\n\n\n\nIk denk dat we genoeg verkennende analyses hebben gedaan om vast te stellen dat bmi en roker samen een synergetisch effect hebben op de prijs, en dat leeftijd ook invloed heeft op de prijs.\nModel bouwen\nMet deze kennis in ons achterhoofd gaan we een model bouwen.\n\n\n\nWe splitsen eerst onze gegevens in training- en testsets. We stratificeren de steekproeftrekking op basis van de status van “roker”, omdat er daar een onevenwicht bestaat en we willen dat ze gelijk vertegenwoordigd zijn in zowel de trainings- als de testdatasets. Dit wordt bereikt door eerst aselecte steekproeven uit te voeren binnen deze klassen.\nEen uitleg van recipe (het pakket recept, zeg maar):\nWe gaan het effect van bmi, leeftijd en roker op prijs modelleren. We specificeren in deze stap geen interacties omdat het recept interacties als stap afhandelt.\nWe maken dummy variabelen (step_dummy) voor alle nominale voorspellers, dus roker wordt roker_ja en roker_nee wordt “geïmpliceerd” door weglating (dus als een rij roker_ja == 0 heeft) omdat in sommige modellen niet alle dummy variabelen als kolom aanwezig kunnen zijn. Om alle dummy variabelen op te nemen, kunt u one_hot = TRUE gebruiken.\nVervolgens normaliseren we alle numerieke voorspellers behalve onze uitkomstvariabele(step_normalize(all_numeric(), -all_outcomes())), omdat je over het algemeen transformaties op uitkomsten wilt vermijden bij het trainen en ontwikkelen van een model, omdat anders een andere dataset die niet consistent is met de dataset die je gebruikt langskomt en je model kapot maakt. Het is het beste om transformaties op uitkomsten te doen voordat je een recipe maakt.\nWe stellen een interactieterm in; bmi en smoker_yes (de dummy variabele voor smoker), hebben allemaal een wisselwerking met elkaar bij het beïnvloeden van de uitkomst. Eerder zagen we al dat oudere patiënten meer moeten betalen, en dat oudere patiënten met een hogere bmi zelfs nog meer moeten betalen. Welnu, oudere patiënten met een hogere bmi die roken worden het meest aangerekend van iedereen in onze dataset. We hebben dit visueel waargenomen toen we naar de plot keken, dus we gaan dit ook testen in het model dat we zullen ontwikkelen.\nLaten we het model specificeren. We gaan werken met een k-Nearest Neighbors model om het later te vergelijken met een ander model. Het KNN-model is eenvoudig als volgt gedefinieerd:\n\nKNN regressie is een niet-parametrische methode die op een intuïtieve manier de associatie tussen onafhankelijke variabelen en de continue uitkomst benadert door het gemiddelde te nemen van de waarnemingen in dezelfde buurt. De grootte van de buurt moet worden ingesteld door de analist of kan worden gekozen met behulp van crossvalidatie (dit komt later aan de orde) om de grootte te kiezen die de gemiddelde kwadratische fout zo klein mogelijk maakt.\n\nOm het eenvoudig te houden, gaan we geen kruisvalidatie gebruiken om de optimale k te vinden. In plaats daarvan zeggen we gewoon k = 10.\n\n\n\nWe hebben het model knn_spec gespecificeerd door het model zelf aan te roepen vanuit parsnip. Daarna hebben we set_engine en de modus op regressie gezet. Let op de neighbors parameter in nearest_neighbor. Dat komt overeen met de k in knn.\nVervolgens passen we het model met behulp van de modelspecificatie toe op onze gegevens. Omdat we al kolommen hebben berekend voor de bmi en smoker_yes interactie, hoeven we de interactie niet opnieuw formeel weer te geven.\nLaten we dit model evalueren om te zien hoe het het doet.\n\n# A tibble: 2 x 6\n  .metric .estimator     mean     n  std_err .config             \n  <chr>   <chr>         <dbl> <int>    <dbl> <chr>               \n1 rmse    standard   4916.       10 274.     Preprocessor1_Model1\n2 rsq     standard      0.827    10   0.0194 Preprocessor1_Model1\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1122    4740    9382   13270   16640   63770 \n\nWe stellen vfold_cv in (dat is de crossvalidatie waar de meeste mensen bekend mee zijn, waarbij de trainingsdata wordt verdeeld in V vouwen en dan wordt getraind op V-1 vouwen om een voorspelling te doen op de laatste vouw, en wordt herhaald zodat alle vouwen worden getraind en gebruikt als voorspellingsvouw) op een prop van 0.9, wat hetzelfde is als het specificeren van 9 trainingsvouwen en 1 testvouw (binnen onze trainingsdata).\nTenslotte voeren we de crossvalidatie uit door fit_resamples te gebruiken. Zoals je kunt zien, hebben we ons workflow object als invoer gebruikt.\nTenslotte roepen we collect_metrics op om de effectiviteit van het model te onderzoeken. We eindigen met een rmse van 4,915 en een rsq van 0,82. De RMSE zou suggereren dat, gemiddeld, onze voorspellingen verschilden van de waargenomen waarden met een absolute maatstaf van 4.915, in dit geval, dollars in charges. De R^2 zou suggereren dat onze regressie een fit heeft van ~82%, hoewel een hoge R^2 niet altijd betekent dat het model een goede fit heeft en een lage R^2 niet altijd betekent dat een model een slechte fit heeft.\n\n\n\nHierboven is een demonstratie van onze regressie op een lijn. Er is een grote cluster van waarden die ons model gewoon niet weergeeft, en we zouden meer kunnen leren over deze punten, maar in plaats daarvan gaan we verder met het toepassen van ons model op onze testgegevens, die we veel eerder in dit project hebben gedefinieerd.\n\n# A tibble: 334 x 2\n    .pred charges\n    <dbl>   <dbl>\n 1  4339.   3757.\n 2 27038.  27809.\n 3  2231.   1837.\n 4  6500.   6204.\n 5  2794.   4688.\n 6  6057.   6314.\n 7 14335.  12630.\n 8  1663.   2211.\n 9  5655.   3580.\n10 39401.  37743.\n# … with 324 more rows\n\nWe hebben nu ons model toegepast op test_proc, de test set nadat we de recipes voorbewerkingsstappen erop hebben toegepast om ze op dezelfde manier te transformeren als we onze trainingsdata hebben getransformeerd. We verbinden de resulterende voorspellingen met de werkelijke charges gevonden in de training data om een twee-koloms tabel te maken met onze voorspellingen en de overeenkomstige werkelijke waarden die we probeerden te voorspellen.\n\n\n\n\n# A tibble: 1 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       4985.\n# A tibble: 2 x 6\n  .metric .estimator     mean     n  std_err .config             \n  <chr>   <chr>         <dbl> <int>    <dbl> <chr>               \n1 rmse    standard   4916.       10 274.     Preprocessor1_Model1\n2 rsq     standard      0.827    10   0.0194 Preprocessor1_Model1\n\nMooi zo. De RMSE gegenereerd door onze testgegevens verschilt niet significant van die gegenereerd door onze crossvalidatie! Dat betekent dat ons model op betrouwbare wijze voorspellingen kan reproduceren met ongeveer hetzelfde foutenniveau.\nEen ander groot voordeel van tidymodels is dat het het proces van het vergelijken van voorspellende prestaties tussen twee verschillende modellen stroomlijnt. Laat mij jou dat demonstreren.\nLineaire Regressie\nWe hebben het recept (recipe) al. Nu moeten we alleen nog een lineair model specificeren en het model kruisvalideren om het te testen op de testgegevens.\n\n\n\nWe herhalen sommige van dezelfde stappen die we voor KNN deden, maar dan nu voor het lineaire model. We kunnen zelfs crossvalideren door (bijna) hetzelfde commando te gebruiken:\n\n# A tibble: 2 x 6\n  .metric .estimator     mean     n  std_err .config             \n  <chr>   <chr>         <dbl> <int>    <dbl> <chr>               \n1 rmse    standard   4866.       10 251.     Preprocessor1_Model1\n2 rsq     standard      0.832    10   0.0162 Preprocessor1_Model1\n# A tibble: 2 x 6\n  .metric .estimator     mean     n  std_err .config             \n  <chr>   <chr>         <dbl> <int>    <dbl> <chr>               \n1 rmse    standard   4916.       10 274.     Preprocessor1_Model1\n2 rsq     standard      0.827    10   0.0194 Preprocessor1_Model1\n\nFascinerend! Het blijkt dat het goede, ouderwetse lineaire model k-Nearest Neighbors verslaat zowel in termen van RMSE als van R^2 over 10 kruisvalidatie-voudigingen.\n\n# A tibble: 334 x 2\n    .pred charges\n    <dbl>   <dbl>\n 1  6335.   3757.\n 2 31938.  27809.\n 3  3171.   1837.\n 4  7878.   6204.\n 5  3081.   4688.\n 6  7815.   6314.\n 7 14070.  12630.\n 8  2656.   2211.\n 9  3498.   3580.\n10 36293.  37743.\n# … with 324 more rows\n\nNu we onze voorspellingen hebben, laten we eens kijken hoe goed het lineaire model het deed:\n\n\n\nHet lijkt erop dat het gebied linksonder de grootste concentratie ladingen had, en het grootste deel van de lm fit verklaart. Kijkend naar deze beide plots vraag ik me af of er een beter model was dat we hadden kunnen gebruiken, maar ons model voldeed gezien onze doelstellingen en nauwkeurigheidsniveau.\n\n\n\nHierboven is een vergelijking van de twee methoden met hun respectieve voorspellingen, en met de stippellijn die de “juiste” waarden weergeeft. In dit geval verschilden de twee modellen niet zo veel van elkaar dat hun verschillen gemakkelijk konden worden waargenomen wanneer ze tegen elkaar werden uitgezet. Maar er zullen zich in de toekomst gevallen voordoen waarin uw twee modellen toch aanzienlijk verschillen. Het zo doen zal je helpen het ene model boven het andere te verkiezen.\nConclusie\nHier konden wij een KNN-model bouwen met onze trainingsgegevens en het gebruiken om waarden in onze testgegevens te voorspellen. Om dit te doen, hebben we:\neen EDA uitgevoerd;\nonze gegevens voorbewerkt en met recipe ons model gespecificeerd als KNN;\nhet toegepast op onze trainingsgegevens;\ncrossvalidatie uitgevoerd om nauwkeurige foutstatistieken te produceren;\nvoorspelde waarden vastgesteld in onze testset;\nde waargenomen testwaarden met onze voorspellingen vergeleken;\neen ander model gespecificeerd (lm);\nook hier een crossvalidatie uitgevoerd;\nontdekt dat lm het betere model was.\nHij is zeer enthousiast om door te gaan met het gebruik van tidymodels in R als een manier om machine-learning methoden toe te passen. Als je geïnteresseerd bent, raad ik je aan om Tidy Modeling with R by Max Kuhn and Julia Silge te bekijken.\n\n\n\n",
    "preview": "posts/2021-07-18-verzekeringskosten/verzekeringskosten_files/figure-html5/unnamed-chunk-10-1.png",
    "last_modified": "2021-07-18T21:47:07+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-18-classificeren-met-tidymodels/",
    "title": "Classificeren met Tidymodels",
    "description": "Dit is bewerking van een blog die [Rahul Raoniar, Towards data science](https://towardsdatascience.com/modelling-binary-logistic-regression-using-tidymodels-library-in-r-part-1-c1bdce0ac055) begin 2021 schreef.",
    "author": [
      {
        "name": "Rahul Raoniar, bewerking Harrie Jonkman",
        "url": {}
      }
    ],
    "date": "2021-07-05",
    "categories": [],
    "contents": "\nEen gids om stap voor stap een logissche regressie uit te voeren met gebruik van het tidymodels pakket\nDit is bewerking van een blog die Rahul Raoniar, Towards data science begin 2021 schreef.\nIn de wereld van ‘supervised machine learning’ worden vaak twee soorten analyses uitgevoerd. De ene heet regressie (voorspellen van continue waarden), de andere heet classificatie (voorspellen van discrete waarden). In deze blog geef ik een voorbeeld van een binair classificatiealgoritme, “Binaire Logistische Regressie” genaamd. Dat valt onder de Binomiale familie met een logit koppelingsfunctie. Binaire logistische regressie wordt gebruikt voor het voorspellen van binaire klassen. Bijvoorbeeld in gevallen waarin je ja/nee, winst/verlies, negatief/positief, waar/onwaar enzovoort wilt voorspellen.\nDeze blog leidt jou door een proces van hoe het ‘tidymodels’-pakket te gebruiken om een model toe te passen en te evalueren met heel weinig en eenvoudige stappen.\nAchtergrond van de data\nIn dit voorbeeld maak je gebruik maken van de Pima Indian Diabetes 2 data, verkregen uit de UCI Repository van de machine learning data (Newman et al. 1998).\nDeze data zijn oorspronkelijk afkomstig van het ‘National Institute of Diabetes and Digestive and Kidney Diseases’. Het doel van de dataset is diagnostisch te voorspellen of een patiënt al dan niet diabetes heeft, op basis bepaalde diagnostische metingen die in de dataset zijn opgenomen. Bij de selectie van deze data uit een grotere databank werden verschillende beperkingen opgelegd. In het bijzonder zijn alle patiënten hier vrouwen van ten minste 21 jaar oud van Pima Indiaanse afkomst. De Pima Indian Diabetes 2-data is de verfijnde versie (alle ontbrekende waarden zijn toegewezen als NA) van de Pima Indian diabetes-gegevens. De dataset bevat de volgende onafhankelijke en afhankelijke variabelen.\nOnafhankelijke variabelen (met symbool: O) - O1: pregnant: Aantal keren zwanger\n- O2: glucose: Plasma glucose concentratie (glucose tolerantie test)\n- O3: pressure: Diastolische bloed druk (mm Hg)\n- O4: triceps: Triceps huidplooidikte (mm)\n- O5: insulin: 2-uur serum insuline (mu U/ml)\n- O6: mass: Body mass index (gewicht in kg/(lengte in m)\\²)\n- O7: pedigree: Diabetes pedigree functie\n- O8: age: Leeftijd (jaren)\nDependent Variable (met symbool: A)\n- A1: diabetes: diabetes geval (pos/neg)\nDoel van de modellering\naanpassen van een binair logistisch regressie-machineleermodel met behulp van de bibliotheek tidymodels\nhet testen van de voorspellingskracht van het getrainde model (evaluatie van het model) op de ongeziene/geteste dataset met behulp van verschillende evaluatiemetrieken.\nBibliotheken en Datasets laden\nStap1: Eerst moeten we de volgende pakketten worden geïnstalleerd met de install.packages( ) functie (als ze al niet zijn geïnstalleerd en ze laden met de library( ) functie.\n\n\n\nStap2: Vervolgens moet je de dataset binnen halen uit het mlbench pakket met behulp van de data( ) functie.\nNa het laden van de data, is de volgende essentiële stap het uitvoeren van een verkennende data-analyse, die zal helpen bij het vertrouwd raken met de data. Gebruik de head( ) functie om de bovenste zes rijen van de data te bekijken.\n\n  pregnant glucose pressure triceps insulin mass pedigree age\n1        6     148       72      35      NA 33.6    0.627  50\n2        1      85       66      29      NA 26.6    0.351  31\n3        8     183       64      NA      NA 23.3    0.672  32\n4        1      89       66      23      94 28.1    0.167  21\n5        0     137       40      35     168 43.1    2.288  33\n6        5     116       74      NA      NA 25.6    0.201  30\n  diabetes\n1      pos\n2      neg\n3      pos\n4      neg\n5      pos\n6      neg\n\nDe Diabetes-gegevensreeks telt 768 waarnemingen en negen variabelen. De eerste acht variabelen zijn van het numerieke type en de afhankelijke/output variabele (diabetes) is een factor/categorische variabele. Het is ook merkbaar dat veel variabelen NA waarden bevatten (missende waarde). Onze volgende taak is het de gegevens te verfijnen/wijzigen, zodat ze compatibel worden met het modelleeralgoritme. Eerst nog eens beter naar de data kijken.\n\nRows: 768\nColumns: 9\n$ pregnant <dbl> 6, 1, 8, 1, 0, 5, 3, 10, 2, 8, 4, 10, 10, 1, 5, 7, …\n$ glucose  <dbl> 148, 85, 183, 89, 137, 116, 78, 115, 197, 125, 110,…\n$ pressure <dbl> 72, 66, 64, 66, 40, 74, 50, NA, 70, 96, 92, 74, 80,…\n$ triceps  <dbl> 35, 29, NA, 23, 35, NA, 32, NA, 45, NA, NA, NA, NA,…\n$ insulin  <dbl> NA, NA, NA, 94, 168, NA, 88, NA, 543, NA, NA, NA, N…\n$ mass     <dbl> 33.6, 26.6, 23.3, 28.1, 43.1, 25.6, 31.0, 35.3, 30.…\n$ pedigree <dbl> 0.627, 0.351, 0.672, 0.167, 2.288, 0.201, 0.248, 0.…\n$ age      <dbl> 50, 31, 32, 21, 33, 30, 26, 29, 53, 54, 30, 34, 57,…\n$ diabetes <fct> pos, neg, pos, neg, pos, neg, pos, neg, pos, pos, n…\n\nVoorbereiding van de gegevens\nDe eerste stap is het verwijderen van data rijen met NA waarden met behulp van na.omit( ) functie. De volgende stap is nogmaals het controleren van de gegevens met behulp van de glimpse( ) functie.\n\nRows: 392\nColumns: 9\n$ pregnant <dbl> 1, 0, 3, 2, 1, 5, 0, 1, 1, 3, 11, 10, 1, 13, 3, 3, …\n$ glucose  <dbl> 89, 137, 78, 197, 189, 166, 118, 103, 115, 126, 143…\n$ pressure <dbl> 66, 40, 50, 70, 60, 72, 84, 30, 70, 88, 94, 70, 66,…\n$ triceps  <dbl> 23, 35, 32, 45, 23, 19, 47, 38, 30, 41, 33, 26, 15,…\n$ insulin  <dbl> 94, 168, 88, 543, 846, 175, 230, 83, 96, 235, 146, …\n$ mass     <dbl> 28.1, 43.1, 31.0, 30.5, 30.1, 25.8, 45.8, 43.3, 34.…\n$ pedigree <dbl> 0.167, 2.288, 0.248, 0.158, 0.398, 0.587, 0.551, 0.…\n$ age      <dbl> 21, 33, 26, 53, 59, 51, 31, 33, 32, 27, 51, 41, 22,…\n$ diabetes <fct> neg, pos, pos, pos, pos, pos, pos, neg, pos, neg, p…\n\nDe uiteindelijke (voorbereide) gegevens bevatten 392 waarnemingen en 9 kolommen. De onafhankelijke variabelen zijn van het type numeriek/dubbel, terwijl de afhankelijke/uitgaande binaire variabele van het type factor/categorie is (neg/ pos).\nGegevensniveaus\nWe kunnen het referentieniveau van de afhankelijke variabele controleren met de functie levels( ). We kunnen zien dat het referentieniveau neg is (het allereerste niveau).\n\n[1] \"neg\" \"pos\"\n\nInstellen referentieniveau\nVoor een betere interpretatie (later voor het uitzetten van de ROC curve) moeten we het referentieniveau van onze afhankelijke variabele “diabetes” op positief (pos) zetten met de relevel( ) functie.\n\n[1] \"pos\" \"neg\"\n\nSplitsing training en testset\nDe volledige dataset wordt in het algemeen opgesplitst in 75% train en 25% test data set (algemene vuistregel). 75% van de trainingsdata wordt gebruikt om het model te trainen, terwijl de overige 25% wordt gebruikt om te controleren hoe het model generaliseerde op ongeziene/test data set.\nOm een split object te maken kun je de initial_split( ) functie gebruiken waar je de dataset, proportie en een strata argument voor moet opgeven. Door de afhankelijke variabele in het strata-attribuut op te geven, wordt gestratificeerde steekproeftrekking uitgevoerd. Gestratificeerde steekproeftrekking is nuttig als je afhankelijke variabele een ongelijke klasse heeft.\nDe volgende stap is het aanroepen van de training( ) en testing( ) functies op het split object (d.w.z. diabetes_split) om de trainings- (diabetes_train) en test- (diabetes_test) datasets op te slaan.\nDe training set bevat 295 waarnemingen, terwijl de test set 97 waarnemingen bevat.\n\n[1] 295\n[1] 97\n\nFitten van logistische regressie\nJe kunt met tidymodels elk type model pasklaar maken met behulp van de volgende stappen. l Stap 1: roep de modelfunctie op: hier gebruiken we logistic_reg( ) omdat we een logistisch regressiemodel willen draaien.\nStap 2: gebruik de set_engine( ) functie om de familie van het model op te geven. We geven het glm argument op, omdat logistische regressie onder de ‘Generalized Linear Regression’-familie valt.\nStap 3: gebruik de set_mode( ) functie en geef het type model op dat je wilt toepassen. Hier willen we pos vs neg classificeren, dus het is een classificatie.\nStap 4: Vervolgens moet je de fit( ) functie gebruiken om het model te fitten en daarbinnen moet je de formule notatie en de dataset (diabetes_train) opgeven.\nplus notatie → diabetes ~ ind_variable 1 + ind_variable 2 + …….so on\ntilde punt notatioe →\ndiabetes~. betekent dat diabetes wordt voorspeld door de rest van de variabelen in het gegevensbestand (d.w.z. alle onafhankelijke variabelen), behalve de afhankelijke variabele, d.w.z. diabetes.\nNa het draaien van het model is de volgende stap het genereren van de modeloverzichtstabel. Je kunt een mooie tabel maken met behulp van de tidy( ) functie van de broom bibliotheek (die is ingebouwd in de tidymodels bibliotheek). De gerapporteerde coëfficiënten zijn in log-odds termen.\n\n# A tibble: 9 x 5\n  term         estimate std.error statistic  p.value\n  <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)  8.64       1.35       6.42   1.36e-10\n2 pregnant    -0.0669     0.0591    -1.13   2.57e- 1\n3 glucose     -0.0352     0.00642   -5.49   4.05e- 8\n4 pressure     0.00931    0.0130     0.715  4.74e- 1\n5 triceps      0.00123    0.0196     0.0629 9.50e- 1\n6 insulin      0.000802   0.00151    0.530  5.96e- 1\n7 mass        -0.0715     0.0302    -2.37   1.79e- 2\n8 pedigree    -0.840      0.452     -1.86   6.31e- 2\n9 age         -0.0375     0.0199    -1.88   6.01e- 2\n\nOpgelet: Het teken en de waarde van de coëfficiënten veranderen afhankelijk van de referentie die u voor de afhankelijke variabele hebt ingesteld (in ons geval is pos het referentieniveau) en de waarneming die u op basis van de aselecte steekproefselectie in de opleidingssteekproef hebt opgenomen [bovenstaande resultaten zijn slechts een voorbeeld].\nDe interpretatie van coëfficiënten in de log-odds term heeft niet veel zin als je die moet rapporteren in je artikel of publicatie. Daarom werd het begrip odds ratio geïntroduceerd.\nDe ODDS is de verhouding van de kans dat een gebeurtenis zich voordoet tot de kans dat de gebeurtenis zich niet voordoet. Wanneer we een verhouding van twee zulke kansen nemen, noemen we dat Odds Ratio.\nOdds ratioWiskundig kan men de odds ratio berekenen door de exponent van de geschatte coëfficiënten te nemen. Je kunt bijvoorbeeld direct de odds ratio’s van de coëfficiënten krijgen door de exponentiate = True mee te geven in de tidy( ) functie.\nHet resultaat is alleen afhankelijk van de steekproeven die we hebben verkregen tijdens het splitsen. Je kunt een ander resultaat krijgen (odds ratio waarden).\n\n# A tibble: 9 x 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept) 5660.      1.35       6.42   1.36e-10\n2 pregnant       0.935   0.0591    -1.13   2.57e- 1\n3 glucose        0.965   0.00642   -5.49   4.05e- 8\n4 pressure       1.01    0.0130     0.715  4.74e- 1\n5 triceps        1.00    0.0196     0.0629 9.50e- 1\n6 insulin        1.00    0.00151    0.530  5.96e- 1\n7 mass           0.931   0.0302    -2.37   1.79e- 2\n8 pedigree       0.432   0.452     -1.86   6.31e- 2\n9 age            0.963   0.0199    -1.88   6.01e- 2\n\nSignificante kansen\nDe tabel geproduceerd door tidy( ) functie kan worden gefilterd. Hier hebben we de variabelen uitgefilterd waarvan de p-waarden lager zijn dan 0.05 (5%) significant niveau. Voor onze steekproef hebben glucose en massa een significante invloed op diabetes.\n\n# A tibble: 3 x 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept) 5660.      1.35         6.42 1.36e-10\n2 glucose        0.965   0.00642     -5.49 4.05e- 8\n3 mass           0.931   0.0302      -2.37 1.79e- 2\n\nModel voorspelling\nVoorspelling van de testgegevensklasse\nDe volgende stap is het genereren van de testvoorspellingen die we kunnen gebruiken voor de evaluatie van het model. Om de klassevoorspelling (pos/neg) te genereren kunnen wij de predict-functie gebruiken en het getrainde modelobject, de testdataset en het type opgeven, dat hier “klasse” is, aangezien wij de klassevoorspelling willen, geen waarschijnlijkheden.\n\n# A tibble: 5 x 1\n  .pred_class\n  <fct>      \n1 pos        \n2 neg        \n3 neg        \n4 pos        \n5 pos        \n\nTestdata klasse waarschijnlijkheden\nWe kunnen ook voorspellingen genereren voor de klassenwaarschijnlijkheden door het argument “prob” in het type-attribuut mee te geven.\n\n# A tibble: 5 x 2\n  .pred_pos .pred_neg\n      <dbl>     <dbl>\n1     0.820    0.180 \n2     0.352    0.648 \n3     0.150    0.850 \n4     0.932    0.0683\n5     0.875    0.125 \n\nVoorbereiding van de uiteindelijke gegevens voor de evaluatie van het model\nDe volgende stap is het voorbereiden van een gegevensframe dat de kolom diabetes uit de oorspronkelijke testdataset, de voorspelde klasse en de klassevoorspellingswaarschijnlijkheid bevat. We gaan dit dataframe gebruiken voor de evaluatie van het model.\n\n   diabetes .pred_class .pred_pos  .pred_neg\n14      pos         pos 0.8197657 0.18023425\n17      pos         neg 0.3520320 0.64796802\n36      neg         neg 0.1499556 0.85004441\n44      pos         pos 0.9317037 0.06829634\n54      pos         pos 0.8752335 0.12476646\n\nModelevaluatie\nConfusiematrix\nWe kunnen een confusiematrix genereren met de conf_mat( )-functie door het uiteindelijke dataframe, diabetes_results, de waarheidskolom, diabetes en voorspelde klasse (.pred_class) in het schattingsattribuut op te geven.\nUit de confusiematrix blijkt dat de testdataset 65 gevallen van negatieve (neg) en 32 gevallen van positieve (pos) waarnemingen bevat. Het getrainde model classificeert 61 negatieven (neg) en 18 positieven (pos) accuraat.\n\n          Truth\nPrediction pos neg\n       pos  18   4\n       neg  14  61\n\nWe kunnen ook het yardstick pakket gebruiken dat bij het tidymodels pakket hoort om verschillende evaluatie metrieken te genereren voor de testdata set.\nNauwkeurigheid\nWe kunnen de classificatienauwkeurigheid berekenen met de accuracy( )-functie door het uiteindelijke dataframe, diabetes_results, de waarheidskolom, diabetes en voorspelde klasse (.pred_class) in het schattingsattribuut op te geven. De classificatienauwkeurigheid van het model op de testdataset is ongeveer 81,4%.\n\n# A tibble: 1 x 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.814\n\nSensitiviteit\nDe sensitiviteit van een classificator is de verhouding tussen het aantal dat correct als positief wordt geïdentificeerd (TP) en het aantal dat daadwerkelijk positief is (FN+TP).\nSensitivity = TP / FN+TP\nDe geschatte sensitiviteitswaarde is 0,562, wat wijst op een slechte detectie van positieve klassen in de testdataset.\n\n# A tibble: 1 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 sens    binary         0.562\n\nSpecificiteit\nSpecificiteit van een classificator is de verhouding tussen het aantal dat correct als negatief werd geclassificeerd (TN) en het aantal dat werkelijk negatief was (FP+TN).\nSpecificity = TN/FP+TN\nDe geschatte specificiteitswaarde is 0,938, wat wijst op een algemeen goede detectie van negatieve klassen in de testdataset.\n\n# A tibble: 1 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 spec    binary         0.938\n\nPrecisie\nHoeveel van alle positieven werden correct als positief geclassificeerd?\nPrecisie = TP/TP+FP\nDe geschatte precisie waarde is 0.818.\n\n# A tibble: 1 x 3\n  .metric   .estimator .estimate\n  <chr>     <chr>          <dbl>\n1 precision binary         0.818\n\nRecall\nRecall en sensitiviteit zijn hetzelfde.\nRecall = TP / FN+TP\nDe geschatte recall-waarde is 0.562.\n\n# A tibble: 1 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 recall  binary         0.562\n\nF-maat\nF-maat is een gewogen harmonisch gemiddelde van precisie en recall met de beste score 1 en de slechtste score 0. De F-maatscore geeft het evenwicht tussen precisie en recall weer. De F1-score is ongeveer 0,667, wat betekent dat het getrainde model een classificatiekracht van 66,7% heeft.\n\n# A tibble: 1 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 f_meas  binary         0.667\n\nKappa\nCohen Kappa geeft informatie over hoeveel beter een model is dan de willekeurige classificator. Kappa kan gaan van -1 tot +1. De waarde <0 betekent geen overeenstemming, terwijl 1,0 een perfecte overeenstemming aangeeft. Uit de geschatte kappastatistieken bleek een matige overeenkomst.\n\n# A tibble: 1 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 kap     binary         0.544\n\nMatthews Correlatie Coefficient (MCC)\nDe Matthews correlatiecoëfficiënt (MCC) wordt gebruikt als maatstaf voor de kwaliteit van een binaire classificator. De waarde varieert van -1 tot +1.\nMCC: -1 wijst op totale onenigheid MCC: 0 wijst op geen overeenstemming MCC: +1 wijst op totale overeenstemming\nUit de geschatte MCC-statistieken bleek een matige overeenstemming.\n\n# A tibble: 1 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 mcc     binary         0.562\n\nEvaluatiematen genereren\nWe kunnen de custom_metrics( )-functie gebruiken om verschillende metrieken tegelijk te genereren.\nStap 1: laat eerst zien wat je wilt laten zien door metric_set( ) te gebruiken Step 2: gebruik decustom_metrics( ) functie en betrek dit op de diabetes_results dataframe, diabaets kolom en op de voorspelde klasse (.pred_class).\n\n# A tibble: 8 x 3\n  .metric   .estimator .estimate\n  <chr>     <chr>          <dbl>\n1 accuracy  binary         0.814\n2 sens      binary         0.562\n3 spec      binary         0.938\n4 precision binary         0.818\n5 recall    binary         0.562\n6 f_meas    binary         0.667\n7 kap       binary         0.544\n8 mcc       binary         0.562\n\nROC-AUC\nROC-AUC is a performance measurement for the classification problem at various thresholds settings. ROC_AUC tells how much the model is capable of distinguishing between classes. The trained logistic regression model has a ROC-AUC of 0.921 indicating overall good predictive performance.\n\n# A tibble: 1 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.921\n\nROC-curve\nROC-AUC is een evaluatiemaat voor het classificatieprobleem bij verschillende drempelinstellingen. ROC-AUC geeft aan in welke mate het model in staat is een onderscheid te maken tussen de klassen. Het getrainde logistische regressiemodel heeft een ROC-AUC van 0,921, wat wijst op een algemeen goede voorspellende prestatie.\nDe ROC-curve wordt uitgezet met TPR (Sensitiviteit) tegen de FPR/ (1- Specificiteit), waarbij Sensitiviteit op de y-as staat en 1-Specificiteit op de x-as. Een lijn wordt diagonaal getrokken om de 50-50 verdeling van de grafiek aan te geven. Als de kromme dichter bij de lijn ligt, is de prestatie van de classificeerder lager en dan niet beter dan een toevallige gok.\nJe kunt een ROC Curve genereren met de roc_curve( ) functie waarbij je de waarheidskolom (diabetes) en de voorspelde kansen voor de positieve klasse (.pred_pos) moet opgeven.\nOns model heeft een ROC-AUC score van 0.921 wat aangeeft dat het een goed model is dat onderscheid kan maken tussen patiënten met diabetes en zonder diabetes.\n\n\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction pos neg\n       pos  18   4\n       neg  14  61\n                                          \n               Accuracy : 0.8144          \n                 95% CI : (0.7227, 0.8862)\n    No Information Rate : 0.6701          \n    P-Value [Acc > NIR] : 0.001173        \n                                          \n                  Kappa : 0.5441          \n                                          \n Mcnemar's Test P-Value : 0.033895        \n                                          \n            Sensitivity : 0.5625          \n            Specificity : 0.9385          \n         Pos Pred Value : 0.8182          \n         Neg Pred Value : 0.8133          \n             Prevalence : 0.3299          \n         Detection Rate : 0.1856          \n   Detection Prevalence : 0.2268          \n      Balanced Accuracy : 0.7505          \n                                          \n       'Positive' Class : pos             \n                                          \n\nBinaire logistische regressie is nog steeds een enorm populair ML-algoritme (voor binaire classificatie) in het bèta/technische onderzoeksdomein. Het is nog steeds zeer eenvoudig te trainen en te interpreteren, in vergelijking met veel complexere modellen.\nReferenties\nNewman, C. B. D. & Merz, C. (1998). UCI Repository of machine learning databases, Technical report, University of California, Irvine, Dept. of Information and Computer Sciences.\nShrikant I. Bangdiwala (2018). Regression: binary logistic, International Journal of Injury Control and Safety Promotion, DOI: 10.1080/17457300.2018.1486503\n\n\n\n",
    "preview": "posts/2021-07-18-classificeren-met-tidymodels/classificeren-met-tidymodels_files/figure-html5/unnamed-chunk-25-1.png",
    "last_modified": "2021-07-18T21:37:04+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-22-tidymodels-opnieuw/",
    "title": "Tidymodels opnieuw",
    "description": "Blog van Rebecca Barter onder de titel 'Tidymodels: tidy machine learning in R'",
    "author": [
      {
        "name": "Rebecca Barter, bewerking Harrie Jonkman",
        "url": {}
      }
    ],
    "date": "2021-04-22",
    "categories": [],
    "contents": "\nTidymodels: een nette en consistente manier om met machine learning in R te werken\nTidyverse is misschien wel een van de grootste successen van R de laatste jaren. Het is een basispakket (een suite van pakketten) waarmee je heel veel statistiscche bewerkingen goed en betrekkelijk eenvoudig kunt uitvoeren. De laatste jaren is tidymodels ontwikkeld dat voor het modelleren van data het basispakket moet worden en het ontwikkelt zich vergelijkbaar de gereedschapskist van tidyverse maar dan op het gebied van machine learning.\nWaarom tidymodels? Nou, het blijkt dat R een consistentieprobleem heeft. Omdat alles rondom machine learning door verschillende mensen is gemaakt, allemaal met verschillende principes, heeft alles een net iets andere interface gekregenen. Om de boel in lijn te houden is onderhand een frustrerende bezigheid. Enkele jaren geleden ontwikkelde Max Kuhn (nu bij RStudio in dienst) het caret R-pakket, dat is zo’n uniforme interface voor een groot aantal machine learning-modellen die er in R zijn. Het programma caret bestaat nog steeds, was in veel opzichten geweldig en is nog steeds goed te gebruiken. Maar in andere opzichten is het beperkt. Zo kan het vrij traag zijn, zelfs bij gebuik van data in bescheiden omvang.\ncaret was een geweldig uitgangspunt, dus RStudio heeft Max Kuhn ingehuurd om te werken aan een tidy versie van caret. Hij en veel anderen ontwikkelden de afgelopen jaren tidymodels.tidymodels is al een paar jaar in ontwikkeling en delen ervan waren al eerder uitgebracht. Die volledige versie is in het voorjaar van 2020 gepresenteerd en Barter schreef vlak daarvoor deze tutoriol. Ondertussen is het voldoende ontwikkeld als je het wil leren! Terwijl caret niet verder ontwikkeld wordt (je kunt caret blijven gebruiken en je bestaande caret-code werkt nog steeds, het pakket wordt alleen niet onderhouden), zal tidymodels het uiteindelijk overbodig maken.\nDeze tutorial van Barter is gebaseerd op Alison Hill’s dia’s van Introduction to Machine Learning with the Tidyverse, die alle dia’s bevat voor de cursus die ze met Garrett Grolemund voor RStudio heeft voorbereid::conf(2020), en Edgar Ruiz’s Gentle introduction to tidymodels op de website van RStudio. In deze tutorial gaat zij ervan uit dat de gebruiker bepaalde basiskennis heeft, voornamelijk omgaan met dplyr (b.v. piping %>% en een functie zoals mutate()).\nWat is tidymodels?\nNet als tidyverse, dat uit verschillende pakketten bestaat zoals ggplot2 en dplyr, zitten er ook in tidymodels enkele kernpakketten, zoals\nrsample: voor het uit elkaar halen van een datasample (b.v. train/test of cross-validatie);\nrecipes: voor pre-procesfuncties;\nparsnip: voor het specificeren van het model;\nyardstick: voor het evalueren van van het model;\ntune: voor het afstemmen van parameters;\nworkflow: om alles samen te brengen.\nNet zoals je de hele suite aan pakketten van tidyverse kunt binnenhalen door library(tidyverse) in te tikken. tidymodels bestaat dus uit verschillende pakketten en soms zal ik hieronder individuele pakketten noemen.\nEerst maar eens de boel klaarzetten\nAls je deze pakketten nog niet hebt geïnstalleerd, moet je dat wel eerst doen (slechts één keer) door install.packages(\"tidymodels\") te gebruiken. Vervolgens laad je bepaalde bibliotheken: tidymodels en tidyverse.\n\n\n\n\nRows: 768\nColumns: 9\n$ pregnant <dbl> 6, 1, 8, 1, 0, 5, 3, 10, 2, 8, 4, 10, 10, 1, 5, 7, …\n$ glucose  <dbl> 148, 85, 183, 89, 137, 116, 78, 115, 197, 125, 110,…\n$ pressure <dbl> 72, 66, 64, 66, 40, 74, 50, 0, 70, 96, 92, 74, 80, …\n$ triceps  <dbl> 35, 29, 0, 23, 35, 0, 32, 0, 45, 0, 0, 0, 0, 23, 19…\n$ insulin  <dbl> 0, 0, 0, 94, 168, 0, 88, 0, 543, 0, 0, 0, 0, 846, 1…\n$ mass     <dbl> 33.6, 26.6, 23.3, 28.1, 43.1, 25.6, 31.0, 35.3, 30.…\n$ pedigree <dbl> 0.627, 0.351, 0.672, 0.167, 2.288, 0.201, 0.248, 0.…\n$ age      <dbl> 50, 31, 32, 21, 33, 30, 26, 29, 53, 54, 30, 34, 57,…\n$ diabetes <fct> pos, neg, pos, neg, pos, neg, pos, neg, pos, pos, n…\n\nWe zullen gebruik maken van de Pima Indian Women’s diabetes-dataset dat informatie bevat over de diabetes status van 768 Pima Indian vrouwen(diabetes). In de dataset zitten daarnaast enkele predictoren zoals het aantal zwangerschappen (pregnant), concentratie glucose (glucose), diastolische bloeddruk (pressure), triceps huidplooidikte (triceps), 2 uur serum insuline (insuline), BMI (mass), diabetes stamboom functie (pedigree) en hun leeftijd (age). Voor het geval je het je afvraagt, de Pima Indianen zijn een groep indianen die leven in een gebied dat bestaat uit wat nu centraal en zuidelijk Arizona is. De korte naam “Pima” zou afkomstig zijn van een zinsnede die “ik weet het niet” betekent, die ze herhaaldelijk gebruikten in hun eerste ontmoetingen met Spaanse kolonisten. Wikipedia bedankt!\n\n\n\n\n    pregnant glucose pressure triceps insulin mass pedigree age\n1          6     148       72      35       0 33.6    0.627  50\n2          1      85       66      29       0 26.6    0.351  31\n3          8     183       64       0       0 23.3    0.672  32\n4          1      89       66      23      94 28.1    0.167  21\n5          0     137       40      35     168 43.1    2.288  33\n6          5     116       74       0       0 25.6    0.201  30\n7          3      78       50      32      88 31.0    0.248  26\n8         10     115        0       0       0 35.3    0.134  29\n9          2     197       70      45     543 30.5    0.158  53\n10         8     125       96       0       0  0.0    0.232  54\n11         4     110       92       0       0 37.6    0.191  30\n12        10     168       74       0       0 38.0    0.537  34\n13        10     139       80       0       0 27.1    1.441  57\n14         1     189       60      23     846 30.1    0.398  59\n15         5     166       72      19     175 25.8    0.587  51\n16         7     100        0       0       0 30.0    0.484  32\n17         0     118       84      47     230 45.8    0.551  31\n18         7     107       74       0       0 29.6    0.254  31\n19         1     103       30      38      83 43.3    0.183  33\n20         1     115       70      30      96 34.6    0.529  32\n21         3     126       88      41     235 39.3    0.704  27\n22         8      99       84       0       0 35.4    0.388  50\n23         7     196       90       0       0 39.8    0.451  41\n24         9     119       80      35       0 29.0    0.263  29\n25        11     143       94      33     146 36.6    0.254  51\n26        10     125       70      26     115 31.1    0.205  41\n27         7     147       76       0       0 39.4    0.257  43\n28         1      97       66      15     140 23.2    0.487  22\n29        13     145       82      19     110 22.2    0.245  57\n30         5     117       92       0       0 34.1    0.337  38\n31         5     109       75      26       0 36.0    0.546  60\n32         3     158       76      36     245 31.6    0.851  28\n33         3      88       58      11      54 24.8    0.267  22\n34         6      92       92       0       0 19.9    0.188  28\n35        10     122       78      31       0 27.6    0.512  45\n36         4     103       60      33     192 24.0    0.966  33\n37        11     138       76       0       0 33.2    0.420  35\n38         9     102       76      37       0 32.9    0.665  46\n39         2      90       68      42       0 38.2    0.503  27\n40         4     111       72      47     207 37.1    1.390  56\n41         3     180       64      25      70 34.0    0.271  26\n42         7     133       84       0       0 40.2    0.696  37\n43         7     106       92      18       0 22.7    0.235  48\n44         9     171      110      24     240 45.4    0.721  54\n45         7     159       64       0       0 27.4    0.294  40\n46         0     180       66      39       0 42.0    1.893  25\n47         1     146       56       0       0 29.7    0.564  29\n48         2      71       70      27       0 28.0    0.586  22\n49         7     103       66      32       0 39.1    0.344  31\n50         7     105        0       0       0  0.0    0.305  24\n51         1     103       80      11      82 19.4    0.491  22\n52         1     101       50      15      36 24.2    0.526  26\n53         5      88       66      21      23 24.4    0.342  30\n54         8     176       90      34     300 33.7    0.467  58\n55         7     150       66      42     342 34.7    0.718  42\n56         1      73       50      10       0 23.0    0.248  21\n57         7     187       68      39     304 37.7    0.254  41\n58         0     100       88      60     110 46.8    0.962  31\n59         0     146       82       0       0 40.5    1.781  44\n60         0     105       64      41     142 41.5    0.173  22\n61         2      84        0       0       0  0.0    0.304  21\n62         8     133       72       0       0 32.9    0.270  39\n63         5      44       62       0       0 25.0    0.587  36\n64         2     141       58      34     128 25.4    0.699  24\n65         7     114       66       0       0 32.8    0.258  42\n66         5      99       74      27       0 29.0    0.203  32\n67         0     109       88      30       0 32.5    0.855  38\n68         2     109       92       0       0 42.7    0.845  54\n69         1      95       66      13      38 19.6    0.334  25\n70         4     146       85      27     100 28.9    0.189  27\n71         2     100       66      20      90 32.9    0.867  28\n72         5     139       64      35     140 28.6    0.411  26\n73        13     126       90       0       0 43.4    0.583  42\n74         4     129       86      20     270 35.1    0.231  23\n75         1      79       75      30       0 32.0    0.396  22\n76         1       0       48      20       0 24.7    0.140  22\n77         7      62       78       0       0 32.6    0.391  41\n78         5      95       72      33       0 37.7    0.370  27\n79         0     131        0       0       0 43.2    0.270  26\n80         2     112       66      22       0 25.0    0.307  24\n81         3     113       44      13       0 22.4    0.140  22\n82         2      74        0       0       0  0.0    0.102  22\n83         7      83       78      26      71 29.3    0.767  36\n84         0     101       65      28       0 24.6    0.237  22\n85         5     137      108       0       0 48.8    0.227  37\n86         2     110       74      29     125 32.4    0.698  27\n87        13     106       72      54       0 36.6    0.178  45\n88         2     100       68      25      71 38.5    0.324  26\n89        15     136       70      32     110 37.1    0.153  43\n90         1     107       68      19       0 26.5    0.165  24\n91         1      80       55       0       0 19.1    0.258  21\n92         4     123       80      15     176 32.0    0.443  34\n93         7      81       78      40      48 46.7    0.261  42\n94         4     134       72       0       0 23.8    0.277  60\n95         2     142       82      18      64 24.7    0.761  21\n96         6     144       72      27     228 33.9    0.255  40\n97         2      92       62      28       0 31.6    0.130  24\n98         1      71       48      18      76 20.4    0.323  22\n99         6      93       50      30      64 28.7    0.356  23\n100        1     122       90      51     220 49.7    0.325  31\n101        1     163       72       0       0 39.0    1.222  33\n102        1     151       60       0       0 26.1    0.179  22\n103        0     125       96       0       0 22.5    0.262  21\n104        1      81       72      18      40 26.6    0.283  24\n105        2      85       65       0       0 39.6    0.930  27\n106        1     126       56      29     152 28.7    0.801  21\n107        1      96      122       0       0 22.4    0.207  27\n108        4     144       58      28     140 29.5    0.287  37\n109        3      83       58      31      18 34.3    0.336  25\n110        0      95       85      25      36 37.4    0.247  24\n111        3     171       72      33     135 33.3    0.199  24\n112        8     155       62      26     495 34.0    0.543  46\n113        1      89       76      34      37 31.2    0.192  23\n114        4      76       62       0       0 34.0    0.391  25\n115        7     160       54      32     175 30.5    0.588  39\n116        4     146       92       0       0 31.2    0.539  61\n117        5     124       74       0       0 34.0    0.220  38\n118        5      78       48       0       0 33.7    0.654  25\n119        4      97       60      23       0 28.2    0.443  22\n120        4      99       76      15      51 23.2    0.223  21\n121        0     162       76      56     100 53.2    0.759  25\n122        6     111       64      39       0 34.2    0.260  24\n123        2     107       74      30     100 33.6    0.404  23\n124        5     132       80       0       0 26.8    0.186  69\n125        0     113       76       0       0 33.3    0.278  23\n126        1      88       30      42      99 55.0    0.496  26\n127        3     120       70      30     135 42.9    0.452  30\n128        1     118       58      36      94 33.3    0.261  23\n129        1     117       88      24     145 34.5    0.403  40\n130        0     105       84       0       0 27.9    0.741  62\n131        4     173       70      14     168 29.7    0.361  33\n132        9     122       56       0       0 33.3    1.114  33\n133        3     170       64      37     225 34.5    0.356  30\n134        8      84       74      31       0 38.3    0.457  39\n135        2      96       68      13      49 21.1    0.647  26\n136        2     125       60      20     140 33.8    0.088  31\n137        0     100       70      26      50 30.8    0.597  21\n138        0      93       60      25      92 28.7    0.532  22\n139        0     129       80       0       0 31.2    0.703  29\n140        5     105       72      29     325 36.9    0.159  28\n141        3     128       78       0       0 21.1    0.268  55\n142        5     106       82      30       0 39.5    0.286  38\n143        2     108       52      26      63 32.5    0.318  22\n144       10     108       66       0       0 32.4    0.272  42\n145        4     154       62      31     284 32.8    0.237  23\n146        0     102       75      23       0  0.0    0.572  21\n147        9      57       80      37       0 32.8    0.096  41\n148        2     106       64      35     119 30.5    1.400  34\n149        5     147       78       0       0 33.7    0.218  65\n150        2      90       70      17       0 27.3    0.085  22\n151        1     136       74      50     204 37.4    0.399  24\n152        4     114       65       0       0 21.9    0.432  37\n153        9     156       86      28     155 34.3    1.189  42\n154        1     153       82      42     485 40.6    0.687  23\n155        8     188       78       0       0 47.9    0.137  43\n156        7     152       88      44       0 50.0    0.337  36\n157        2      99       52      15      94 24.6    0.637  21\n158        1     109       56      21     135 25.2    0.833  23\n159        2      88       74      19      53 29.0    0.229  22\n160       17     163       72      41     114 40.9    0.817  47\n161        4     151       90      38       0 29.7    0.294  36\n162        7     102       74      40     105 37.2    0.204  45\n163        0     114       80      34     285 44.2    0.167  27\n164        2     100       64      23       0 29.7    0.368  21\n165        0     131       88       0       0 31.6    0.743  32\n166        6     104       74      18     156 29.9    0.722  41\n167        3     148       66      25       0 32.5    0.256  22\n168        4     120       68       0       0 29.6    0.709  34\n169        4     110       66       0       0 31.9    0.471  29\n170        3     111       90      12      78 28.4    0.495  29\n171        6     102       82       0       0 30.8    0.180  36\n172        6     134       70      23     130 35.4    0.542  29\n173        2      87        0      23       0 28.9    0.773  25\n174        1      79       60      42      48 43.5    0.678  23\n175        2      75       64      24      55 29.7    0.370  33\n176        8     179       72      42     130 32.7    0.719  36\n177        6      85       78       0       0 31.2    0.382  42\n178        0     129      110      46     130 67.1    0.319  26\n179        5     143       78       0       0 45.0    0.190  47\n180        5     130       82       0       0 39.1    0.956  37\n181        6      87       80       0       0 23.2    0.084  32\n182        0     119       64      18      92 34.9    0.725  23\n183        1       0       74      20      23 27.7    0.299  21\n184        5      73       60       0       0 26.8    0.268  27\n185        4     141       74       0       0 27.6    0.244  40\n186        7     194       68      28       0 35.9    0.745  41\n187        8     181       68      36     495 30.1    0.615  60\n188        1     128       98      41      58 32.0    1.321  33\n189        8     109       76      39     114 27.9    0.640  31\n190        5     139       80      35     160 31.6    0.361  25\n191        3     111       62       0       0 22.6    0.142  21\n192        9     123       70      44      94 33.1    0.374  40\n193        7     159       66       0       0 30.4    0.383  36\n194       11     135        0       0       0 52.3    0.578  40\n195        8      85       55      20       0 24.4    0.136  42\n196        5     158       84      41     210 39.4    0.395  29\n197        1     105       58       0       0 24.3    0.187  21\n198        3     107       62      13      48 22.9    0.678  23\n199        4     109       64      44      99 34.8    0.905  26\n200        4     148       60      27     318 30.9    0.150  29\n201        0     113       80      16       0 31.0    0.874  21\n202        1     138       82       0       0 40.1    0.236  28\n203        0     108       68      20       0 27.3    0.787  32\n204        2      99       70      16      44 20.4    0.235  27\n205        6     103       72      32     190 37.7    0.324  55\n206        5     111       72      28       0 23.9    0.407  27\n207        8     196       76      29     280 37.5    0.605  57\n208        5     162      104       0       0 37.7    0.151  52\n209        1      96       64      27      87 33.2    0.289  21\n210        7     184       84      33       0 35.5    0.355  41\n211        2      81       60      22       0 27.7    0.290  25\n212        0     147       85      54       0 42.8    0.375  24\n213        7     179       95      31       0 34.2    0.164  60\n214        0     140       65      26     130 42.6    0.431  24\n215        9     112       82      32     175 34.2    0.260  36\n216       12     151       70      40     271 41.8    0.742  38\n217        5     109       62      41     129 35.8    0.514  25\n218        6     125       68      30     120 30.0    0.464  32\n219        5      85       74      22       0 29.0    1.224  32\n220        5     112       66       0       0 37.8    0.261  41\n221        0     177       60      29     478 34.6    1.072  21\n222        2     158       90       0       0 31.6    0.805  66\n223        7     119        0       0       0 25.2    0.209  37\n224        7     142       60      33     190 28.8    0.687  61\n225        1     100       66      15      56 23.6    0.666  26\n226        1      87       78      27      32 34.6    0.101  22\n227        0     101       76       0       0 35.7    0.198  26\n228        3     162       52      38       0 37.2    0.652  24\n229        4     197       70      39     744 36.7    2.329  31\n230        0     117       80      31      53 45.2    0.089  24\n231        4     142       86       0       0 44.0    0.645  22\n232        6     134       80      37     370 46.2    0.238  46\n233        1      79       80      25      37 25.4    0.583  22\n234        4     122       68       0       0 35.0    0.394  29\n235        3      74       68      28      45 29.7    0.293  23\n236        4     171       72       0       0 43.6    0.479  26\n237        7     181       84      21     192 35.9    0.586  51\n238        0     179       90      27       0 44.1    0.686  23\n239        9     164       84      21       0 30.8    0.831  32\n240        0     104       76       0       0 18.4    0.582  27\n241        1      91       64      24       0 29.2    0.192  21\n242        4      91       70      32      88 33.1    0.446  22\n243        3     139       54       0       0 25.6    0.402  22\n244        6     119       50      22     176 27.1    1.318  33\n245        2     146       76      35     194 38.2    0.329  29\n246        9     184       85      15       0 30.0    1.213  49\n247       10     122       68       0       0 31.2    0.258  41\n248        0     165       90      33     680 52.3    0.427  23\n249        9     124       70      33     402 35.4    0.282  34\n250        1     111       86      19       0 30.1    0.143  23\n251        9     106       52       0       0 31.2    0.380  42\n252        2     129       84       0       0 28.0    0.284  27\n253        2      90       80      14      55 24.4    0.249  24\n254        0      86       68      32       0 35.8    0.238  25\n255       12      92       62       7     258 27.6    0.926  44\n256        1     113       64      35       0 33.6    0.543  21\n257        3     111       56      39       0 30.1    0.557  30\n258        2     114       68      22       0 28.7    0.092  25\n259        1     193       50      16     375 25.9    0.655  24\n260       11     155       76      28     150 33.3    1.353  51\n261        3     191       68      15     130 30.9    0.299  34\n262        3     141        0       0       0 30.0    0.761  27\n263        4      95       70      32       0 32.1    0.612  24\n264        3     142       80      15       0 32.4    0.200  63\n265        4     123       62       0       0 32.0    0.226  35\n266        5      96       74      18      67 33.6    0.997  43\n267        0     138        0       0       0 36.3    0.933  25\n268        2     128       64      42       0 40.0    1.101  24\n269        0     102       52       0       0 25.1    0.078  21\n270        2     146        0       0       0 27.5    0.240  28\n271       10     101       86      37       0 45.6    1.136  38\n272        2     108       62      32      56 25.2    0.128  21\n273        3     122       78       0       0 23.0    0.254  40\n274        1      71       78      50      45 33.2    0.422  21\n275       13     106       70       0       0 34.2    0.251  52\n276        2     100       70      52      57 40.5    0.677  25\n277        7     106       60      24       0 26.5    0.296  29\n278        0     104       64      23     116 27.8    0.454  23\n279        5     114       74       0       0 24.9    0.744  57\n280        2     108       62      10     278 25.3    0.881  22\n281        0     146       70       0       0 37.9    0.334  28\n282       10     129       76      28     122 35.9    0.280  39\n283        7     133       88      15     155 32.4    0.262  37\n284        7     161       86       0       0 30.4    0.165  47\n285        2     108       80       0       0 27.0    0.259  52\n286        7     136       74      26     135 26.0    0.647  51\n287        5     155       84      44     545 38.7    0.619  34\n288        1     119       86      39     220 45.6    0.808  29\n289        4      96       56      17      49 20.8    0.340  26\n290        5     108       72      43      75 36.1    0.263  33\n291        0      78       88      29      40 36.9    0.434  21\n292        0     107       62      30      74 36.6    0.757  25\n293        2     128       78      37     182 43.3    1.224  31\n294        1     128       48      45     194 40.5    0.613  24\n295        0     161       50       0       0 21.9    0.254  65\n296        6     151       62      31     120 35.5    0.692  28\n297        2     146       70      38     360 28.0    0.337  29\n298        0     126       84      29     215 30.7    0.520  24\n299       14     100       78      25     184 36.6    0.412  46\n300        8     112       72       0       0 23.6    0.840  58\n301        0     167        0       0       0 32.3    0.839  30\n302        2     144       58      33     135 31.6    0.422  25\n303        5      77       82      41      42 35.8    0.156  35\n304        5     115       98       0       0 52.9    0.209  28\n305        3     150       76       0       0 21.0    0.207  37\n306        2     120       76      37     105 39.7    0.215  29\n307       10     161       68      23     132 25.5    0.326  47\n308        0     137       68      14     148 24.8    0.143  21\n309        0     128       68      19     180 30.5    1.391  25\n310        2     124       68      28     205 32.9    0.875  30\n311        6      80       66      30       0 26.2    0.313  41\n312        0     106       70      37     148 39.4    0.605  22\n313        2     155       74      17      96 26.6    0.433  27\n314        3     113       50      10      85 29.5    0.626  25\n315        7     109       80      31       0 35.9    1.127  43\n316        2     112       68      22      94 34.1    0.315  26\n317        3      99       80      11      64 19.3    0.284  30\n318        3     182       74       0       0 30.5    0.345  29\n319        3     115       66      39     140 38.1    0.150  28\n320        6     194       78       0       0 23.5    0.129  59\n321        4     129       60      12     231 27.5    0.527  31\n322        3     112       74      30       0 31.6    0.197  25\n323        0     124       70      20       0 27.4    0.254  36\n324       13     152       90      33      29 26.8    0.731  43\n325        2     112       75      32       0 35.7    0.148  21\n326        1     157       72      21     168 25.6    0.123  24\n327        1     122       64      32     156 35.1    0.692  30\n328       10     179       70       0       0 35.1    0.200  37\n329        2     102       86      36     120 45.5    0.127  23\n330        6     105       70      32      68 30.8    0.122  37\n331        8     118       72      19       0 23.1    1.476  46\n332        2      87       58      16      52 32.7    0.166  25\n333        1     180        0       0       0 43.3    0.282  41\n334       12     106       80       0       0 23.6    0.137  44\n335        1      95       60      18      58 23.9    0.260  22\n336        0     165       76      43     255 47.9    0.259  26\n337        0     117        0       0       0 33.8    0.932  44\n338        5     115       76       0       0 31.2    0.343  44\n339        9     152       78      34     171 34.2    0.893  33\n340        7     178       84       0       0 39.9    0.331  41\n341        1     130       70      13     105 25.9    0.472  22\n342        1      95       74      21      73 25.9    0.673  36\n343        1       0       68      35       0 32.0    0.389  22\n344        5     122       86       0       0 34.7    0.290  33\n345        8      95       72       0       0 36.8    0.485  57\n346        8     126       88      36     108 38.5    0.349  49\n347        1     139       46      19      83 28.7    0.654  22\n348        3     116        0       0       0 23.5    0.187  23\n349        3      99       62      19      74 21.8    0.279  26\n350        5       0       80      32       0 41.0    0.346  37\n351        4      92       80       0       0 42.2    0.237  29\n352        4     137       84       0       0 31.2    0.252  30\n353        3      61       82      28       0 34.4    0.243  46\n354        1      90       62      12      43 27.2    0.580  24\n355        3      90       78       0       0 42.7    0.559  21\n356        9     165       88       0       0 30.4    0.302  49\n357        1     125       50      40     167 33.3    0.962  28\n358       13     129        0      30       0 39.9    0.569  44\n359       12      88       74      40      54 35.3    0.378  48\n360        1     196       76      36     249 36.5    0.875  29\n361        5     189       64      33     325 31.2    0.583  29\n362        5     158       70       0       0 29.8    0.207  63\n363        5     103      108      37       0 39.2    0.305  65\n364        4     146       78       0       0 38.5    0.520  67\n365        4     147       74      25     293 34.9    0.385  30\n366        5      99       54      28      83 34.0    0.499  30\n367        6     124       72       0       0 27.6    0.368  29\n368        0     101       64      17       0 21.0    0.252  21\n369        3      81       86      16      66 27.5    0.306  22\n370        1     133      102      28     140 32.8    0.234  45\n371        3     173       82      48     465 38.4    2.137  25\n372        0     118       64      23      89  0.0    1.731  21\n373        0      84       64      22      66 35.8    0.545  21\n374        2     105       58      40      94 34.9    0.225  25\n375        2     122       52      43     158 36.2    0.816  28\n376       12     140       82      43     325 39.2    0.528  58\n377        0      98       82      15      84 25.2    0.299  22\n378        1      87       60      37      75 37.2    0.509  22\n379        4     156       75       0       0 48.3    0.238  32\n380        0      93      100      39      72 43.4    1.021  35\n381        1     107       72      30      82 30.8    0.821  24\n382        0     105       68      22       0 20.0    0.236  22\n383        1     109       60       8     182 25.4    0.947  21\n384        1      90       62      18      59 25.1    1.268  25\n385        1     125       70      24     110 24.3    0.221  25\n386        1     119       54      13      50 22.3    0.205  24\n387        5     116       74      29       0 32.3    0.660  35\n388        8     105      100      36       0 43.3    0.239  45\n389        5     144       82      26     285 32.0    0.452  58\n390        3     100       68      23      81 31.6    0.949  28\n391        1     100       66      29     196 32.0    0.444  42\n392        5     166       76       0       0 45.7    0.340  27\n393        1     131       64      14     415 23.7    0.389  21\n394        4     116       72      12      87 22.1    0.463  37\n395        4     158       78       0       0 32.9    0.803  31\n396        2     127       58      24     275 27.7    1.600  25\n397        3      96       56      34     115 24.7    0.944  39\n398        0     131       66      40       0 34.3    0.196  22\n399        3      82       70       0       0 21.1    0.389  25\n400        3     193       70      31       0 34.9    0.241  25\n401        4      95       64       0       0 32.0    0.161  31\n402        6     137       61       0       0 24.2    0.151  55\n403        5     136       84      41      88 35.0    0.286  35\n404        9      72       78      25       0 31.6    0.280  38\n405        5     168       64       0       0 32.9    0.135  41\n406        2     123       48      32     165 42.1    0.520  26\n407        4     115       72       0       0 28.9    0.376  46\n408        0     101       62       0       0 21.9    0.336  25\n409        8     197       74       0       0 25.9    1.191  39\n410        1     172       68      49     579 42.4    0.702  28\n411        6     102       90      39       0 35.7    0.674  28\n412        1     112       72      30     176 34.4    0.528  25\n413        1     143       84      23     310 42.4    1.076  22\n414        1     143       74      22      61 26.2    0.256  21\n415        0     138       60      35     167 34.6    0.534  21\n416        3     173       84      33     474 35.7    0.258  22\n417        1      97       68      21       0 27.2    1.095  22\n418        4     144       82      32       0 38.5    0.554  37\n419        1      83       68       0       0 18.2    0.624  27\n420        3     129       64      29     115 26.4    0.219  28\n421        1     119       88      41     170 45.3    0.507  26\n422        2      94       68      18      76 26.0    0.561  21\n423        0     102       64      46      78 40.6    0.496  21\n424        2     115       64      22       0 30.8    0.421  21\n425        8     151       78      32     210 42.9    0.516  36\n426        4     184       78      39     277 37.0    0.264  31\n427        0      94        0       0       0  0.0    0.256  25\n428        1     181       64      30     180 34.1    0.328  38\n429        0     135       94      46     145 40.6    0.284  26\n430        1      95       82      25     180 35.0    0.233  43\n431        2      99        0       0       0 22.2    0.108  23\n432        3      89       74      16      85 30.4    0.551  38\n433        1      80       74      11      60 30.0    0.527  22\n434        2     139       75       0       0 25.6    0.167  29\n435        1      90       68       8       0 24.5    1.138  36\n436        0     141        0       0       0 42.4    0.205  29\n437       12     140       85      33       0 37.4    0.244  41\n438        5     147       75       0       0 29.9    0.434  28\n439        1      97       70      15       0 18.2    0.147  21\n440        6     107       88       0       0 36.8    0.727  31\n441        0     189      104      25       0 34.3    0.435  41\n442        2      83       66      23      50 32.2    0.497  22\n443        4     117       64      27     120 33.2    0.230  24\n444        8     108       70       0       0 30.5    0.955  33\n445        4     117       62      12       0 29.7    0.380  30\n446        0     180       78      63      14 59.4    2.420  25\n447        1     100       72      12      70 25.3    0.658  28\n448        0      95       80      45      92 36.5    0.330  26\n449        0     104       64      37      64 33.6    0.510  22\n450        0     120       74      18      63 30.5    0.285  26\n451        1      82       64      13      95 21.2    0.415  23\n452        2     134       70       0       0 28.9    0.542  23\n453        0      91       68      32     210 39.9    0.381  25\n454        2     119        0       0       0 19.6    0.832  72\n455        2     100       54      28     105 37.8    0.498  24\n456       14     175       62      30       0 33.6    0.212  38\n457        1     135       54       0       0 26.7    0.687  62\n458        5      86       68      28      71 30.2    0.364  24\n459       10     148       84      48     237 37.6    1.001  51\n460        9     134       74      33      60 25.9    0.460  81\n461        9     120       72      22      56 20.8    0.733  48\n462        1      71       62       0       0 21.8    0.416  26\n463        8      74       70      40      49 35.3    0.705  39\n464        5      88       78      30       0 27.6    0.258  37\n465       10     115       98       0       0 24.0    1.022  34\n466        0     124       56      13     105 21.8    0.452  21\n467        0      74       52      10      36 27.8    0.269  22\n468        0      97       64      36     100 36.8    0.600  25\n469        8     120        0       0       0 30.0    0.183  38\n470        6     154       78      41     140 46.1    0.571  27\n471        1     144       82      40       0 41.3    0.607  28\n472        0     137       70      38       0 33.2    0.170  22\n473        0     119       66      27       0 38.8    0.259  22\n474        7     136       90       0       0 29.9    0.210  50\n475        4     114       64       0       0 28.9    0.126  24\n476        0     137       84      27       0 27.3    0.231  59\n477        2     105       80      45     191 33.7    0.711  29\n478        7     114       76      17     110 23.8    0.466  31\n479        8     126       74      38      75 25.9    0.162  39\n480        4     132       86      31       0 28.0    0.419  63\n481        3     158       70      30     328 35.5    0.344  35\n482        0     123       88      37       0 35.2    0.197  29\n483        4      85       58      22      49 27.8    0.306  28\n484        0      84       82      31     125 38.2    0.233  23\n485        0     145        0       0       0 44.2    0.630  31\n486        0     135       68      42     250 42.3    0.365  24\n487        1     139       62      41     480 40.7    0.536  21\n488        0     173       78      32     265 46.5    1.159  58\n489        4      99       72      17       0 25.6    0.294  28\n490        8     194       80       0       0 26.1    0.551  67\n491        2      83       65      28      66 36.8    0.629  24\n492        2      89       90      30       0 33.5    0.292  42\n493        4      99       68      38       0 32.8    0.145  33\n494        4     125       70      18     122 28.9    1.144  45\n495        3      80        0       0       0  0.0    0.174  22\n496        6     166       74       0       0 26.6    0.304  66\n497        5     110       68       0       0 26.0    0.292  30\n498        2      81       72      15      76 30.1    0.547  25\n499        7     195       70      33     145 25.1    0.163  55\n500        6     154       74      32     193 29.3    0.839  39\n501        2     117       90      19      71 25.2    0.313  21\n502        3      84       72      32       0 37.2    0.267  28\n503        6       0       68      41       0 39.0    0.727  41\n504        7      94       64      25      79 33.3    0.738  41\n505        3      96       78      39       0 37.3    0.238  40\n506       10      75       82       0       0 33.3    0.263  38\n507        0     180       90      26      90 36.5    0.314  35\n508        1     130       60      23     170 28.6    0.692  21\n509        2      84       50      23      76 30.4    0.968  21\n510        8     120       78       0       0 25.0    0.409  64\n511       12      84       72      31       0 29.7    0.297  46\n512        0     139       62      17     210 22.1    0.207  21\n513        9      91       68       0       0 24.2    0.200  58\n514        2      91       62       0       0 27.3    0.525  22\n515        3      99       54      19      86 25.6    0.154  24\n516        3     163       70      18     105 31.6    0.268  28\n517        9     145       88      34     165 30.3    0.771  53\n518        7     125       86       0       0 37.6    0.304  51\n519       13      76       60       0       0 32.8    0.180  41\n520        6     129       90       7     326 19.6    0.582  60\n521        2      68       70      32      66 25.0    0.187  25\n522        3     124       80      33     130 33.2    0.305  26\n523        6     114        0       0       0  0.0    0.189  26\n524        9     130       70       0       0 34.2    0.652  45\n525        3     125       58       0       0 31.6    0.151  24\n526        3      87       60      18       0 21.8    0.444  21\n527        1      97       64      19      82 18.2    0.299  21\n528        3     116       74      15     105 26.3    0.107  24\n529        0     117       66      31     188 30.8    0.493  22\n530        0     111       65       0       0 24.6    0.660  31\n531        2     122       60      18     106 29.8    0.717  22\n532        0     107       76       0       0 45.3    0.686  24\n533        1      86       66      52      65 41.3    0.917  29\n534        6      91        0       0       0 29.8    0.501  31\n535        1      77       56      30      56 33.3    1.251  24\n536        4     132        0       0       0 32.9    0.302  23\n537        0     105       90       0       0 29.6    0.197  46\n538        0      57       60       0       0 21.7    0.735  67\n539        0     127       80      37     210 36.3    0.804  23\n540        3     129       92      49     155 36.4    0.968  32\n541        8     100       74      40     215 39.4    0.661  43\n542        3     128       72      25     190 32.4    0.549  27\n543       10      90       85      32       0 34.9    0.825  56\n544        4      84       90      23      56 39.5    0.159  25\n545        1      88       78      29      76 32.0    0.365  29\n546        8     186       90      35     225 34.5    0.423  37\n547        5     187       76      27     207 43.6    1.034  53\n548        4     131       68      21     166 33.1    0.160  28\n549        1     164       82      43      67 32.8    0.341  50\n550        4     189      110      31       0 28.5    0.680  37\n551        1     116       70      28       0 27.4    0.204  21\n552        3      84       68      30     106 31.9    0.591  25\n553        6     114       88       0       0 27.8    0.247  66\n554        1      88       62      24      44 29.9    0.422  23\n555        1      84       64      23     115 36.9    0.471  28\n556        7     124       70      33     215 25.5    0.161  37\n557        1      97       70      40       0 38.1    0.218  30\n558        8     110       76       0       0 27.8    0.237  58\n559       11     103       68      40       0 46.2    0.126  42\n560       11      85       74       0       0 30.1    0.300  35\n561        6     125       76       0       0 33.8    0.121  54\n562        0     198       66      32     274 41.3    0.502  28\n563        1      87       68      34      77 37.6    0.401  24\n564        6      99       60      19      54 26.9    0.497  32\n565        0      91       80       0       0 32.4    0.601  27\n566        2      95       54      14      88 26.1    0.748  22\n567        1      99       72      30      18 38.6    0.412  21\n568        6      92       62      32     126 32.0    0.085  46\n569        4     154       72      29     126 31.3    0.338  37\n570        0     121       66      30     165 34.3    0.203  33\n571        3      78       70       0       0 32.5    0.270  39\n572        2     130       96       0       0 22.6    0.268  21\n573        3     111       58      31      44 29.5    0.430  22\n574        2      98       60      17     120 34.7    0.198  22\n575        1     143       86      30     330 30.1    0.892  23\n576        1     119       44      47      63 35.5    0.280  25\n577        6     108       44      20     130 24.0    0.813  35\n578        2     118       80       0       0 42.9    0.693  21\n579       10     133       68       0       0 27.0    0.245  36\n580        2     197       70      99       0 34.7    0.575  62\n581        0     151       90      46       0 42.1    0.371  21\n582        6     109       60      27       0 25.0    0.206  27\n583       12     121       78      17       0 26.5    0.259  62\n584        8     100       76       0       0 38.7    0.190  42\n585        8     124       76      24     600 28.7    0.687  52\n586        1      93       56      11       0 22.5    0.417  22\n587        8     143       66       0       0 34.9    0.129  41\n588        6     103       66       0       0 24.3    0.249  29\n589        3     176       86      27     156 33.3    1.154  52\n590        0      73        0       0       0 21.1    0.342  25\n591       11     111       84      40       0 46.8    0.925  45\n592        2     112       78      50     140 39.4    0.175  24\n593        3     132       80       0       0 34.4    0.402  44\n594        2      82       52      22     115 28.5    1.699  25\n595        6     123       72      45     230 33.6    0.733  34\n596        0     188       82      14     185 32.0    0.682  22\n597        0      67       76       0       0 45.3    0.194  46\n598        1      89       24      19      25 27.8    0.559  21\n599        1     173       74       0       0 36.8    0.088  38\n600        1     109       38      18     120 23.1    0.407  26\n601        1     108       88      19       0 27.1    0.400  24\n602        6      96        0       0       0 23.7    0.190  28\n603        1     124       74      36       0 27.8    0.100  30\n604        7     150       78      29     126 35.2    0.692  54\n605        4     183        0       0       0 28.4    0.212  36\n606        1     124       60      32       0 35.8    0.514  21\n607        1     181       78      42     293 40.0    1.258  22\n608        1      92       62      25      41 19.5    0.482  25\n609        0     152       82      39     272 41.5    0.270  27\n610        1     111       62      13     182 24.0    0.138  23\n611        3     106       54      21     158 30.9    0.292  24\n612        3     174       58      22     194 32.9    0.593  36\n613        7     168       88      42     321 38.2    0.787  40\n614        6     105       80      28       0 32.5    0.878  26\n615       11     138       74      26     144 36.1    0.557  50\n616        3     106       72       0       0 25.8    0.207  27\n617        6     117       96       0       0 28.7    0.157  30\n618        2      68       62      13      15 20.1    0.257  23\n619        9     112       82      24       0 28.2    1.282  50\n620        0     119        0       0       0 32.4    0.141  24\n621        2     112       86      42     160 38.4    0.246  28\n622        2      92       76      20       0 24.2    1.698  28\n623        6     183       94       0       0 40.8    1.461  45\n624        0      94       70      27     115 43.5    0.347  21\n625        2     108       64       0       0 30.8    0.158  21\n626        4      90       88      47      54 37.7    0.362  29\n627        0     125       68       0       0 24.7    0.206  21\n628        0     132       78       0       0 32.4    0.393  21\n629        5     128       80       0       0 34.6    0.144  45\n630        4      94       65      22       0 24.7    0.148  21\n631        7     114       64       0       0 27.4    0.732  34\n632        0     102       78      40      90 34.5    0.238  24\n633        2     111       60       0       0 26.2    0.343  23\n634        1     128       82      17     183 27.5    0.115  22\n635       10      92       62       0       0 25.9    0.167  31\n636       13     104       72       0       0 31.2    0.465  38\n637        5     104       74       0       0 28.8    0.153  48\n638        2      94       76      18      66 31.6    0.649  23\n639        7      97       76      32      91 40.9    0.871  32\n640        1     100       74      12      46 19.5    0.149  28\n641        0     102       86      17     105 29.3    0.695  27\n642        4     128       70       0       0 34.3    0.303  24\n643        6     147       80       0       0 29.5    0.178  50\n644        4      90        0       0       0 28.0    0.610  31\n645        3     103       72      30     152 27.6    0.730  27\n646        2     157       74      35     440 39.4    0.134  30\n647        1     167       74      17     144 23.4    0.447  33\n648        0     179       50      36     159 37.8    0.455  22\n649       11     136       84      35     130 28.3    0.260  42\n650        0     107       60      25       0 26.4    0.133  23\n651        1      91       54      25     100 25.2    0.234  23\n652        1     117       60      23     106 33.8    0.466  27\n653        5     123       74      40      77 34.1    0.269  28\n654        2     120       54       0       0 26.8    0.455  27\n655        1     106       70      28     135 34.2    0.142  22\n656        2     155       52      27     540 38.7    0.240  25\n657        2     101       58      35      90 21.8    0.155  22\n658        1     120       80      48     200 38.9    1.162  41\n659       11     127      106       0       0 39.0    0.190  51\n660        3      80       82      31      70 34.2    1.292  27\n661       10     162       84       0       0 27.7    0.182  54\n662        1     199       76      43       0 42.9    1.394  22\n663        8     167      106      46     231 37.6    0.165  43\n664        9     145       80      46     130 37.9    0.637  40\n665        6     115       60      39       0 33.7    0.245  40\n666        1     112       80      45     132 34.8    0.217  24\n667        4     145       82      18       0 32.5    0.235  70\n668       10     111       70      27       0 27.5    0.141  40\n669        6      98       58      33     190 34.0    0.430  43\n670        9     154       78      30     100 30.9    0.164  45\n671        6     165       68      26     168 33.6    0.631  49\n672        1      99       58      10       0 25.4    0.551  21\n673       10      68      106      23      49 35.5    0.285  47\n674        3     123      100      35     240 57.3    0.880  22\n675        8      91       82       0       0 35.6    0.587  68\n676        6     195       70       0       0 30.9    0.328  31\n677        9     156       86       0       0 24.8    0.230  53\n678        0      93       60       0       0 35.3    0.263  25\n679        3     121       52       0       0 36.0    0.127  25\n680        2     101       58      17     265 24.2    0.614  23\n681        2      56       56      28      45 24.2    0.332  22\n682        0     162       76      36       0 49.6    0.364  26\n683        0      95       64      39     105 44.6    0.366  22\n684        4     125       80       0       0 32.3    0.536  27\n685        5     136       82       0       0  0.0    0.640  69\n686        2     129       74      26     205 33.2    0.591  25\n687        3     130       64       0       0 23.1    0.314  22\n688        1     107       50      19       0 28.3    0.181  29\n689        1     140       74      26     180 24.1    0.828  23\n690        1     144       82      46     180 46.1    0.335  46\n691        8     107       80       0       0 24.6    0.856  34\n692       13     158      114       0       0 42.3    0.257  44\n693        2     121       70      32      95 39.1    0.886  23\n694        7     129       68      49     125 38.5    0.439  43\n695        2      90       60       0       0 23.5    0.191  25\n696        7     142       90      24     480 30.4    0.128  43\n697        3     169       74      19     125 29.9    0.268  31\n698        0      99        0       0       0 25.0    0.253  22\n699        4     127       88      11     155 34.5    0.598  28\n700        4     118       70       0       0 44.5    0.904  26\n701        2     122       76      27     200 35.9    0.483  26\n702        6     125       78      31       0 27.6    0.565  49\n703        1     168       88      29       0 35.0    0.905  52\n704        2     129        0       0       0 38.5    0.304  41\n705        4     110       76      20     100 28.4    0.118  27\n706        6      80       80      36       0 39.8    0.177  28\n707       10     115        0       0       0  0.0    0.261  30\n708        2     127       46      21     335 34.4    0.176  22\n709        9     164       78       0       0 32.8    0.148  45\n710        2      93       64      32     160 38.0    0.674  23\n711        3     158       64      13     387 31.2    0.295  24\n712        5     126       78      27      22 29.6    0.439  40\n713       10     129       62      36       0 41.2    0.441  38\n714        0     134       58      20     291 26.4    0.352  21\n715        3     102       74       0       0 29.5    0.121  32\n716        7     187       50      33     392 33.9    0.826  34\n717        3     173       78      39     185 33.8    0.970  31\n718       10      94       72      18       0 23.1    0.595  56\n719        1     108       60      46     178 35.5    0.415  24\n720        5      97       76      27       0 35.6    0.378  52\n721        4      83       86      19       0 29.3    0.317  34\n722        1     114       66      36     200 38.1    0.289  21\n723        1     149       68      29     127 29.3    0.349  42\n724        5     117       86      30     105 39.1    0.251  42\n725        1     111       94       0       0 32.8    0.265  45\n726        4     112       78      40       0 39.4    0.236  38\n727        1     116       78      29     180 36.1    0.496  25\n728        0     141       84      26       0 32.4    0.433  22\n729        2     175       88       0       0 22.9    0.326  22\n730        2      92       52       0       0 30.1    0.141  22\n731        3     130       78      23      79 28.4    0.323  34\n732        8     120       86       0       0 28.4    0.259  22\n733        2     174       88      37     120 44.5    0.646  24\n734        2     106       56      27     165 29.0    0.426  22\n735        2     105       75       0       0 23.3    0.560  53\n736        4      95       60      32       0 35.4    0.284  28\n737        0     126       86      27     120 27.4    0.515  21\n738        8      65       72      23       0 32.0    0.600  42\n739        2      99       60      17     160 36.6    0.453  21\n740        1     102       74       0       0 39.5    0.293  42\n741       11     120       80      37     150 42.3    0.785  48\n742        3     102       44      20      94 30.8    0.400  26\n743        1     109       58      18     116 28.5    0.219  22\n744        9     140       94       0       0 32.7    0.734  45\n745       13     153       88      37     140 40.6    1.174  39\n746       12     100       84      33     105 30.0    0.488  46\n747        1     147       94      41       0 49.3    0.358  27\n748        1      81       74      41      57 46.3    1.096  32\n749        3     187       70      22     200 36.4    0.408  36\n750        6     162       62       0       0 24.3    0.178  50\n751        4     136       70       0       0 31.2    1.182  22\n752        1     121       78      39      74 39.0    0.261  28\n753        3     108       62      24       0 26.0    0.223  25\n754        0     181       88      44     510 43.3    0.222  26\n755        8     154       78      32       0 32.4    0.443  45\n756        1     128       88      39     110 36.5    1.057  37\n757        7     137       90      41       0 32.0    0.391  39\n758        0     123       72       0       0 36.3    0.258  52\n759        1     106       76       0       0 37.5    0.197  26\n760        6     190       92       0       0 35.5    0.278  66\n761        2      88       58      26      16 28.4    0.766  22\n762        9     170       74      31       0 44.0    0.403  43\n763        9      89       62       0       0 22.5    0.142  33\n764       10     101       76      48     180 32.9    0.171  63\n765        2     122       70      27       0 36.8    0.340  27\n766        5     121       72      23     112 26.2    0.245  30\n767        1     126       60       0       0 30.1    0.349  47\n768        1      93       70      31       0 30.4    0.315  23\n    diabetes\n1        pos\n2        neg\n3        pos\n4        neg\n5        pos\n6        neg\n7        pos\n8        neg\n9        pos\n10       pos\n11       neg\n12       pos\n13       neg\n14       pos\n15       pos\n16       pos\n17       pos\n18       pos\n19       neg\n20       pos\n21       neg\n22       neg\n23       pos\n24       pos\n25       pos\n26       pos\n27       pos\n28       neg\n29       neg\n30       neg\n31       neg\n32       pos\n33       neg\n34       neg\n35       neg\n36       neg\n37       neg\n38       pos\n39       pos\n40       pos\n41       neg\n42       neg\n43       neg\n44       pos\n45       neg\n46       pos\n47       neg\n48       neg\n49       pos\n50       neg\n51       neg\n52       neg\n53       neg\n54       pos\n55       neg\n56       neg\n57       pos\n58       neg\n59       neg\n60       neg\n61       neg\n62       pos\n63       neg\n64       neg\n65       pos\n66       neg\n67       pos\n68       neg\n69       neg\n70       neg\n71       pos\n72       neg\n73       pos\n74       neg\n75       neg\n76       neg\n77       neg\n78       neg\n79       pos\n80       neg\n81       neg\n82       neg\n83       neg\n84       neg\n85       pos\n86       neg\n87       neg\n88       neg\n89       pos\n90       neg\n91       neg\n92       neg\n93       neg\n94       pos\n95       neg\n96       neg\n97       neg\n98       neg\n99       neg\n100      pos\n101      pos\n102      neg\n103      neg\n104      neg\n105      neg\n106      neg\n107      neg\n108      neg\n109      neg\n110      pos\n111      pos\n112      pos\n113      neg\n114      neg\n115      pos\n116      pos\n117      pos\n118      neg\n119      neg\n120      neg\n121      pos\n122      neg\n123      neg\n124      neg\n125      pos\n126      pos\n127      neg\n128      neg\n129      pos\n130      pos\n131      pos\n132      pos\n133      pos\n134      neg\n135      neg\n136      neg\n137      neg\n138      neg\n139      neg\n140      neg\n141      neg\n142      neg\n143      neg\n144      pos\n145      neg\n146      neg\n147      neg\n148      neg\n149      neg\n150      neg\n151      neg\n152      neg\n153      pos\n154      neg\n155      pos\n156      pos\n157      neg\n158      neg\n159      neg\n160      pos\n161      neg\n162      neg\n163      neg\n164      neg\n165      pos\n166      pos\n167      neg\n168      neg\n169      neg\n170      neg\n171      pos\n172      pos\n173      neg\n174      neg\n175      neg\n176      pos\n177      neg\n178      pos\n179      neg\n180      pos\n181      neg\n182      neg\n183      neg\n184      neg\n185      neg\n186      pos\n187      pos\n188      pos\n189      pos\n190      pos\n191      neg\n192      neg\n193      pos\n194      pos\n195      neg\n196      pos\n197      neg\n198      pos\n199      pos\n200      pos\n201      neg\n202      neg\n203      neg\n204      neg\n205      neg\n206      neg\n207      pos\n208      pos\n209      neg\n210      pos\n211      neg\n212      neg\n213      neg\n214      pos\n215      pos\n216      pos\n217      pos\n218      neg\n219      pos\n220      pos\n221      pos\n222      pos\n223      neg\n224      neg\n225      neg\n226      neg\n227      neg\n228      pos\n229      neg\n230      neg\n231      pos\n232      pos\n233      neg\n234      neg\n235      neg\n236      pos\n237      pos\n238      pos\n239      pos\n240      neg\n241      neg\n242      neg\n243      pos\n244      pos\n245      neg\n246      pos\n247      neg\n248      neg\n249      neg\n250      neg\n251      neg\n252      neg\n253      neg\n254      neg\n255      pos\n256      pos\n257      neg\n258      neg\n259      neg\n260      pos\n261      neg\n262      pos\n263      neg\n264      neg\n265      pos\n266      neg\n267      pos\n268      neg\n269      neg\n270      pos\n271      pos\n272      neg\n273      neg\n274      neg\n275      neg\n276      neg\n277      pos\n278      neg\n279      neg\n280      neg\n281      pos\n282      neg\n283      neg\n284      pos\n285      pos\n286      neg\n287      neg\n288      pos\n289      neg\n290      neg\n291      neg\n292      pos\n293      pos\n294      pos\n295      neg\n296      neg\n297      pos\n298      neg\n299      pos\n300      neg\n301      pos\n302      pos\n303      neg\n304      pos\n305      neg\n306      neg\n307      pos\n308      neg\n309      pos\n310      pos\n311      neg\n312      neg\n313      pos\n314      neg\n315      pos\n316      neg\n317      neg\n318      pos\n319      neg\n320      pos\n321      neg\n322      pos\n323      pos\n324      pos\n325      neg\n326      neg\n327      pos\n328      neg\n329      pos\n330      neg\n331      neg\n332      neg\n333      pos\n334      neg\n335      neg\n336      neg\n337      neg\n338      pos\n339      pos\n340      pos\n341      neg\n342      neg\n343      neg\n344      neg\n345      neg\n346      neg\n347      neg\n348      neg\n349      neg\n350      pos\n351      neg\n352      neg\n353      neg\n354      neg\n355      neg\n356      pos\n357      pos\n358      pos\n359      neg\n360      pos\n361      pos\n362      neg\n363      neg\n364      pos\n365      neg\n366      neg\n367      pos\n368      neg\n369      neg\n370      pos\n371      pos\n372      neg\n373      neg\n374      neg\n375      neg\n376      pos\n377      neg\n378      neg\n379      pos\n380      neg\n381      neg\n382      neg\n383      neg\n384      neg\n385      neg\n386      neg\n387      pos\n388      pos\n389      pos\n390      neg\n391      neg\n392      pos\n393      neg\n394      neg\n395      pos\n396      neg\n397      neg\n398      pos\n399      neg\n400      pos\n401      pos\n402      neg\n403      pos\n404      neg\n405      pos\n406      neg\n407      pos\n408      neg\n409      pos\n410      pos\n411      neg\n412      neg\n413      neg\n414      neg\n415      pos\n416      pos\n417      neg\n418      pos\n419      neg\n420      pos\n421      neg\n422      neg\n423      neg\n424      neg\n425      pos\n426      pos\n427      neg\n428      pos\n429      neg\n430      pos\n431      neg\n432      neg\n433      neg\n434      neg\n435      neg\n436      pos\n437      neg\n438      neg\n439      neg\n440      neg\n441      pos\n442      neg\n443      neg\n444      pos\n445      pos\n446      pos\n447      neg\n448      neg\n449      pos\n450      neg\n451      neg\n452      pos\n453      neg\n454      neg\n455      neg\n456      pos\n457      neg\n458      neg\n459      pos\n460      neg\n461      neg\n462      neg\n463      neg\n464      neg\n465      neg\n466      neg\n467      neg\n468      neg\n469      pos\n470      neg\n471      neg\n472      neg\n473      neg\n474      neg\n475      neg\n476      neg\n477      pos\n478      neg\n479      neg\n480      neg\n481      pos\n482      neg\n483      neg\n484      neg\n485      pos\n486      pos\n487      neg\n488      neg\n489      neg\n490      neg\n491      neg\n492      neg\n493      neg\n494      pos\n495      neg\n496      neg\n497      neg\n498      neg\n499      pos\n500      neg\n501      neg\n502      neg\n503      pos\n504      neg\n505      neg\n506      neg\n507      pos\n508      neg\n509      neg\n510      neg\n511      pos\n512      neg\n513      neg\n514      neg\n515      neg\n516      pos\n517      pos\n518      neg\n519      neg\n520      neg\n521      neg\n522      neg\n523      neg\n524      pos\n525      neg\n526      neg\n527      neg\n528      neg\n529      neg\n530      neg\n531      neg\n532      neg\n533      neg\n534      neg\n535      neg\n536      pos\n537      neg\n538      neg\n539      neg\n540      pos\n541      pos\n542      pos\n543      pos\n544      neg\n545      neg\n546      pos\n547      pos\n548      neg\n549      neg\n550      neg\n551      neg\n552      neg\n553      neg\n554      neg\n555      neg\n556      neg\n557      neg\n558      neg\n559      neg\n560      neg\n561      pos\n562      pos\n563      neg\n564      neg\n565      neg\n566      neg\n567      neg\n568      neg\n569      neg\n570      pos\n571      neg\n572      neg\n573      neg\n574      neg\n575      neg\n576      neg\n577      neg\n578      pos\n579      neg\n580      pos\n581      pos\n582      neg\n583      neg\n584      neg\n585      pos\n586      neg\n587      pos\n588      neg\n589      pos\n590      neg\n591      pos\n592      neg\n593      pos\n594      neg\n595      neg\n596      pos\n597      neg\n598      neg\n599      pos\n600      neg\n601      neg\n602      neg\n603      neg\n604      pos\n605      pos\n606      neg\n607      pos\n608      neg\n609      neg\n610      neg\n611      neg\n612      pos\n613      pos\n614      neg\n615      pos\n616      neg\n617      neg\n618      neg\n619      pos\n620      pos\n621      neg\n622      neg\n623      neg\n624      neg\n625      neg\n626      neg\n627      neg\n628      neg\n629      neg\n630      neg\n631      pos\n632      neg\n633      neg\n634      neg\n635      neg\n636      pos\n637      neg\n638      neg\n639      pos\n640      neg\n641      neg\n642      neg\n643      pos\n644      neg\n645      neg\n646      neg\n647      pos\n648      pos\n649      pos\n650      neg\n651      neg\n652      neg\n653      neg\n654      neg\n655      neg\n656      pos\n657      neg\n658      neg\n659      neg\n660      pos\n661      neg\n662      pos\n663      pos\n664      pos\n665      pos\n666      neg\n667      pos\n668      pos\n669      neg\n670      neg\n671      neg\n672      neg\n673      neg\n674      neg\n675      neg\n676      pos\n677      pos\n678      neg\n679      pos\n680      neg\n681      neg\n682      pos\n683      neg\n684      pos\n685      neg\n686      neg\n687      neg\n688      neg\n689      neg\n690      pos\n691      neg\n692      pos\n693      neg\n694      pos\n695      neg\n696      pos\n697      pos\n698      neg\n699      neg\n700      neg\n701      neg\n702      pos\n703      pos\n704      neg\n705      neg\n706      neg\n707      pos\n708      neg\n709      pos\n710      pos\n711      neg\n712      neg\n713      pos\n714      neg\n715      neg\n716      pos\n717      pos\n718      neg\n719      neg\n720      pos\n721      neg\n722      neg\n723      pos\n724      neg\n725      neg\n726      neg\n727      neg\n728      neg\n729      neg\n730      neg\n731      pos\n732      pos\n733      pos\n734      neg\n735      neg\n736      neg\n737      neg\n738      neg\n739      neg\n740      pos\n741      pos\n742      neg\n743      neg\n744      pos\n745      neg\n746      neg\n747      pos\n748      neg\n749      pos\n750      pos\n751      pos\n752      neg\n753      neg\n754      pos\n755      pos\n756      pos\n757      neg\n758      pos\n759      neg\n760      pos\n761      neg\n762      pos\n763      neg\n764      neg\n765      neg\n766      neg\n767      pos\n768      neg\n\nEen snelle verkenning van de dataset toont aan dat er meer nullen in de gegevens zitten dan verwacht (vooral omdat een BMI of tricep huiddikte van 0 onmogelijk is), wat betekent dat ontbrekende waarden als nullen worden geregistreerd. Zie bijvoorbeeld het histogram van de tricep huidplooidikte, waar de nullen voor dikte opvallen.\n\n\n\nDit fenomeen is ook te zien in de glucose-, druk-, insuline- en massavariabelen. We zetten eerst de 0-scores in alle variabelen (behalve “zwanger”) over naar NA (missende waarde). Daarvoor gebruiken we de mutate_at()functie (die binnenkort wordt vervangen door mutate() met across()) om aan te geven op welke variabelen we onze muterende functie willen toepassen. We gebruiken de if_else()functie om aan te geven waar we de waarde mee moeten vervangen als de voorwaarde waar of onwaar is.\n\n\n\nOnze gegevens zijn klaar. Laten we beginnen met het maken van een aantal tidymodels!\nHaal train/test sets uit elkaar\nLaten we onze data verdelen in trainings- en testdata. De trainingsdata worden gebruikt om ons model te vinden en de parameters in te stellen (tune). De testdata gebruiken we alleen om de werking van het finale model vast te stellen. Dat splitten kunnen we doen door de inital_split() functie (van het rsample pakket). Dat creëert een speciaal “split” object.\n\n<Analysis/Assess/Total>\n<576/192/768>\n\ndiabetes_split, ons gesplitste object, vertelt ons hoeveel waarnemingen we hebben in de trainingsset, de testset en de gehele dataset: <train/test/totaal> (576/192/768).\nDe trainings- en testsets kunnen uit het “split”-object worden gehaald met behulp van de training() en testing() functies. Hoewel we deze objecten niet echt zullen gebruiken in de pipeline (daarvoor zullen we het diabetes_split-object zelf gebruiken).\n\n\n\nOp een gegeven moment zullen we de parameters hiervan wat willen tuenen (afstemmen). Dat doen we met cross-validatie. Zo ontstaat er met vfold_cv() een cross-validatie versie van de trainingsset waar we zo op terugkomen.\n\n\n\nDefineeer een recipe\nMet het pakket recipes kun je de variabelen een rol geven, als uitkomst of voorspellende variabele (gebruik een “formule”) b.v.. Maar met recipe kun je ook andere voorbereidingsstappen zetten die je nodig acht (zoals standaardiseren, imputeren, PCA, etc). Een recipe voer je uit in delen (gelaagd op elkaar door pipes %>% te gebruiken):\nSpecificeer de formule (recipe()): specificeer eerst wat is de uitkomstvariabele en wat zijn de predictoren;\nSpecificeer pre-processing steps (step_zzz()): defineer voorbereidingsstappen, zoals imputatie, creëren van dummy variabelen, schalen en wat al niet meer\nZo kunnen we bijvoorbeeld de volgende recipe maken.\n\n\n\nAls je ooit eerder formules hebt gezien (bijvoorbeeld met behulp van de lm() functie in R), dan weet je misschien dat we onze formule veel efficiënter hadden kunnen schrijven met behulp van een shortcut, waarbij de . alle variabelen in de gegevens vertegenwoordigt: outcome ~ .\nDe volledige lijst van beschikbare voorbewerkingsstappen is hier te vinden. In de bovenstaande chunck hebben we de functies all_numeric() en all_predictors() gebruikt als argumenten van voorbereiding. Deze worden “rolselecties” genoemd en geven aan dat we de stap willen toepassen op “alle numerieke” variabelen of “alle predictoren”. De lijst van alle potentiële rolselectoren kan worden gevonden door ?selectis in je console te typen.\nMerk op dat we het originele diabetes_clean data-object hebben gebruikt (we stellen recipe(..., data = diabetes_clean)), in plaats van het diabetes_train-object of het diabetes_split-object. Het blijkt dat we deze allemaal hadden kunnen gebruiken. Alle recipes die op dit punt uit het dataobject worden gehaald zijn de namen en rollen van de uitkomst en de voorspellende variabelen. We zullen deze recipe later toepassen op specifieke datasets. Dit betekent dat voor grote datasets een kleinere dataset gebruikt wordt om tijd en geheugen te besparen.\nInderdaad, als we een samenvatting van het diabetes_recipe object printen, dan laat het ons gewoon zien hoeveel voorspellingsvariabelen we hebben gespecificeerd en welke stappen we hebben gespecificeerd (maar het implementeert ze eigenlijk nog niet!).\n\nData Recipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          8\n\nOperations:\n\nCentering and scaling for all_numeric()\nK-nearest neighbor imputation for all_predictors()\n\nAls je de voorbewerkte dataset zelf wilt extraheren, kunt je eerst prep() het recept voor een specifieke dataset en juice() het voorbewerkte recept om de voorbewerkte gegevens te extraheren. Het blijkt dat het extraheren van de voorbewerkte data eigenlijk niet nodig is voor de pipeline, omdat dit onder de motorkap gebeurt als het model geschikt is. Soms is het toch nuttig.\n\n# A tibble: 576 x 9\n   pregnant glucose pressure triceps insulin   mass pedigree     age\n      <dbl>   <dbl>    <dbl>   <dbl>   <dbl>  <dbl>    <dbl>   <dbl>\n 1   0.673   0.837   -0.0581  0.616   0.328   0.187    0.531  1.37  \n 2  -0.824  -1.21    -0.537   0.0354 -0.770  -0.831   -0.359 -0.205 \n 3   1.27    1.98    -0.697   0.229   1.33   -1.31     0.676 -0.122 \n 4  -1.12    0.478   -2.61    0.616   0.0669  1.57     5.89  -0.0396\n 5   0.374  -0.205    0.102  -0.700  -0.404  -0.977   -0.843 -0.287 \n 6   1.87   -0.238    0.229   0.229   0.558   0.435   -1.06  -0.370 \n 7  -0.525   2.43    -0.218   1.58    3.15   -0.264   -0.982  1.61  \n 8   1.27    0.0877   1.86   -0.178   0.863   0.318   -0.743  1.70  \n 9   0.0743 -0.401    1.54    0.674   0.139   0.769   -0.875 -0.287 \n10   1.87    0.544    0.581  -0.448   0.666  -0.758    3.16   1.94  \n# … with 566 more rows, and 1 more variable: diabetes <fct>\n\nSpecificeer het model\nTot nu toe hebben we onze data verdeeld in training en test-sets en onze pre-proces stappen gespecificeerd door een recipe te gebruiken. Nu willen we ons model definiëren en daarvoor gebruiken we het parsnip pakket dat in tidymodels zit.\nParsnip biedt een uniforme interface voor de enorme verscheidenheid aan modellen die er in R bestaan. Dit betekent dat je slechts één manier hoeft te leren om een model te specificeren en dan kun je dit gebruiken voor allerlei verschillende modellen, vaak met enkele coderegel.\nEr zijn een paar primaire componenten in de modelspecificatie opgeslagen:\nHet model type: wat voor soort model wil je gebruiken, zoals rand_forest() voor het random forest-model, logistic_reg() voor het logistisch regressie-model, svm_poly() voor een polynomiaal SVM-model, enz. De volledige lijst van modellen die beschikbaar zijn via parsnip kan [hier] (link naar website) vinden.\nDe arguments: de model parameter waarden (de benaming is consistent over verschillende modellen), door het gebruik van set_args().\nDe engine: het onderliggende pakket waar het model van wegkomt (bv. “ranger” voor implementatie van Random Forest), door het gebuik van set_engine().\nDe mode: het type voorspelling - omdat verschillende pakketten zowel classificatie (binaire/categoriale voorspelling) en regressie (continue voorspelling) kunnen uitvoeren, door het gebruik van set_mode().\nAls we bijvoorbeeld een random forest model willen gebruiken, zoals dat in het ranger pakket zit, met als doel classificatie en we willen de try parameter tunen (het afstemmen van het aantal willekeurig gekozen variabelen dat bij elke splitsing in aanmerking moet worden genomen), dan moeten we de volgende modelspecificatie definiëren:\n\n\n\nAls je later het variabele belang van jouw uiteindelijke model wilt kunnen onderzoeken, moet je het engine argument opnieuw instellen. De volgende code specificeert bijvoorbeeld een logistisch regressiemodel uit het glm pakket.\n\n\n\nDeze code draait niet het model. Net als de recipe, is het veel meer een beschrijving van het model. Echter, wanneer je een parameter op tune() zet wordt het later gestemd in de stemfase van de pipeline (bv. om de waarde vast te stellen van de parameter die de beste performance geeft). Je kunt ook zelf een bepaalde waarde aan de parameter geven wanneer je het niet wilt afstemmen, bv door set_args(mtry = 4) te gebruiken. Een ander ding om op te merken is dat niets wat deze modelspecificatie betreft specifiek is voor de diabetes-dataset.\nAlles in een workflow samenbrengen\nWe zijn klaar om het model en de recipes in een workflow te plaatsen. Een workflow zet je op door het gebruik van workflow() (van het workflows pakket) en dan kun je een recipe en een model toevoegen.\n\n\n\nMerk op dat we de voorbewerkingsstappen nog niet in de recipe hebben geïmplementeerd noch dat we het model hebben gepast. We hebben alleen maar het raamwerk geschreven. Pas als we de parameters hebben afgestemd of in het model hebben gepast, worden het recept en het model daadwerkelijk geïmplementeerd.\nAfstemmen van de parameters\nOmdat er een parameter is ontwikkeld om af te stemmen (mtry), moeten we dat daar voor gebruiken (bv. de waarde kiezen die de beste performance laat zien) voordat we het model passen. Als je geen parameters hebt om af te stemmen, kun je dit deel overslaan.\nDat afstemmen doen we door een cross-validation object (diabetes_cv) te kiezen. Om dat te doen specificeren we de range van mtry waarden die we willen gebruiken en dan voegen we een stemmingslaag toe aan onze workflow door tune_grid() te gebruiken (van het tune pakket). We richten ons op twee maten: accuracy en roc_auc (van het yardstick pakket). Die vertellen ons welke maten we het beste kunnen gebruiken.\n\n\n\nJe kunt verschillende parameters afstemmen door verschillende parameters aan de expand.grid() functie toe te voegen, bv. expand.grid(mtry = c(3, 4, 5), trees = c(100, 500)).\nHet is altijd goed om de resultaten van de cross-validatie goed te onderzoeken. collect_metrics() is echt een handige functie die in verschillende omstandigheden kan worden gebruikt om te vergelijken die zijn berekend in het object dat is gebruikt. In dit geval komen de maten van de cross-validatie performance over de verschillende waarden van de performance.\n\n# A tibble: 6 x 7\n   mtry .metric  .estimator  mean     n std_err .config             \n  <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1     3 accuracy binary     0.750    10  0.0225 Preprocessor1_Model1\n2     3 roc_auc  binary     0.829    10  0.0154 Preprocessor1_Model1\n3     4 accuracy binary     0.758    10  0.0226 Preprocessor1_Model2\n4     4 roc_auc  binary     0.828    10  0.0149 Preprocessor1_Model2\n5     5 accuracy binary     0.753    10  0.0232 Preprocessor1_Model3\n6     5 roc_auc  binary     0.826    10  0.0153 Preprocessor1_Model3\n\nTen opzichte van accuracy en AUC laat mtry = 4 de beste performance zien (hoogste gemiddelde waarden).\nAfronden van de workflow\nWe willen een laag aan onze workflow toevoegen die overeenkomt met de afgestemde parameter, d.w.z. dat we mtry instellen als de waarde die de beste resultaten opleverde. Als je geen parameters hebt afgestemd, kun je deze stap overslaan.\nWe kunnen de beste waarde voor de nauwkeurigheidsmetriek extraheren door de select_best()functie toe te passen op het afstemmingsobject.\n\n# A tibble: 1 x 2\n   mtry .config             \n  <dbl> <chr>               \n1     4 Preprocessor1_Model2\n\nDan kunnen we deze parameter aan de workflow toevoegen door de finalize_workflow() functie te gebruiken.\n\n\n\nEvalueren van het model op de test set\nNu we ons recipe en ons model hebben gedefinieerd en de parameters van het model hebben getuned, zijn we klaar om daadwerkelijk het uiteindelijke model te draaien. Aangezien al deze informatie in het workflow-object zit, zullen we de last_fit() functie toepassen op onze workflow en ons train/test-splitsingsobject. Dit zal automatisch het door de workflow gespecificeerde model trainen met behulp van de trainingsgegevens en evaluaties produceren op basis van de testset.\n\n\n\nMerk op dat het object dat wordt gecreëerd een data-frame-achtig object is; het is een tibble met listkolommen.\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 x 6\n  splits      id        .metrics    .notes    .predictions   .workflow\n  <list>      <chr>     <list>      <list>    <list>         <list>   \n1 <rsplit [5… train/te… <tibble [2… <tibble … <tibble [192 … <workflo…\n\nDit is echt een aardige eigenschap van tidymodels (en ook waarom je zo goed kunt werken met tidyverse) omdat je al je nette handelingen op het modelobject kunt uitvoeren.\nAangezien we het trainings/testobject al hebben geleverd op het moment dat we in de workflow werken, worden de maten geëvalueerd op de testset. Wanneer we nu de collect_metrics() functie gebruiken (herinner ons dat we deze hebben gebruikt bij het afstemmen van onze parameters), haalt deze de prestaties van het uiteindelijke model (aangezien rf_fit nu bestaat uit een enkel definitief model) toegepast op de test set.\n\n# A tibble: 2 x 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.745 Preprocessor1_Model1\n2 roc_auc  binary         0.831 Preprocessor1_Model1\n\nOverall is de performance heel goed, met een accuracy van 0.74 en een AUC van 0.82. Maar deze waarden zijn vaak lager dan in de trainingsset.\nJe kunt de test set voorspellingen zelf gebruiken met de collect_predictions() functie. Let op dat er 192 rijen in het voorspellingsobject zitten dat overeenkomt met de test set observaties (juist om jou te laten zien dat deze gebaseerd zijn op de testset meer dan op de trainingsset).\n\n# A tibble: 192 x 7\n   id        .pred_neg .pred_pos  .row .pred_class diabetes .config   \n   <chr>         <dbl>     <dbl> <int> <fct>       <fct>    <chr>     \n 1 train/te…    1.00     0.00045     4 neg         neg      Preproces…\n 2 train/te…    0.967    0.0330      7 neg         pos      Preproces…\n 3 train/te…    0.0393   0.961      12 pos         pos      Preproces…\n 4 train/te…    0.708    0.292      19 neg         neg      Preproces…\n 5 train/te…    0.636    0.364      21 neg         neg      Preproces…\n 6 train/te…    0.541    0.459      25 neg         pos      Preproces…\n 7 train/te…    0.482    0.518      27 pos         pos      Preproces…\n 8 train/te…    0.692    0.308      29 neg         neg      Preproces…\n 9 train/te…    0.951    0.0493     34 neg         neg      Preproces…\n10 train/te…    0.369    0.631      35 pos         neg      Preproces…\n# … with 182 more rows\n\nOmndat dit een normaal data frame/tibble object is, kunnen we de samenvattingen genereren en een confusie matrix plotten.\n\n          Truth\nPrediction neg pos\n       neg 106  33\n       pos  16  37\n\nWe kunnen ook de voorspelde kansverdelingen voor elke klasse in kaart brengen.\n\n\n\nDe voorspellingen kun je ook als volgt laten zien:\n\n[[1]]\n# A tibble: 192 x 6\n   .pred_neg .pred_pos  .row .pred_class diabetes .config             \n       <dbl>     <dbl> <int> <fct>       <fct>    <chr>               \n 1    1.00     0.00045     4 neg         neg      Preprocessor1_Model1\n 2    0.967    0.0330      7 neg         pos      Preprocessor1_Model1\n 3    0.0393   0.961      12 pos         pos      Preprocessor1_Model1\n 4    0.708    0.292      19 neg         neg      Preprocessor1_Model1\n 5    0.636    0.364      21 neg         neg      Preprocessor1_Model1\n 6    0.541    0.459      25 neg         pos      Preprocessor1_Model1\n 7    0.482    0.518      27 pos         pos      Preprocessor1_Model1\n 8    0.692    0.308      29 neg         neg      Preprocessor1_Model1\n 9    0.951    0.0493     34 neg         neg      Preprocessor1_Model1\n10    0.369    0.631      35 pos         neg      Preprocessor1_Model1\n# … with 182 more rows\n\nHet laatste model\nIn de vorige paragraaf is het model dat is getraind op de trainingsgegevens geëvalueerd aan de hand van de testgegevens. Maar als je eenmaal jouw definitieve model hebt bepaald, wil je het vaak trainen op je volledige dataset en het dan gebruiken om de respons voor nieuwe gegevens te voorspellen.\nAls je jouw model wilt gebruiken om de respons voor nieuwe waarnemingen te voorspellen, moet je de fit()functie op jouw workflow gebruiken en de dataset waarop je het uiteindelijke model wilt laten passen (bijvoorbeeld de volledige training + testdataset).\n\n\n\nHet final_model object bevat een aantal zaken, waaronder het ranger-object dat getraind is met de parameters die via de workflow in rf_workflow zijn vastgelegd op basis van de gegevens in diabetes_clean (de gecombineerde trainings- en testgegevens).\n\n══ Workflow [trained] ════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ──────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_normalize()\n• step_knnimpute()\n\n── Model ─────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~4,      x), importance = ~\"impurity\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  500 \nSample size:                      768 \nNumber of independent variables:  8 \nMtry:                             4 \nTarget node size:                 10 \nVariable importance mode:         impurity \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.1576627 \n\nAls we de diabetes status van een nieuwe vrouw willen voorspellen, kunnen we de predict() functie gebruiken.\nBijvoorbeeld, definieren we de data voor een nieuwe vrouw.\n\n# A tibble: 1 x 8\n  pregnant glucose pressure triceps insulin  mass pedigree   age\n     <dbl>   <dbl>    <dbl>   <dbl>   <dbl> <dbl>    <dbl> <dbl>\n1        2      95       70      31     102  28.2     0.67    47\n\nDe voorspelde diabetes status van deze nieuwe vrouw is “negatief”.\n\n# A tibble: 1 x 1\n  .pred_class\n  <fct>      \n1 neg        \n\nVariabele belang\nAls je de belangrijkheid van een variabele uit je model wilt vaststellen, voor zover je dat kan zien, moet je het modelobject uit het fit() object halen (dat voor ons final_model heet). De functie die het model extraheert is pull_workflow_fit() en dan moet je het fit-object pakken dat de output bevat.\n\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, mtry = min_cols(~4,      x), importance = ~\"impurity\", num.threads = 1, verbose = FALSE,      seed = sample.int(10^5, 1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  500 \nSample size:                      768 \nNumber of independent variables:  8 \nMtry:                             4 \nTarget node size:                 10 \nVariable importance mode:         impurity \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.1576627 \n\nVervolgens kun je het belang van de variabele uit het ranger-object zelf halen (variable.importance is een specifiek object in de ranger-output - dit zal moeten worden aangepast voor het specifieke objecttype van andere modellen).\n\npregnant  glucose pressure  triceps  insulin     mass pedigree \n16.33289 80.62800 17.08757 21.43870 51.76331 42.41799 30.79204 \n     age \n34.36002 \n\n\n\n\n",
    "preview": "posts/2021-04-22-tidymodels-opnieuw/tidymodels-opnieuw_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2021-07-18T13:15:25+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-10-machine-learning/",
    "title": "Machine Learning met Tidymodels",
    "description": "Enkele blogs zal ik aan Machine Learning besteden. Ik zal enkele tutorials bewerken die mij veel geleerd hebben. Lisa Lendway, van wie ik al twee keer eerder materiaal gebruikte, schreef een goede blog over tidymodels. Zie hieronder.",
    "author": [
      {
        "name": "Lisa Lendway, vertaling Harrie Jonkman",
        "url": {}
      }
    ],
    "date": "2021-03-11",
    "categories": [],
    "contents": "\nLaad tidyverse, tidymodels en enkele andere pakketten en zet theme (optioneel) voor een bepaalde vormgeving van de figuren.\n\n\n\nLees de King County Housing-data in en kijk eens naar de eerste vijf rijen.\n\n# A tibble: 5 x 21\n  id         date        price bedrooms bathrooms sqft_living sqft_lot\n  <chr>      <date>      <dbl>    <int>     <dbl>       <int>    <int>\n1 7129300520 2014-10-13 221900        3      1           1180     5650\n2 6414100192 2014-12-09 538000        3      2.25        2570     7242\n3 5631500400 2015-02-25 180000        2      1            770    10000\n4 2487200875 2014-12-09 604000        4      3           1960     5000\n5 1954400510 2015-02-18 510000        3      2           1680     8080\n# … with 14 more variables: floors <dbl>, waterfront <lgl>,\n#   view <int>, condition <fct>, grade <fct>, sqft_above <int>,\n#   sqft_basement <int>, yr_built <int>, yr_renovated <int>,\n#   zipcode <fct>, lat <dbl>, long <dbl>, sqft_living15 <int>,\n#   sqft_lot15 <int>\n\nOver de house_prices data lezen we het volgende:\n\n“Deze dataset bevat huizenverkoopprijzen voor King County, waar Seattle deel van uitmaakt. Het omvat huizen verkocht tussen mei 2014 en mei 2015. Deze dataset is verkregen via Kaggle.com.” De beschrijving van de variabelen in de dataset in de documentatie lijkt niet helemaal te kloppen. Een meer accurate beschrijving is hieronder te vinden. In ieder geval willen we hier de prijs van woningen modelleren.\n\n\nExploratie\nKijk eerst eens naar de verdelingen van alle variabelen om te zien of er iets onregelmatigs aan de hand is.\nKwantitatieve variabelen:\n\n\n\nDingen die opvielen en eerste gedachten over het opstartproces: * ‘Right-skewness’ in de variabele price en alle variabelen betreffende vierkante meters –> log transformeren indien lineaire regressie wordt gebruikt. * Veel 0’s in sqft_basement, view, en yr_renovated –> maak indicator variabelen van het hebben van dat kenmerk vs. niet hebben , dat wil zeggen een variabele genaamd basement waar een 0 aangeeft geen kelder (sqft_basement = 0) en wel een kelder bij (sqft_basement > 0).\n* Leeftijd van huis is misschien een betere, interpreteerbare variabele dan bouwjaar –> age_at_sale = year(date) - yr_built.\n\n\n\nData splitsen (in train- en test-sets)\nEerst splitsen we de gegevens op in training- en test-datasets. We gebruiken de trainingsgegevens om verschillende soorten modellen te proberen en de parameters van die modellen zo nodig aan te passen. De testdataset wordt bewaard voor het allerlaatst om een kleine subset van modellen te vergelijken. De initial_split() functie uit de rsample bibliotheek (onderdeel van tidymodels) wordt gebruikt om tot deze splitsing te komen. We splitsen deze dataset random, maar er zijn andere mogelijkheden om tot gestratificeerde steekproeven te komen. Daarna gebruiken we training() en testing() om de twee datasets, house_training en house_testing, te extraheren.\n\n<Analysis/Assess/Total>\n<16210/5403/21613>\n\nLater zullen we 5-voudige cross-validatie gebruiken om het model te evalueren en de modelparameters aan te passen. We zetten de vijfvoud van de trainingsdata op met de vfold_cv() functie. We zullen dit later in meer detail uitleggen.\n\n\n\nData voorspel: recipe() en step_xxx()\nWe gebruiken de recipe()-functie om de uitkomstvariabele en de predictoren te definiëren.\nEen verscheidenheid van step_xxx() functies kan worden gebruikt om data te bewerken/transformeren. Vind ze allemaal hier. Ik heb er een paar gebruikt, met korte beschrijvingen in de code. Ik heb ook een aantal selectiefuncties gebruikt, zoals all_predictors() en all_nominal() om de juiste variabelen te selecteren.\nWe gebruiken ook update_roles() om de rollen van sommige variabelen te veranderen. Voor ons zijn dit variabelen die we misschien willen meenemen voor evaluatiedoeleinden, maar die niet gebruikt zullen worden bij het bouwen van het model. Ik heb gekozen voor de rol evaluative, maar je kunt die rol elke naam geven die je maar wilt, bijvoorbeeld id, extra, junk (misschien een slecht idee?).\n\n\n\nPas het toe op de trainings-dataset, gewoon om te zien wat er gebeurt. Let op de namen van de variabelen.\n\n# A tibble: 16,210 x 36\n   id        date       bedrooms bathrooms sqft_living sqft_lot floors\n   <fct>     <date>        <int>     <dbl>       <dbl>    <dbl>  <dbl>\n 1 71293005… 2014-10-13        3      1           3.07     3.75      1\n 2 64141001… 2014-12-09        3      2.25        3.41     3.86      2\n 3 56315004… 2015-02-25        2      1           2.89     4         1\n 4 24872008… 2014-12-09        4      3           3.29     3.70      1\n 5 19544005… 2015-02-18        3      2           3.23     3.91      1\n 6 72375503… 2014-05-12        4      4.5         3.73     5.01      1\n 7 13214000… 2014-06-27        3      2.25        3.23     3.83      2\n 8 20080002… 2015-01-15        3      1.5         3.03     3.99      1\n 9 24146001… 2015-04-15        3      1           3.25     3.87      1\n10 37935001… 2015-03-12        3      2.5         3.28     3.82      2\n# … with 16,200 more rows, and 29 more variables: waterfront <dbl>,\n#   view <dbl>, sqft_above <dbl>, zipcode <fct>, lat <dbl>,\n#   long <dbl>, price <dbl>, basement <dbl>, renovated <dbl>,\n#   age_at_sale <dbl>, condition_X2 <dbl>, condition_X3 <dbl>,\n#   condition_X4 <dbl>, condition_X5 <dbl>, grade_X7 <dbl>,\n#   grade_X8 <dbl>, grade_X9 <dbl>, grade_high <dbl>,\n#   date_month_Feb <dbl>, date_month_Mar <dbl>, date_month_Apr <dbl>,\n#   date_month_May <dbl>, date_month_Jun <dbl>, date_month_Jul <dbl>,\n#   date_month_Aug <dbl>, date_month_Sep <dbl>, date_month_Oct <dbl>,\n#   date_month_Nov <dbl>, date_month_Dec <dbl>\n\nHet model definiëren en workflows creëren\nNu we de gegevens hebben opgesplitst en voorbewerkt, zijn we klaar om te modelleren! Eerst zullen we price (die nu eigenlijk log(price) is) modelleren met eenvoudige lineaire regressie.\nWe zullen dit doen met behulp van enkele modelleringsfuncties uit het parsnip pakket. Vind alle beschikbare functies hier. Hier is lineaire regressie meer in detail.\nOm ons model te definiëren, moeten we de volgende stappen zetten:\nBepaal het modeltype, dat is het algemene moeltype dat u wilt draaien.\nStel de motor in, die het pakket/de functie bepaalt die zal worden gebruikt om het model te draaien.\nStel de modus in, die ofwel “regressie” is voor continue uitkomstvariabelen of “classificatie” voor binaire/categorische uitkomstvariabelen. (Merk op dat voor lineaire regressie, het alleen “regressie” kan zijn, dus we hebben deze stap in dit geval niet NODIG).\n(OPTIONEEL) Stel argumenten in om af te stemmen (‘tunen’). We zullen hier later een voorbeeld van zien.\n\n\n\nDit is slechts het opzetten van het proces. We hebben het model nog niet aan de gegevens aangepast en er is nog één stap voordat we dat doen - een workflow maken! Hier wordt de voorbewerking en de stappen in het model gecombineerd.\n\n══ Workflow ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ──────────────────────────────────────────────────────\n6 Recipe Steps\n\n● step_rm()\n● step_log()\n● step_mutate()\n● step_rm()\n● step_date()\n● step_dummy()\n\n── Model ─────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\nModelleren en evalueren\nNu zijn we eindelijk klaar om het model te draaien! Na al dat werk, lijkt dit deel eenvoudig. We gebruiken eerst de fit() functie om het model te fitten, door te vertellen op welke dataset we het model willen draaien. Daarna gebruiken we enkele andere functies om de resultaten mooi weer te geven.\n\n# A tibble: 31 x 5\n   term        estimate std.error statistic p.value\n   <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n 1 (Intercept)    4.01      0.048     83.4        0\n 2 bedrooms      -0.018     0.002    -11.2        0\n 3 bathrooms      0.036     0.003     14.3        0\n 4 sqft_living    0.294     0.025     11.7        0\n 5 sqft_lot      -0.038     0.003    -11.0        0\n 6 floors         0.021     0.003      6.87       0\n 7 waterfront     0.194     0.013     15.0        0\n 8 view          -0.061     0.004    -15.6        0\n 9 sqft_above     0.149     0.025      5.99       0\n10 basement      -0.042     0.005     -9.14       0\n# … with 21 more rows\n\nOm het model te evalueren, gebruiken we crossvalidatie (CV), specifiek de 5-voudige CV. (Ik veronderstel dat we niet zowel de vorige stap van het passen van een model op de trainingsgegevens EN deze stap moeten doen, maar ik kon er niet achter komen hoe we het uiteindelijke model uit de CV-gegevens kunnen halen … dus dit was mijn oplossing voor nu). We passen het model dus aan met de 5-voudige dataset die we in het begin hebben gemaakt. Voor een diepere discussie over crossvalidatie, raad ik Bradley Boehmke’s Resampling sectie van Hands on Machine Learning with R aan.\n\n# A tibble: 10 x 5\n   id    .metric .estimator .estimate .config             \n   <chr> <chr>   <chr>          <dbl> <chr>               \n 1 Fold1 rmse    standard       0.135 Preprocessor1_Model1\n 2 Fold1 rsq     standard       0.662 Preprocessor1_Model1\n 3 Fold2 rmse    standard       0.137 Preprocessor1_Model1\n 4 Fold2 rsq     standard       0.644 Preprocessor1_Model1\n 5 Fold3 rmse    standard       0.137 Preprocessor1_Model1\n 6 Fold3 rsq     standard       0.638 Preprocessor1_Model1\n 7 Fold4 rmse    standard       0.133 Preprocessor1_Model1\n 8 Fold4 rsq     standard       0.655 Preprocessor1_Model1\n 9 Fold5 rmse    standard       0.135 Preprocessor1_Model1\n10 Fold5 rsq     standard       0.642 Preprocessor1_Model1\n# A tibble: 2 x 6\n  .metric .estimator  mean     n  std_err .config             \n  <chr>   <chr>      <dbl> <int>    <dbl> <chr>               \n1 rmse    standard   0.135     5 0.000668 Preprocessor1_Model1\n2 rsq     standard   0.648     5 0.00437  Preprocessor1_Model1\n# A tibble: 2 x 5\n# Groups:   .metric [2]\n  .metric .estimator  mean     n  std_err\n  <chr>   <chr>      <dbl> <int>    <dbl>\n1 rmse    standard   0.135     5 0.000668\n2 rsq     standard   0.648     5 0.00437 \n\nVoorspellen en evalueren van testgegevens\nIn dit eenvoudige scenario zijn we wellicht geïnteresseerd in hoe het model presteert op de testgegevens die werden weggelaten. De onderstaande code past het model toe op de trainingsgegevens en past het toe op de testgegevens. Er zijn andere manieren waarop we dit hadden kunnen doen, maar de manier waarop we het hier doen zal nuttig zijn wanneer we complexere modellen gaan gebruiken waarbij we de modelparameters moeten afstellen.\nNadat het model is aangepast en toegepast, verzamelen we de prestatiecijfers en geven we ze weer en tonen we de voorspellingen van de testgegevens.\n\n# A tibble: 2 x 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.135 Preprocessor1_Model1\n2 rsq     standard       0.655 Preprocessor1_Model1\n# A tibble: 5,403 x 5\n   id               .pred  .row price .config             \n   <chr>            <dbl> <int> <dbl> <chr>               \n 1 train/test split  5.58    12  5.67 Preprocessor1_Model1\n 2 train/test split  5.53    17  5.60 Preprocessor1_Model1\n 3 train/test split  5.90    27  5.97 Preprocessor1_Model1\n 4 train/test split  5.58    29  5.64 Preprocessor1_Model1\n 5 train/test split  5.67    31  5.76 Preprocessor1_Model1\n 6 train/test split  5.88    38  5.81 Preprocessor1_Model1\n 7 train/test split  5.69    40  5.78 Preprocessor1_Model1\n 8 train/test split  5.79    41  5.80 Preprocessor1_Model1\n 9 train/test split  5.77    42  5.89 Preprocessor1_Model1\n10 train/test split  5.66    44  5.84 Preprocessor1_Model1\n# … with 5,393 more rows\n\nDe onderstaande code maakt een eenvoudige plot om de voorspelde vs. de werkelijke prijs van de huisgegevens te onderzoeken.\n\n\n\n\n\n\nHoe zal het model worden gebruikt?\nWanneer we modellen creëren is het belangrijk na te denken over hoe het model zal worden gebruikt en met name hoe het model schade zou kunnen berokkenen. Wat opvalt in de bovenstaande grafieken is dat de prijs van woningen met een lagere prijs gemiddeld wordt overschat, terwijl de prijs van woningen met een hogere prijs gemiddeld wordt onderschat.\nWat als dit model werd gebruikt om de prijs van woningen te bepalen voor de onroerendgoedbelasting? Dan zouden lager geprijsde huizen te zwaar worden belast en hoger geprijsde huizen te weinig.\nMer complexe modellen met tuning parameters\nNu gaan we de Least Absolute Shrinkage and Selection Operator (LASSO) regressie proberen. Deze methode krimpt sommige coëfficiënten tot 0 op basis van een strafterm. We zullen crossvalidatie gebruiken om ons te helpen de beste strafterm te vinden.\nHet model opzetten\nWe zetten het model op zoals we het lineaire model hebben opgezet, maar voegen nu een set_args() functie toe. We vertellen het model dat we de penalty parameter later gaan aanpassen.\n\n\n\nDe workflow updaten\nEn dan creëren we een LASSO workflow.\n\n══ Workflow ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ──────────────────────────────────────────────────────\n6 Recipe Steps\n\n● step_rm()\n● step_log()\n● step_mutate()\n● step_rm()\n● step_date()\n● step_dummy()\n\n── Model ─────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n\nAfstemmen van de strafparameter\nWe gebruiken de grid_regular() functie uit de dials bibliotheek om een aantal waarden van de penalty parameter voor ons te kiezen. Als alternatief kunnen we ook een vector van waarden opgeven die we willen proberen.\n\n# A tibble: 20 x 1\n    penalty\n      <dbl>\n 1 1.00e-10\n 2 3.36e-10\n 3 1.13e- 9\n 4 3.79e- 9\n 5 1.27e- 8\n 6 4.28e- 8\n 7 1.44e- 7\n 8 4.83e- 7\n 9 1.62e- 6\n10 5.46e- 6\n11 1.83e- 5\n12 6.16e- 5\n13 2.07e- 4\n14 6.95e- 4\n15 2.34e- 3\n16 7.85e- 3\n17 2.64e- 2\n18 8.86e- 2\n19 2.98e- 1\n20 1.00e+ 0\n\nGebruik de tune_grid() functie om het model te draaien met behulp van crossvalidatie voor alle penalty_grid waarden en evalueer op alle vouwen.\n\n# Tuning results\n# 5-fold cross-validation \n# A tibble: 5 x 4\n  splits                id    .metrics          .notes          \n  <list>                <chr> <list>            <list>          \n1 <rsplit [12968/3242]> Fold1 <tibble [40 × 5]> <tibble [2 × 1]>\n2 <rsplit [12968/3242]> Fold2 <tibble [40 × 5]> <tibble [2 × 1]>\n3 <rsplit [12968/3242]> Fold3 <tibble [40 × 5]> <tibble [2 × 1]>\n4 <rsplit [12968/3242]> Fold4 <tibble [40 × 5]> <tibble [2 × 1]>\n5 <rsplit [12968/3242]> Fold5 <tibble [40 × 5]> <tibble [2 × 1]>\n\nBekijk de resultaten van de cross-validatie.\n\n# A tibble: 100 x 6\n   id     penalty .metric .estimator .estimate .config              \n   <chr>    <dbl> <chr>   <chr>          <dbl> <chr>                \n 1 Fold1 1.00e-10 rmse    standard       0.135 Preprocessor1_Model01\n 2 Fold1 3.36e-10 rmse    standard       0.135 Preprocessor1_Model02\n 3 Fold1 1.13e- 9 rmse    standard       0.135 Preprocessor1_Model03\n 4 Fold1 3.79e- 9 rmse    standard       0.135 Preprocessor1_Model04\n 5 Fold1 1.27e- 8 rmse    standard       0.135 Preprocessor1_Model05\n 6 Fold1 4.28e- 8 rmse    standard       0.135 Preprocessor1_Model06\n 7 Fold1 1.44e- 7 rmse    standard       0.135 Preprocessor1_Model07\n 8 Fold1 4.83e- 7 rmse    standard       0.135 Preprocessor1_Model08\n 9 Fold1 1.62e- 6 rmse    standard       0.135 Preprocessor1_Model09\n10 Fold1 5.46e- 6 rmse    standard       0.135 Preprocessor1_Model10\n# … with 90 more rows\n# A tibble: 20 x 7\n    penalty .metric .estimator  mean     n  std_err .config           \n      <dbl> <chr>   <chr>      <dbl> <int>    <dbl> <chr>             \n 1 1.00e-10 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\n 2 3.36e-10 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\n 3 1.13e- 9 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\n 4 3.79e- 9 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\n 5 1.27e- 8 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\n 6 4.28e- 8 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\n 7 1.44e- 7 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\n 8 4.83e- 7 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\n 9 1.62e- 6 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\n10 5.46e- 6 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\n11 1.83e- 5 rmse    standard   0.135     5 0.000644 Preprocessor1_Mod…\n12 6.16e- 5 rmse    standard   0.135     5 0.000640 Preprocessor1_Mod…\n13 2.07e- 4 rmse    standard   0.135     5 0.000623 Preprocessor1_Mod…\n14 6.95e- 4 rmse    standard   0.135     5 0.000591 Preprocessor1_Mod…\n15 2.34e- 3 rmse    standard   0.136     5 0.000522 Preprocessor1_Mod…\n16 7.85e- 3 rmse    standard   0.142     5 0.000513 Preprocessor1_Mod…\n17 2.64e- 2 rmse    standard   0.161     5 0.000515 Preprocessor1_Mod…\n18 8.86e- 2 rmse    standard   0.191     5 0.000896 Preprocessor1_Mod…\n19 2.98e- 1 rmse    standard   0.228     5 0.000960 Preprocessor1_Mod…\n20 1.00e+ 0 rmse    standard   0.228     5 0.000960 Preprocessor1_Mod…\n\n# A tibble: 1 x 2\n   penalty .config              \n     <dbl> <chr>                \n1 0.000207 Preprocessor1_Model13\n\nUpdate de workflow voor best afgestemde parameter\nPas de workflow aan om de beste afstemparameter (kleinste rmse, met select_best() in vorige stap) in het model op te nemen. Er zijn andere manieren om modellen te selecteren, zoals select_by_one_std_error() die “het meest eenvoudige model selecteert dat binnen één standaardfout van de numeriek optimale resultaten ligt”.\n\n══ Workflow ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ──────────────────────────────────────────────────────\n6 Recipe Steps\n\n● step_rm()\n● step_log()\n● step_mutate()\n● step_rm()\n● step_date()\n● step_dummy()\n\n── Model ─────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = 0.000206913808111479\n  mixture = 1\n\nComputational engine: glmnet \n\nPas de beste afstelling toe op de trainingsgegevens\nNu kunnen we dit toepassen op de trainingsgegevens en het resulterende model bekijken. De uitvoer van het model was niet wat ik verwachtte. Volgens Julia Silge’s antwoord op mijn vraag hier, zou dit verholpen moeten zijn als je parsnip installeert vanaf GitHub] met devtools::install_github(\"tidymodels/parsnip\") van de devtools bibliotheek.\n\n══ Workflow [trained] ════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ──────────────────────────────────────────────────────\n6 Recipe Steps\n\n● step_rm()\n● step_log()\n● step_mutate()\n● step_rm()\n● step_date()\n● step_dummy()\n\n── Model ─────────────────────────────────────────────────────────────\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~1) \n\n   Df  %Dev   Lambda\n1   0  0.00 0.153800\n2   1  7.71 0.140200\n3   1 14.11 0.127700\n4   1 19.42 0.116400\n5   1 23.83 0.106000\n6   1 27.49 0.096610\n7   1 30.53 0.088030\n8   1 33.06 0.080210\n9   1 35.15 0.073080\n10  2 37.42 0.066590\n11  2 39.41 0.060670\n12  2 41.06 0.055280\n13  2 42.43 0.050370\n14  3 43.93 0.045900\n15  3 45.24 0.041820\n16  3 46.32 0.038100\n17  4 47.48 0.034720\n18  5 48.59 0.031640\n19  6 49.53 0.028830\n20  7 50.48 0.026260\n21  7 51.84 0.023930\n22  8 52.98 0.021810\n23  8 54.00 0.019870\n24  9 54.93 0.018100\n25 11 56.10 0.016490\n26 10 57.40 0.015030\n27 11 58.29 0.013690\n28 11 59.06 0.012480\n29 12 59.70 0.011370\n30 12 60.25 0.010360\n31 13 60.71 0.009439\n32 13 61.10 0.008600\n33 13 61.43 0.007836\n34 13 61.71 0.007140\n35 14 61.95 0.006506\n36 15 62.18 0.005928\n37 15 62.40 0.005401\n38 16 62.57 0.004921\n39 17 62.96 0.004484\n40 18 63.28 0.004086\n41 18 63.56 0.003723\n42 20 63.80 0.003392\n43 20 64.00 0.003091\n44 20 64.16 0.002816\n45 20 64.30 0.002566\n46 21 64.42 0.002338\n\n...\nand 32 more lines.\n# A tibble: 31 x 3\n   term        estimate  penalty\n   <chr>          <dbl>    <dbl>\n 1 (Intercept)   4.12   0.000207\n 2 bedrooms     -0.0174 0.000207\n 3 bathrooms     0.0355 0.000207\n 4 sqft_living   0.307  0.000207\n 5 sqft_lot     -0.0376 0.000207\n 6 floors        0.0210 0.000207\n 7 waterfront    0.191  0.000207\n 8 view         -0.0615 0.000207\n 9 sqft_above    0.139  0.000207\n10 basement     -0.0405 0.000207\n# … with 21 more rows\n\nWe kunnen het belang van de variabele visualiseren\n\n\n\nEvalueren op testgegevens\nTen slotte passen we het model toe op de testgegevens en onderzoeken we enkele definitieve metrieken. We tonen ook de metriek van het gewone lineaire model. Het lijkt erop dat de prestaties van het LASSO-model iets beter zijn, maar het scheelt niet veel.\n\n# A tibble: 2 x 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.135 Preprocessor1_Model1\n2 rsq     standard       0.655 Preprocessor1_Model1\n# A tibble: 2 x 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.135 Preprocessor1_Model1\n2 rsq     standard       0.655 Preprocessor1_Model1\n\nBronnen\nDank gaat uit naar verschillende mensen voor het delen van materiaal over tidymodels, waaronder\n En natuurlijk Lisa zelf, haar voeg ik hier zelf aan toe\n\nDit zijn de bronnen die bij deze blog ondersteuning boden:\nRebecca Barter’s blog\ntidymodels website (Alison Hill, Max Kuhn, Desirée De Leon, Julia Silge)\nJulia Silge’s tidymodels example\nUiteraard vooral Lisa Lendway via:\nLisa Lendway/2020_north_tidymodels\n\n\n\n",
    "preview": "posts/2021-03-10-machine-learning/images/house_prices_variables.png",
    "last_modified": "2021-04-20T21:16:53+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-10-github/",
    "title": "GitHub voor samenwerking",
    "description": "Lisa Lendway heeft een aantal interessante repositories op haar GitHub account staan, [zie hier](https://github.com/llendway). Ze zijn vaak kort, maar helder en concreet. Haar stijl en de consistentie daarin bevallen mij zeer. Van haar manier van doen leer ik veel. Zij maakt haar stukken vaak voor haar statistieklessen en deelt zo haar kennis met haar studenten en anderen buiten haar klas. Ik heb mij voorgenomen om er een aantal goed te lezen, te vertalen en te bewerken waar nodig, en deze op mijn website over te nemen. Vorige maand deed ik dat al met een blof over Distill en nu een over GitHub.",
    "author": [
      {
        "name": "Lisa Lendway, vertaling Harrie Jonkman",
        "url": {}
      }
    ],
    "date": "2021-03-10",
    "categories": [],
    "contents": "\nGitHub\nIk was al vaker van plan hier een stukje over te schrijven. Lisa Lendways tekst hierover vind ik heel duidelijk. Lisa, ik hoop dat je het goed vindt dat ik mij aan jou op trek zo. Dank je wel.\nMijn eigen GitHub accountLisa Lendway heeft veel van haar materiaal weghaald uit Happy git with R by Jenny Bryan. Dat is inderdaad een uitstekende bron, maar bevat ook veel informatie die we niet altijd nodig hebben. Als je Git en GitHub op meer geavanceerde manieren wilt gebruiken of als dit stuk onduidelijk is voor je, dan moet je het zeker bekijken. Het is trouwens überhaupt een goede bron.\nVideo uitleg\n\n\nVoicethread tutorial\nGit en GitHub\nGit is een versie controle systeem. Het is net zoiets als Googledocs, maar het biedt ruimte aan veel soorten bestanden, ook bestanden waar Google docs niets mee kan … zoals .rmd bestanden! GitHub is een online interface om met Git te werken.\nWaarom leren we deze dingen?!\nGitHub is goed geïntegreerd met R Studio. Dus, we zullen geen command-line functies hoeven te gebruiken, tenminste niet nadat we alles hebben ingesteld.\nJe bent verplicht om R te gebruiken voor je eindproject. De presentatie of paper moet worden opgeslagen als een .rmd document dat kan worden ‘geknit’ tot een html document. Door GitHub te gebruiken, kun je gemakkelijk met je groep samenwerken, ook als je niet bij elkaar bent.\nGitHub leert je een aantal goede gewoontes aan. Je wordt gedwongen om na te denken over wanneer je ieets opslaat en om notities te maken over welke wijzigingen je hebt gemaakt, bijvoorbeeld.\nMaak eerst een GitHub account aan\nGa naar http://github.com\nGebruik een username … zie Jenny Bryan’s tips. Incorporeer jouw eigen naam, maar gebruik een andere usernaam die je verder gebrukkt, neem iets waar jouw toekomstige baas zich prettig bij voelt. De username van de universiteit is misschien een goede optie.\nInstalleer Git\n1. Controleer of je Git al geïnstalleerd hebt. Dit zal alleen het geval zijn als je het ergens anders gebruikt hebt. Om dit te doen, open je de commandoregel of, in R Studio, vouw je de Console uit. Er zou een tabblad moeten zijn dat Terminal zegt. In dat gebied type je\nwhich git\nHet geeft iets terug als\n/usr/bin/git\ndan ben je klaar en hoef je Git niet meer te installeren. Op een Windows machine kun je misschien niet eens het which git commando succesvol intypen. Dit zou verholpen moeten zijn door git te installeren. Of je zult de shell moeten gebruiken.\nAls je Git niet geïnstalleerd hebt, moet je het installeren. De instructies zijn iets anders voor Windows en Macs.\nVoor een Windows machine:\nInstalleer Git for Windows. Als er gevraagd wordt naar “Aanpassen van uw PATH omgeving”, zorg er dan voor dat u “Git vanaf de commandoregel en ook van software van derden” selecteert. Anders denken we dat het goed is om de standaardinstellingen te accepteren.\nR Studio voor Windows geeft er de voorkeur aan dat Git geïnstalleerd wordt onder C:/Program Files en dit lijkt de standaard te zijn. Dit houdt bijvoorbeeld in dat de Git executable op mijn Windows systeem te vinden is in C:/Program Files/Git/bin/git.exe. Tenzij je specifieke redenen hebt om anders te doen, volg deze conventie.\nVoor een Mac machine:\nGa naar jouw shell/terminal en voer één van deze commando’s in om een aanbod te krijgen om developer command line tools te installeren. Accepteer het aanbod … klik op installeren.\ngit --version\ngit config\nSommigen van jullie die op een Mac werken moeten misschien eerst het volgende doen in de terminal als je een project zonder succes probeert te openen.\nxcode-select --install\nJe komt er zo achter of dit het geval is.\nGa nu terug naar de Console in R Studio en installeer het usethis pakket in R Studio. Sluit vervolgens R Studio en open het opnieuw.\nLaad de usethis bibliotheek door het volgende stukje code in de console uit te voeren:\nlibrary(usethis)\nVoer de volgende code uit in de console met enkele kleine wijzigingen. De user.name is je Git gebruikersnaam. Dit kan anders zijn dan je GitHub gebruikersnaam, hoewel het misschien een goed idee is om het gewoon hetzelfde te houden. De user.email MOET hetzelfde zijn als je GitHub gebruikers email.\nuse_git_config(user.name = \"Jane Doe\", user.email =       \"jane@example.org\")\nMaak een eerste repo (repository) en gebruik RStudio daarbij\nHet woord “repo” is een afkorting van “repository”, en dat is precies wat het is: een plaats waar dingen (onze bestanden, in dit geval) worden opgeslagen. Het is als de map die je gemaakt hebt om al je werk voor deze les in op te slaan.\nLaten we naar GitHub gaan en inloggen. Nadat je ingelogd bent, zou je een klein icoontje in de rechter bovenhoek moeten zien. Het mijne is een afbeelding van mij. Als ik daar op klik verschijnt er een drop-down en kan ik “Your repositories” kiezen. Doe dat. Je zou nu zoiets als dit moeten zien:\n\nKlik op de “New” knop. Geef jouw repository een naam, bv NAME_test_repo, waar NAME eigenlijk jouw naam is. Kies Public en klik de README file aan. Klik dan op Create repository.\n\nEr zijn dingen die je direct binnen GitHub kunt doen, maar we zullen ons richten op de integratie met R Studio.\nKlonen van een repo\nDenk aan het klonen van een repo als het “kopiëren” van de repository naar je computer. Maar als met het kopiëren doet, houdt het de verbinding met de online repo.\nLaten we dit doen. Op je mijn_test_repo pagina, kies je de groene knop met Code en kopieer je het pad door het icoontje met een pijl erop te selecteren en erop te klikken.\nGa nu naar R Studio. Klik op Bestand –> Nieuw Project … Je zou nu een venster moeten zien dat er als volgt uitziet:\n\nKies Version Control. Dan zie je een scherm dat er zo’n beetje zo uitziet:\n\nKies Git. Dan zou je een scherm moeten zien dat er uitziet als dit, zonder alle details ingevuld. De Repository URL is waar je de repo URL moet plakken die je gekloond hebt van github. Het zal ook de Project mapnaam invullen. Laat die gewoon staan. Let op waar de project directory zich bevindt en verander het naar een betere directory indien nodig. Klik op Create Project.\n\nAls je in de Bestanden tab kijkt in het rechter ondervenster van R Studio, dan zou je het .gitignore bestand moeten zien, het project bestand (eindigt op .Rproj), en het README.md bestand. Je zou ook een Git tab moeten zien in het rechter bovenvenster van R Studio. Als je nu op de Git tab klikt, zul je daar niets zien.\nMet de Git tab open, laten we het README.md bestand in R Studio openen. Maak een kleine wijziging in het bestand door de zin “Ik verander iets in dit bestand.” toe te voegen. Klik dan op het save icoon. Als je dit doet, zul je README.md zien verschijnen in de Git tab.\nKlik nu op de Commit knop in de Git tab. Zet een vinkje in het vakje naast het README.md bestand onder het woord Staged (in de toekomst kun je meerdere bestanden tegelijk stagen door de vakjes naast meerdere bestanden aan te vinken) en voeg een commentaar toe aan het commit vakje.\nHet zou er ongeveer zo uit moeten zien:\n\nKlik tenslotte op commit. Je krijgt dan een bericht dat het voltooid is. Het bericht kan cryptisch overkomen als je er niet aan gewend bent. Het ziet er ongeveer zo uit:\n\nDe wijziging die je hebt gemaakt is nu gecommit in het lokale geheugen. Het gewijzigde bestand is alleen gewijzigd op je computer, NIET online als je op GitHub kijkt … ga maar eens kijken. Klik op de Diff knop in de Git tab en je kunt de geschiedenis van je commits zien.\nVervolgens gaan we die wijzigingen naar GitHub pushen door op de groene pijl omhoog in de Git tab te klikken. Dit zal je een bericht geven dat er ongeveer zo uitziet:\n\nWerd je gevraagd om een gebruikersnaam en wachtwoord? Probeer een andere wijziging te maken, vast te leggen en te pushen. Wordt er nog steeds om een gebruikersnaam en wachtwoord gevraagd? Zo ja, dan kun je hier zien hoe je dat doet Jenny Bryans bron.\nPROBEER HET EENS!!\nVoeg een .rmd bestand toe aan je project. Doe dit door te gaan naar Bestand –> Nieuw bestand –> R Markdown … Voeg wat woorden en een R code chunk toe aan het .rmd bestand. Sla het op, commit het (vergeet het bericht niet!), en push het. Controleer GitHub online om er zeker van te zijn dat je het .rmd bestand daar ziet.\nNu, knit je het bestand lokaal. Commit de wijzigingen (zorg ervoor dat je een vinkje zet naast alles wat je ge-staged wilt hebben - .rmd, .html, etc.) en push ze naar GitHub. Controleer GitHub online om er zeker van te zijn dat je alles ziet wat je verwacht.\nPartners toevoegen\nTot nu toe hebben we eigenlijk alleen technieken geleerd om GitHub te gebruiken om onze eigen bestanden te beheren, maar het coolste eraan zijn de samenwerkingsmogelijkheden. De manier waarop we dit gaan leren is door medewerkers aan de repo toe te voegen.\nZoek iemand om mee samen te werken. Als er een oneven aantal is, maak dan een groepje van drie. In je groepje, voeg elkaar toe als medewerkers aan je project. In GitHub, op de repo pagina, ga naar Instellingen. Een van de opties aan de linkerkant is Collaborators. Klik daarop en doe wat er staat.\nDe persoon die is uitgenodigd om samen te werken zal een email ontvangen en zou ook in staat moeten zijn om de uitnodiging op GitHub te zien. Ze moeten deze accepteren. Eenmaal geaccepteerd, zouden jullie beiden (of alle drie) toegang moeten hebben om wijzigingen in het bestand vast te leggen.\nCommit –> Push –> Pull –> … (en Communicatie)\nZodra je medewerkers hebt toegevoegd, kunnen alle medewerkers committen en pushen. Maar, wat gebeurt er als iemand iets commit en terugzet en jij gaat er dan aan werken op je computer… hoe krijg je dan die wijzigingen? … PULL!\nProbeer in jullie groepen het volgende. Jullie moeten allemaal meewerken aan elkaars projecten, dus je kunt van rol wisselen nadat je het één keer gedaan hebt.\nDe medewerker moet eerst de repo klonen waaraan hij gevraagd is om mee te werken. Als ze een ander project open hebben, sla dan op, commit, en push alle wijzigingen. Sluit dan dat project en open het project waar ze om gevraagd is om aan mee te werken door de GitHub repo te klonen. De medewerker moet het project open hebben in R Studio.\nDe medewerker moet proberen te trekken (pullen) door op de aqua pijl omlaag te klikken in de Git tab. Je zou een bericht moeten krijgen dat er als volgt uitziet:\n\nDe persoon die de repo heeft aangemaakt, maakt een wijziging in zijn .rmd bestand. Het kan een kleine wijziging zijn, zoals het toevoegen van een zin. Diezelfde persoon slaat het bestand op, commit (staged en schrijft een commit boodschap), en pushed het naar GitHub. Controleer online om er zeker van te zijn dat de meest recente wijzigingen zijn gepushed.\nDe medewerker haalt nu die wijzigingen naar zijn lokale map (naar zijn computer). Klik op het pull icoon. Je zou een bericht moeten zien dat er ongeveer zo uitziet:\n\nEn controleer het bestand waarin een wijziging is aangebracht om er zeker van te zijn dat de wijziging wordt weerspiegeld in het bestand op uw computer.\nGa nog een paar keer heen en weer en breng kleine wijzigingen aan. Degene die eigenaar is van de repo zou de wijziging moeten maken en de medewerker zou het moeten binnenhalen. Wissel dan van rol. Als je wisselt, wees er dan zeker van dat je aan het juiste project werkt.\nConflicten samenvoegen\nAls je samen aan een project werkt, is de kans groot dat je tegen een moment aanloopt waarop twee van jullie hetzelfde bestand tegelijkertijd aan het bewerken zijn. Soms, als je allebei je wijzigingen probeert te pushen, zul je wat genoemd wordt een “merge conflict” krijgen. GitHub zal niet weten welke te gebruiken. Dus, zal het je dwingen om te beslissen.\nAls je probeert je wijzigingen naar GitHub te pushen en iemand anders heeft zijn wijzigingen met betrekking tot hetzelfde bestand al gepushed, dan zul je een bericht als dit krijgen:\n\nDan, wanneer je de wijzigingen binnenhaalt, krijg je een bericht zoals dit:\n\nMerk op dat het je vertelt in welk bestand het samenvoegconflict zich voordeed. Je moet dat bestand openen en beslissen hoe de conflicterende informatie samengevoegd moet worden. In het begin zal het er ongeveer zo uitzien:\n\nHet deel na het woord HEAD is wat in je lokale bestand staat. Alles na de ====== is wat in het bestand op afstand staat (d.w.z. de wijzigingen die je medewerker heeft gemaakt). Je kunt besluiten om dit op te lossen op elke manier die je wilt: combineer de twee ideeën, verwijder ze allebei, houd er maar een over, etc. Als je klaar bent, zorg er dan voor dat je de <<<<<<< HEAD en >>>>>>> verwijdert, gevolgd door de alfanumerieke string, plus alle andere vreemde tekens.\nSla het bestand dan op en doe de gebruikelijke commit en push. Je zou de wijzigingen naar GitHub gepushed moeten zien worden.\nLaten we dit eens proberen!\nIn groepjes van 3-4, oefen je GitHub vaardigheden.\nKies iemand om een nieuwe repo aan te maken op GitHub genaamd our_collaborative_graph.\nDe maker voegt de anderen toe als collaborators.\nDe medewerkers moeten hun e-mail controleren en accepteren dat ze medewerkers zijn.\nDe maker en de medewerkers klonen de repo lokaal.\nEen medewerker voegt lokaal een .rmd bestand aan het project toe. De titel moet zijn “Onze grafiek” en voeg alle groepsleden toe als auteurs. Voeg een R code stuk toe dat de tidyverse bibliotheek laadt. Sla het bestand op, commit met bericht, en push naar GitHub. Controleer online om er zeker van te zijn dat het goed gepushed is.\nAlle andere groepsleden halen de wijzigingen lokaal op.\nEen andere medewerker voegt een ander R code stuk toe. Maak met de mpg dataset een scatterplot met hwy op de y-as, displ op de x-as en kleur de punten met drv. Sla de wijzigingen op. Brei het bestand. commit dan met bericht en push naar GitHub. Zorg ervoor dat je alle bestanden in de commit staged. Controleer online om er zeker van te zijn dat je de wijzigingen ziet.\nAlle andere groepsleden halen de wijzigingen lokaal op.\nEen ander groepslid (medewerker of maker als je maar 3 groepsleden hebt) wijzigt de R code chunk die de grafiek maakt, door mooie x en y labels toe te voegen en te veranderen naar theme_minimal(). Sla de wijzigingen op. Brei het bestand. commit dan met bericht en push naar GitHub. Zorg ervoor dat je alle bestanden in de commit staged. Controleer online om er zeker van te zijn dat je de veranderingen ziet.\nAlle grop leden trekken. Degene die net gepushed heeft zou moeten zien dat ze al up to date zijn. Alle anderen zouden de wijzigingen lokaal terug moeten zien.\nNu moeten alle groepsleden iets toevoegen aan het .rmd bestand. Vertel elkaar niet wat je toevoegt. Als je klaar bent, sla op, brei, commit, en push naar GitHub. Ten minste één van jullie zal een samenvoeg conflict krijgen, dus zal het je vragen om wijzigingen van GitHub binnen te halen en het conflict op te lossen. Doe dat. Deze keer zul je het .rmd bestand moeten aanpassen in plaats van de README zoals ik eerder liet zien.\nAls je klaar bent, moeten 112 leerlingen een project opzetten met hun eigenlijke groepsprojectleden. Neem een .rmd-bestand op met de naam “ideas.rmd” waarin je ideeën kunt uitwisselen, inclusief onderwerpen en gegevens die je misschien zou willen analyseren. 155 leerlingen moeten de enquête over het groepsproject op de Moodle-pagina invullen.\nBron\nHappy git with R door Jenny Bryan. Leer meer over Distill via https://rstudio.github.io/distill.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-04-20T21:09:01+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-19-website-met-distill/",
    "title": "Website met distill",
    "description": "Website met blog maken",
    "author": [
      {
        "name": "Lisa Lendway, vertaling Harrie Jonkman",
        "url": {}
      }
    ],
    "date": "2021-02-19",
    "categories": [],
    "contents": "\nDistill\nMijn eigen Harrie’s Hoekje blog, over een aantal ontwikkelingen in de dataanalyse, maak ik ook met het gebruik van het pakket Distill. Met Distill kun je wetenschappelijke websites maken, een blog en artikelen schrijven. Over hoe je dat doet schreef Lisa Lendway een kort en krachtige blog. Dat kan ik niet beter. Dank je, Lisa, hiervoor. Haar blog staat hier\nWaarom een website?\nNou, eindelijk heb ik het gedaan!, zo begint zij haar blog dat ik (Harrie) hier verder volg.\nIk (Lisa) heb een website gemaakt. En om dat te vieren, ga ik met jullie delen hoe ik het gedaan heb. En waarom heb ik dat gedaan? Twee belangrijke redenen zijn er: 1. om materiaal te delen dat nuttig kan zijn voor anderen, 2. om wat dingen voor mezelf te documenteren, allemaal op één plek.\nIk koos voor een {distill} site omdat het me genoeg vrijheid leek te geven om mijn site aan te passen en ook weer niet zo veel vrijheid dat ik zou verzanden in details (bv. kleuren kiezen … oeps, daar heb ik toch nog veel tijd aan besteed).\nBronnen\nVoordat ik begin, zal ik wat bronnen delen die ik heb gebruikt.\nAlison Hill en Desirée De Leon’s webinar over Sharing on Short Notice. KIJK HIERNAAR voordat je verder gaat. Hier werd ik voor het eerst geïntroduceerd op netlify en toen zag ik pas hoe makkelijk het is om html-files tot een website om te vormen website. Je zou daar zelfs eerst mee kunnen beginnen voordat jij je op een website springt. Misschien spreken sommige van de andere opties die ze bespreken jou meer aan dan {distill}.\nDe distill documentatie, ook al in de vorm van een … distill website!\nDistill websites van anderen: Ijeamaka Anyene, Shannon Pileggi(aka Piping Hot Data), Miles McBain, Tom Mock, en meer!\nAlison Hill’s website voor hoog niveau inhoud en design inspiratie. Iedere keer vind ik wel een nieuw bron als ik haar website bezoek. Bijvoorbeeld, bekijk eens haar praatje op ‘Recent updates in the R markdown family’.\nEn meer! Ik zal proberen in dit blog op enkele bronnen terug te komen.\nKijk ook naar de video die Lisa maakte en die je hier vindt.\nBouwen van de site\nLaten we nu verder gaan met het maken van de site. Onderweg kom ik terug op de YouTube-video. Ik kom steeds terug op dezelfde YouTube-video, maar ik zal ze daar zetten waar ik het op dat moment over heb. Zo is het makkelijker voor je om delen over te slaan als je dat wilt.\nEen GitHub repo opzetten & het project starten\nKijk naar Tom Mock’s post hier. Ik denk dat zijn manier om dit te doen logischer is dan de mijne. Helaas, zag ik het toen ik mijn ding had gedaan :(\nIk probeer er een gewoonte van te maken om al mijn projecten met een GitHub repo te beginnen. Dus, dat is wat ik hier ook heb gedaan. Hier zijn alle stappen:\nMaak een repo\nCreëer project in R Studio door de repo te klonen * Laad de {distill} bibliotheek\nMaak een “starter” site met de create_website() functie. Ik heb dit gebruikt in plaats van create_blog() omdat ik van mijn hoofdpagina een About pagina wilde maken in plaats van een blog. Ik zal het blog gedeelte later toevoegen. Lees de {distill} documentatie om je te helpen bij het beslissingsproces. Omdat ik eerst mijn GitHub repo heb gemaakt, moest ik wat rare dingen doen om de mappenstructuur te fixen. Het werkt, maar het is een beetje lelijk.\nVerplaats alle bestanden behalve het .Rproj bestand van de zojuist gemaakte map naar de hoofdmap van het archief.\nVerwijder de website map (zou leeg moeten zijn, behalve het .Rproj bestand).\nVerwijder het README.md bestand in de hoofdmap van het archief (als ik dat niet deed, bouwde de site later niet).\n\nOf kijk naar dit deel van de video (tot minuut 8:04):\n\n\nDe site voor de eerste keer bouwen\nVervolgens willen we de site bouwen. Om dit op een eenvoudige manier te doen, sla je jouw bestanden op, sluit je RStudio en open je het opnieuw, waarbij jij ervoor zorgt dat je je in het project van jouw distill-site bevindt. Wanneer je dit doet, zou er een Build tab moeten verschijnen in jouw paneel aan de rechterbovenhoek (of waar u gewoonlijk jouw Environment, History, enz. hebt). Klik op het Build Website-icoon en je zou je site moeten zien! (8:25 in de video, als je het mij wilt zien doen).\nOp dit punt zijn er veel verschillende richtingen die je op kunt gaan. Ik zal je vertellen wat ik gedaan heb. Als je niet veel meer wilt aanpassen, kun je naar ?? gaan om een eenvoudige manier te vinden om je website te publiceren.\nAanpassen van de home page\nIk wilde dat mijn “Home” pagina mijn “About” pagina zou worden. Om dit te doen, heb ik eerst wat veranderingen aangebracht in het _site.yml bestand, het “About” gedeelte van de navigatiebalk verwijderd en de tekst voor de homepage hernoemd naar “About”.\nDan, om te beginnen met het aanpassen van mijn “About” pagina, voeg ik een foto van mezelf toe aan het index.Rmd bestand en plaats ik wat plaatshouders voor plaatsen waar ik wat informatie zal schrijven.\nBekijk dit in de video (tot minuut 17:35):\n\n\nVoeg het blog toe en maak jouw eerste post\nAls je de blog route vanaf het begin hebt gevolgd, hoef je dit deel niet te doen. Merk op dat ik in de video de dingen in de verkeerde volgorde deed\nVoeg een post toe met create_post(\"mijnpost\"). Dit genereert een R Markdown bestand met de naam mypost.Rmd (tenzij je de slug verandert), een _posts map, en een map die de datum en de naam van het bericht heeft. Door te beginnen met de datum, houdt het je berichten in een mooie volgorde :)\nBewerk jouuw blog post-Rmarkdownbestand naar believen. Zorg ervoor dat je dit bestand knit zodat het op de blog verschijnt. Deze bestanden worden niet automatisch gebreid. Dat is met opzet.\nMaak een nieuw R Markdown bestand met ALLEEN een yaml kop met een titel en listing. Sla het op in de hoofd repository.\nWijzig het _site.yml bestand om de listing pagina te linken. De tekst kan zijn wat je maar wilt - dit is wat er op de navigatiebalk komt te staan. De href waarde is de .html van het listing .Rmd bestand.\nVoeg een aangepaste blog preview afbeelding toe. Zet de afbeelding die je hiervoor wilt gebruiken in de map voor de blog post. In de yaml kop van het R Markdown bestand van uw blog, voeg je preview: image.png toe, waar image.png de naam van jouw afbeelding is. Standaard zal de preview de eerste plot zijn die gegenereerd wordt in uw R code.\nBekijk dit in de video (tot minuut 33:27):\n\n\nPas_site.yml aan\nIn dit deel voeg ik enkele aangepaste iconen toe aan de bovenste navigatiebalk van de site. Deze bevatten een persoonlijke favicon aan de linkerkant (die ik uiteindelijk toch weer weghaal) en links naar mijn GitHub, LinkedIn en Twitter pagina’s (en later voeg ik er een toe aan mijn YouTube kanaal).\nVoeg het volgende toe aan het _site.yml bestand na de navbar koptekst. Wees voorzichtig met inspringen. Je kunt mijn bestand hier bekijken (ik heb meer bewerkt sinds het maken van de video, dat wel).\n- icon: fa fa-github\n  href: https://github.com/YOUR_USERNAME\n- icon: fa fa-linkedin\n  href: https://www.linkedin.com/in/YOUR_LINKEDIN/\n- icon: fa fa-twitter\n  href: https://twitter.com/YOUR_TWITTER\nOm een gepersonaliseerde favicon toe te voegen, voeg het volgende toe na navbar:, waar ll.png de persoonlijke favicon is. Je kunt ook een link naar een website toevoegen waar hij naartoe gaat als je er op klikt. Nogmaals, wees voorzichtig met inspringen.\n  logo:\n    image: ll.png\nVolg de video hieronder (tot minuut 44:22). Toen ik dit de eerste keer deed, maakte ik wat fouten, dus ik laat je dat deel van de video overslaan.\n\n\nPubliseer de site via netlify\nNu je een website hebt, kun je die gemakkelijk publiceren via netlify. Ik zal je laten zien hoe je deze aan je GitHub repo kunt koppelen, zodat iedere keer dat je wijzigingen naar GitHub stuurt, je website die wijzigingen zal weergeven. Ik raad aan om eerst een account op netlify aan te maken.\nBekijk de video om te zien hoe ik het doe (tot minuut 48:22):\n\n\nMaak het je eigen!\nHet laatste stuk is om wat aanpassingen te doen. Dankzij de geweldige {distill} auteurs, kunnen we de create_theme() functie gebruiken om ons door het aanpassen van wat css te leiden. Ik ben een echte beginner als het op css aankomt, dus er is van een makkelijkere manier. Ik raad ten zeerste aan om de documentatie over theming en de recente updates door te lezen. En lees grondig de tekst van de website (misschien heb ik dat de eerste keer niet gedaan)!\nVoegtheme: \"my_theme.css\" aan de bodem van de _site.yml file toe.\nJe kunt de video tot het einde bekijken:\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-04-20T21:04:31+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-11-17-testen-met-bayes/",
    "title": "Testen met Bayes",
    "description": "Resultaten testen met Bayesiaanse onderzoekstechnieken.",
    "author": [
      {
        "name": "Makowski en anderen, vertaling Harrie Jonkman",
        "url": "https://easystats.github.io/bayestestR/"
      }
    ],
    "date": "2021-02-14",
    "categories": [],
    "contents": "\nKorte inleiding\nDe laatste weken lees ik weer regelmatig over de achtergronden, de principes en de voordelen van bayesiaanse onderzoekstechnieken. De update van Statistical Rethinking. A Bayesian Course with Examples in R and Stan (McElreath, 2020) en het nieuwe boek Regression and other stories (Gelman, Hill & Vehtari, 2020) geven veel inspiratie. Daarover later meer. Ondertussen verscheen vorig jaar het R-pakket bayestestR met een hele duidelijke bijbehorende website waarin een aantal uitgangspunten heel duidelijk worden uitgelegd en de voordelen van deze manier van onderzoek doen worden vergeleken met de klassieke onderzoekstechniek. Ik kon het niet laten om een aantal lessen te vertalen om dit goed in mijn vingers te krijgen. Mogelijk dat ik hier later nog een keer aandacht aan besteed. De website is gebaseerd op twee artikelen waar de wetenschappers naar refereren. Natuurlijk moet ik deze artikelen hier aan het begin noemen.\nMakowski, D., Ben-Shachar, M. S., & Lüdecke, D. (2019). bayestestR: Describing Effects and their Uncertainty, Existence and Significance within the Bayesian Framework. Journal of Open Source Software, 4(40), 1541. 10.21105/joss.01541\nMakowski, D., Ben-Shachar, M. S., Chen, S. H. A., & Lüdecke, D. (2019). Indices of Effect Existence and Significance in the Bayesian Framework. Frontiers in Psychology 2019;10:2767. 10.3389/fpsyg.2019.02767\nWaarom zou je het Bayesiaanse kader gebruiken?\nHet Bayesiaanse statistische raamwerk wint snel aan populariteit onder wetenschappers, wat samenhangt met de algemene verschuiving naar open en eerlijke wetenschap. Redenen om de voorkeur te geven aan deze aanpak zijn betrouwbaarheid, nauwkeurigheid (in rommelige data en kleine steekproeven), de mogelijkheid om prior kennis in de analyse te introduceren en, kritisch gezien, de intuïtiviteit van de resultaten en hun rechtstreekse interpretatie (Andrews & Baguley, 2013; Etz & Vandekerckhove, 2016; Kruschke, 2010; Kruschke, Aguinis, & Joo, 2012; Wagenmakers et al., 2018).\nIn het algemeen wordt de frequentisttische aanpak geassocieerd met de focus op null hypothesetests, en het misbruik van p-waarden blijkt kritisch bij te dragen aan de reproduceerbaarheidscrisis van psychologische wetenschap (Chambers, Feredoes, Muthukumaraswamy, & Etchells, 2014; Szucs & Ioannidis, 2016). Men is het er algemeen over eens dat de veralgemening van de Bayesiaanse aanpak een manier is om deze problemen te overwinnen (Benjamin et al., 2018; Etz & Vandekerckhove, 2016).\nAls we het er eenmaal over eens zijn dat het Bayesiaanse raamwerk de juiste weg is, kun je je vervolgens afvragen wat het Bayesiaanse raamwerk is.\nWaar gaat al dat gedoe over?\nWat is het Bayesiaanse kader?\nHet aannemen van het Bayesiaanse raamwerk is meer een verschuiving in het paradigma dan een verandering in de methodologie. Inderdaad, alle gemeenschappelijke statistische procedures (t-tests, correlaties, ANOVA’s, regressies, …) kunnen nog steeds worden uitgevoerd met behulp van het Bayesiaanse raamwerk. Een van de kernverschillen is dat in het frequentische perspectief (de “klassieke” statistiek, met p- en t-waarden, evenals met die rare vrijheidsgraden), de effecten vastliggen (maar onbekend zijn) en data random zijn. Aan de andere kant wordt in het Bayesiaanse inferentieproces, in plaats van schattingen van het “ware effect”, de waarschijnlijkheid van verschillende effecten berekend gegeven de waargenomen gegevens. Dat resulteert in een verdeling van mogelijke waarden voor de parameters, de zogenaamde posterior-distributie.\nDe onzekerheid in de Bayesiaanse inferentie kan bijvoorbeeld worden samengevat door de mediaan van de verdeling, evenals een reeks waarden van de posterior distributie die de 95% meest waarschijnlijke waarden omvat (het 95% waarschijnlijke interval). Cum grano salis, deze worden beschouwd als de tegenhangers van de punt-schatting en het betrouwbaarheidsinterval in een frequentistisch kader. Om het verschil in interpretatie te illustreren, laat het Bayesiaanse raamwerk toe om te zeggen “gezien de geobserveerde gegevens, heeft het effect een 95% kans om binnen dit bereik te vallen”, terwijl het minder eenvoudige alternatief voor de frequentist zou zijn “wanneer herhaaldelijk betrouwbaarheidsintervallen uit deze reeks gegevens worden berekend, is er een 95% kans dat het effect binnen een bepaald bereik valt”. In wezen geven de Bayesiaanse sampling algoritmen (zoals MCMC-bemonstering) een waarschijnlijkheidsverdeling (de posterior) van een effect dat compatibel is met de waargenomen gegevens. Zo kan een effect worden beschreven door de posterior verdeling te karakteriseren in relatie tot de centraliteit (punt-schattingen), de onzekerheid, en het bestaan en de betekenis ervan.\nMet andere woorden, als we de wiskunde achterwege laten, kunnen we dat zeggen:\nDe frequentist probeert “het reële effect” in te schatten, bijvoorbeeld, de “echte” waarde van de correlatie tussen x en y. Vandaar dat de modellen van frequentisten een “punt-schatting” opleveren. (d.w.z. één enkele waarde) van de “echte” correlatie (bv. r = 0,42) die wordt geschat op basis van een aantal onduidelijke veronderstellingen (minimaal, aangezien de gegevens willekeurig worden onttrokken van een “ouder”, meestal een normale verdeling).\nDe Bayesiaan gaat niet van zoiets uit. De gegevens zijn wat ze zijn. Op basis van deze geobserveerde gegevens (en een eerdere overtuiging over het resultaat) geeft het Bayesiaanse samplingsalgoritme (soms ook wel MCMC sampling genoemd) een waarschijnlijkheidsverdeling (de zogenaamde posterior) van het effect dat compatibel is met de geobserveerde gegevens. Voor de correlatie tussen x en y geeft het een verdeling, die bijvoorbeeld zegt: “het meest waarschijnlijke effect is 0,42, maar deze gegevens zijn ook compatibel met correlaties tussen 0,12 en 0,74”.\nOm onze effecten te karakteriseren is geen behoefte aan p-waarden of andere cryptische indices. We beschrijven gewoon de posterior verdeling van het effect. We kunnen bijvoorbeeld de mediaan, de 89% Credible Interval of andere indices rapporteren.\nMet andere woorden, als we de wiskunde even achterwege laten, kunnen we zeggen dat:\n\nHoewel het doel van dit pakket is het gebruik van Bayesiaanse statistieken te verdedigen, zijn er serieuze argumenten die de frequentie-indexen ondersteunen (zie bijvoorbeeld hier). Zoals altijd is de wereld niet zwart-wit (p < .001).\n\nNou… hoe werkt het?\nEen eenvoudig voorbeeld\nInstallatie van BayestestR\nU kunt bayestestR samen met de hele easystats suite installeren (of alleen bayestestR omdat de suite installeren bij mij niet werkte) door het volgende uit te voeren: ## A simple example\n\n\n\nLaten we ook het pakket rstanarm installeren en laden, die het mogelijk maakt om de Bayesiaanse modellen, evenals de bayestestR, te werken.\n\n\n\nTraditionele lineaire regressie\nLaten we beginnen met het aanbrengen van een eenvoudige frequentistische lineaire regressie (de lm() functie staat voor lineair model) tussen twee numerieke variabelen, Sepal.Length en Petal.Length uit de beroemde iris-dataset, standaard opgenomen in R.\n\n\nCall:\nlm(formula = Sepal.Length ~ Petal.Length, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.24675 -0.29657 -0.01515  0.27676  1.00269 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   4.30660    0.07839   54.94   <2e-16 ***\nPetal.Length  0.40892    0.01889   21.65   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4071 on 148 degrees of freedom\nMultiple R-squared:   0.76, Adjusted R-squared:  0.7583 \nF-statistic: 468.6 on 1 and 148 DF,  p-value: < 2.2e-16\n\nDeze analyse suggereert dat er een significante (wat dat ook moge betekenen) en een *positieve** (met een coëfficiënt van 0,41) lineaire relatie bestaat tussen de twee variabelen.\nHet aanpassen en interpreteren van frequentiemodellen is zo eenvoudig dat het duidelijk is dat mensen het gebruiken in plaats van het Bayesiaanse kader… toch?\nNiet meer.\nBayesiaanse lineaire regressie\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000416 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 4.16 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.042514 seconds (Warm-up)\nChain 1:                0.06525 seconds (Sampling)\nChain 1:                0.107764 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2.1e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.21 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.045785 seconds (Warm-up)\nChain 2:                0.065693 seconds (Sampling)\nChain 2:                0.111478 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.9e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.051297 seconds (Warm-up)\nChain 3:                0.068238 seconds (Sampling)\nChain 3:                0.119535 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.6e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.045173 seconds (Warm-up)\nChain 4:                0.059105 seconds (Sampling)\nChain 4:                0.104278 seconds (Total)\nChain 4: \n# Description of Posterior Distributions\n\nParameter    | Median |         89% CI |      pd |        89% ROPE | % in ROPE |  Rhat |      ESS\n-------------------------------------------------------------------------------------------------\n(Intercept)  |  4.306 | [4.183, 4.424] | 100.00% | [-0.083, 0.083] |         0 | 1.000 | 4051.872\nPetal.Length |  0.409 | [0.379, 0.437] | 100.00% | [-0.083, 0.083] |         0 | 1.000 | 3934.690\n\nDat is het! Je hebt een Bayesiaanse versie van het model gedraaid door eenvoudigweg stan_glm() te gebruiken in plaats van lm() en hebt de posterior distributie van de parameters beschreven. De conclusie die we kunnen trekken, voor dit voorbeeld, zijn zeer vergelijkbaar. Het effect (de mediaan van de posterior verdeling van het effect) is ongeveer 0,41, en het kan ook als significant worden beschouwd in de Bayesiaanse zin (meer daarover later).\nDus, klaar om meer te leren?\n1. Initiatie tot Bayesiaanse modellen\nNu je de beginsectie hebt gelezen, laten we een duik nemen in de subtiliteiten van Bayesiaanse modellering met behulp van R.\nLaden van pakketten\nAls je een keer de benodigde pakketten hebt geïnstalleerd, kun je rstanarm laden (om de modellen te draaien) en ook bayestestR (om bruikbare indices te berekenen) en insight (om toegang te krijgen tot de parameters).\n\n\n\nEenvoudig lineair model (ook wel regressie genoemd)\nWe beginnen met het uitvoeren van een eenvoudige lineaire regressie om het verband tussen Petal.Length (onze voorspeller, of onafhankelijke, variabele) en Sepal.Length (onze respons-, of afhankelijke-variabele) te testen vanuit de irisdataset die standaard is opgenomen in R.\nPassend bij het model\nLaten we beginnen met het draaien van de frequentistische versie van het model, gewoon om een referentiepunt te hebben:\n\n\nCall:\nlm(formula = Sepal.Length ~ Petal.Length, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.24675 -0.29657 -0.01515  0.27676  1.00269 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   4.30660    0.07839   54.94   <2e-16 ***\nPetal.Length  0.40892    0.01889   21.65   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4071 on 148 degrees of freedom\nMultiple R-squared:   0.76, Adjusted R-squared:  0.7583 \nF-statistic: 468.6 on 1 and 148 DF,  p-value: < 2.2e-16\n\nIn dit model is de lineaire relatie tussen Petal.Length en Sepal.Length positief en significant (beta = 0,41, t(148) = 21,6, p < .001). Dit betekent dat je voor elke toename van Petal.Length (de voorspeller) met één eenheid kunt verwachten dat de Sepal.Length (het antwoord) met 0,41 zal toenemen. Dit effect kan worden gevisualiseerd door de voorspellingswaarden op de x-as en de responswaarden als y te plotten met behulp van het ggplot2 pakket:\n\n\n\nLaten we nu een Bayesiaanse versie van het model draaien door gebruik te maken van de stan_glm-functie dat in het rstanarmpakket zit:\n\n\n\nJe ziet dat het samplingsalgoritme draait.\nDe posterior eruit halen\nLaten we, als het bovenstaande eenmaal gedaan is, de parameters (d.w.z. de coëfficiënten) van het model extraheren.\n\n  (Intercept) Petal.Length\n1    4.350261    0.3945665\n2    4.217188    0.4308909\n3    4.270616    0.4095938\n4    4.341964    0.3950089\n5    4.276013    0.4220744\n6    4.347153    0.3964028\n\nZoals we kunnen zien, hebben de parameters de vorm van een lange dataframe met twee kolommen, die overeenkomen met de intercept en het effect van Petal.Length. Deze kolommen bevatten de posterior distributies van deze twee parameters. Eenvoudig gezegd is de posterior distributie een set van verschillende plausibele waarden voor elke parameter.\nOver de posterior trekkingen\nLaten we eerst eens kijken naar de lengtes van de posteriors.\n\n[1] 4000\n\n\nWaarom zijn dit er 4000, en niet meer of minder?\n\nIn de eerste plaats worden deze waarnemingen (de rijen) meestal aangeduid als posterior ‘draws’ (trekkingen). De achterliggende gedachte is dat het Bayesiaanse samplingsalgoritme (b.v. Monte Carlo Markov Chains - MCMC) zal putten uit de verborgen ware posterior distributie . Het is dus door middel van deze ‘posterior draws’ dat we de onderliggende ware posterior distribution kunnen inschatten. Hoe meer trekkingen je hebt, hoe beter je de posterior distriubtion kunt inschatten. Meer trekkingen betekent echter ook een langere rekentijd.\nAls we kijken naar de documentatie (?sampling) voor het rstanarm“sampling”-algoritme dat standaard in het bovenstaande model wordt gebruikt, kunnen we verschillende parameters zien die het aantal posterior draws beïnvloeden. Standaard zijn er 4 ketens (je kunt het zien als aparte sampling runs), die elk 2000 iter (trekkingen, iteraties) aanmaken. Echter, slechts de helft van deze iteraties wordt behouden, aangezien de helft wordt gebruikt voor de opwarming (het convergeren van het algoritme). Het totaal is dus 4 ketens * (2000 iteraties - 1000 warming-up) = 4000 posterior trekkingen. Dat kunnen we aanpassen, bijvoorbeeld:\n\n\n\nIn dit geval hebben we, zoals verwacht, 2 ketens * (1000 iteraties - 250 warming-up) = 1500 posterior trekkingen. Maar laten we ons eerste model de standaard instelling aanhouden (omdat het meer trekkingen heeft).\nHet visualiseren van de posterieure verdeling\nNu we hebben begrepen waar deze waarden vandaan komen, laten we er eens naar kijken. We zullen beginnen met het visualiseren van de posterieure distributie van de parameter waarin we geïnteresseerd zijn, het effect van Petal.Length.\n\n\n\nDeze verdeling vertegenwoordigt de waarschijnlijkheid (de y-as) van verschillende effecten (de x-as). De centrale waarden zijn waarschijnlijker dan de extreme waarden. Zoals u ziet varieert deze verdeling van ongeveer 0,35 tot 0,50, waarbij het grootste deel rond 0,41 ligt.\n\nGefeliciteerd! Je hebt zojuist je posterior distribution beschreven.\n\nEn dit is het hart van de Bayesiaanse analyse. We hebben geen p-waarden, t-waarden of vrijheidsgraden nodig: Alles is aanwezig, binnen deze posterior verdeling.\nOnze beschrijving hierboven is consistent met de waarden verkregen uit de frequentistische regressie (die resulteerde in een bèta van 0,41). Dit is geruststellend! Inderdaad, in de meeste gevallen verandert een Bayesiaanse analyse de resultaten niet drastisch of hun interpretatie. Het maakt de resultaten wel beter interpreteerbaar en intuïtief, en uiteindelijk gemakkelijker te begrijpen en te beschrijven.\nWe kunnen nu doorgaan en deze posterior verdeling nauwkeurig karakteriseren.\nDe Posterior beschrijven\nHelaas, het is vaak niet praktisch om de hele posterieure verdelingen als grafiek te rapporteren. We moeten een beknopte manier vinden om het samen te vatten. We raden aan om de posterior verdeling te beschrijven op basis van 3 elementen:\nEen puntschatting die een samenvatting is van één waarde (vergelijkbaar met de bèta in frequente regressies).\nEen credible interval die de bijbehorende onzekerheid weergeeft.\nSommige indices van betekenis, die informatie geven over het relatieve belang van dit effect.\nPuntschatting\nWelke ene waarde kan het beste mijn posterior distributie representeren?\nCentrum indices, zoals het gemiddelde, de mediaan of de modus worden meestal gebruikt als puntschatting - maar wat is het verschil tussen het frequentische en Bayesiaanse raamwerk? Laten we dit beantwoorden door eerst het gemiddelde te inspecteren:\n\n[1] 0.4085763\n\nDit ligt dicht bij de frequentistische beta. Maar zoals we weten, is het gemiddelde vrij gevoelig voor uitschieters of extremen. Misschien is de mediaan robuuster?\n\n[1] 0.408726\n\nNou, dit is zeer dicht bij het gemiddelde (en identiek als de waarden worden afgerond). Misschien kunnen we de modus nemen, dat wil zeggen, de piek van de posterior verdeling? In het Bayesiaanse kader wordt deze waarde de Maximum A Posteriori (MAP) genoemd. Laten we daar eens kijken:\n\nMAP = 0.41\n\nZe zitten allemaal heel dichtbij elkaar! Laten we deze waarden visualiseren op de posterior distributie:\n\n\n\nNou, al deze waarden geven zeer gelijkaardige resultaten. Dus we zullen de mediaan kiezen, omdat deze waarde een directe betekenis heeft vanuit een probabilistisch perspectief: er is 50% kans dat het werkelijke effect hoger is en 50% kans dat het effect lager is (omdat het de verdeling in twee gelijke delen verdeelt).\nOnzekerheid\nNu we een puntschatting hebben, moeten we de onzekerheid beschrijven. We zouden het bereik kunnen berekenen:\n\n[1] 0.3420445 0.4783965\n\nMaar heeft het zin om al deze extreme waarden op te nemen? Waarschijnlijk niet. Dus, we zullen een credible interval berekenen. Lang verhaal kort, het lijkt een beetje op een frequentistische confidence interval, maar is makkelijker te interpreteren en gemakkelijker te berekenen - en het is logischer.\nWe zullen dit credible interval berekenen op basis van het Highest Density Interval (HDI). Het geeft ons het bereik dat de 89% meest waarschijnlijke effectwaarden bevat. We zullen 89% CIs gebruiken in plaats van 95% CIs (zoals in het frequentistische kader), omdat het 89%-niveau stabielere resultaten geeft (Kruschke, 2014) en ons herinnert aan de willekeur van dergelijke conventies (McElreath, 2020).\n\n# Highest Density Interval\n\n89% HDI     \n------------\n[0.38, 0.44]\n\nMooi, dus we kunnen concluderen dat het effect 89% kans heeft om binnen het [0,38, 0,44] bereik te vallen. We hebben zojuist de twee belangrijkste stukken informatie berekend om onze effecten te beschrijven.\nEffect significantie\nOp veel wetenschappelijke gebieden is het echter niet voldoende om alleen de effecten te beschrijven. Wetenschappers willen ook weten of dit effect betekenis heeft in praktische of statistische termen. Of, om het met andere woorden te zeggen, of het effect belangrijk is. Wijkt het effect af van 0? Dus hoe berekenen we de significantie van een effect. Hoe kunnen we dit doen?\nWel, in dit specifieke geval is het zeer welsprekend: Alle mogelijke effectwaarden (d.w.z. de hele posterior distributie) zijn positief en meer dan 0,35, wat al een substantieel bewijs is dat het effect niet nul is.\nMaar toch willen we een objectief beslissingscriterium, om te zeggen of het effect ja of nee ‘significant’ is. Een benadering, vergelijkbaar met het frequentistisch kader, zou zijn om te kijken of het Credible Interval een 0 bevat. Als dat niet het geval is, zou dat betekenen dat ons effect ‘significant’ is.\nMaar deze index is toch niet erg fijnmazig? Kunnen we het beter doen? Ja.\nEen lineair model met een categorische voorspeller\nStel je voor dat je geïnteresseerd bent in hoe het gewicht van de kippen varieert, afhankelijk van twee verschillende voedersoorten. Voor dit examen zullen we beginnen met het selecteren van twee voor ons interessante voersoorten uit de chickwts-dataset (zit ook in basis R) (we hebben wel bijzondere interesses): vleesmaaltijden (‘meat meals’) en zonnebloemen (‘sunflowers’).\nData voorbereiden en model draaien\n\n\n\nLaten we nog een Bayesiaanse regressie uitvoeren om het gewicht te voorspellen met de twee voertypesoorten.\n\n\n\nPosterior beschrijving\n\n\n\nDit representeert de posterior distributie van het verschil tussen ‘meatmeal’ en ‘sunflowers’. Het lijkt erop dat het verschil eerder positief is (de waarden lijken geconcentreerd aan de rechterkant van 0)… Het eten van zonnebloemen maakt je dikker (tenminste, als je een kip bent). Maar, door hoeveel?  Laten we de mediaan en de CI berekenen:\n\n[1] 51.56878\n\n\n# Highest Density Interval\n\n89% HDI       \n--------------\n[11.09, 90.35]\n\nHet maakt je met ongeveer 51 gram (de mediaan) dikker. De onzekerheid is echter vrij groot: er is 89% kans dat het verschil tussen de twee voersoorten tussen 14 en 91 ligt.\n\nVerschilt dit effect van 0?\n\nROPE Percentage\nTesten of deze verdeling anders is dan 0 heeft geen zin, omdat 0 een enkele waarde is (en de kans dat een verdeling anders is dan een enkele waarde is oneindig).\nEen manier om significantie te beoordelen kan echter zijn om een gebied rond 0 te definiëren, wat als praktisch equivalent van nul zal worden beschouwd (d.w.z. afwezigheid van, of verwaarloosbaar, effect). Dit wordt de ‘Region of Practical Equivalence’ (ROPE) genoemd en is een manier om de betekenis van de parameters te testen.\nHoe definiëren we dit gebied?\n\nTringgg Tringgg\n\n– U spreekt met het easystatsteam. Hoe kunnen we u helpen?\n– Ja met Prof. Sanders. Ik ben kippenexpert. Ik bel u vanwegen mijn expertkennis. Een effect tussen -20 en 20 is verwaarloosbaar. Tot ziens.\nNou, dat komt goed uit. Nu weten we dat we de ROPE kunnen definiëren als het [-20, 20] bereik. Alle effecten binnen dit bereik worden als nihil (te verwaarlozen) beschouwd. We kunnen nu het aandeel van de 89% meest waarschijnlijke waarden (de 89% CI) berekenen die niet nul zijn, d.w.z., die buiten dit bereik liggen.\n\n# Proportion of samples inside the ROPE [-20.00, 20.00]:\n\ninside ROPE\n-----------\n4.80 %     \n\n5% van de 89% CI kan als nihil worden beschouwd. Is dat veel? Gebaseerd op onze richtlijnen, ja, het is te veel. Op basis van deze specifieke definitie van ROPE concluderen we dat dit effect niet significant is (de kans dat het verwaarloosbaar is, is te groot).\nHoewel, om eerlijk te zijn, heb ik een aantal twijfels over deze Prof. Sanders. Ik vertrouw zijn definitie van ROPE** niet echt. Is er een meer objectieve manier om het te definiëren?\nJa. Een betrouwbare manier is bijvoorbeeld het gebruik van een tiende (1/10 = 0,1) van de standaardafwijking (SD) van de responsvariabele, die als een “verwaarloosbare” effectomvang kan worden beschouwd (Cohen, 1988).\n\n[1] -6.17469  6.17469\n\nLaten we onze ROPE opnieuw definiëren als de regio binnen het [-6.2, 6.2] bereik. Merk op dat dit direct kan worden verkregen met de rope_range functie :)\n\n[1] -6.17469  6.17469\n\nLaten we nu het percentage in ROPE opnieuw berekenen:\n\n# Proportion of samples inside the ROPE [-6.17, 6.17]:\n\ninside ROPE\n-----------\n0.00 %     \n\nMet deze redelijke definitie van ROPE stellen we vast dat de 89% van de posterior distributie van het effect niet overlapt met de ROPE. We kunnen dus concluderen dat het effect significant is (in de zin van belangrijk genoeg om op te merken).\nWaarschijnlijkheid van Richting (Probability of Direction (pd))\nMisschien zijn we niet geïnteresseerd in de vraag of het effect niet te verwaarlozen is. Misschien willen we alleen weten of dit effect positief of negatief is. In dit geval kunnen we eenvoudigweg berekenen welk deel van de posterior distributie positief is, ongeacht de “grootte” van het effect.\n\n[1] 98.05\n\nWe kunnen concluderen dat het effect positief is met een waarschijnlijkheid van 98%. We noemen deze index de Waarschijnlijkheid van Richting (pd). Het kan in feite gemakkelijker worden berekend met het volgende:\n\npd = 98.05%\n\nInteressant is dat deze index meestal sterk gecorreleerd is met de meest frequente p-waarde. We kunnen de overeenkomstige p-waarde bijna ruwweg afleiden met een eenvoudige transformatie:\nInterestingly, it so happens that this index is usually highly correlated with the frequentist p-value. We could almost roughly infer the corresponding p-value with a simple transformation:\n\n[1] 0.0436\n\nAls we ons model in het frequentistisch kader hebben uitgevoerd, zouden we ongeveer een effect moeten waarnemen met een p-waarde van 0.04. Is dat waar?\nVergelijking met frequentisten\n\n\nCall:\nlm(formula = weight ~ feed, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-123.909  -25.913   -6.917   32.091  103.091 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     276.91      17.20  16.097 2.74e-13 ***\nfeedsunflower    52.01      23.82   2.184   0.0405 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 57.05 on 21 degrees of freedom\nMultiple R-squared:  0.1851,    Adjusted R-squared:  0.1463 \nF-statistic: 4.769 on 1 and 21 DF,  p-value: 0.04047\n\nHet frequentistische model vertelt ons dat het verschil positief en significant (beta = 52, p = 0.04) is.\nAlhoewel we tot een gelijkaardige conclusie kwamen, liet het Bayesiaanse kader ons toe om een meer diepgaand en intuïtief begrip te ontwikkelen van ons effect en van de onzekerheid van de inschatting ervan.\nAlles met één functie\nEn toch, ik ben het ermee eens, het was een beetje omslachtig om alle indices eruit te halen en te berekenen. Maar wat als ik je vertel dat we dit allemaal kunnen doen, en meer, met slechts één functie?\n\nZie, beschrijf_posterior!\n\nDeze functie berekent alle genoemde indexen, en kan direct op het model worden uitgevoerd:\n\n# Description of Posterior Distributions\n\nParameter     |  Median |             89% CI |      pd |        89% ROPE | % in ROPE |        BF |  Rhat |      ESS\n-------------------------------------------------------------------------------------------------------------------\n(Intercept)   | 277.410 | [249.059, 305.796] | 100.00% | [-6.175, 6.175] |         0 | 5.121e+11 | 1.000 | 3071.333\nfeedsunflower |  51.569 | [ 11.086,  90.349] |  98.05% | [-6.175, 6.175] |         0 |     0.764 | 1.000 | 3260.635\n\nTada! Daar hebben we het! De mediaan, de CI, de pd en het ROPE percentage!\nHet begrijpen en beschrijven van posterior distributies is slechts één aspect van Bayesiaanse modellering… Ben je klaar voor meer? \nBevestiging van Bayesiaanse vaardigheden\nNu het beschrijven en begrijpen van posterior distributies van lineaire regressies voor jou geen geheimen meer heeft, zullen we een stap terug doen en wat eenvoudigere modellen bestuderen: correlaties en t-testen.\nMaar laten we eerst even stilstaan bij het feit dat alle statistische basisprocedures zoals correlaties, t-testen, ANOVA’s of Chisquare-testen ** lineaire regressies** zijn (we raden deze uitstekende demonstratie ten zeerste aan). Op basis van deze eenvoudige modellen introduceren we een complexere index, zoals de Bayes-factor.\nCorrelaties\nFrequentistische versie\nLaten we opnieuw beginnen met een frequentistische correlatie tussen twee continue variabelen, de breedte en de lengte van de kelkbladen van sommige bloemen (‘sepals’). De gegevens zijn beschikbaar in R als de iris dataset (dezelfde die we hierboven hebben gebruikt).\nWe zullen een Pearson’s correlatietest berekenen, de resultaten opslaan in een object met de naam resultaat en vervolgens deze resultaten weergeven:\n\n\n    Pearson's product-moment correlation\n\ndata:  iris$Sepal.Width and iris$Sepal.Length\nt = -1.4403, df = 148, p-value = 0.1519\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.27269325  0.04351158\nsample estimates:\n       cor \n-0.1175698 \n\nZoals je in de output kunt zien, heeft de test die we hebben gedaan eigenlijk twee hypothesen vergeleken: de nul-hypothese (h0; geen correlatie) met de alternatieve hypothese (h1; een niet-nul-correlatie). Op basis van de p-waarde kan de nulhypothese niet worden verworpen: de correlatie tussen de twee variabelen is negatief maar niet significant (r = -.12, p > .05).\nBayesiaanse correlatie\nOm een Bayesiaanse correlatietest te berekenen, hebben we het BayesFactor-pakket nodig (u kunt het installeren door install.packages (“BayesFactor”) uit te voeren). We kunnen dan dit pakket laden, de correlatie berekenen met behulp van de correlatieBF() functie en de resultaten op een vergelijkbare manier opslaan.\n\n\n\nLaten we nu eens onze describe_posterior()-functie hierop los:\n\n  Parameter     Median CI     CI_low    CI_high    pd ROPE_CI\n1       rho -0.1102375 89 -0.2294536 0.02167853 0.922      89\n  ROPE_low ROPE_high ROPE_Percentage        BF Prior_Distribution\n1     -0.1       0.1       0.4509969 0.5090175             cauchy\n  Prior_Location Prior_Scale\n1              0   0.3333333\n\nWe zien hier weer veel dingen, maar de belangrijke indices voor nu zijn de mediaan van de posterior distributie, -.11. Dit komt (weer) dicht in de buurt van de frequentistische correlatie. We zouden, zoals eerder, het credible interval, de pd of het ROPE-percentage kunnen beschrijven, maar we zullen ons hier richten op een andere index die door het Bayesiaanse kader wordt geboden, de Bayes-factor (BF).\nBayes-factor (BF)\nWe zeiden eerder dat een correlatietest eigenlijk twee hypothesen vergelijkt, een nul (afwezigheid van effect) met een alarmerende (aanwezigheid van een effect). De Bayes-factor (BF) laat dezelfde vergelijking toe en bepaalt onder welke van twee modellen de geobserveerde gegevens waarschijnlijker zijn: een model met het effect waarin we geinteresseerd zijn, en een nulmodel zonder het effect daarvan. We kunnen de bayes-factor() gebruiken om de Bayes-factor specifiek te berekenen bij het vergelijken van die modellen:\n\n# Bayes Factors for Model Comparison\n\n  Model             BF\n  [2] (rho != 0) 0.509\n\n* Against Denominator: [1] (rho = 0)\n*   Bayes Factor Type: JZS (BayesFactor)\n\nWe hebben een BF van 0,51. Wat betekent dat?\nBayes-factoren zijn continue metingen van het relatieve bewijs, waarbij een Bayes-factor groter dan 1 bewijs geeft ten gunste van één van de modellen (vaak de teller genoemd), en een Bayes-factor kleiner dan 1 die bewijs geeft ten gunste van het andere model (de noemer).\n\nJa, je hebt het goed gehoord, bewijs ten gunste van de nul!\n\nDat is een van de redenen waarom het Bayesiaanse kader soms als superieur wordt beschouwd aan het frequentistische kader. Onthoud uit je statistiekenlessen, dat de p waarde alleen gebruikt kan worden om h0 af te wijzen, maar niet om het te accepteren. Met de Bayes-factor kunt je -evidentie meten tegen - en ook ten gunste van - de nul.\nBF’s die het bewijs voor het alternatief tegen de null vertegenwoordigen kunnen worden teruggedraaid met 𝐵𝐹01=1/𝐵𝐹10 (de 01 en 10 komen respectievelijk overeen met h0 tegen h1 en h1 tegen h0) om het bewijs voor de null weer te geven. Dit verbetert de leesbaarheid in gevallen waarin het BF van het alternatief tegen de nul kleiner is dan 1 (d.w.z. ter ondersteuning van de nul).\nIn ons geval, BF = 1/0,51 = 2, geeft aan dat de gegevens 2 keer meer waarschijnlijk zijn onder de null in vergelijking met de alternatieve hypothese. Die weliswaar de voorkeur geeft aan de nul-hypothese, maar slechts als anekdotisch bewijs moet wordt beschouwd.\nWe kunnen dus concluderen dat er anecdotisch bewijs is ten gunste van de hypothese ‘gebrek aan correlatie tussen de twee variabelen’ (mediaan = 0,11, BF = 0,51), wat veel meer informatie geeft dan wat we kunnen doen met de frequentistische statistiek.\nEn dat is nog niet alles!\nVisualiseren van de Bayes-factor\nIn het algemeen zijn taartgrafieken een absolute ‘no-go’ in datavisualisatie, omdat het waarnemingssysteem van onze hersenen de gepresenteerde informatie op deze manier sterk vervormt. Toch is er één uitzondering: pizzagrafieken.\nHet is een intuïtieve manier om de bewijskracht van BFs te interpreteren als een soort verrassing\nDergelijke “pizzapercelen” kunnen direct worden aangemaakt via het zie visualisatiepakket voor easystats (u kunt het installeren door het uitvoeren van\nDergelijke ‘pizzagrafieken’ kunnen direct worden aangemaakt met het visualisatiepakket voor easystats (u kunt het installeren door install.packages(\"see\")) uit te voeren):\n\n\n\nDus, na het zien van deze pizza, ben je dan nog verrast door de uitkomst?\nt-testen\n\n“Ik weet dat ik niets weet, en vooral niet als versicolor en virginica verschillen in termen van Sepal.Width”, zei de beroemde Socrates.\n\nTijd om eindelijk een antwoord te geven op deze cruciale vraag!\nVersicolor vs. virginica\nBayesiaanse t-testen kunnen worden uitgevoerd op een zeer vergelijkbare manier als correlaties. We zijn met name geïnteresseerd in twee niveaus van de Specie factor, versicolor en virginica. We zullen beginnen met het uit iris uitfilteren van de niet-relevante waarnemingen die overeenkomen met de setosa specie, en we zullen dan de waarnemingen en de distributie van de Sepal.Width variabele visualiseren.\n\n\n\nBereken de Bayesiaanse t-test\nHet lijkt er (visueel) op dat virgnica bloemen gemiddeld een iets grotere kelkbladbreedte hebben. Laten we dit verschil statistisch beoordelen met behulp van de ttestBF in het BayesFactor pakket.\n\n   Parameter    Median CI    CI_low   CI_high    pd ROPE_CI\n1 Difference 0.1871487 89 0.0946914 0.2936964 0.998      89\n    ROPE_low ROPE_high ROPE_Percentage       BF Prior_Distribution\n1 -0.0332751 0.0332751               0 17.71872             cauchy\n  Prior_Location Prior_Scale\n1              0   0.7071068\n\nOp basis van de indexen kunnen we zeggen dat het verschil tussen virginica en versicolor (van Sepal.Width) een kans heeft van 100% om negatief te zijn [van de pd en het teken van de mediaan] (mediaan = -0,19, 89% CI [-0,29, -0,092]). De gegevens leveren een sterk bewijs tegen de nulhypothese (BF = 18).\nHoud dat in gedachten, want we zullen een andere manier zien om deze vraag te onderzoeken.\nLogistisch Model\nEen hypothese waarvoor men een t-test gebruikt, kan ook getest worden met een binomiaal model (bv. een logistisch model). Het is inderdaad mogelijk om de volgende hypothese te herformuleren, “er is een belangrijk verschil in deze variabele tussen de twee groepen” door “deze variabele in staat te stellen om te discrimineren tussen (of te classificeren in) de twee groepen”. Deze modellen zijn echter veel krachtiger dan een gewone t-test.\nIn het geval van het verschil van Sepal.Width tussen virginica en versicolor wordt de vraag, hoe goed kunnen we de twee soorten classificeren met alleen Sepal.Width.\nHet model fitten\n\n\n\nPrestatie en parameters\nEerst prestatie van het model in kaart brengen.\n\nCan't calculate log-loss.\n# Indices of model performance\n\nELPD   | ELPD_SE |  LOOIC | LOOIC_SE |   WAIC |   R2 | RMSE | Sigma | Score_log | Score_spherical\n-------------------------------------------------------------------------------------------------\n-66.25 |    3.04 | 132.51 |     6.08 | 132.50 | 0.10 | 1.11 |  1.00 |   -105.55 |            0.01\n\nVervolgens de resultaten van enkele indices presenteren.\n\n# Description of Posterior Distributions\n\nParameter   | Median |           89% CI |      pd |        89% ROPE | % in ROPE |     BF |  Rhat |      ESS\n-----------------------------------------------------------------------------------------------------------\n(Intercept) | -6.081 | [-9.268, -2.676] | 100.00% | [-0.181, 0.181] |         0 | 12.135 | 1.000 | 2931.465\nSepal.Width |  2.118 | [ 0.916,  3.198] | 100.00% | [-0.181, 0.181] |         0 | 17.399 | 1.001 | 2912.719\n\nReferenties\nAndrews, M., & Baguley, T. (2013). Prior approval: The growth of bayesian methods in psychology. British Journal of Mathematical and Statistical Psychology, 66(1), 1–7.\nBenjamin, D. J., Berger, J. O., Johannesson, M., Nosek, B. A., Wagenmakers, E.-J., Berk, R., … others. (2018). Redefine statistical significance. Nature Human Behaviour, 2(1), 6.\nChambers, C. D., Feredoes, E., Muthukumaraswamy, S. D., & Etchells, P. (2014). Instead of ’playing the game’ it is time to change the rules: Registered reports at aims neuroscience and beyond. AIMS Neuroscience, 1(1), 4–17.\nEtz, A., & Vandekerckhove, J. (2016). A bayesian perspective on the reproducibility project: Psychology. PloS One, 11(2), e0149794.\nKruschke, J. K. (2010). What to believe: Bayesian methods for data analysis. Trends in Cognitive Sciences, 14(7), 293–300.\nKruschke, J. K., Aguinis, H., & Joo, H. (2012). The time has come: Bayesian methods for data analysis in the organizational sciences. Organizational Research Methods, 15(4), 722–752.\nSzucs, D., & Ioannidis, J. P. (2016). Empirical assessment of published effect sizes and power in the recent cognitive neuroscience and psychology literature. BioRxiv, 071530.\nWagenmakers, E.-J., Marsman, M., Jamil, T., Ly, A., Verhagen, J., Love, J., … others. (2018). Bayesian inference for psychology. Part i: Theoretical advantages and practical ramifications. Psychonomic Bulletin & Review, 25(1), 35–57.\n\n\n\n",
    "preview": "posts/2020-11-17-testen-met-bayes/testen-met-bayes_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2021-02-14T12:26:03+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-11-01-nogmaals-grafieken/",
    "title": "Het Goede, het Slechte en het Lelijke: Data effectief visualiseren en communiceren",
    "description": "Shirin Elsinghorst schreef deze blog onlangs op [Codecentric](https://blog.codecentric.de/2020/10/goodbadugly/). Omdat ze op mijn werk de vormgeving van de uitgaven hebben aangepast, wilde ik het maken van figuren aan de nieuwe kleursetting van mijn werk aanpassen. Shirin's blog was een mooie oefenplaats voor mij. Tegelijk is het een mooie introductie op datavisualisatie en daarom de moeite waard het in het Nederlands te bewerken.    [Hier](https://docs.google.com/presentation/d/e/2PACX-1vR4pD2EmW9Gzxr1Q3qwgjEYkU64o2-ThlX1mXqfNQ2EKteVUVt6Qg2ImEKKi9XLv-Iutb3lD8esLyU7/pub?start=false&loop=false&delayms=3000&slide=id.g58b36409ef_0_0) vind je de presentatie die zij zelf hierover op 20 oktober 2020 in Duitsland gaf.",
    "author": [
      {
        "name": "Shirin Elsinghorst, vertaling Harrie Jonkman",
        "url": "https://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/"
      }
    ],
    "date": "2020-11-01",
    "categories": [],
    "contents": "\nEnkele handelingen vooraf\nEerst maar eens de pakketten laden die gebruikt worden:\n\n\n\nVervolgens de kleuren instellen.\nDe dataset\n\n# A tibble: 6 x 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm\n  <fct>   <fct>              <dbl>         <dbl>             <int>\n1 Adelie  Torgersen           39.1          18.7               181\n2 Adelie  Torgersen           39.5          17.4               186\n3 Adelie  Torgersen           40.3          18                 195\n4 Adelie  Torgersen           NA            NA                  NA\n5 Adelie  Torgersen           36.7          19.3               193\n6 Adelie  Torgersen           39.3          20.6               190\n# … with 3 more variables: body_mass_g <int>, sex <fct>, year <int>\n\n\n\n[1] 0.3926991 1.1780972 1.9634954 2.7488936 3.5342917 4.3196899\n[7] 5.1050881 5.8904862\n\n\n\n\n\n\n\n\n\n\n0. Datavisualisatie, cruciaal voor begrip en communicatie\nDatavisualisatie is een cruciaal onderdeel van elke analyse. Of het nu “voor jezelf” is om verbanden en resultaten beter te begrijpen of om resultaten te presenteren en te “verkopen” aan anderen. Omdat goede grafieken de gegevens intuïtief toegankelijk maken, vertellen ze een verhaal en laten ze duidelijk patronen, trends of uitschieters zien. Daarom is Explorative Data Analyse (EDA) meestal de eerste stap van elke gegevensanalyse en -modellering. Alleen als we onze gegevens begrijpen, kunnen we de juiste voorbewerkingsstappen en analysemethoden, statistieken of ’deep learning’technieken toepassen. En aangezien mensen veel beter zijn in het visueel begrijpen van getallen in een grafiek dan in tabellen, moeten we de kracht van datavisualisatie gebruiken! Vooral bij het maken van visualisaties voor rapporten of publicaties is het cruciaal dat deze zowel feitelijk correct als visueel aantrekkelijk zijn.\n\nDatavisualisatie is deels kunst en deels wetenschap. De uitdaging is om de kunst goed te krijgen zonder dat de wetenschap het bij het verkeerde eind heeft en vice versa. (Wilke 2019)\n\nHet doel van een goede illustratie is dat het in één oogopslag begrijpelijk is en een duidelijke uitspraak doet. Slechte grafieken variëren van eenvoudigweg lelijk tot (opzettelijk?) misleidend of zelfs verkeerd. In dit artikel leg ik uit wat goede grafieken zijn (en wat niet) en hoe we ze kunnen maken met behulp van de Grammatica van Grafieken. En ik introduceer enkele van de meest gebruikte soorten grafieken, samen met negatieve voorbeelden “uit de vrije natuur”.\nWat maakt een goede grafiek?\n1. Data\nHet belangrijkste aspect en de basis van elke grafiek zijn de gebruikte gegevens! Ze moeten correct zijn en we moeten altijd controleren op mogelijke (meet)fouten. Zoals bijvoorbeeld in deze afbeelding:\n\nHet is duidelijk dat er een fout in de voorspelling zit, want temperaturen van -100°C, zelfs op een hele speciale dag, zijn echt heel onwaarschijnlijk! En niet alleen toont deze grafiek duidelijk verkeerde gegevens; deze uitschietwaarde comprimeert de rest van de gegevens in de grafiek zodanig dat de curven niet meer duidelijk zichtbaar zijn en de waarden van verschillende dagen moeilijk te vergelijken zijn.\nTot goede praktijk van datavisualisatie behoort tevens het specificeren van de gegevensbron.\n2. Overzichtelijkheid & kleuren\nEen goede grafiek is juist zo complex omdat ze haar boodschap in één oogopslag moet overbrengen; ze moet duidelijk zijn en geen “chart junk” bevatten. Edward Tufte beschouwde alle visuele elementen in een grafiek die ofwel niet nodig zijn om de grafiek te begrijpen, ofwel zelfs afleiden van de centrale informatie als grafiektroep (Tufte 1983). Drie voorbeelden daarvan zijn in deze figuur te zien:\n\nIedereen die regelmatig kranten leest zal merken dat “chart junk” extreem vaak voorkomt en vooral populair is bij populaire media om een chart te kruiden. In veel gevallen kan een onschuldige chartjunk worden afgedaan als “artistieke vrijheid”, maar hoe ernstiger jouw grafiek wilt maken, hoe meer je die moet vermijden.\nDuidelijkheid omvat ook de keuze van geschikte kleuren en duidelijke contrasten. Te veel, felle of grillige kleuren maken een plot visueel weerzinwekkend en zorgen ervoor dat het er overbeladen en verwarrend uitziet. Daarnaast moet je er bij de keuze van de kleuren ook op letten dat het voor kleurenblinden mogelijk is om de grafiek te lezen. Dit moet worden gedaan met behulp van een geschikt kleurenpalet, evenals redundante functies zoals verschillende vormen, patronen of lijntypes die het mogelijk maken om de grafiek te lezen, zelfs zonder kleurinformatie.\nKleuren\n3. etikettering & assen\nEen goede grafiek is in één oogopslag te begrijpen, wat betekent dat ze voor zichzelf spreken. Essentieel hiervoor is de juiste etikettering met titel, aslabels (met eenheden!), legenda’s en bijschriften! Ook het aantal assentikken moet op de juiste manier worden gekozen. En vooral: de afstanden tussen de assen moeten voor de numerieke waarden regelmatig zijn, d.w.z. dat de afstand tussen de teken in geen geval mag variëren, zoals te zien is op de y-as in deze vreselijke illustratie:\n\nDe grammatica van de grafiek\nWe kennen nu de belangrijkste aspecten van goede grafieken. Maar wat is de beste manier om goede grafieken te maken? Er zijn vele manieren om grafieken te genereren: met de hand tekenen, met Excel of met verschillende programmeertalen zoals R, Python, Java, enz. De beste manier, hoewel met een hogere instapdrempel, is het gebruik van programmeertalen. Dit is de eenvoudigste manier om ervoor te zorgen dat de gegevens schoon zijn en op een traceerbare manier kunnen worden verwerkt. Excel kan ook gebruikt worden om grafieken te maken, maar het programma heeft een paar valkuilen: Excel-format kan makkelijk leiden tot fouten in de gegevens. En het maakt het erg moeilijk om grafieken te reproduceren, omdat het niet documenteert welke stappen handmatig werden uitgevoerd en in welke volgorde.\nR en Python zijn bijzonder geschikt omdat ze de meest gebruikte programmeertalen zijn voor het genereren van grafieken en omdat ze pakketten aanbieden die het analyseren van gegevens en het maken van grafieken zeer efficiënt maken. Hier presenteer ik de (naar mijn mening) beste manier om op een gestructureerde manier grafieken te genereren: met de pakketten ggplot2 voor R of plotnine voor Python (gebaseerd op ggplot2).\nMet ggplot2 heeft Hadley Wickham een implementatie gemaakt van de 1999 Grammatica van Graphics voor de door Leland Wilkinson beschreven R-programmeertaal, die ik hieronder introduceer met codevoorbeelden (Wilkinson et al. 1999; Wickham 2010). Deze Grammatica van Graphics beschrijft een raamwerk voor het gestructureerd maken van grafieken die bestaan uit lagen die op elkaar voortbouwen (Wickham en Grolemund 2017). Hieronder laat ik een paar voorbeelden zien. Een overzicht van alle mogelijke opties is te vinden in het ggplot2-cheatsheet.\n1. Data\nOok voor de Grammatica van Graphics zijn data het belangrijkste en meest fundamentele element. Hier gebruik ik een voorbeelddataset met verschillende groottes van drie pinguïnsoorten (Gorman, 2014). De centrale functie van het ggplot2-pakket wordt ggplot() genoemd en neemt een dataset als invoer. Deze functie creëert eerst een leeg coördinatensysteem waarop we met de volgende lagen kunnen bouwen en zo onze grafiek stap voor stap kunnen creëren, aanpassen en uitbreiden.\n2. Esthetiek\nHet tweede argument dat we definiëren in de ggplot() functie is de esthetiek (aes()). Esthetiek beschrijft grafische elementen zoals X- en Y-waarden, grootte, kleuren, vormen, enz. Voor een eenvoudige scatterplot moeten we ten minste de X- en Y-posities specificeren. Hiervoor moeten we eerst beslissen welke gegevens (variabelen) we in kaart willen brengen. Bijvoorbeeld hier de snavellengte van de pinguïn op de X-as tegen de vinlengte op de Y-as:\n\n\n\nZelfs met een bepaalde esthetiek krijgen we nog steeds geen echte grafiek te zien, maar we hebben de volgende laag nodig, de zogenaamde geometrie. Omdat esthetiek en geometrie zeer nauw met elkaar verbonden zijn en deels van elkaar afhankelijk zijn, laat ik hieronder extra esthetiek zien.\n3. Geometriek\nGeometrische objecten of geometrieën beschrijven hoe de gegevens die we in de esthetiek hebben gedefinieerd, moeten worden weergegeven. Dit kan bijvoorbeeld een puntgrafiek (geom_point()) of een lijngrafiek (geom_line()) zijn, die nu ook als grafiek in deze laag wordt weergegeven:\n\n\n\nEen lijngrafiek is echter niet nuttig voor de gegevens hier; meer hierover in de sectie Grafiektypen - Lijngrafieken. Andere geometrieën zijn staafdiagrammen (geom_bar()) of boxplots (geom_boxplot()). Geometrie en esthetiek zijn onderling afhankelijk in die zin dat het gegevenstype van de esthetische variabelen alleen bepaalde geometrieën toelaat of zinvol is. Zo zijn strooi- en lijndiagrammen geschikt voor doorlopende X- en Y-assen (rationele getallen, tijden of datum). Voor staafdiagrammen moeten de X-as gegevens categorisch zijn. Voordat ik in de loop van latere lagen meer in detail zal ingaan op geometrieën en esthetiek, zal ik eerst facetten introduceren.\n4. Facetten\nFacetten betekent het splitsen van een grafiek in verschillende subplots. In onze voorbeelddataset worden de meetwaarden van drie verschillende pinguïnsoorten verzameld. Het bovenstaande strooiplot laat echter niet toe om een onderscheid te maken tussen de drie soorten, wat natuurlijk een belangrijke bijkomende informatie is in de gegevens. Daarom moeten we dit in onze grafiek weergeven. Een manier om dit te doen is door gebruik te maken van facetten:\n\n\n\nNu zien we de punten voor elke soort pinguïn in een aparte subplot. Facetten kunnen worden gecreëerd voor één of meer categorische variabelen, maar meer dan twee facetten zullen in het algemeen verwarrend zijn. Standaard gebruikt ggplot2 dezelfde X- en Y-asafmetingen om de subplots vergelijkbaar te maken. Met facetten is het in dit geval echter niet zo eenvoudig om de drie typen te vergelijken. Als alternatief kunnen we de drie pinguïnsoorten zichtbaar maken met behulp van verschillende kleuren. Deze mogelijkheid valt onder schaalvergroting.\n5. Schaalvergroting\nMet schaalvergroting kunnen we naast de twee X- en Y-dimensies nog extra dimensies tonen, vergelijkbaar met wat we al gedaan hebben voor de pinguïnsoorten met facetten. Zo kunnen we bijvoorbeeld een kleurenschaal kiezen. In ggplot2 worden schalen gegeven door extra variabelen in de esthetiek:\n\n\n\nDe bijbehorende legende wordt automatisch aangemaakt. Andere schalen zijn maatschalen, punt- of lijntypes. Niet alle schalen zijn geschikt voor elk datatype. Terwijl kleuren ook continue getallen kunnen vertegenwoordigen, zijn punt- en lijntypes slechts voor een beperkt aantal categorieën beschikbaar.\n\n\n\nIn principe kan elk aantal dimensies van de gegevens in een grafiek worden weergegeven, ook al kunnen meer dan vier dimensies de grafiek meestal te chaotisch en verwarrend maken.\nEen ander type schaling is de asschaalverdeling. Zo kunnen we bijvoorbeeld de assen omdraaien zodat de waarden niet van links/naar beneden = laag naar rechts/boven = hoog worden weergegeven, maar de hoge waarden wel links/naar beneden worden weergegeven:\n\n\n\n6. Statistische Transformaties\nStats, afkorting voor statistische transformaties, worden gebruikt om statistische waarden of berekeningen toe te voegen aan een plot of om deze te definiëren. Dit kunnen bijvoorbeeld gemiddelde waarden zijn, mediaan, betrouwbaarheidsintervallen, standaardafwijkingen, enz.\nIn deze figuur is een staafdiagram weergegeven met een numerieke waarde:\n\n\n\nOmdat de standaardstatistiek voor staafdiagrammen in ggplot2 „Aantal (count)“ ist, kunnen deze verhoudingen mit de stat „identity“ worden veranderd.\nEen van de meest gebruikte statistieken zijn ‘Smoothed Conditional Means’, om bijvoorbeeld de samenhang van de X- en Y-Variabelen met gladde lijnen en bijbehorende foutmarges aan te tonen:\n\n\n\n7. Coördinatensystemen\nDe laatste laag in de Grammatica van Graphics zijn coördinatensystemen. Coördinatensystemen bepalen hoe de assen van onze grafiek moeten worden gerangschikt. Meestal is de X-as horizontaal en de Y-as verticaal (cartesiaans coördinatenstelsel); maar er zijn ook gevallen waarin we radiale of gebogen assen hebben, bijvoorbeeld in een taartdiagram of kaartweergave. Een taartdiagram is dus niets meer dan een staafdiagram waarin we het coördinatensysteem hebben veranderd:\n\n\n\nDiagramtypen Met deze Grammatica van Graphics kunnen nu alle gangbare diagramtypen eenvoudig en flexibel worden gegenereerd en uitgebreid. De meest gebruikte diagramtypen zijn:\nPuntdiagrammen\n\n\n\nPuntdiagrammen worden vaak gebruikt wanneer we numerieke X-waarden tegen numerieke Y-waarden willen weergeven en dus hun correlatie willen laten zien. Plots kunnen verschillende kleuren, vormen en maten hebben. Meestal zijn puntdiagrammen gemakkelijk te begrijpen, maar ze kunnen ook verwarrend worden als er te veel overlappende punten zijn.\nLijndiagrammen\n\n\n\nLijndiagrammen zijn meestal vergelijkbaar met puntdiagramma, met dat verschil dat de (denkbeeldige) punten met elkaar verbonden zijn door lijnen. Deze verbonden lijnen vertegenwoordigen de denkbeeldige tussenliggende waarden tussen twee meetpunten; punten moeten dus alleen verbonden worden als deze veronderstelling wordt gemaakt! Om deze reden is een lijngrafiek niet bruikbaar voor het bovenstaande voorbeeld, omdat we onafhankelijke metingen van individuele personen laten zien. Lijndiagrammen zijn vooral nuttig voor tijdreeksen.\nStaafdiagrammen\n\n\n\nStaafdiagrammen tonen ofwel het aantal gebeurtenissen of ze tonen een numerieke waarde Y ter vergelijking tussen verschillende categorieën. Vooral bij staafdiagrammen vinden we veel negatieve voorbeelden met misleidende voorstellingen (waarschijnlijk omdat ze zo gemakkelijk te maken zijn met de eenvoudigste tekenprogramma’s zonder enige gegevensbasis). Hier zijn twee zeer opvallende negatieve voorbeelden van de coronavirus situatie: in beide voorbeelden komen de staafhoogtes niet overeen met de waarden op de (niet getoonde) Y-as!\n\n\nEen verzameling van andere veelgebruikte diagramtypen met illustraties en negatieve voorbeelden is te vinden in de dia’s bij deze lezing.\nReferenties\nGorman, Tony D. AND Fraser, Kristen B. AND Williams. 2014. “Ecological Sexual Dimorphism and Environmental Variability Within a Community of Antarctic Penguins (Genus Pygoscelis).” PLOS ONE 9 (3): 1–14. https://doi.org/10.1371/journal.pone.0090081.\nTufte, Edward R. 1983. The Visual Display of Quantitative Information. Graphics Press.\nWickham, Hadley. 2010. “A Layered Grammar of Graphics.” Journal of Computational and Graphical Statistics 19 (1): 3–28. https://doi.org/10.1198/jcgs.2009.07098. Wickham, Hadley, and Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 1st ed. O’Reilly Media, Inc. https://r4ds.had.co.nz/.\nWilke, C. O. 2019. Fundamentals of Data Visualization: A Primer on Making Informative and Compelling Figures. O’Reilly Media. https://books.google.de/books?id=L3ajtgEACAAJ.\nWilkinson, L., D. Wills, J. Chambers, R. Dubbs, W. Eddy, A. Norton, and W. Haerdie. 1999. The Grammar of Graphics. Statistics and Computing. Springer New York. https://books.google.de/books?id=5boZAQAAIAAJ.\n\n\n\n",
    "preview": "posts/2020-11-01-nogmaals-grafieken/nogmaals-grafieken_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-07-11T15:28:57+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-10-17-een-eenvoudige-introductie-op-machine-learning/",
    "title": "Een eenvoudige introductie op `tidymodels`",
    "description": "Edgar Ruiz' eenvoudige introductie op machine learning met de inzet van het pakket `tidymodels`.",
    "author": [
      {
        "name": "Edgar Ruiz, vertaling Harrie Jonkman",
        "url": "https://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/"
      }
    ],
    "date": "2020-10-17",
    "categories": [],
    "contents": "\nIntroductie Harrie\nIk heb me voorgenomen om wat machine learning te leren. Het komt mij allemaal nog wat onbekend voor. Door soms tutorials van anderen over te zetten en te kijken wat er gebeurt, wil ik hierin verder komen. Edgar Ruiz schreef een korte inleiding en zijn tutorial heb ik, brutaal als ik ben, naar het Nederlands overgezet. Edgar Ruiz, ik hoop dat je dit goed vindt. Onderaan vermeld ik waar de lezer jouw oorspronkelijke tutorial kan vinden.\nEen rustige introductie op tidymodels\nThe WorkflowOnlangs had Edgar Ruiz de gelegenheid om tidymodels te laten zien in workshops en gesprekken. Omdat hij zichzelf meer ziet als gebruiker dan ontwikkelaar, zou het wel eens waardevol en interessant kunnen zijn, zo dacht hij, om te delen wat hij tot nu toe had geleerd. Laten we eerst eens bekijken wat tidymodels in onze analyseprojecten kan betekenen, dat was het doel van zijn korte en duidelijke introductie tidymodels.\nHet figuur hierboven is gebaseerd op een figuur uit R voor Data Science boek, het boek van Wickham en Grolemund en wordt heel vaak gebruikt. Alleen wordt hier het onderdeel modeleren met tidymodels uitgevoerd en dat is nieuw. In de introductie laat hij zien welke stappen hier gezet moeten worden. Modeleren kan baat hebben bij een ‘nette’ interface, dat is waar tidymodels een rol speelt.\nHet is belangrijk om te verduidelijken dat de groep van pakketten die deel uitmaken van tidymodels niet zelf statistische modellen implementeren. In plaats daarvan richten ze zich vooral op het makkelijker maken van alle taken die te maken hebben met modeleren. Deze taken omvatten het voorbewerken van gegevens tot en met het valideren van resultaten.\nIn zekere zin kent het modeleren enkele substappen. Voor deze substappen levert tidymodels één of meerdere pakketten. Dit artikel toont functies uit vier tidymodels pakketten, die alle vier in de suite tidymodels zijn opgesloten:\nrsample - Verschillende types van re-samples recipes - Transformaties om data voor te bewerken voor modeleren parnip - Een algemene interface voor modelcreatie yardstick - Meten hoe het model het doet\nHet volgend figuur illustrates elkw modelleerstapt, en laat de tidymodels pakketten zien die we in dit artikel zullen gebruiken:\nThe WorkflowIn een bepaalde analyse kan al dan niet het tidyverse pakket worden gebruikt. Niet alle projecten hoeven te werken met tijdsvariabelen, dus het is niet altijd nodig om functies uit het hms pakket te gebruiken. Hetzelfde idee geldt voor tidymodels. Afhankelijk van wat voor soort modelering er gedaan gaat worden, zullen alleen functies uit sommige van de pakketten gebruikt worden.\nEen voorbeeld\nWe zullen de iris dataset hiervoor gebruiken. De data zijn al binnen gehaald en voldoende opgeschoond om direct te modeleren.\nLaad alleen de tidymodels bibliotheek\nDit is mogelijk het eerste artikel dat hij heeft geschreven waarbij hij slechts één pakket heeft aangeroepen via de bibliotheek(). Naast het laden van de kernpakketten voor het modelleren, laadt tidymodels, ook handig, een aantal tidyverse pakketten, waaronder dplyr en ggplot2. Gedurende deze oefening zullen we enkele functies uit die pakketten gebruiken, maar we hoeven ze dus niet expliciet in onze R-sessie te laden.\n\n\n\nHet voorwerk\nDeze eerstestap richt zich op het geschikt maken van data voor modellering. Daarbij wordt gebruik gemaakt van datatransformaties. Alle transformaties kunnen worden uitgevoerd met dplyr, of andere tidyverse pakketten Overweeg het gebruik van tidymodels pakketten wanneer dit deel zwaarder en complexer is.\nData Sampling\nDe initial_split() functie is vooral gebouwd om de dataset op te splitsen in een trainings en test set. Standaard wordt 3/4 van de data voor training en de rest voor testen gebruikt. Dat kan aangepast worden door het prop functie te gebruiken. Dit genereert een rplit object, geen dataframe. De geprinte output laat het aantal rijen voor testen, trainen en het totaal zien.\n\n<Analysis/Assess/Total>\n<90/60/150>\n\nOm toegang te krijgen tot de observaties van de trainingsset, gebruik je de training() functie. Hetzelfde voor testset waar je toegang toe krijgt via testing().\n\nRows: 90\nColumns: 5\n$ Sepal.Length <dbl> 4.7, 5.4, 5.0, 4.4, 4.9, 4.8, 4.3, 5.8, 5.1, 5…\n$ Sepal.Width  <dbl> 3.2, 3.9, 3.4, 2.9, 3.1, 3.0, 3.0, 4.0, 3.8, 3…\n$ Petal.Length <dbl> 1.3, 1.7, 1.5, 1.4, 1.5, 1.4, 1.1, 1.2, 1.5, 1…\n$ Petal.Width  <dbl> 0.2, 0.4, 0.2, 0.2, 0.1, 0.1, 0.1, 0.2, 0.3, 0…\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa…\n\nDeze samplingfuncties zijn mogelijk met behulp van het rsample pakket, dat deel uitmaakt van tidymodels.\nPre-proces interface\nIn tidymodels biedt het recipes pakket een interface dat gespecialiseerd is in het voorbewerken van gegevens. Binnen het pakket worden de functies die de gegevenstransformaties starten, of uitvoeren, vernoemd naar kookacties. Dat maakt de interface gebruiksvriendelijker. Bijvoorbeeld:\nrecipe() - start een nieuwe set van toe te passen transformaties, vergelijkbaar met het ggplot() commando. Het belangrijkste argument is de formule van het model.\nprep() - Voert de transformaties uit bovenop de geleverde gegevens (meestal de trainingsgegevens).\nElke datatransformatie is een stap. Functies komen overeen met specifieke soorten stappen, die elk een voorvoegsel van step_ hebben. Er zijn verschillende step_ functies; in dit voorbeeld gebruiken we er drie:\nstep_corr() - Verwijdert variabelen die sterk correleren met andere variabelen\nstep_center() - Normaliseert numerieke data die een gemiddelde van nul krijgen\nstep_scale() - Normaliseert numerieke data die een standard deviatie van één krijgen\nEen ander aardig kenmerk van deze step is dat deze kan worden toegepast op een specifieke variabele, groepen variabelen of alle variabelen. De all_outocomes() en all_predictors() functie bieden een hele prettige manier om specifieke groepen variabelen te specificeren. Bijvoorbeeld, als we de step_corr() willen gebruiken om alleen de predictorvariabelen te analyseren, gebruiken we step_corr(all_predictors()). Zo hoeven we niet elke variabele op te sommen.\nIn het volgende voorbeeld, brengen we de recipe(), prep() en step-functies om een recipe object te creëren. De training() functie wordt gebruikt om de dataset uit de eerder aangemaakte gesplitste dataset te halen.\n\n\n\nAls we het iris_recipe object oproepen, zal het details hierover afdrukken. De Operations sectie beschrijft wat er met de gegevens is gedaan. Een van de bewerkingen in het voorbeeld legt uit dat de correlatiestap de Petal.Length variabele heeft verwijderd.\n\nData Recipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          4\n\nTraining data contained 90 data points and no missing data.\n\nOperations:\n\nCorrelation filter removed Petal.Length [trained]\nCentering for Sepal.Length, ... [trained]\nScaling for Sepal.Length, ... [trained]\n\nUitvoeren van het pre-proces\nDe testgegevens kunnen nu worden getransformeerd met behulp van precies dezelfde stappen, gewichten en categorisatie als bij de voorbewerking van de trainingsgegevens. Hiervoor wordt een andere functie met een kookterm gebruikt: bake(). Merk op dat de functie testing() wordt gebruikt om de juiste dataset te extraheren.\nThe testing data can now be transformed using the exact same steps, weights, and categorization used to pre-process the training data. To do this, another function with a cooking term is used: bake(). Notice that the testing() function is used in order to extract the appropriate data set.\n\nRows: 60\nColumns: 4\n$ Sepal.Length <dbl> -0.9090229, -1.1427716, -1.4933947, -1.0258972…\n$ Sepal.Width  <dbl> 1.0457854, -0.1076544, 0.1230336, 1.2764734, 0…\n$ Petal.Width  <dbl> -1.3566929, -1.3566929, -1.3566929, -1.3566929…\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa…\n\nHet uitvoeren van dezelfde operatie over de trainingsgegevens is overbodig, omdat die gegevens al zijn voorgeprogrammeerd. Om de voorbereide trainingsgegevens in een variabele te laden, gebruiken we juice(). Het zal de gegevens uit het iris_recipe object halen.\n\nRows: 90\nColumns: 4\n$ Sepal.Length <dbl> -1.37652034, -0.55839976, -1.02589723, -1.7271…\n$ Sepal.Width  <dbl> 0.3537215, 1.9685372, 0.8150974, -0.3383423, 0…\n$ Petal.Width  <dbl> -1.3566929, -1.0918288, -1.3566929, -1.3566929…\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa…\n\nModel training\nIn R zijn er meerdere pakketten die op hetzelfde type model passen. Het is gebruikelijk dat elk pakket een unieke interface biedt. Met andere woorden, zaken als een argument voor hetzelfde modelattribuut wordt voor elk pakket anders gedefinieerd. Bijvoorbeeld, de ranger en randomForest-pakketten passen bij Random Forest-modellen. In de ranger() functie gebruiken we num.trees om het aantal bomen te definiëren. In randomForest wordt dat argument dan weer ntree genoemd. Het is niet gemakkelijk om te wisselen tussen pakketten om hetzelfde model te draaien.\nIn plaats van het modelleerpakket te vervangen, vervangt tidymodels de interface. Beter gezegd, tidymodels biedt een enkele set functies en argumenten om een model te definiëren. Een specifiek pakket wordt dan aangepast in het model en algemeen gemaakt.\nIn het onderstaande voorbeeld wordt de rand_forest() functie gebruikt om een Random Forest model te initialiseren. Om het aantal bomen te definiëren wordt het treesargument gebruikt. Om de ranger versie van Random Forest te gebruiken, wordt de set_engine() functie gebruikt. Tenslotte wordt de fit() functie gebruikt om het model uit te voeren. De verwachte argumenten zijn de formule en de gegevens. Merk op dat het model boven op de gesausde getrainde gegevens draait.\n\n\n\nAls we nu hetzelfde model niet met ranger maar met randomForest willen draaien, hoeven we alleen maar de waarde in set_engine() te veranderen in randomForest.\n\n\n\nHet is ook het vermelden waard dat het model niet in een enkele, grote functie met veel argumenten is gedefinieerd. De definitie van het model is gescheiden in kleinere functies zoals fit() en set_engine(). zo krijgen we een flexibelere - en gemakkelijker te leren - interface.\nVoorspellingen\nIn plaats van een vector geeft de predictfunctie tibble terug. Standaard wordt de voorspellingsvariabele .pred_class genoemd. Merk op dat in het voorbeeld de ’kook’testgegevens worden gebruikt.\n\n# A tibble: 60 x 1\n   .pred_class\n   <fct>      \n 1 setosa     \n 2 setosa     \n 3 setosa     \n 4 setosa     \n 5 setosa     \n 6 setosa     \n 7 setosa     \n 8 setosa     \n 9 setosa     \n10 setosa     \n# … with 50 more rows\n\nHet is eenvoudig om de voorspellingen toe te voegen aan de ’kook’testgegevens door gebruik te maken van dplyr’s bind_cols() functie.\n\nRows: 60\nColumns: 5\n$ .pred_class  <fct> setosa, setosa, setosa, setosa, setosa, setosa…\n$ Sepal.Length <dbl> -0.9090229, -1.1427716, -1.4933947, -1.0258972…\n$ Sepal.Width  <dbl> 1.0457854, -0.1076544, 0.1230336, 1.2764734, 0…\n$ Petal.Width  <dbl> -1.3566929, -1.3566929, -1.3566929, -1.3566929…\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa…\n\nModel validatie\nGebruik de metrics() functie om de prestaties van het model te meten. Het zal automatisch metrieken kiezen die geschikt zijn voor een bepaald type model. De functie verwacht een tibble dat de werkelijke resultaten bevat (waarheid) en wat het model heeft voorspeld (schatting).\n\n# A tibble: 2 x 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy multiclass     0.983\n2 kap      multiclass     0.975\n\nDoor de consistentie van de nieuwe interface is het meten van de resultaten voor het randomForest-model eenvoudig omdat je alleen maar de modelvariabele aan de bovenkant van de code hoeft te vervangen (nu iris_rf ipv iris_ranger.\n\n# A tibble: 2 x 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy multiclass     0.983\n2 kap      multiclass     0.975\n\nPer classificator metriek\nHet is eenvoudig om de waarschijnlijkheid voor elke mogelijke voorspelde waarde te verkrijgen door het type-argument op prob te zetten. Dat levert een tibble op met zoveel mogelijk variabelen als er mogelijke voorspelde waarden zijn. Hun naam zal standaard op de oorspronkelijke waarde worden gezet, voorafgegaan door .pred_.\n\nRows: 60\nColumns: 7\n$ .pred_setosa     <dbl> 0.99800000, 0.90737302, 0.97193651, 0.9780…\n$ .pred_versicolor <dbl> 0.000000000, 0.062146825, 0.006666667, 0.0…\n$ .pred_virginica  <dbl> 0.002000000, 0.030480159, 0.021396825, 0.0…\n$ Sepal.Length     <dbl> -0.9090229, -1.1427716, -1.4933947, -1.025…\n$ Sepal.Width      <dbl> 1.0457854, -0.1076544, 0.1230336, 1.276473…\n$ Petal.Width      <dbl> -1.3566929, -1.3566929, -1.3566929, -1.356…\n$ Species          <fct> setosa, setosa, setosa, setosa, setosa, se…\n\nOok hier, gebruik bind_cols() om de voorspellingen toe te voegen aan de test dataset die je hiervoor hebt voorbereid.\n\nRows: 148\nColumns: 5\n$ .level          <chr> \"setosa\", \"setosa\", \"setosa\", \"setosa\", \"se…\n$ .n              <dbl> 0, 2, 3, 6, 7, 8, 10, 11, 12, 13, 14, 15, 1…\n$ .n_events       <dbl> 0, 2, 3, 6, 7, 8, 10, 11, 12, 13, 14, 15, 1…\n$ .percent_tested <dbl> 0.000000, 3.333333, 5.000000, 10.000000, 11…\n$ .percent_found  <dbl> 0.00000, 9.52381, 14.28571, 28.57143, 33.33…\n\nNu alles in een tibble zit, is het eenvoudig om curve-methoden te berekenen. In dit geval gebruiken we de gain_curve().\n\n\n\nIn de curve-methoden zit ook een autoplot() functie dat makkelijk kan worden omgezet naar een ggplot2 visualizatie.\n\n\n\nDit is een voorbeeld van een roc_curve(). Nogmaals, vanwege de consistentie van de interface, hoeft maar een functienaam te worden omgezet; zelfs de argument waarden blijven hetzelfde.\n\n\n\nOm de gecombineerde enkelvoudige voorspelde waarde en de waarschijnlijkheid van elke mogelijke waarde te meten, combineer je de twee voorspellingsmodi (met en zonder prop type). In dit voorbeeld is met het gebruik van dplyr’s select() de resulterende tibble makkelijker af te lezen.\n\nRows: 60\nColumns: 5\n$ .pred_setosa     <dbl> 0.99800000, 0.90737302, 0.97193651, 0.9780…\n$ .pred_versicolor <dbl> 0.000000000, 0.062146825, 0.006666667, 0.0…\n$ .pred_virginica  <dbl> 0.002000000, 0.030480159, 0.021396825, 0.0…\n$ .pred_class      <fct> setosa, setosa, setosa, setosa, setosa, se…\n$ Species          <fct> setosa, setosa, setosa, setosa, setosa, se…\n\nPipe de resultatentabel in metrics(). In dit geval, specificeer de .pred_class als de schatting.\n\n# A tibble: 4 x 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 accuracy    multiclass     0.983\n2 kap         multiclass     0.975\n3 mn_log_loss multiclass     0.167\n4 roc_auc     hand_till      0.990\n\nSlotopmerkingen van Edgar Ruiz\nDit voorbeeld is bedoeld als een hele voorzichtige kennismaking met tidymodels. Het aantal functies en de mogelijkheden van dergelijke functies zijn voor deze demonstratie tot een minimum beperkt, maar er kan nog veel meer worden gedaan met deze prachtige suite van pakketten. Hopelijk helpt dit artikel jou op weg en moedigt het u misschien zelfs aan om uw kennis verder uit te breiden.\nDank je wel!\nEdgar Ruiz wil graag Max Kuhn en Davis Vaughan, de ontwikkelaars van tidymodels, bedanken. Ze waren hem genadig in het geven van instructie, feedback en begeleiding tijdens zijn reis om tidymodels te leren.\nSlotopmerkingen van Harrie Jonkman\nVoor mij was dit inderdaad een van de eerste kennismakingen met tidymodels. Ik kende het pakket caret en ook het mlr. tidymodels is een modernisering van deze eerste interfaces en moet net zo’n RStudio succes worden als tidyverse. Ondertussen het ik de website van tidymodels gelezen, de interactieve cursus van Julia Silge en de introducties van Alison Hill. Ook het nieuwe boek van Max Kuhn en Julia Silge ben ik op dit moment aan het lezen. Hieronder vind je die literatuur die ik op dit moment wat bij elkaar aan het zoeken ben. Voor mij is er nog een lange weg te gaan maar het artikel van Edgar Ruiz was voor mij een eerste uitstapje. Ik wil hem hartelijk dank voor zijn uitnodiging.\nLiteratuur en verwijzingen\nEdgar Ruiz (A Gentle introduction to tidymodels)[https://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/]\nTidymodels (website)[https://www.tidymodels.org/]\nHefin Rhys (Machine learning with R)[https://education.rstudio.com/blog/2020/02/conf20-intro-ml/]\nJulia Silge (Supervised learning course)[https://juliasilge.com/blog/supervised-ml-course/]\nMax Kuhn and Julia Silge (Tidy modeling with R)[https://www.tmwr.org/]\nAlison Hill (Introduction to machine learning)[https://education.rstudio.com/blog/2020/02/conf20-intro-ml/]\nRebecca Barter (Tidymodels: tidy machine learning in R)[http://www.rebeccabarter.com/blog/2020-03-25_machine_learning/]\n\n\n\n",
    "preview": "posts/2020-10-17-een-eenvoudige-introductie-op-machine-learning/een-eenvoudige-introductie-op-machine-learning_files/figure-html5/unnamed-chunk-16-1.png",
    "last_modified": "2020-11-01T20:31:54+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-08-21-survival-analyse-met-r/",
    "title": "Survival analyse met R",
    "description": "Dit is een deel van een tutorial die Emily Zabore schreef over survival analyse met R.",
    "author": [
      {
        "name": "Emily Zabore, bewerking Harrie Jonkman",
        "url": "www.harriejonkman.nl"
      }
    ],
    "date": "2020-08-21",
    "categories": [],
    "contents": "\nVoorwoord\nOnlangs volgde ik een webinar bij STATA over analyse van survival data (https://www.stata.com/training/webinar_series/survival-analysis/). Vervolgens ben ik op zoek gegaan naar een tutorial in R over survival analyse en kwam terecht by deze interessante tutorial van Emily Zabore. Deze had ik al eens eerder zien staan maar dit was het moment om er eens goed naar te kijken. Om mij de techniek goed eigen te maken heb ik deze voor het grootste deel naar het Nederlands toe omgezet en waar nodig bewerkt. Alle credits gaan naar Emily Zabore die dit zo helder en duidelijk op rij heeft gezet (hartelijke dank, Emily). Deze tutorial is dus een introductie op survival analyse en hoe deze techniek in R uit te voeren. Emily Zabore presenteerde dit voor op R-Presentatie serie van het Memorial Sloan Kettering Cancer Center in New York op 30 augustus 2018. Daarna heeft ze deze aangepast voor een meer intensievere training op hetzelfde centrum in Maart 2019. De informatie kun je vinden op haar Github repository waar je deze tutorial en de bron-files kunt vinden.\nDeel 1: Introductie op Survival Analyse\nDeze tutorial bevat enige basisinformatie over survival analyse en de volgende uitgaven kunnen jou hierbij verder helpen:\n\nClark, T., Bradburn, M., Love, S., & Altman, D. (2003). Survival analysis part I: Basic concepts and first analyses. 232-238. ISSN 0007-0920.\n\n\nM J Bradburn, T G Clark, S B Love, & D G Altman. (2003). Survival Analysis Part II: Multivariate data analysis – an introduction to concepts and methods. British Journal of Cancer, 89(3), 431-436.\n\n\nBradburn, M., Clark, T., Love, S., & Altman, D. (2003). Survival analysis Part III: Multivariate data analysis – choosing a model and assessing its adequacy and fit. 89(4), 605-11.\n\n\nClark, T., Bradburn, M., Love, S., & Altman, D. (2003). Survival analysis part IV: Further concepts and methods in survival analysis. 781-786. ISSN 0007-0920.\n\nPakketten\nEnkele R-pakketten zullen in ieder geval gebruikt worden:\nlubridate\nsurvival\nsurvminer\nZorg ervoor dat ze geïnstalleerd zijn (wel doen, anders werkt het niet) en open ze vervolgens zo:\n\n\n\nWat zijn survival data?\nHet gaat om Tijd-tot-gebeurtenis (Time-to-event) data die bestaan uit een aparte starttijd en eindtijd.\nVoorbeelden uit kankeronderzoek zijn:\nTijd van operatie tot dood;\nTijd van start van de behandeling tot progressie;\nTijd van antwoord op herhaling.\nVoorbeelden uit andere velden:\nTijd-tot-gebeurtenis data zijn in verschillende velden algemeen, waaronder:\nTijd van HIV infectie tot ontwikkeling van AIDS;\nTijd tot hardaanval;\nTijd tot begin van alcoholmisbruik;\nTijd tot initiatie van sexuele activiteit;\nTijd tot machine niet meer goed functioneert.\nHoe survival analyse ook wel wordt genoemd\nOmdat survival analyse in allerlei velden wordt gebruikt, kent het ook verschillende namen:\nBetrouwbaarheids-analyse;\nDuur-analyse;\nGebeurtenisgeschiedenis-analyse;\nTijd-tot-gebeurtenis-analyse.\nDe longdataset\nDe long dataset is beschikbaar via het survival pakket in R. De data bevatten subjecten met gevorderde longkanker van de North Central Cancer Treatment Group. Enkele variabelen (waarvoor ik de Engelse namen gebruik zoals ze in de dataset voorkomen) die we zullen gebruiken om methodes aan te tonen zijn:\ntime: Overlevingstijd (Survival tijd) in dagen;\nstatus: censoring status 1=censored, 2=dood;\nsex: Man=1 Vrouw=2.\nWat is censoring?\n\n\n\n\nRICH JT, NEELY JG, PANIELLO RC, VOELKER CCJ, NUSSENBAUM B, WANG EW. A PRACTICAL GUIDE TO UNDERSTANDING KAPLAN-MEIER CURVES. Otolaryngology head and neck surgery: official journal of American Academy of Otolaryngology Head and Neck Surgery. 2010;143(3):331-336. doi:10.1016/j.otohns.2010.05.007.\n\nTypen van censoring\nEen subject kan gecensored zijn vanwege:\nOmdat we ze niet meer te volgen zijn;\nZe uit de studie zijn gestapt;\nEr geen gebeurtenis kon worden vastgesteld bij het einde van de vaststaande studieperiode.\nDit zijn voorbeelden van rechts censoring.\nLinks censoring en interval censoring zijn ook mogelijk en er zijn methodes om dit soort data te analyseren, maar hier beperken we ons tot rechts censoring.\nGecensorde survival data\n\n\n\nHoe zouden we in dit voorbeeld de proportie vaststellen van hen die gebeurtenisvrij zijn bij 10 jaar?\nSubjecten 6 en 7 zijn gebeurtenisvrij bij 10 jaar. Subjecten 2, 9 en 10 hadden de gebeurtenis voor 10 jaar. Subjecten 1, 3, 4, 5 en 8 zijn gecensord voor 10 jaar, dus we weten niet of ze de gebeurtenis hebben of niet bij 10 jaar - hoe kunnen we deze subjecten incorpereren in onze schatting?\nDistributie van follow-up tijd\nGecensorde subjecten geven nog steeds informatie dus zijn geschikt om in de analyse mee te nemen;\nDistributie van follow-up tijden is scheef en kunnen verschillen tussen gecensorde patiënten en hen met gebeurtenissen;\nFollow-up tijden zijn altijd positief.\n\n\n\nComponenten van survival data\nVoor subject \\(i\\):\nGebeurtenistijd \\(T_i\\)\nCensortijd \\(C_i\\)\nGebeurtenis indicator \\(\\delta_i\\):\n1 als gebeurtenis geobserveerd (bv. \\(T_i \\leq C_i\\))\n0 als gecensord (bv. \\(T_i > C_i\\))\n\nGeobserveerde tijd: \\(Y_i = \\min(T_i, C_i)\\)\nDe geobserveerde tijden en een gebeurtenis indicator zitten in de long data\ntime: Survival tijd in dagen (\\(Y_i\\))\nstatus: censor status 1=gecensord, 2=dood (\\(\\delta_i\\))\n\ninst\ntime\nstatus\nage\nsex\nph.ecog\nph.karno\npat.karno\nmeal.cal\nwt.loss\n3\n306\n2\n74\n1\n1\n90\n100\n1175\nNA\n3\n455\n2\n68\n1\n0\n90\n90\n1225\n15\n3\n1010\n1\n56\n1\n0\n90\n90\nNA\n15\n5\n210\n2\n57\n1\n1\n90\n60\n1150\n11\n1\n883\n2\n60\n1\n0\n100\n90\nNA\n0\n12\n1022\n1\n74\n1\n1\n50\n80\n513\n0\n\nOmgaan met data (van datum) in R\nData laten vaak de start en einddata zien eerder dan de voorberekende survival tijden. De eerste stap is om er zeker van te zijn dat deze als data in R zijn geformatteerd.\nOm het duidelijk te maken, laten we eens een kleine dataset als voorbeeld maken met de variabelen sx_date voor operatiedatum en last_fup_date voor de laatste follow-up datum.\n\n\n# A tibble: 3 x 2\n  sx_date    last_fup_date\n  <chr>      <chr>        \n1 2007-06-22 2017-04-15   \n2 2004-02-13 2018-07-04   \n3 2010-10-27 2016-10-31   \n\nWe zien dat het beide chr-variabelen betreft, wat vaker het geval is. Maar het is nodig ze naar data-variabelen te formatteren.\nFormatteren van data - basis R\n\n\n# A tibble: 3 x 2\n  sx_date    last_fup_date\n  <date>     <date>       \n1 2007-06-22 2017-04-15   \n2 2004-02-13 2018-07-04   \n3 2010-10-27 2016-10-31   \n\nMerk op dat in basis R het format zowel de onderscheider als het symbool moet omvatten. Dus als jouw data in format m/d/Y staan dan heb je het format = \"%m/%d/%Y\"nodig.\nVoor een hele lijst van dataformat symbolen kun je kijken naar https://www.statmethods.net/input/dates.html\nFormatteren van data-lubridate pakket\nWe kunnen ook het lubridate pakket gebruien om data te formatteren. In in dit geval, gebruik de ymd functie\n\n\n# A tibble: 3 x 2\n  sx_date    last_fup_date\n  <date>     <date>       \n1 2007-06-22 2017-04-15   \n2 2004-02-13 2018-07-04   \n3 2010-10-27 2016-10-31   \n\nMerk op dat hier de onderscheiders in de R optie niet hoeven worden toegepast.\nDe help pagina voor ?dmy zullen je alle format opties kunnen geven.\nCalculeren van survival tijden - basis R\nNu de data zijn geformatteerd, moeten we het verschil tussen start en eindtijd in een bepaalde eenheid uitdrukken,meestal maanden of jaren. In basis R, wordtdifftime gebruikt om het aantal dagen te berekenen tussen onze twee data en dit te converteren naar een numerieke waarde met as.numeric. Dan zetten we het over naar dagen door het te delen door 365.25, het gemiddelde aantal dagen in een jaar.\n\n\n\n\n\n# A tibble: 3 x 3\n  sx_date    last_fup_date os_yrs\n  <date>     <date>         <dbl>\n1 2007-06-22 2017-04-15      9.82\n2 2004-02-13 2018-07-04     14.4 \n3 2010-10-27 2016-10-31      6.01\n\nBerekenen van survival tijden - lubridate\nAls we het lubridate pakket gebruiken, dan drukt %--% een tijd interval uit, dat dan geconverteerd wordt naar het aantal seconden door as.duration te gebruiken en tenslotte naar jaren door het te delen door dyears(1), wat het aantal seconden in een jaar geeft.\n\n\n# A tibble: 3 x 3\n  sx_date    last_fup_date os_yrs\n  <date>     <date>         <dbl>\n1 2007-06-22 2017-04-15      9.82\n2 2004-02-13 2018-07-04     14.4 \n3 2010-10-27 2016-10-31      6.01\n\nNotitie: we moeten het lubridate pakket laden door library te gebruiken en zo toegang te krijgen tot specifieke functies.\nGebeurtenis indicator\nVoor de componenten van survivaldata noemde ik eerder de gebeurtenisindicator:\nGebeurtenisindicator \\(\\delta_i\\):\n1 als de gebeurtenis heeft plaatsgevonden (bv. \\(T_i \\leq C_i\\))\n0 als het gecensord is (i.e. \\(T_i > C_i\\))\nEchter, in R accepteert de Surv-function ook ‘TRUE/FALSE’ (’TRUE = gebeurtenis) of 1/2 (2 = gebeurtenis).\nIn de lung data, hebben we:\nstatus: censoring status 1=gecensord, 2=dood\nSurvivalfunctie\nDe waarschijnlijkheid dat een subject zal overleven voorbij een bepaalde gespecificeerde tijd\n\\[S(t) = Pr(T>t) = 1 - F(t)\\]\n\\(S(t)\\): survivalfunctie \\(F(t) = Pr(T \\leq t)\\): cumulatieve distributiefunctie\nIn theorie is de survivalfunctie gelijdelijk; in de praktijk observeren we gebeurtenissen op een discrete tijdschaal.\nSurvivalwaarschijnlijkheid\nSurvivalwaarschijnlijkheid op een bepaalde tijd, \\(S(t)\\), is een conditionele waarschijnlijkheid van overleven voorbij die tijd gegeven dat een individu overleefd heeft juist voorafgaand aan die tijd.\nHet kan worden geschat als het aantal patiënten die leven zonder verlies van follow-up gegevens op die tijd, gedeeld door het aantal patiënten die leven voorafgaand aan dat moment.\nDe Kaplan-Meier schatting van survivalwaarschijnlijkheid is het product van deze conditionele waarschijnlijkheden tot op dat moment.\nOp tijd 0, de survivalwaarschijnlijkheid is 1, bv. \\(S(t_0) = 1\\)\nCreëren van survivalobjecten\nDe Kaplan-Meier methode is de meest algemene manier om survivaltijden en -waarschijnlijkheden te schatten. Het is een niet-parametrische benadering die resulteert in een stapsgewijze-functie, met steeds een stap naar beneden iedere keer wanneer de gebeurtenis plaatsvindt.\nDe Surv functie van het survival pakket creëert een survival object voor gebruik als antwoord in een modelformule. Er is een ingang voor elk subject dat de survivaltijd is, dat wordt gevolgd door een + als het subject gecensord is. Laten we eens naar de eerste tien observaties kijken, dat maakt meer duidelijk:\n\n\n [1]  306   455  1010+  210   883  1022+  310   361   218   166 \n\nSchatten van de survivalcurves met de Kaplan-Meier methode\nDe survfit functie creëert survivalcurves die gebaseerd zijn op een formule. Laten we de overallsurvivalcurve eens genereren voor de hele cohort, benoem het als het object f1, en kijk naar de namen(names) van dat object:\n\n\n [1] \"n\"         \"time\"      \"n.risk\"    \"n.event\"   \"n.censor\" \n [6] \"surv\"      \"std.err\"   \"cumhaz\"    \"std.chaz\"  \"type\"     \n[11] \"logse\"     \"conf.int\"  \"conf.type\" \"lower\"     \"upper\"    \n[16] \"call\"     \n\nEnkele sleutelcomponenten van dit survfit object dat wordt gebruikt om survivalcurves te maken omvatten:\ntime, die de start en eindpunten van elk tijdinterval inhouden\nsurv, die de survivalwaarschijnlijkheid inhouden die corresponderen met elke tijd (time)\nKaplan-Meier grafiek - basis R\nNu plotten we het survfit object met basis R om de Kaplan-Meier grafiek te krijgen.\n\n\n\nDe standaard grafiek in basis R toont de stapfunctie (doorlopende lijn) met de betrouwbaarheidsintervallen die ermee samenhangen (stippellijnen);\nHorizontale lijnen representeren de survivalduur voor het interval;\nDe interval beëindigd door een gebeurtenis;\nDe hoogte van de verticale lijnen laat de verandering in cumulatievewaarschijnlijkheid zien;\nGecensorde observaties, aangegeven met vinkjes, reduceren de cumulatieve survival tussen intervallen. (Opmerking de vinkjes voor gecensorde patiënten worden niet in deze standaardgrafiek getoond, maar worden toegevoegd door de optie mark.time = TRUE).\n\n\n\nKaplan-Meier grafiek - ggsurvplot\nAls alternatief kun je de ggsurvplot functie gebruiken van het survminer pakket dat op ggplot2 is gebouwd, en dat kan worden gebruikt om de Kaplan-Meier grafieken te maken. Bekijk de cheatsheet maar eens voor het survminer pakket.\n\n\n\nDe standaardgrafiek van ggsurvplot laat de stapfunctie (doorlopende lijn) zien met bijbehorende betrouwbaarheidsbanden (donkere gebied);\nDe vinkjes voor gecensorde patienten worden hier wel standaard getoond, enigszins onduidelijk aangegeven in de lijn in dit voorbeeld en kan worden weggehaald met de optie censor = FALSE.\nSchatten van \\(x\\)-jaar overleven\nEen kwantiteit waar we in de survivalanalyse vaak in geïnteresseerd zijn is de waarschijnlijkheid van overleven voorbij een bepaald aantal (\\(x\\)) jaren.\nBijvoorbeeld, om de waarschijnlijkheid te schatten van overleven bij \\(1\\) jaar, gebruik je summary met het times argument (Opgelet de time variabele in de lung data is eigenlijk in dagen, dus we moeten times = 365.25 gebruiken).\n\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = lung)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n  365     65     121    0.409  0.0358        0.345        0.486\n\nWe vinden dat de \\(1\\)-jaar waarschijnlijkheid van overleven in deze study 41% is.\nDe boven- en ondergrens van het 95% betrouwbaarheidsinterval wordt ook getoond.\n\\(x\\)-jaar survival en de survivalcurve\nDe \\(1\\)-jaar survivalwaarschijnlijkheid is het punt op de y-as dat correspondeert met \\(1\\) jaar op de x-as voor de survivalcurve.\n\n\n\n\\(x\\)-jaar survival wordt vaak niet correct geschat\nWat gebeurt als je een “naïeve” schatting gebruikt?\n121 van de 228 patiënten stierven bij \\(1\\) jaar dus:\n\\[\\Big(1 - \\frac{121}{228}\\Big) \\times 100 = 47\\%\\] - Je krijgt een incorrecte schatting van de \\(1\\)-jaar survivalwaarschijnlijheid als het feit over het hoofd ziet dat 42 patiënen waren gecensord voor \\(1\\) jaar.\nHerinner dat de correcte schatting van de \\(1\\)-jaar survivalwaarschijnlijkheid 41% was.\nImpact op \\(x\\)-jaar survival door censoring over het hoofd te zien\nStel twee studies, met elk 228 subjecten. In iedere studie zijn er 165 doden. Geen censoring bij de een (oranje lijn), 63 patiënten gecensord in de andere (blauwe lijn);\nOntkennen van censoring leidt tot een overschatting van de overall survivalwaarschijnlijkheid, omdat de gecensorde subjecten alleen informatie bijdragen voor het deel van de follow-up tijd en dan vallen ze uit de risk set, dus drukken ze de cumulatieve waarschijnlijkheid van de survival naar beneden.\n\n\n\nSchatten van de mediaan survivaltijd\nEen andere kwantiteit die vaak interessant is voor overlevingsanalyse is de gemiddelde overlevingstijd, die we kwantificeren met behulp van de mediaan. Er wordt niet verwacht dat de overlevingstijd normaal wordt verdeeld, dus het gemiddelde is geen passende samenvatting.\nWe kunnen dit rechtstreeks verkrijgen uit ons survfit object\n\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = lung)\n\n      n  events  median 0.95LCL 0.95UCL \n    228     165     310     285     363 \n\nWe zien dat de mediaan survivaltijd 310 dagen is. De onder- en bovengrens van het 95% betrouwbaarheidsinterval worden ook gegeven.\nMediaan overlevingstijd and de overlevingscurve\nMediaan survival is de tijd die correspondeert met de overlevingswaarschijnlijkheid van \\(0.5\\):\n\n\n\nDe mediaan van overleving wordt vaak verkeerd geschat\nWat gebeurt als je een “naïeve” schatting gebruikt?\nVat de mediaan overlevingstijd samen onder de 165 patiënten die stierven\n\n\n  median_surv\n1         226\n\nJe krijgt een verkeerde schatting van de mediaan overlevingstijd van 226 dagen als je het feit ontkent dat gecensorde patiënten ook bijdragen aan de follow-up tijd.\nOnthoud dat de correcte schatting van de mediaan overlevingstijd 310 dagen is.\nImpact op de overlevingsmediaan wanneer je censoring negeert\nNegeren van censoring creëert een kunstmatig lagere overlevinscurve omdat de follow-up tijd die gecensorde patiënten bijdragen er buiten wordt gehouden (paarse lijn).\nDe ware overlevingscurve voor de lung data zie je voor de vergelijking in blauw staan.\n\n\n\nVergelijken van overlevingstijd tussen groepen\nWe kunnen significantie testen tussen-groepen uitvoeren met een ‘log-rank test’.\nDe ‘log-rank test’ weegt observaties gelijk over de hele follow-up tijd en is de meest algemene manier om overlevingstijden tussen groepen te vergelijken.\nEr zijn ook versie die meer gewicht geven aan de eerdere of latere follow-up die meer geschikt zijn afhankelijk van de onderzoeksvraag (zie ?survdiff voor verschillende test opties).\nWe krijgen de log-rank p-waarde wanneer we de survdiff functie gebruiken. Bijvoorbeeld, we kunnen testen of er een verschil was in overlevingstijd wat sexe betreft in de lung data.\n\n\nCall:\nsurvdiff(formula = Surv(time, status) ~ sex, data = lung)\n\n        N Observed Expected (O-E)^2/E (O-E)^2/V\nsex=1 138      112     91.6      4.55      10.3\nsex=2  90       53     73.4      5.68      10.3\n\n Chisq= 10.3  on 1 degrees of freedom, p= 0.001 \n\nExtraheren van informatie van een survdiff object\nHet is lastig om een p-waarde te extraheren van de resultaten van survdiff. Hier is een coderegel om dat te doen.\n\n\n[1] 0.001311165\n\nOf er is de sdp functie in het ezfun pakket, dat je kunt installeren via devtools::install_github(\"zabore/ezfun\"). Het geeft een opgemaakt p-value terug.\n\n\n[1] 0.001\n\nHet Cox regressiemodel\nWe kunnen een effectgrootte voor een enkele variabele kwantificeren of meer dan één variabele opnemen in een regressiemodel wanneer we rekening willen houden met de effecten van meerdere variabelen.\nHet Cox-regressiemodel is een semi-parametrisch model dat kan worden gebruikt voor univariabele en multivariabele regressiemodellen met overlevingsuitkomsten.\n\\[h(t|X_i) = h_0(t) \\exp(\\beta_1 X_{i1} + \\cdots + \\beta_p X_{ip})\\]\n\\(h(t)\\): gevaar (‘hazard’), of de mate waarin de gebeurtenis plaatsvindt \\(h_0(t)\\): onderliggende baseline van het gevaar (‘hazard’).\nEnkele aannames van het model:\nnon-informatieve censoring\nproportionele ‘hazards’\nOpgelet: parametrische regressiemodels voor overlevingsuitkomsten zijn ook beschikbaar, daar zal hier niet op worden ingegaan.\nWe kunnen regressiemodellen voor survivaldata draaien door de coxph functie te gebruiken, die een Surv object aan de linkerkant neemt en een standaard regressie formule in R aan de rechterkant hanteert.\n\n\nCall:\ncoxph(formula = Surv(time, status) ~ sex, data = lung)\n\n       coef exp(coef) se(coef)      z       p\nsex -0.5310    0.5880   0.1672 -3.176 0.00149\n\nLikelihood ratio test=10.63  on 1 df, p=0.001111\nn= 228, number of events= 165 \n\nFormatteren van Cox regressieresultaten\nWe kunnen een opgeschoonde versie van de output zien door de tidy functie van het broom pakket te gebruiken:\n\nterm\nestimate\nstd.error\nstatistic\np.value\nsex\n0.5880028\n0.1671786\n-3.176385\n0.0014912\n\nOf door tbl_regression te gebruiken van het gtsummary pakket.\n\nhtml {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#bthodvqsjj .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#bthodvqsjj .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#bthodvqsjj .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#bthodvqsjj .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 4px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#bthodvqsjj .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#bthodvqsjj .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#bthodvqsjj .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#bthodvqsjj .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#bthodvqsjj .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#bthodvqsjj .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#bthodvqsjj .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#bthodvqsjj .gt_group_heading {\n  padding: 8px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#bthodvqsjj .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#bthodvqsjj .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#bthodvqsjj .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#bthodvqsjj .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#bthodvqsjj .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#bthodvqsjj .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 12px;\n}\n\n#bthodvqsjj .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#bthodvqsjj .gt_first_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n}\n\n#bthodvqsjj .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#bthodvqsjj .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#bthodvqsjj .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#bthodvqsjj .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#bthodvqsjj .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding: 4px;\n}\n\n#bthodvqsjj .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#bthodvqsjj .gt_sourcenote {\n  font-size: 90%;\n  padding: 4px;\n}\n\n#bthodvqsjj .gt_left {\n  text-align: left;\n}\n\n#bthodvqsjj .gt_center {\n  text-align: center;\n}\n\n#bthodvqsjj .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#bthodvqsjj .gt_font_normal {\n  font-weight: normal;\n}\n\n#bthodvqsjj .gt_font_bold {\n  font-weight: bold;\n}\n\n#bthodvqsjj .gt_font_italic {\n  font-style: italic;\n}\n\n#bthodvqsjj .gt_super {\n  font-size: 65%;\n}\n\n#bthodvqsjj .gt_footnote_marks {\n  font-style: italic;\n  font-size: 65%;\n}\nCharacteristic\n      HR1\n      95% CI1\n      p-value\n    sex\n      0.59\n      0.42, 0.82\n      0.001\n    \n        \n          1\n          \n           \n          HR = Hazard Ratio, CI = Confidence Interval\n          \n      \n    \n\nHazard ratio’s\nDe interesse bij een Cox-regressiemodel gaat uit naar een hazard ratio (HR). De HR staat voor de verhouding van de gevaren tussen twee groepen op een bepaald moment;\nDe HR wordt geïnterpreteerd als het momentane tempo waarin de gebeurtenis zich voordoet bij degenen die nog steeds risico lopen voor de gebeurtenis. Het is geen risico, maar wordt wel als zodanig geïnterpreteerd;\nAls je een regressieparameter hebt van \\(\\beta\\) (van kolom estimate in ons coxph) dan is HR = \\(\\exp(\\beta)\\);\nEen HR < 1 duidt op een verminderd doodsgevaar, terwijl een HR > 1 duidt op een verhoogd doodsgevaar;\nDus onze HR = 0.59 betekent dat er ongeveer 0.6 keer zoveel vrouwen sterven als mannen, op een bepaald moment.\n\n\n\nDeel 2: Oriëntatiepuntanalyse en Tijd Afhankelijke Covariaten\nIn deel 1 hebben we met behulp van log-rank tests en Cox regressie de associaties tussen covariaten die van belang zijn en overlevingsresultaten onderzocht.\nMaar deze analyses zijn gebaseerd op het meten van het covariaat op baseline, dat wil zeggen, voordat de vervolgtijd voor het evenement begint.\nWat gebeurt er als u geïnteresseerd bent in een covariaat dat wordt gemeten nadat de vervolgtijd begint?\nVoorbeeld: Tumorreactie\nIn het algemeen wordt de totale overleving gemeten vanaf het begin van de behandeling en de interesse gaat daarbij uit naar de associatie tussen de volledige respons op de behandeling en de overleving.\nAnderson et al. (JCO, 1983) beschreven waarom tradionele methoden zoals log-rank tests of Cox regressie in dit scenario bevooroordeeld zijn ten gunste van de responders en stelden de oriëntatiepuntaanpak voor. De nul-hypothese in de landmarkbenadering is dat het overleven van een oriëntatiepunt niet afhankelijk is van de status van de respons bij dat punt.\n\nAnderson, J., Cain, K., & Gelber, R. (1983). Analysis of survival by tumor response. Journal of Clinical Oncology : Official Journal of the American Society of Clinical Oncology, 1(11), 710-9.\n\nAndere voorbeelden\nEnkele andere mogelijke covariaten die van belang zijn bij kankeronderzoek en die niet op de basislijn kunnen worden gemeten, zijn onder andere:\ntransplantatiefout\nent versus gastheer ziekte\ntweede resectie\nadjuvante therapie\nnaleving\nongunstige gebeurtenissen\nVoorbeeld data - de BMT dataset van het SemiCompRisks pakket\nGegevens over 137 beenmergpatiënten. Variabelen van belang zijn onder meer:\nT1 tijd(in dagen) tot dood or laatste follow-up\ndelta1 dood indicator; 1-Dood, 0-Levend\nTA tijd (in dagen) tot ent versus gastheer ziekte\ndeltaA acute ent versus gastheer ziekte indicator; 1-Ontwikkelt ent versus gastheer ziekte, 0-Nooit een ent versus gastheer ziekte ontwikkelt.\nLaten we de gegevens laden voor gebruik in de voorbeelden.\n\n\n\nOriëntatiepuntmethode\nSelecteer een vaste tijd na de basislijn als uw oriëntatiepunt. Opgelet: dit moet worden gedaan op basis van klinische informatie, voorafgaand aan de inspectie van de gegevens.\nSubset populatie voor degenen die volgen ten minste tot aan de mijlpaal tijd. Opgelet: rapporteer altijd het nummer dat is uitgesloten vanwege het evenement van belang of de censuur vóór het tijdstip van de mijlpaal.\nBereken de follow-up van de landmarktijd en voer de traditionele log-ranktests of Cox-regressie toe.\nIn de BMT data interesseert men zich voor de associatie tussen acute ent versus gastheer ziekte (op zijn Engels ‘aGVHD’) en overleving. Maar dit wordt beoordeeld na de transplantatie, wat onze basislijntijd is of bij het begin van de follow-up-tijd.\nStap 1 Selecteer een mijlpaal tijd\nMeestal treedt de ziekte op binnen de eerste 90 dagen na de transplantatie, dus gebruiken we een 90-dagen oriëntatiepunt.\nDe interesse gaat uit naar de associatie tussen acute enting versus gastheerziekte (‘aGVHD’) en overleving. Maar ‘aGVHD’ wordt beoordeeld na de transplantatie, wat onze basislijn is, of het begin van de follow-up, tijd.\nStep 2 Subset populatie voor degenen die gevolgd zijn ten minste tot mijlpaal tijd\n\n\n\nDit reduceert onze sampleomvang van 137 tot 122.\nAlle 15 data zijn exclusief patiënten die voor het 90-daags oriëntatiepunt zijn overleden.\nDe belangstelling gaat uit naar het verband tussen acute enting versus gastheerziekte (‘aGVHD’) en overleving. Maar ‘aGVHD’ wordt vastgesteld na de transplantatie, wat onze basislijn is of het begin van de follow-up-tijd.\nStep 3 Berekenen van follow-up tijd vanuit het oriëntatiepunt en toepassen van traditionele methodes.\n\n\n\n\n\n\nCox-regressie voor oriëntatievoorbeeld bij gebruik van BMT data\nIn de Cox-regressie kun je de subset option in coxph gebruiken om die patiënten uit te sluiten die niet zijn gevolgd gedurende de oriëntatietijd.\n\nhtml {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#dmpqwnvorn .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#dmpqwnvorn .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#dmpqwnvorn .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#dmpqwnvorn .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 4px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#dmpqwnvorn .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#dmpqwnvorn .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#dmpqwnvorn .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#dmpqwnvorn .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#dmpqwnvorn .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#dmpqwnvorn .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#dmpqwnvorn .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#dmpqwnvorn .gt_group_heading {\n  padding: 8px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#dmpqwnvorn .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#dmpqwnvorn .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#dmpqwnvorn .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#dmpqwnvorn .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#dmpqwnvorn .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#dmpqwnvorn .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 12px;\n}\n\n#dmpqwnvorn .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#dmpqwnvorn .gt_first_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n}\n\n#dmpqwnvorn .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#dmpqwnvorn .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#dmpqwnvorn .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#dmpqwnvorn .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#dmpqwnvorn .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding: 4px;\n}\n\n#dmpqwnvorn .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#dmpqwnvorn .gt_sourcenote {\n  font-size: 90%;\n  padding: 4px;\n}\n\n#dmpqwnvorn .gt_left {\n  text-align: left;\n}\n\n#dmpqwnvorn .gt_center {\n  text-align: center;\n}\n\n#dmpqwnvorn .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#dmpqwnvorn .gt_font_normal {\n  font-weight: normal;\n}\n\n#dmpqwnvorn .gt_font_bold {\n  font-weight: bold;\n}\n\n#dmpqwnvorn .gt_font_italic {\n  font-style: italic;\n}\n\n#dmpqwnvorn .gt_super {\n  font-size: 65%;\n}\n\n#dmpqwnvorn .gt_footnote_marks {\n  font-style: italic;\n  font-size: 65%;\n}\nCharacteristic\n      HR1\n      95% CI1\n      p-value\n    deltaA\n      1.08\n      0.57, 2.07\n      0.8\n    \n        \n          1\n          \n           \n          HR = Hazard Ratio, CI = Confidence Interval\n          \n      \n    \n\nTijd-afhankelijke covariaat\nEen alternatief voor de oriëntatieanalyse is de incorporatie van een tijd-afhankelijke covariaat. Dit is beter geschikt wanneer\nde waarde van covariaat over tijd verandert;\nals er geen duidelijke oriëntatietijd is;\nwanneer de oriëntatie tot heel veel uitsluitingen leidt.\nData setup van een tijd-afhankelijke covariaat\nAnalyse van tijdafhankelijke covariaten in R veronderstelt de setup van een speciale dataset. Informatie hierover vind je in een gedetailleerd artikel door de auteur van het survival pakket Using Time Dependent Covariates and Time Dependent Coefficients in the Cox Model.\nEr was geen ID variable in het BMT data, dat nodig is om die special dataset te maken. Dus maak er een die heet my_id.\n\n\n\nGebruik de tmerge functie met de event en tdc functieopties om die speciale dataset te creëren.\ntmerge creëert een lange dataset met verschillende tijdintervallen voor de verschillende covariaatwaarden voor elke patiënt;\nevent creëert de nieuwe tijdindicator die samengaat met de gecreëerde tijdintervallen;\ntdc creëert de tijd-afhankelijke covariaatindicator die samengaat met de gecreëerde tijdintervallen.\n\n\n\nTime-afhankelijke covariaat - voorbeeld van enkele patiënten\nOm te zien wat dit doet, laten we eens kijken naar de eerste vijf individuele patiënten.\nDe variabelen waar onze interesse naar uitgaan zien er in de originele data als volgt uit:\n\n\n  my_id   T1 delta1   TA deltaA\n1     1 2081      0   67      1\n2     2 1602      0 1602      0\n3     3 1496      0 1496      0\n4     4 1462      0   70      1\n5     5 1433      0 1433      0\n\nDe nieuwe dataset voor dezelfde patiënten ziet er als volgt uit:\n\n\n  my_id   T1 delta1 id tstart tstop death agvhd\n1     1 2081      0  1      0    67     0     0\n2     1 2081      0  1     67  2081     0     1\n3     2 1602      0  2      0  1602     0     0\n4     3 1496      0  3      0  1496     0     0\n5     4 1462      0  4      0    70     0     0\n6     4 1462      0  4     70  1462     0     1\n7     5 1433      0  5      0  1433     0     0\n\nTijd-afhankelijke covariaat - Cox regressie\nNu kunnen we de tijd-afhankelijke covariaat analyseren zoals we gewend zijn met Coxregressie met coxph en een aanpassing in het gebruik van Surv door zowel de argumenten time en time2 op te nemen.\n\nhtml {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#vcipwnwpuw .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#vcipwnwpuw .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#vcipwnwpuw .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#vcipwnwpuw .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 4px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#vcipwnwpuw .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#vcipwnwpuw .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#vcipwnwpuw .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#vcipwnwpuw .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#vcipwnwpuw .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#vcipwnwpuw .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#vcipwnwpuw .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#vcipwnwpuw .gt_group_heading {\n  padding: 8px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#vcipwnwpuw .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#vcipwnwpuw .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#vcipwnwpuw .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#vcipwnwpuw .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#vcipwnwpuw .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#vcipwnwpuw .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 12px;\n}\n\n#vcipwnwpuw .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#vcipwnwpuw .gt_first_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n}\n\n#vcipwnwpuw .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#vcipwnwpuw .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#vcipwnwpuw .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#vcipwnwpuw .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#vcipwnwpuw .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding: 4px;\n}\n\n#vcipwnwpuw .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#vcipwnwpuw .gt_sourcenote {\n  font-size: 90%;\n  padding: 4px;\n}\n\n#vcipwnwpuw .gt_left {\n  text-align: left;\n}\n\n#vcipwnwpuw .gt_center {\n  text-align: center;\n}\n\n#vcipwnwpuw .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#vcipwnwpuw .gt_font_normal {\n  font-weight: normal;\n}\n\n#vcipwnwpuw .gt_font_bold {\n  font-weight: bold;\n}\n\n#vcipwnwpuw .gt_font_italic {\n  font-style: italic;\n}\n\n#vcipwnwpuw .gt_super {\n  font-size: 65%;\n}\n\n#vcipwnwpuw .gt_footnote_marks {\n  font-style: italic;\n  font-size: 65%;\n}\nCharacteristic\n      HR1\n      95% CI1\n      p-value\n    agvhd\n      1.40\n      0.81, 2.43\n      0.2\n    \n        \n          1\n          \n           \n          HR = Hazard Ratio, CI = Confidence Interval\n          \n      \n    \n\nSamenvatting\nWe vinden dat acute ent versus gastheer ziekte niet significant geassocieerd is met de dood met behulp van hetzij oriëntatieanalyse of met inzet van een tijdafhankelijke covariate.\nVaak zal men oriëntatieanalyse willen gebruiken voor de visualisatie van een enkel covariaat en Cox regressie met een tijdsafhankelijk covariaat voor univariabele en multivariabele modellering.\nPart 3: Concurrerende Risico’s\nPakketten\nHet belangrijkste pakket om te gebruiken in concurrerende risico analyse is:\ncmprsk\n\n\n\nWat zijn concurrerende risico’s?\nHier is sprake van wanneer subjecten meerdere mogelijke gebeurtenissen kennen in een tijd tot gebeurtenissetting.\nVoorbeelden:\nherhaling\nsterfte door ziekte\ndood door andere oorzaken\nbehandelingsreactie\nAl deze of sommige van deze (onder andere) gebeurtenissen kunnen in een bepaalde studie mogelijk zijn.\nDus wat is het probleem?\nOnopgemerkte afhankelijkheid tussen de momenten van het evenement is het fundamentele probleem dat leidt tot de noodzaak om speciale aandacht hieraan te besteden.\nMen kan zich bijvoorbeeld voorstellen dat patiënten die terugkomen meer kans hebben om te sterven en daarom zouden tijden om terug te keren en tijden om te sterven geen onafhankelijke gebeurtenissen zijn.\nAchtergronden van concurrerende risico’s\nTwee benaderingen van de analyse in de aanwezigheid van meerdere potentiële uitkomsten:\nOorzaak-specifiek gevaar van een bepaalde gebeurtenis: dit is het percentage per tijdseenheid van de gebeurtenis onder degenen die niet hebben gefaald op andere gebeurtenissen.\nCumulatieve incidentie van een bepaald evenement: dit is het deel per tijdseenheid van het evenement en de invloed van concurrerende evenementen.\nElk van deze benaderingen kan slechts één belangrijk aspect van de gegevens belichten, terwijl andere mogelijk worden verdoezeld, en de gekozen aanpak moet afhangen van de kwestie van het belang.\nEen aantal aanvullende aantekeningen en referenties\nWanneer de gebeurtenissen onafhankelijk zijn (bijna nooit waar), zijn de oorzaak-specifieke gevaren onbevooroordeeld.\nWanneer de gebeurtenissen afhankelijk zijn, kunnen afhankelijk van de instelling verschillende resultaten worden verkregen\nCumulatieve incidentie met Kaplan-Meier is altijd >= cumulatieve incidentie met behulp van concurrerende risicomethoden, dus kan alleen leiden tot een overschatting van de cumulatieve incidentie, de hoogte van de overschatting hangt af van het aantal gebeurtenissen en de afhankelijkheid van de gebeurtenissen.\nOm vast te stellen dat een covariaat inderdaad reageert op de gebeurtenis dat van belang is, kan de voorkeur worden gegeven aan oorzaken-specifieke gevaren voor de behandeling of voor het testen van het pronostiek-markeereffect.\nOm het algemene voordeel vast te stellen, kan de voorkeur worden gegeven aan subdistributierisico’s voor het bouwen van prognostische nomogrammen of het overwegen van gezondheidseconomische effecten om een beter gevoel te krijgen van de invloed van de behandeling en andere covariaten op een absolute schaal.\n\nDignam JJ, Zhang Q, Kocherginsky M. The use and interpretation of competing risks regression models. Clin Cancer Res. 2012;18(8):2301-8.\n\n\nKim HT. Cumulative incidence in competing risks data and competing risks regression analysis. Clin Cancer Res. 2007 Jan 15;13(2 Pt 1):559-65.\n\n\nSatagopan JM, Ben-Porat L, Berwick M, Robson M, Kutler D, Auerbach AD. A note on competing risks in survival data analysis. Br J Cancer. 2004;91(7):1229-35.\n\n\nAustin, P., & Fine, J. (2017). Practical recommendations for reporting Fine‐Gray model analyses for competing risk data. Statistics in Medicine, 36(27), 4391-4400.\n\nCumulatieve incidentie voor concurrerende risico’s\n– Niet-parametrische schatting van de cumulatieve incidentie;\n- Schat het cumulatieve effect van het evenement van de rente in;\n- Op elk moment is de som van de cumulatieve incidentie van elke gebeurtenis gelijk aan de totale cumulatieve incidentie van elke gebeurtenis (niet waar in de oorzaak-specifieke setting);\n- Gray’s test is een aangepaste Chi-kwadraat test die wordt gebruikt om 2 of meer groepen te vergelijken.\nVoorbeeld Melanoma data van het MASS pakket\nWe gebruiken de Melanoma data van het MASS pakket om deze concepten duidelijk te maken. Deze omvatten de volgende variabelen:\ntime overlevingstijd in dagen, mogelijk gecensord.\nstatus 1 dood vanwege melanoma, 2 levend, 3 dood vanwege andere oorzaken.\nsex 1 = man, 0 = vrouw.\nage leeftijd in jaren.\nyear van de operatie.\nthickness tumor dikte in mm.\nulcer 1 = aanwezig, 0 = afwezig.\n\n\n\nCumulatieve incidentie in de Melanoma data\nSchat de cumulatieve incidentie in de context van concurrerende risico’s met gebruik van de cuminc functie.\nOpgelet: in de Melanoma data, gecensorde patiënten zijn gecodeerd als \\(2\\) voor status, dus we kunnen niet de cencode standaardoptie gebruiken van \\(0\\)\n\n\nEstimates and Variances:\n$est\n          1000       2000       3000      4000      5000\n1 1 0.12745714 0.23013963 0.30962017 0.3387175 0.3387175\n1 3 0.03426709 0.05045644 0.05811143 0.1059471 0.1059471\n\n$var\n            1000         2000         3000        4000        5000\n1 1 0.0005481186 0.0009001172 0.0013789328 0.001690760 0.001690760\n1 3 0.0001628354 0.0002451319 0.0002998642 0.001040155 0.001040155\n\nPlotten van de cumulatieve incidentie - basis R\nGenereer een basis R plot met al de standaards.\n\n\n\n\n\n\nIn de legende:\nHet eerste getal geeft de groep aan, in dit geval is er slechts een globale schatting, dus het is \\(1\\) voor beide…\nHet tweede getal geeft het type gebeurtenis aan, in dit geval is het de ononderbroken lijn \\(1\\) voor de dood door melanoom en de stippellijn \\(3\\) voor de dood door andere oorzaken.\nZet de cumulatieve incidentie uit - ggcompetingrisks\nWe kunnen ook de cumulatieve incidentie berekenen met behulp van de ggs-competingrisks functie van het survminer pakket.\nIn dit geval krijgen we een panel gelabeld volgens de groep en een legenda gelabeld evenement, met vermelding van het type evenement voor elke regel.\nIn the legenda:\nHet eerste getal geeft de groep aan, in dit geval is er alleen een overall schatte en is \\(1\\) voor beiden;\nHet tweede getal geeft de gebeurtenis type aan, in dit geval is de doorlopende lijn \\(1\\) voor overleden aan melanoma en de stippellijn is \\(3\\) voor overleden vanwege overleden vanwege andere oorzaken.\nPlot de cumulatieve incidentie - ‘ggcompetingrisks’\nWe kunnen de cumulatieve incidentie ook plotten met de ggscompetingrisks functie van het survminer pakket.\nIn dit geval krijgen we een panel gelabeld volgens de groep en een legenda gelabeld evenement, met vermelding van het type evenement voor elke regel.\nOpgelet\nJe kunt de optie multiple_panels = FALSE gebruiken om alle groepen op een enkel panel afgedrukt te krijgen;\nanders dan bij R basis gaat de y-as niet standaard naar 1, dus die moet je handmatig aanpassen\n\n\n\nVergelijken van cumulatieve incidentie tussen groepen\nIn cuminc wordt de Gray’s test gebruikt voor tussen-groepen tests.\nAls voorbeeld vergelijken we de Melanoma uitkomsten volgens ulcer, de aan- of afwezigheid van ulceratie (zweervorming). De resultaten hiervan vind je in Tests.\n\n\n       stat           pv df\n1 26.120719 3.207240e-07  1\n3  0.158662 6.903913e-01  1\n\nPlotten van cumulatieve incidentie volgens groep - ‘ggcompetingrisks’\n\n\n\nPlotten van cumulatieve incidentie per groep - handmatig\nOpgelet Persoonlijk vind ik ggcompetingrisks functie lastig, vooral vergeleken met ggsurvplot. Het plotten doe ik veelal door eerst te zorgen voor een schone dataset van de cuminc fit resultaten en pas daarna de resultaten te plotten. Kijk maar eens naar de broncode hieronder.\n\n\n\nPlotten van een enkele gebeurtenis type - handmatig\nVaak is slechts een van de soorten evenementen interessant, hoewel we nog steeds rekenschap willen afleggen van de concurrerende gebeurtenis. In dat geval kan de gebeurtenis van belang alleen worden uitgezet. Nogmaals, ik doe dit handmatig door eerst een nette dataset te maken van de cuminc fit resultaten en dan de resultaten te plotten. Zie hieronder:\n\n\n\nGetallen toevoegen aan de risicotabel\nMisschien wil je de getallen van de risicotabel toevoegen aan een cumulatieve incidentiegrafiek. Hier weet ik geen eenvoudige manier voor.\nMaak een grafiek met basis R, ggcompetingrisks of ggplot\nHaal het getal van de risico tabel vanggsurvplot door survfit te gebruiken waar alle gebeurtenissen tellen als een enkel eindpunt\nDwing de assen zodat ze dezelfde limieten, breekpunten en titels hebben;\nWees er zeker van dat de kleuren/lijntypen matchen met de labels;\nZorg ervoor dat de lettergrootte steeds hetzelfde is;\n\nCombineer dan de grafiek en de risicotabel. Ik gebruik hiervoor de plot_grid functie van het cowplot pakket;\nIk weet niet hoe ik de tekstgrootte van “Number at risk” moet veranderen…\n\n\n\n\nConcurrerende risico regressie\nTwee benaderingen:\nOorzaak-specifieke gevaren (’hazards)\nsnelheid van het optreden van het gegeven type van gebeurtenis die op dit moment gebeurtenis-vrij zijn\ngeschat met behulp van Cox regressie (coxph functie)\n\nSubdistributierisico’s\nsnelheid van het optreden van het gegeven type gebeurtenis bij proefpersonen die nog geen gebeurtenis van dat type hebben meegemaakt\ngeschat met behulp van Fine-Gray regressie (crr functie)\n\nConcurrerende risico regressie in Melanoma data - subdistributie hazard benadering\nLaten we zeggen dat we geïnteresseerd zijn in het effect van leeftijd en geslacht op de dood door melanoom, met de dood door andere oorzaken als een concurrerende gebeurtenis.\nOpmerkingen:\ncrr vereist specificatie van covariaten als een matrix\nAls er meer dan één gebeurtenis van belang is, kunt u de resultaten voor een andere gebeurtenis opvragen met behulp van de failcode optie, standaard worden de resultaten geretourneerd voor failcode = 1.\nLet’s say we’re interested in looking at the effect of age and sex on death from melanoma, with death from other causes as a competing event.\n\n\nconvergence:  TRUE \ncoefficients:\n    sex     age \n0.58840 0.01259 \nstandard errors:\n[1] 0.271800 0.009301\ntwo-sided p-values:\n sex  age \n0.03 0.18 \n\nIn het vorige voorbeeld werden zowel sex en agegecodeerd als numerieke variabelen. De crr functie kan niet op een natuurlijke manier omgaan met karaktervariabelen en je krijgt een fout. Dus als er karaktervariabelen aanwezig zijn moeten we dummy-variabelen maken met behulp van model.matrix.\n\n\n\nFormatteren van de resultaten van van crr\nOutput van crr wordt niet ondersteund door broom::tidy() of noch door gtsummary::tbl_regression() op dit moment. Als alternatief, gebruik de (niet flexibele, maar beter dan niks?) mvcrrres van mijn ezfun pakket\n\n\nHR (95% CI)\np-value\nsex\n1.8 (1.06, 3.07)\n0.03\nage\n1.01 (0.99, 1.03)\n0.18\n\nConcurrerende risico’s regressie in de gegevens van Melanoom - oorzaak-specifieke gevarenbenadering\nCensor alle onderwerpen die niet voor de gebeurtenis van belang zijn, in dit geval de dood door melanoom, en gebruik coxph zoals voorheen. Dus patiënten die zijn overleden aan andere oorzaken worden nu gecensord voor de oorzaak-specifieke gevarenbenadering van concurrerende risico’s.\nDe resultaten kunnen worden geformatteerd met broom::tidy() of gtsummary::tbl_regression().\n\nterm\nestimate\nstd.error\nstatistic\np.value\nsex\n1.818949\n0.2676386\n2.235323\n0.0253961\nage\n1.016679\n0.0086628\n1.909514\n0.0561958\nhtml {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#tjxokapgnw .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#tjxokapgnw .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#tjxokapgnw .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#tjxokapgnw .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 4px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#tjxokapgnw .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#tjxokapgnw .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#tjxokapgnw .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#tjxokapgnw .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#tjxokapgnw .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#tjxokapgnw .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#tjxokapgnw .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#tjxokapgnw .gt_group_heading {\n  padding: 8px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#tjxokapgnw .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#tjxokapgnw .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#tjxokapgnw .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#tjxokapgnw .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#tjxokapgnw .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#tjxokapgnw .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 12px;\n}\n\n#tjxokapgnw .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#tjxokapgnw .gt_first_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n}\n\n#tjxokapgnw .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#tjxokapgnw .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#tjxokapgnw .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#tjxokapgnw .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#tjxokapgnw .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding: 4px;\n}\n\n#tjxokapgnw .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#tjxokapgnw .gt_sourcenote {\n  font-size: 90%;\n  padding: 4px;\n}\n\n#tjxokapgnw .gt_left {\n  text-align: left;\n}\n\n#tjxokapgnw .gt_center {\n  text-align: center;\n}\n\n#tjxokapgnw .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#tjxokapgnw .gt_font_normal {\n  font-weight: normal;\n}\n\n#tjxokapgnw .gt_font_bold {\n  font-weight: bold;\n}\n\n#tjxokapgnw .gt_font_italic {\n  font-style: italic;\n}\n\n#tjxokapgnw .gt_super {\n  font-size: 65%;\n}\n\n#tjxokapgnw .gt_footnote_marks {\n  font-style: italic;\n  font-size: 65%;\n}\nCharacteristic\n      HR1\n      95% CI1\n      p-value\n    sex\n      1.82\n      1.08, 3.07\n      0.025\n    age\n      1.02\n      1.00, 1.03\n      0.056\n    \n        \n          1\n          \n           \n          HR = Hazard Ratio, CI = Confidence Interval\n          \n      \n    \n\nWat hebben we gedaan?\nDe basis van de overlevingsanalyse met inbegrip van de Kaplan-Meier overlevingsfunctie en Cox-regressie\nOriëntatieanalyse en tijdsafhankelijke covariaten\nCumulatieve incidentie en regressie voor concurrerende risicoanalyses\n",
    "preview": "posts/2020-08-21-survival-analyse-met-r/img/trial_anatomy.png",
    "last_modified": "2020-08-21T11:07:13+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-05-31-grafieken/",
    "title": "Grafieken",
    "description": "Hier een serie grafieken die je kunt maken. Dit is gebaseerd op blog van het Urban Institute in de Verenigde Staten. Zij maken hun grafieken altijd op eenzelfde manier. Hoe ze dat doen kan je hierlezen",
    "author": [
      {
        "name": "Urban Institute, bewerking Harrie Jonkman",
        "url": "www.harriejonkman.nl"
      }
    ],
    "date": "2020-05-31",
    "categories": [],
    "contents": "\nR is een krachtige, open-source programmeertaal en -omgeving. R blinkt uit in databeheer en bewerking, traditionele statistische analyse, machine learning en reproduceerbaar onderzoek. Maar is waarschijnlijk nog het meest bekend om zijn grafieken. In deze blog staan voorbeelden en instructies voor populaire en minder bekende plottechnieken in R. Het bevat ook instructies voor het gebruik van urbnthemes, het R-pakket van het Urban Institute voor het maken van bijna-publicatie-klare plots met ggplot2. Als u vragen heeft, zo staat op hun site, aarzel dan niet om contact op te nemen met Aaron Williams of Kyle Ueyama.\nAchtergrond\nHet R-pakket (library(urbnthemes)) maakt ggplot2 output dat verbonden is met Urban Institute’s Data Visualisatie stijl gids. Tzijn pakket produceert ** geen publicatieklare grafieken**. Visuele stijlen moeten nog steeds worden bewerkt met behulp van de normale bewerkingsworkflow van het project. Door grafieken te exporteren als pdf kunnen ze gemakkelijker worden bewerkt. Zie de sectie Plots opslaan voor meer informatie.\nHet vaste thema dat hier gebruikt wordt is getest met ggplot2 versie 3.0.0. Het zal niet goed functioneren met oudere versies van ggplot2.\nGebruik library(urbnthemes)\nJe moet in ieder geval de volgende code gebruiken om urbnthemes te installeren of te updaten:\n# install.packages(\"devtools\")\n# devtools::install_github(\"UrbanInstitute/urbnthemes\")\nVoer de volgende code bovenaan elk script uit. Als je dit hebt gedaan, kun je aan de slag:\n\n\nlibrary(tidyverse)\nlibrary(urbnthemes)\nlibrary(ggrepel)\nlibrary(extrafont)\n\nset_urbn_defaults(style = \"print\")\n\n\n\nIAls het nog niet is geïnstalleerd, installeer dan het gratis Lato-lettertype van Google-lettertypen. Als je op een Mac werkt sla je Lato op in je font-book. Als je op Windows werkt, moet je eerst Ghostscript installeren. Vertel dan in R waar uw ghostscript-bestand zich bevindt. Bewerk het bestandspad als het uwe zich op een andere plaats bevindt.\nSys.setenv(R_GSCMD=\"C:/Program Files/gs/gs9.05/bin/gswin32c.exe\")\nVoer dit script één keer uit om Lato te importeren en te registreren:\n# install.packages(c(\"ggplot2\", \"ggrepel\", \"extrafont\"))\n# urbnthemes::lato_install()\nHet laden en importeren van dit lettertype kan enkele minuten duren.\nGrammar of Graphics en de conventies\nHadley Wickham’s ggplot2 is gebaseerd op Leland Wilkinsons The Grammar of Graphics en Wickhams A Layered Grammar of Graphics. De gelaagde Grammer of Graphics is een gestructureerde manier van denken over de componenten van een plot, die zich vervolgens lenen voor de eenvoudige structuur van ggplot2.\nData zijn wat in een plot wordt gevisualiseerd en mappings zijn aanwijzingen voor hoe gegevens in een plot in kaart worden gebracht op een manier die door de mens kan worden waargenomen.\nGegevens zijn weergaven van de werkelijke gegevens zoals punten, lijnen en balken.\nStatistieken zijn statistische transformaties die samenvattingen van de gegevens weergeven, zoals histogrammen.\nScales kaartwaarden in de dataruimte naar waarden in de esthetische ruimte. Schalen tekenen legendes en assen.\nCoördinatensystemen beschrijven hoe geomen in het vlak van de grafiek in kaart worden gebracht.\nFacetten splitsen de gegevens op in betekenisvolle deelverzamelingen zoals kleine veelvouden. *Thema’s** controleren de fijnere punten van een plot zoals lettertypes, lettergroottes en achtergrondkleuren.\nMeer informatie vind je hier: ggplot2: Elegant Graphics for Data Analysis\nTips en trucs\nggplot2 verwacht dat de gegevens in dataframes of tibbles zitten. Het heeft de voorkeur dat de dataframes “netjes” zijn met elke variabele als een kolom, elke obseravtion als een rij, en elke observatie-eenheid als een aparte tabel. De dplyr en tidyr bevatten beknopte en effectieve hulpmiddelen voor het “opruimen” van gegevens.\nR staat toe dat functie-argumenten expliciet bij naam en impliciet bij positie worden aangeroepen. De codeervoorbeelden in deze handleiding bevatten alleen benoemde argumenten voor de duidelijkheid.\nGrafieken zullen soms verschillend worden weergeven op verschillende besturingssystemen. Dit zal geen probleem zijn als de afbeeldingen eenmaal zijn opgeslagen.\nDoorlopende x-assen hebben tikken. Discrete x-assen hebben geen teken. Gebruik remove_ticks() om teken te verwijderen.\nStaaf grafieken\nEen kleur\n\n\nmtcars %>%\n  count(cyl) %>%\n  ggplot(mapping = aes(x = factor(cyl), y = n)) +\n  geom_col() +\n  geom_text(mapping = aes(label = n), vjust = -1) +    \n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.1))) +\n  labs(x = \"Cilinders\",\n       y = NULL) +\n  remove_ticks() +\n  remove_axis() \n\n\n\n\nEen kleur (Geroteerd)\nDit introduceert coord_flip() en remove_axis(axis = \"x\", flip = TRUE). remove_axis() komt van library(urbnthemes) en creëert een aangepast thema voor geroteerde staafgrafieken.\n\n\nmtcars %>%\n  count(cyl) %>%\n  ggplot(mapping = aes(x = factor(cyl), y = n)) +\n  geom_col() +\n  geom_text(mapping = aes(label = n), hjust = -1) +  \n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.1))) +\n  labs(x = \"Cilinders\",\n       y = NULL) +  \n  coord_flip() +\n  remove_axis(axis = \"x\", flip = TRUE)\n\n\n\n\nDrie kleuren\nDit is identiek aan de vorige grafiek, behalve dat kleuren en een legenda zijn toegevoegd met fill = cyl. Door x om te zetten in een factor met factor(cyl) worden 5 en 7 op de x-as overgeslagen. Het toevoegen van fill = cyl zonder factor() zou een doorlopend kleurenschema en een legenda hebben gecreëerd.\n\n\nmtcars %>%\n  mutate(cyl = factor(cyl)) %>%\n  count(cyl) %>%\n  ggplot(mapping = aes(x = cyl, y = n, fill = cyl)) +\n  geom_col() +\n  geom_text(mapping = aes(label = n), vjust = -1) +    \n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.1))) +\n  labs(x = \"Cylinders\",\n       y = NULL) +\n  remove_ticks() +\n  remove_axis()\n\n\n\n\nGestapelde staafgrafiek\nEen extra esthetiek kan eenvoudig worden toegevoegd aan de staafgrafiek door fill = categorical variable toe te voegen aan de mapping. Hier toont elk onderdeel een subset van een aantal auto’s met verschillende aantallen cilinders.\n\n\nmtcars %>%\n  mutate(am = factor(am, labels = c(\"Automatic\", \"Manual\")),\n         cyl = factor(cyl)) %>%  \n  group_by(am) %>%\n  count(cyl) %>%\n  group_by(cyl) %>%\n  arrange(desc(am)) %>%\n  mutate(label_height = cumsum(n)) %>%\n  ggplot() +\n  geom_col(mapping = aes(x = cyl, y = n, fill = am)) +\n  geom_text(aes(x = cyl, y = label_height - 0.5, label = n, color = am)) +\n  scale_color_manual(values = c(\"white\", \"black\")) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.1))) +\n  labs(x = \"Cylinders\",\n       y = NULL) +  \n  remove_ticks() +\n  remove_axis() +\n  guides(color = FALSE)\n\n\n\n\nGestapelde staafgrafiek met Position = Fill\nDe vorige voorbeelden gebruiken geom_col(), die een y-waarde voor de staafhoogte neemt. Dit voorbeeld gebruikt geom_bar() die de waarden opsomt en een waarde voor de staafhoogte genereert. In dit voorbeeld verandert position = \"fill\" in geom_bar() de y-as van de telling naar de verhouding van elke staaf.\n\n\nmtcars %>%\n  mutate(am = factor(am, labels = c(\"Automatic\", \"Manual\")),\n         cyl = factor(cyl)) %>%  \n  ggplot() +\n  geom_bar(mapping = aes(x = cyl, fill = am), position = \"fill\") +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.1)), labels = scales::percent) +\n  labs(x = \"Cylinders\",\n       y = NULL) +  \n  remove_ticks() +\n  guides(color = FALSE)\n\n\n\n\nOpgedeelde staafgrafiek\nDelen van de staafgrafiek in ggplot2 worden standaard opgestapeld. position = \"dodge\" in geom_col() breidt het staafdiagram uit zodat de subsets naast elkaar verschijnen.\n\n\nmtcars %>%\n  mutate(am = factor(am, labels = c(\"Automatic\", \"Manual\")),\n         cyl = factor(cyl)) %>%\n  group_by(am) %>%\n  count(cyl) %>%\n  ggplot(mapping = aes(cyl, y = n, fill = factor(am))) +\n  geom_col(position = \"dodge\") +\n  geom_text(aes(label = n), position = position_dodge(width = 0.7), vjust = -1) +    \n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.1))) +\n  labs(x = \"Cylinders\",\n       y = NULL) +  \n  remove_ticks() +\n  remove_axis()\n\n\n\n\nLolly grafiek/Cleveland puntgrafiek\nLolly en Cleveland puntgrafiek zijn minimalistische alternatieven voor staafgrafieken. De sleutel tot beide grafieken is om de gegevens te ordenen op basis van de continue variabele met behulp van arrange() en dan de discrete variabele om te zetten in een factor met de geordende niveaus van de continue variabele met behulp van mutate(). Deze stap “slaat” de volgorde van de gegevens op.\nLollygrafiek\n\n\nmtcars %>%\n  rownames_to_column(\"model\") %>%\n  arrange(mpg) %>%\n  mutate(model = factor(model, levels = .$model)) %>%\n  ggplot(aes(mpg, model)) +\n    geom_segment(aes(x = 0, xend = mpg, y = model, yend = model)) +  \n    geom_point() +\n    scale_x_continuous(expand = expand_scale(mult = c(0, 0)), limits = c(0, 40)) +\n    labs(x = NULL, \n         y = \"Miles Per Gallon\")\n\n\n\n\nCleveland puntgrafiek\n\n\nmtcars %>%\n  rownames_to_column(\"model\") %>%\n  arrange(mpg) %>%\n  mutate(model = factor(model, levels = .$model)) %>%\n  ggplot(aes(mpg, model)) +\n    geom_point() +\n    scale_x_continuous(expand = expand_scale(mult = c(0, 0)), limits = c(0, 40)) +\n    labs(x = NULL, \n         y = \"Miles Per Gallon\")\n\n\n\n\nDumbellgrafieken\nPuntengrafieken\nEen kleur puntengrafiek\nPuntengrafieken zijn nuttig voor het tonen van relaties tussen twee of meer variabelen. Gebruik scatter_grid() van library(urbnthemes) om eenvoudig verticale rasterlijnen toe te voegen aan de puntengrafieken.\n\n\nmtcars %>%\n  ggplot(mapping = aes(x = wt, y = mpg)) +\n  geom_point() +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(0, 6),\n                     breaks = 0:6) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \n                     limits = c(0, 40),\n                     breaks = 0:8 * 5) +\n  labs(x = \"Gewicht (duizenden ponden)\",\n       y = \"MPG\") +\n  scatter_grid()\n\n\n\n\nPuntengrafiek met hoge dichtheid met transparantie\nGrote aantallen waarnemingen maken soms strooiplekken soms moeilijk om te interpreteren omdat punten elkaar overlappen. Het toevoegen van alpha = met een getal tussen 0 en 1 voegt transparantie toe aan punten en helderheid aan grafieken.\n\n\ndiamonds %>%\n  ggplot(mapping = aes(x = carat, y = price)) +\n  geom_point(alpha = 0.05) +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(0, 6),\n                     breaks = 0:6) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \n                     limits = c(0, 20000),\n                     breaks = 0:4 * 5000,\n                     labels = scales::dollar) +\n  labs(x = \"Carat\",\n       y = \"Price\") +\n  scatter_grid()\n\n\n\n\nHexus puntengrafiek\nSoms is transparantie niet genoeg om duidelijkheid te brengen in een verstrooide grafiek met veel waarnemingen. Als n toeneemt in de honderdduizenden en zelfs miljoenen, kan geom_hex een van de beste manieren zijn om relaties tussen twee variabelen weer te geven.\n\n\ndiamonds %>%\n  ggplot(mapping = aes(x = carat, y = price)) +\n  geom_hex(mapping = aes(fill = ..count..)) +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(0, 6),\n                     breaks = 0:6) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \n                     limits = c(0, 20000),\n                     breaks = 0:4 * 5000,\n                     labels = scales::dollar) +\n  scale_fill_gradientn(labels = scales::comma) +  \n  labs(x = \"Carat\",\n       y = \"Price\") +\n  scatter_grid() +\n  theme(legend.position = \"right\",\n        legend.direction = \"vertical\")\n\n\n\n\nPuntengrafiek met random vervuiling\nSoms hebben puntengrafieken veel overlappende punten, maar een redelijk aantal waarnemingen. Geom_jitter voegt een kleine hoeveelheid willekeurige ruis toe zodat punten minder snel overlappen. De breedte en hoogte bepalen de hoeveelheid ruis die wordt toegevoegd. Merk in het volgende voor- en naschrijven op hoeveel punten er nog zichtbaar zijn na het toevoegen van jitter.\nVoor\n\n\nmpg %>%\n  ggplot(mapping = aes(x = displ, y = cty)) +\n  geom_point() +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(0, 8),\n                     breaks = 0:8) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \n                     limits = c(0, 40),\n                     breaks = 0:4 * 10) +\n  labs(x = \"Uitval\",\n       y = \"MPG\") +\n  scatter_grid()\n\n\n\n\nNa\n\n\nset.seed(2017)\nmpg %>%\n  ggplot(mapping = aes(x = displ, y = cty)) +\n  geom_jitter() +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(0, 8),\n                     breaks = 0:8) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \n                     limits = c(0, 40),\n                     breaks = 0:4 * 10) +\n  labs(x = \"Uitval\",\n       y = \"MPG\") +\n  scatter_grid()\n\n\n\n\nPuntengrafiek met gevarieerde puntenomvang\nGewichten en populaties kunnen in puntengrafieken met de grootte van de punten in kaart worden gebracht. Hier wordt het aantal huishoudens in elke staat in kaart gebracht op de grootte van elk punt met behulp van aes(size = hhpop). Opmerking: ggplot2::geom_point() wordt gebruikt in plaats van geom_point().\nWel eerst dit pakket laden (wel installeren als je dat nog niet hebt gedaan).\n\n\nlibrary(urbnmapr)\n\n\n\n\n\nurbnmapr::statedata %>%\n  ggplot(mapping = aes(x = medhhincome, y = horate)) +\n  ggplot2::geom_point(mapping = aes(size = hhpop), alpha = 0.3) +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(30000, 80000),\n                     breaks = 3:8 * 10000,\n                     labels = scales::dollar) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \n                     limits = c(0, 0.8),\n                     breaks = 0:4 * 0.2) +\n  scale_radius(range = c(3, 15),\n               breaks = c(2500000, 7500000, 12500000), \n               labels = scales::comma) +\n  labs(x = \"Huishoud inkomen\",\n       y = \"Ratio huizenbezit\") +\n  scatter_grid() +\n  theme(plot.margin = margin(r = 20))\n\n\n\n\nPuntengrafieken met vulling\nEen derde esthetiek kan worden toegevoegd aan puntengrafieken. Hier betekent kleur het aantal cilinders in elke auto. Voordat ggplot() wordt aangeroepen, worden de cilinders aangemaakt met behulp van library(dplyr) en de functie %>%.\n\n\nmtcars %>%\n  mutate(cyl = paste(cyl, \"cylinders\")) %>%\n  ggplot(aes(x = wt, y = mpg, color = cyl)) +\n  geom_point() +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(0, 6),\n                     breaks = 0:6) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \n                     limits = c(0, 40),\n                     breaks = 0:8 * 5) +\n  labs(x = \"Gewicht (duizenden ponden)\",\n       y = \"MPG\") +\n  scatter_grid()\n\n\n\n\nLijngrafieken\n\n\neconomics %>%\n  ggplot(mapping = aes(x = date, y = unemploy)) +\n  geom_line() +\n  scale_x_date(expand = expand_scale(mult = c(0.002, 0)), \n               breaks = \"10 years\",\n               limits = c(as.Date(\"1961-01-01\"), as.Date(\"2020-01-01\")),\n               date_labels = \"%Y\") +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \n                     breaks = 0:4 * 4000,\n                     limits = c(0, 16000),\n                     labels = scales::comma) +\n  labs(x = \"Jaar\", \n       y = \"Aantal werklozen (1,000den)\")\n\n\n\n\nLijngrafieken met meerdere lijnen\n\n\nlibrary(gapminder)\n\ngapminder %>%\n  filter(country %in% c(\"Australia\", \"Netherlands\", \"New Zealand\")) %>%\n  mutate(country = factor(country, levels = c(\"Netherlands\", \"Australia\", \"New Zealand\"))) %>%\n  ggplot(aes(year, gdpPercap, color = country)) +\n  geom_line() +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     breaks = c(1952 + 0:12 * 5), \n                     limits = c(1952, 2007)) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \n                     breaks = 0:8 * 5000,\n                     labels = scales::dollar, \n                     limits = c(0, 40000)) +\n  labs(x = \"Jaap\",\n       y = \"BNP per hoofd van de bevolking (US dollars)\")\n\n\n\n\nHet plotten van meer dan één variabele kan nuttig zijn om de relatie van variabelen in de tijd te zien, maar het vergt een kleine hoeveelheid databewerking.\nDit komt omdat ggplot2 gegevens in een “lang” formaat wil hebben in plaats van een “breed” formaat voor lijnplots met meerdere lijnen. gather() en spread() uit het tidyr pakket maakt het wisselen tussen “lang” en “breed” pijnloos. In wezen gaan variabele titels naar “key” en variabele waarden naar “value”. Dan verandert ggplot2, de verschillende niveaus van de sleutelvariabele (bevolking, werkloosheid) in kleuren.\n\n\nas_tibble(EuStockMarkets) %>%\n  mutate(date = time(EuStockMarkets)) %>%\n  gather(key = \"key\", value = \"value\", -date) %>%\n  ggplot(mapping = aes(x = date, y = value, color = key)) +\n  geom_line() +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(1991, 1999), \n                     breaks = c(1991, 1993, 1995, 1997, 1999)) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \n                     breaks = 0:4 * 2500,\n                     labels = scales::dollar, \n                     limits = c(0, 10000)) +  \n  labs(x = \"Tijd\",\n       y = \"Waarde\")\n\n\n\n\nTrapgrafiek\ngeom_line() verbindt coördinaten met de kortst mogelijke rechte lijn. Soms zijn trapgrafieken nodig omdat de y-waarden niet veranderen tussen de coördinaten. Zo wordt bijvoorbeeld de bovengrens van de Federal Funds Rate met regelmatige tussenpozen ingesteld en blijft deze constant totdat deze wordt gewijzigd.\n\n\n# downloaded from FRED on 2018-12-06\n\n# https://fred.stlouisfed.org/series/DFEDTARU\n\nfed_fund_rate <- read_csv(\n  \"date, fed_funds_rate\n   2014-01-01,0.0025\n   2015-12-16,0.0050\n   2016-12-14,0.0075\n   2017-03-16,0.0100\n   2017-06-15,0.0125\n   2017-12-14,0.0150\n   2018-03-22,0.0175\n   2018-06-14,0.0200\n   2018-09-27,0.0225\n   2018-12-06,0.0225\")\n\nfed_fund_rate %>%\n  ggplot(mapping = aes(x = date, y = fed_funds_rate)) + \n  geom_step() +\n  scale_x_date(expand = expand_scale(mult = c(0.002, 0)), \n               breaks = \"1 year\",\n               limits = c(as.Date(\"2014-01-01\"), as.Date(\"2019-01-01\")),\n               date_labels = \"%Y\") +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \n                     breaks = c(0, 0.01, 0.02, 0.03),\n                     limits = c(0, 0.03),\n                     labels = scales::percent) +  \n  labs(x = \"Tijd\",\n       y = \"Bovengrens van de Federal Funds Rate\")\n\n\n\n\nPadgrafiek\nDe Beveridge-curve is een macro-economisch plot dat een verband toont tussen de werkloosheidsgraad en de vacaturegraad. Bewegingen op de curve duiden op veranderingen in de bedrijfscultuur en horizontale verschuivingen van de curve duiden op structurele veranderingen op de arbeidsmarkt.\nLijnen in de Beveridge-curve bewegen niet monotoon van links naar rechts. Daarom is het noodzakelijk om geom_pad() te gebruiken.\n\n\nlibrary(ggrepel)\n\nbeveridge <- read_csv(\n  [1336 chars quoted with '\"'])\n\nlabels <- beveridge %>%\n  filter(lubridate::month(quarter) == 1)\n\nbeveridge %>%\n  ggplot() +\n  geom_path(mapping = aes(x = unempoyment_rate, y = vacanacy_rate), alpha = 0.5) +\n  geom_point(data = labels, mapping = aes(x = unempoyment_rate, y = vacanacy_rate)) +\n  geom_text_repel(data = labels, mapping = aes(x = unempoyment_rate, y = vacanacy_rate, label = lubridate::year(quarter))) +  \n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(0.04, 0.1),\n                     labels = scales::percent) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)), \n                     breaks = c(0, 0.01, 0.02, 0.03, 0.04, 0.05),\n                     limits = c(0, 0.05),\n                     labels = scales::percent) +  \n  labs(x = \"Seizoen gecontroleerd werkloosheidspercentage\",\n       y = \"Seizoen gecontroleerd beschikbare banenpercentage\") +  \n  scatter_grid()\n\n\n\n\nHellingsgrafiek\n\n\n# https://www.bls.gov/lau/\nlibrary(ggrepel)\n\nunemployment <- tibble(\n  time = c(\"October 2009\", \"October 2009\", \"October 2009\", \"August 2017\", \"August 2017\", \"August 2017\"),\n  rate = c(7.4, 7.1, 10.0, 3.9, 3.8, 6.4),\n  state = c(\"Maryland\", \"Virginia\", \"Washington, D.C.\", \"Maryland\", \"Virginia\", \"Washington, D.C.\")\n)\n\nlabel <- tibble(label = c(\"October 2009\", \"August 2017\"))\noctober <- filter(unemployment, time == \"October 2009\")\naugust <- filter(unemployment, time == \"August 2017\")\n\nunemployment %>%\n  mutate(time = factor(time, levels = c(\"October 2009\", \"August 2017\")),\n         state = factor(state, levels = c(\"Washington, D.C.\", \"Maryland\", \"Virginia\"))) %>%\n  ggplot() + \n  geom_line(aes(time, rate, group = state, color = state), show.legend = FALSE) +\n  geom_point(aes(x = time, y = rate, color = state)) +\n  labs(subtitle = \"Werkloosheidspercentage\") +\n  theme(axis.ticks.x = element_blank(),\n        axis.title.x = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.y = element_blank(), \n        axis.text.y = element_blank(),\n        panel.grid.major.y = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        panel.grid.major.x = element_blank(),\n        axis.line = element_blank()) +\n  geom_text_repel(data = october, mapping = aes(x = time, y = rate, label = as.character(rate)), nudge_x = -0.06) + \n  geom_text_repel(data = august, mapping = aes(x = time, y = rate, label = as.character(rate)), nudge_x = 0.06)\n\n\n\n\nUnivariaat\nEr zijn een aantal manieren om de verdeling van univariate data in R te onderzoeken. Sommige methoden, zoals strookdiagrammen, laten alle datapunten zien. Andere methoden, zoals de box- en whiskerplot, tonen geselecteerde datapunten die belangrijke waarden communiceren zoals de mediaan en het 25e percentiel. Tenslotte tonen sommige methoden geen van de onderliggende data, maar berekenen ze dichtheidsschattingen. Elke methode heeft voor- en nadelen, dus het is de moeite waard om de verschillende vormen te begrijpen. Lees voor meer informatie 40 jaar boxplottem van Hadley Wickham en Lisa Stryjewski.\nStripdiagram\nStripdiagrammen, de eenvoudigste univariate plot, tonen de verdeling van de waarden langs één as. Stripdiagrammen werken het beste met variabelen die veel variatie hebben. Zo niet, dan hebben de punten de neiging om op elkaar te clusteren. Zelfs als de variabele veel variatie heeft, is het vaak belangrijk om transparantie toe te voegen aan de punten met alpha = zodat overlappende waarden zichtbaar zijn.\n\n\nmsleep %>%\n  ggplot(aes(x = sleep_total, y = factor(1))) +\n  geom_point(alpha = 0.2, size = 5) +\n  labs(y = NULL) +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(0, 25), \n                     breaks = 0:5 * 5) +\n  scale_y_discrete(labels = NULL) +\n  labs(title = \"Totale slaaptijd van verschillende zoogdieren\",\n       x = \"Totale slaaptijd (uren)\",\n       y = NULL) +\n  theme(axis.ticks.y = element_blank())\n\n\n\n\nStripdiagram met Highlighting\nOmdat strookdiagrammen alle waarden weergeven, zijn ze nuttig om aan te geven waar de geselecteerde punten in de verdeling van een variabele liggen. De duidelijkste manier om dit te doen is door geom_point() tweemaal toe te voegen met filter() in het gegevensargument. Op deze manier worden de geaccentueerde waarden boven op de niet geaccentueerde waarden getoond.\n\n\nggplot() +\n  geom_point(data = filter(msleep, name != \"Red fox\"), \n                    aes(x = sleep_total, \n                        y = factor(1)),\n             alpha = 0.2, \n             size = 5,\n             color = \"grey50\") +\n  geom_point(data = filter(msleep, name == \"Red fox\"),\n             aes(x = sleep_total, \n                 y = factor(1), \n                 color = name),\n             alpha = 0.8,\n             size = 5) +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(0, 25), \n                     breaks = 0:5 * 5) +  \n  scale_y_discrete(labels = NULL) +\n  labs(title = \"Totale slaaptijd van verschillende zoogdieren\",\n       x = \"Totale slaaptijd (uren)\",\n       y = NULL,\n       legend) +\n  guides(color = guide_legend(title = NULL)) +\n  theme(axis.ticks.y = element_blank())\n\n\n\n\nOndergeschikte Strip Chart\nVoeg een y-variabele toe om de verdelingen van de continue variabele in deelverzamelingen van een categorische variabele te zien.\n\n\nlibrary(forcats)\n\nmsleep %>%\n  filter(!is.na(vore)) %>%\n  mutate(vore = fct_recode(vore, \n                            \"Insectivore\" = \"insecti\",\n                            \"Omnivore\" = \"omni\", \n                            \"Herbivore\" = \"herbi\", \n                            \"Carnivore\" = \"carni\"\n                            )) %>%\n  ggplot(aes(x = sleep_total, y = vore)) +\n  geom_point(alpha = 0.2, size = 5) +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(0, 25), \n                     breaks = 0:5 * 5) +  \n  labs(title = \"Totale slaaptijd van verschillende zoogdieren volgens dieet\",\n       x = \"Totale slaaptijd (uren)\",\n       y = NULL) +\n  theme(axis.ticks.y = element_blank())\n\n\n\n\nHistogrammen\nHistogrammen verdelen de verdeling van een variabele in n staven van gelijke grootte en tellen en tonen vervolgens het aantal waarnemingen in elke staaf. Histogrammen zijn gevoelig voor de breedte van de staven. Zoals ?geom_histogram merkt op, “U moet altijd de waarde van [de standaard staafbreedte] overschrijven, door meerdere breedtes te onderzoeken om het beste te vinden om de verhalen in uw gegevens te illustreren”.\n\n\nggplot(data = diamonds, mapping = aes(x = depth)) + \n  geom_histogram(bins = 100) +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(0, 100)) +  \n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.2)), labels = scales::comma) +\n  labs(x = \"Depth\",\n       y = \"Count\")\n\n\n\n\nBoxplots\nBoxplots zijn in de jaren zeventig uitgevonden door John Tukey. In plaats van de onderliggende gegevens te tonen of het aantal blikken van de onderliggende gegevens, richten ze zich op belangrijke waarden zoals het 25e percentiel, de mediaan en het 75e percentiel.\n\n\nInsectSprays %>%\n  ggplot(mapping =  aes(x = spray, y = count)) +\n  geom_boxplot() +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.2))) +\n  labs(x = \"Type of insect spray\",\n       y = \"Number of dead insects\") +\n  remove_ticks()\n\n\n\n\nGladde kernel verdelingsgrafieken\nDoorlopende variabelen met vloeiende verdelingen worden soms beter weergegeven met afgevlakte kerneldichtheidsschattingen dan histogrammen of boxplots. geom_density() berekent en plot een kerneldichtheidsschatting. Let op de klontjes rond gehele en halve getallen in de volgende verdeling vanwege afrondingen.\n\n\ndiamonds %>%\n  ggplot(mapping = aes(carat)) +\n  geom_density(color = NA) +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(0, NA)) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.2))) +\n  labs(x = \"Carat\",\n       y = \"Density\")\n\n\n\n\n\n\ndiamonds %>%\n  mutate(cost = ifelse(price > 5500, \"More than $5,500 +\", \"$0 to $5,500\")) %>%\n  ggplot(mapping = aes(carat, fill = cost)) +\n  geom_density(alpha = 0.25, color = NA) +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(0, NA)) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.1))) +\n  labs(x = \"Carat\",\n       y = \"Density\")\n\n\n\n\n‘Ridgeline’ grafieken\nRidgeline plots zijn gedeeltelijk overlappende afgevlakte kerneldichtheid plots gefacetteerd door een categorische variabele die veel informatie verpakken in één elegante plot. Onderstaande maakt duidelijk wat we hiermee bedoelen.\n\n\nlibrary(ggridges)\n\nggplot(diamonds, mapping = aes(x = price, y = cut)) +\n  geom_density_ridges(fill = \"#1696d2\") +\n  labs(x = \"Price\",\n       y = \"Cut\")\n\n\n\n\nViool grafieken\nVioolplots zijn symmetrische weergaven van gladde kerneldichtheidplots.\n\n\nInsectSprays %>%\n  ggplot(mapping = aes(x = spray, y = count, fill = spray)) +\n  geom_violin(color = NA) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.2))) +\n  labs(x = \"Type of insect spray\",\n       y = \"Number of dead insects\") +\n  remove_ticks()\n\n\n\n\nBonenplot\nIndividuele uitschieters en belangrijke samenvattende waarden zijn niet zichtbaar in vioolplots of afgevlakte kerneldichtheidsplots. Bonenplots, gemaakt door Peter Kampstra in 2008, zijn vioolplots met gegevens weergegeven als kleine lijnen in een eendimensionale stripplot en grotere lijnen voor het gemiddelde.\n\n\nmsleep %>%\n  filter(!is.na(vore)) %>%\n  mutate(vore = fct_recode(vore, \n                            \"Insectivore\" = \"insecti\",\n                            \"Omnivore\" = \"omni\", \n                            \"Herbivore\" = \"herbi\", \n                            \"Carnivore\" = \"carni\"\n                            )) %>%\n  ggplot(aes(x = vore, y = sleep_total, fill = vore)) +\n  stat_summary(fun.y = \"mean\",\n               colour = \"black\", \n               size = 30,\n               shape = 95,\n               geom = \"point\") +\n  geom_violin(color = NA) +\n  geom_jitter(width = 0,\n              height = 0.05,\n              alpha = 0.4,\n              shape = \"-\",\n              size = 10,\n              color = \"grey50\") +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.2))) +  \n    labs(x = NULL,\n         y = \"Total sleep time (hours)\") +\n  theme(legend.position = \"none\") +\n  remove_ticks()\n\n\n\n\nGebiedsplot\nGestapeld gebied\n\n\ntxhousing %>%\n  filter(city %in% c(\"Austin\",\"Houston\",\"Dallas\",\"San Antonio\",\"Fort Worth\")) %>%\n  group_by(city, year) %>%\n  summarize(sales = sum(sales)) %>%\n  ggplot(aes(x = year, y = sales, fill = city)) +\n  geom_area(position = \"stack\") +\n  scale_x_continuous(expand = expand_scale(mult = c(0, 0)),\n                     limits = c(2000, 2015),\n                     breaks = 2000 + 0:15) +  \n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.2)), \n                     labels = scales::comma) +\n  labs(x = \"Year\",\n       y = \"Home sales\")\n\n\n\n\nGevuld gebied\n\n\ntxhousing %>%\n  filter(city %in% c(\"Austin\",\"Houston\",\"Dallas\",\"San Antonio\",\"Fort Worth\")) %>%\n  group_by(city, year) %>%\n  summarize(sales = sum(sales)) %>%\n  ggplot(aes(x = year, y = sales, fill = city)) +\n  geom_area(position = \"fill\") +\n  scale_x_continuous(expand = expand_scale(mult = c(0, 0)),\n                     limits = c(2000, 2015),\n                     breaks = 2000 + 0:15) +  \n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.02)),\n                     breaks = c(0, 0.25, 0.5, 0.75, 1),\n                     labels = scales::percent) +\n  labs(x = \"Year\",\n       y = \"Home sales\")\n\n\n\n\nWafelkaart / Vierkante taartkaart\nHet wafelpakket {CRAN en Github} maakt vierkante taartkaarten. Het kan ook gecombineerd worden met glyphs voor meer elegantere vormen dan vierkanten. Data hiervoor komen hier vandaan: A Vision for an Equitable DC.\nWafelkaarten vereisen een beetje extra knutselen omdat ze worden genoemd vanuit library(waffle) in plaats van library(ggplot2). Het belangrijkste is dat voor wafeldiagrammen theme_urban(text = element_text(family = \"Lato\")) nodig is voor het lettertype Lato.\nEnkele wafelkaart\n\n\nlibrary(waffle)\n\nparts <- c(`Virginia\\nClinics` = (1000 - 208 - 105), `Maryland\\nClinics` = 208, `D.C.\\nClinics` = 105)\nwaffle(parts, rows = 25, size = 1, colors = c(\"#1696d2\", \"#fdbf11\", \"#000000\"), legend_pos = \"bottom\") +\n  labs(title = \"Free Clinics in the D.C.-Maryland-Virginia Area\",\n       subtitle = \"1 Square == 1 Clinic\") +\n  theme(text = element_text(family = \"Lato\"))\n\n\n\n\nMeervoudige wafelkaarten\nlibrary(waffle) allows multiple waffle charts to be ironed together using iron(). maakt het mogelijk om meerdere wafelkaarten in elkaar te strijken met behulp van iron(). Het samen strijken van meerdere wafeldiagrammen vereist wat trial-and-error om de maten en resolutie goed te krijgen, maar de resultaten kunnen de moeite waard zijn. Vergeet niet theme(text = element_text(family = \"Lato\"))!\n\n\nlibrary(waffle)\n\nwhite <- c(`With Degree` = 169300, `Without Degree` = 800)\nblack <- c(`With Degree` = 174900, `Without Degree` = 34700)\nhispanic <- c(`With Degree` = 27700, `Without Degree` = 12400)\n\niron(\n  waffle(white / 83, rows = 40, size = 0.25, colors = c(\"#1696d2\", \"#fdbf11\"), title = \"White\", keep = FALSE, pad = 10) + \n  theme(text = element_text(family = \"Lato\")),\n  waffle(black / 83, rows = 40, size = 0.25, colors = c(\"#1696d2\", \"#fdbf11\"), title = \"Black\", keep = FALSE) + \n  theme(text = element_text(family = \"Lato\")),\n  waffle(hispanic / 83, rows = 40, size = 0.25, colors = c(\"#1696d2\", \"#fdbf11\"), title = \"Hispanic\", keep = FALSE, pad = 59, xlab = \"1 Square == 83 People\") + \n  theme(text = element_text(family = \"Lato\"))\n) \n\n\n\n\nWarmtekaart\n\n\nlibrary(fivethirtyeight)\n\nbad_drivers %>%\n  filter(state %in% c(\"Maine\", \"New Hampshire\", \"Vermont\", \"Massachusetts\", \"Connecticut\", \"New York\")) %>%\n  mutate(`Number of\\nDrivers` = scale(num_drivers),\n         `Percent\\nSpeeding` = scale(perc_speeding),\n         `Percent\\nAlcohol` = scale(perc_alcohol),\n         `Percent Not\\nDistracted` = scale(perc_not_distracted),\n         `Percent No\\nPrevious` = scale(perc_no_previous),\n         state = factor(state, levels = rev(state))\n         ) %>%\n  select(-insurance_premiums, -losses, -(num_drivers:losses)) %>%\n  gather(`Number of\\nDrivers`:`Percent No\\nPrevious`, key = \"variable\", value = \"SD's from Mean\") %>%\n  ggplot(aes(variable, state)) +\n    geom_tile(aes(fill = `SD's from Mean`)) +\n    labs(x = NULL,\n         y = NULL) + \n    scale_fill_gradientn() +\n    theme(legend.position = \"right\",\n          legend.direction = \"vertical\",\n          axis.line.x = element_blank(),\n          panel.grid.major.y = element_blank()) +\n  remove_ticks()\n\n\n\n#https://learnr.wordpress.com/2010/01/26/ggplot2-quick-heatmap-plotting/\n\n\n\nFaceteren en kleine kaartjes\nfacet_wrap()\nR’s faceteersysteem is een krachtige manier om kleinere kaarten te maken.\nSommige bewerkingen aan het thema kunnen nodig zijn, afhankelijk van het aantal rijen en kolommen in de plot.\n\n\ndiamonds %>%\n  ggplot(mapping = aes(x = carat, y = price)) +\n  geom_point(alpha = 0.05) +\n  facet_wrap(~cut, ncol = 5) +\n  scale_x_continuous(expand = expand_scale(mult = c(0, 0)),\n                     limits = c(0, 6)) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0)),\n                     limits = c(0, 20000), \n                     labels = scales::dollar) +\n  labs(x = \"Karaat\",\n       y = \"Prijs\") +\n  scatter_grid()\n\n\n\n\nfacet_grid()\n\n\ndiamonds %>%\n  filter(color %in% c(\"D\", \"E\", \"F\", \"G\")) %>%\n  ggplot(mapping = aes(x = carat, y = price)) +\n  geom_point(alpha = 0.05) +\n  facet_grid(color ~ cut) +\n  scale_x_continuous(expand = expand_scale(mult = c(0, 0)),\n                     limits = c(0, 4)) +  \n  scale_y_continuous(expand = expand_scale(mult = c(0, 0)),\n                     limits = c(0, 20000), \n                     labels = scales::dollar) +\n  labs(x = \"Carat\",\n       y = \"Price\") +\n  theme(panel.spacing = unit(20L, \"pt\")) +\n  scatter_grid()\n\n\n\n\n‘Smoothers’\ngeom_smooth() past en modelleert op gegevens met twee of meer dimensies.\nHet begrijpen en manipuleren van defaults is belangrijker voor geom_smooth() dan andere ‘geoms’ omdat het een aantal aannames bevat. geom_smooth() gebruikt automatisch loess voor datasets met minder dan 1.000 waarnemingen en een algemeen model met formula = y ~ s(x, bs = \"cs\") voor datasets met meer dan 1.000 waarnemingen. Beide zijn standaard ingesteld op een betrouwbaarheidsinterval van 95%.\nModellen worden gekozen met method = en kunnen worden ingesteld op lm(), glm(), gam(), loess(), rlm(), en meer. Formules kunnen worden opgegeven met formule = en y ~ x syntaxis. Het plotten van de standaardfout wordt omgeschakeld met se = TRUE en se = FALSE, en het niveau wordt gespecificeerd met level =. Zoals altijd is er meer informatie te zien in RStudio met ?geom_smooth().\ngeom_point() voegt een scatterplot toe aan geom_smooth(). De volgorde van de functie-aanroepen is belangrijk. De functie die als tweede wordt aangeroepen wordt bovenop de functie die als eerste wordt aangeroepen gelegd.\n\n\ndiamonds %>%\n  ggplot(mapping = aes(x = carat, y = price)) +\n  geom_point(alpha = 0.05) +\n  geom_smooth(color =  \"#ec008b\") +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(0, 5),\n                     breaks = 0:5) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)),\n                     limits = c(0, 20000), \n                     labels = scales::dollar) +  \n  labs(x = \"Carat\",\n       y = \"Price\") +\n  scatter_grid()\n\n\n\n\ngeom_smooth kan worden onderverdeeld in categorische en factorvariabelen. Dit vereist subgroepen met een behoorlijk aantal waarnemingen en een behoorlijke mate van variabiliteit over de x-as. De betrouwbaarheidsintervallen worden vaak groter aan de uiteinden, zodat speciale zorg nodig is om de grafiek zinvol en leesbaar te maken.\nDit voorbeeld gebruikt Loess met MPG = verplaatsing.\n\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = factor(cyl))) +\n  geom_point(alpha = 0.2) +\n  geom_smooth() +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(0, 7),\n                     breaks = 0:7) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)),\n                     limits = c(0, 60)) +  \n  labs(x = \"Engine displacement\",\n       y = \"Highway MPG\") +\n  scatter_grid()\n\n\n\n\nDit voorbeeld gebruikt liniaire regressie met MPG = displacement.\n\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = factor(cyl))) +\n  geom_point(alpha = 0.2) +\n  geom_smooth(method = \"lm\") +\n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)), \n                     limits = c(0, 7),\n                     breaks = 0:7) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)),\n                     limits = c(0, 60)) +  \n  labs(x = \"Engine displacement\",\n       y = \"Highway MPG\") +\n  scatter_grid()\n\n\n\n\nBenadrukken\nlibrary(gghighlight) maakt intuitief benadrukken van ggplot2 plots mogelijk. gghighlight wijzigt bestaande ggplot2-objecten, dus geen enkele andere code mag veranderen. Alle markering wordt afgehandeld door de functie gghighlight(), die alle soorten geomen kan verwerken.\nWaarschuwing:  R zal een fout maken als er te veel kleuren worden gemarkeerd vanwege het ontwerp van urbnthemes. Verlaag gewoon het aantal gemarkeerde geomen om dit probleem op te lossen.\nEr zijn twee belangrijke manieren om de aandacht te vestigen.\nDrempelwaarde\nDe eerste manier om te markeren is met een drempel. Voeg een logische test toe aan gghighlight() om te beschrijven welke lijnen gemarkeerd moeten worden. Hier worden regels met een maximale verandering in het bruto binnenlands product per hoofd van de bevolking van meer dan $35.000 gemarkeerd met gghighlight(max(pcgpd_change) > 35000, use_direct_label = FALSE).\n\n\nlibrary(gghighlight)\nlibrary(gapminder)\n\ndata <- gapminder %>%\n  filter(continent %in% c(\"Europe\")) %>%\n  group_by(country) %>%\n  mutate(pcgpd_change = ifelse(year == 1952, 0, gdpPercap - lag(gdpPercap))) %>%\n  mutate(pcgpd_change = cumsum(pcgpd_change))\n  \ndata %>%\n  ggplot(aes(year, pcgpd_change, group = country, color = country)) +\n  geom_line() +\n  gghighlight(max(pcgpd_change) > 35000, use_direct_label = FALSE) +  \n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)),\n                     breaks = c(seq(1950, 2010, 10)),\n                     limits = c(1950, 2010)) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)),\n                     breaks = 0:8 * 5000,\n                     labels = scales::dollar,\n                     limits = c(0, 40000)) +\n  labs(x = \"Year\",\n       y = \"Change in per-capita GDP (US dollars)\")\n\n\n\n\nRang\nDe tweede manier om te markeren is door middel van een rangorde. Hier worden de landen met de eerste hoogste waarden voor verandering in het bruto binnenlands product per hoofd van de bevolking benadrukt met gghighlight(max(pcgpd_change), max_highlight = 5, use_direct_label = FALSE).\n\n\ndata %>%\n  ggplot(aes(year, pcgpd_change, group = country, color = country)) +\n  geom_line() +\n  gghighlight(max(pcgpd_change), max_highlight = 5, use_direct_label = FALSE) +  \n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)),\n                     breaks = c(seq(1950, 2010, 10)),\n                     limits = c(1950, 2010)) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)),\n                     breaks = 0:8 * 5000,\n                     labels = scales::dollar,\n                     limits = c(0, 40000)) +\n  labs(x = \"Year\",\n       y = \"Change in per-capita GDP (US dollars)\")\n\n\n\n\nFaceteren\ngghighlight() werkt goed met ggplot2’s facetsysteem.\n\n\ndata %>%\n  ggplot(aes(year, pcgpd_change, group = country)) +\n  geom_line() +\n  gghighlight(max(pcgpd_change), max_highlight = 4, use_direct_label = FALSE) +  \n  scale_x_continuous(expand = expand_scale(mult = c(0.002, 0)),\n                     breaks = c(seq(1950, 2010, 10)),\n                     limits = c(1950, 2010)) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)),\n                     breaks = 0:8 * 5000,\n                     labels = scales::dollar,\n                     limits = c(0, 40000)) +\n  labs(x = \"Year\",\n       y = \"Change in per-capita GDP (US dollars)\") +\n  facet_wrap(~ country) +\n  theme(panel.spacing = unit(20L, \"pt\"))\n\n\n\n\nTekst en annotatie\nVerschillende functies kunnen worden gebruikt om verschillende delen van percelen te annoteren, te labelen en te markeren. geom_text() en geom_text_repel() geven beide variabelen uit dataframes weer. annotate(), die verschillende toepassingen heeft, geeft variabelen en waarden weer die zijn opgenomen in de functie-aanroep.\ngeom_text()\ngeom_text() zet tekstvariabelen in datasets om in geometrische objecten. Dit is nuttig voor het labelen van gegevens in plots. Beide functies hebben x waarden en y waarden nodig om de plaatsing op het coördinatenvlak te bepalen en een tekstvector van labels.\nDit kan gebruikt worden voor het labelen van geom_bar().\n\n\ndiamonds %>%\n  group_by(cut) %>%\n  summarize(price = mean(price)) %>%\n  ggplot(aes(cut, price)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = scales::dollar(price)), vjust = -1) +\n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.2)),\n                     labels = scales::dollar) +\n  labs(title = \"Average Diamond Price by Diamond Cut\",\n       x = \"Cut\",\n       y = \"Price\") +\n  remove_ticks()\n\n\n\n\nHet kan ook gebruikt worden om punten in een scatterplot te labelen.\nHet is zelden nuttig om elk punt in een scatter plot te labelen. Gebruik filter() om een tweede dataset te maken die wordt onderverdeeld en deze door te geven aan de labelfunctie.\n\n\nlabels <- mtcars %>%\n  rownames_to_column(\"model\") %>%\n  filter(model %in% c(\"Toyota Corolla\", \"Merc 240D\", \"Datsun 710\"))\n\nmtcars %>%\n  ggplot() +\n  geom_point(mapping = aes(x = wt, y = mpg)) +\n  geom_text(data = labels, mapping = aes(x = wt, y = mpg, label = model), nudge_x = 0.38) +\n  scale_x_continuous(expand = expand_scale(mult = c(0, 0.002)),\n                     limits = c(0, 6)) + \n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)),\n                     limits = c(0, 40)) +  \n  labs(x = \"Weight (Tons)\",\n       y = \"Miles per gallon (MPG)\") +\n  scatter_grid()\n\n\n\n\nTekst overlapt te vaak met andere tekst of geomen bij gebruik van geom_text(). library(ggrepel) is een library(ggplot2) add-on die automatisch tekst positioneert zodat deze niet overlapt met geomen of andere tekst. Om deze functionaliteit toe te voegen, installeer en laad je library(ggrepel) en gebruikt je vervolgens geom_text_repel() met dezelfde syntaxis als geom_text().\ngeom_text_repel()\n\n\nlibrary(ggrepel)\n\nlabels <- mtcars %>%\n  rownames_to_column(\"model\") %>%\n  top_n(5, mpg)\n\nmtcars %>%\n  ggplot(mapping = aes(x = wt, y = mpg)) +\n  geom_point() +\n  geom_text_repel(data = labels, \n                  mapping = aes(label = model), \n                  nudge_x = 0.38) +\n  scale_x_continuous(expand = expand_scale(mult = c(0, 0.002)),\n                     limits = c(0, 6)) + \n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)),\n                     limits = c(0, 40)) +  \n  labs(x = \"Weight (Tons)\",\n       y = \"Miles per gallon (MPG)\") +\n  scatter_grid()\n\n\n\n\nannotate()\nannotate() gebruikt geen dataframes. In plaats daarvan zijn er waarden nodig voor x = en y =. Het kan tekst, rechthoeken, segmenten en puntenreeksen toevoegen.\n\n\nmsleep %>%\n  filter(bodywt <= 1000) %>%\n  ggplot(aes(bodywt, sleep_total)) +\n  geom_point() +\n  scale_x_continuous(expand = expand_scale(mult = c(0, 0.002)),\n                     limits = c(-10, 1000),\n                     labels = scales::comma) + \n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)),\n                     limits = c(0, 25)) +  \n  annotate(\"text\", x = 500, y = 12, label = \"These data suggest that heavy \\n animals sleep less than light animals\") +\n  labs(x = \"Body weight (pounds)\",\n       y = \"Sleep time (hours)\") +\n  scatter_grid()  \n\n\n\n\n\n\nlibrary(AmesHousing)\n\names <- make_ames()\n\names %>%\n  mutate(square_footage = Total_Bsmt_SF - Bsmt_Unf_SF + First_Flr_SF + Second_Flr_SF) %>%\n  mutate(Sale_Price = Sale_Price / 1000) %>%  \n  ggplot(aes(square_footage, Sale_Price)) +\n  geom_point(alpha = 0.2) +\n  scale_x_continuous(expand = expand_scale(mult = c(0, 0.002)),\n                     limits = c(-10, 12000),\n                     labels = scales::comma) + \n  scale_y_continuous(expand = expand_scale(mult = c(0, 0.002)),\n                     limits = c(0, 800),\n                     labels = scales::dollar) +  \n  annotate(\"rect\", xmin = 6800, xmax = 11500, ymin = 145, ymax = 210, alpha = 0.1) +\n  annotate(\"text\", x = 8750, y = 230, label = \"Unfinished homes\") +\n  labs(x = \"Square footage\", \n       y = \"Sale price (thousands)\") +\n  scatter_grid()   \n\n\n\n\nGelaagde geoms\nGeomen kunnen worden gelaagd in ggplot2. Dit is nuttig voor het ontwerp en de analyse.\nHet is vaak nuttig om punten toe te voegen aan lijngrafieken met een klein aantal waarden over de x-as. Dit voorbeeld uit R voor Data Science laat zien hoe het veranderen van de lijn naar grijs aantrekkelijk kan zijn.\nDesign\nVoor\n\n\ntable1 %>%\n  ggplot(aes(x = year, y = cases)) +\n    geom_line(aes(color = country)) +\n    geom_point(aes(color = country)) +\n    scale_y_continuous(expand = expand_scale(mult = c(0, 0.2)), \n                       labels = scales::comma) +\n    scale_x_continuous(breaks = c(1999, 2000)) +\n    labs(title = \"Changes in Tuberculosis Cases in Three Countries\")\n\n\n\n\nNa\n\n\ntable1 %>%\n  ggplot(aes(year, cases)) +\n    geom_line(aes(group = country), color = \"grey50\") +\n    geom_point(aes(color = country)) +\n    scale_y_continuous(expand = expand_scale(mult = c(0, 0.2)), \n                       labels = scales::comma) +\n    scale_x_continuous(breaks = c(1999, 2000)) +\n    labs(title = \"Changes in Tuberculosis Cases in Three Countries\")\n\n\n\n\nGelaagde geomen zijn ook nuttig voor het toevoegen van trendlijnen en centroïden aan scatterplots.\n\n\n# Simpele lijn\n# Regressie model\n# Centroiden\n\n\n\nCentroiden\n\n\nmpg_summary <- mpg %>%\n  group_by(cyl) %>%\n  summarize(displ = mean(displ), cty = mean(cty))\n\nmpg %>%\n  ggplot() +\n  geom_point(aes(x = displ, y = cty, color = factor(cyl)), alpha = 0.5) +\n  geom_point(data = mpg_summary, aes(x = displ, y = cty), size = 5, color = \"#ec008b\") +\n  geom_text(data = mpg_summary, aes(x = displ, y = cty, label = cyl)) +\n  scale_x_continuous(expand = expand_scale(mult = c(0, 0.002)), \n                     limits = c(0, 8)) +  \n  scale_y_continuous(expand = expand_scale(mult = c(0, 0)), \n                     limits = c(0, 40)) +\n  labs(x = \"Displacement\",\n       y = \"City MPG\") +\n  scatter_grid()\n\n\n\n\nGrafieken opslaan\nggsave() exporteert ggplot2 percelen. De functie kan op twee manieren worden gebruikt. Als plot = niet is gespecificeerd in de functie-aanroep, dan slaat ggsave() automatisch de plot op die het laatst werd weergegeven in het Viewer-venster. Ten tweede, als plot = is gespecificeerd, dan slaat ggsave() het gespecificeerde plot op. ggsave() raadt het type grafische soort dat gebruikt moet worden bij het exporteren (.png, .pdf, .svg, etc.) van de bestandsextensie in de bestandsnaam.\nmtcars %>%\n  ggplot(aes(x = wt, y = mpg)) +\n  geom_point()\n\nggsave(filename = \"cars.png\")\n\nplot2 <- mtcars %>%\n  ggplot(aes(x = wt, y = mpg)) +\n  geom_point()\n\nggsave(filename = \"cars.png\", plot = plot2)\nGeëxporteerde plots zien er zelden identiek uit als de plots die in het Viewervenster in RStudio verschijnen omdat de totale grootte en de beeldverhouding van de Viewer vaak anders is dan de standaardinstellingen voor ggsave(). Specifieke afmetingen, beeldverhoudingen en resoluties kunnen worden gecontroleerd met argumenten in ggsave(). RStudio heeft een nuttig cheatsheet genaamd “How Big is Your Graph?” dat zou moeten helpen bij het kiezen van de beste grootte, beeldverhouding en resolutie.\nLettertypen zijn niet standaard in PDF’s opgenomen. Om lettertypes in te sluiten in PDF’s, neem device = cairo_pdf op in ggsave().\nplot <- mtcars %>%\n  ggplot(aes(x = wt, y = mpg)) +\n  geom_point()\n\nggsave(filename = \"cars.pdf\", plot = plot2, width = 6.5, height = 4, device = cairo_pdf)\nBibliography and Session Information\nNote: Examples present in this document by Aaron Williams were created during personal time.\nBob Rudis and Dave Gandy (2017). waffle: Create Waffle Chart Visualizations in R. R package version 0.7.0. https://CRAN.R-project.org/package=waffle\nChester Ismay and Jennifer Chunn (2017). fivethirtyeight: Data and Code Behind the Stories and Interactives at ‘FiveThirtyEight’. R package version 0.3.0. https://CRAN.R-project.org/package=fivethirtyeight\nHadley Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2009.\nHadley Wickham (2017). tidyverse: Easily Install and Load the ‘Tidyverse’. R package version 1.2.1. https://CRAN.R-project.org/package=tidyverse\nHadley Wickham (2017). forcats: Tools for Working with Categorical Variables (Factors). R package version 0.2.0. https://CRAN.R-project.org/package=forcats\nJennifer Bryan (2017). gapminder: Data from Gapminder. R package version 0.3.0. https://CRAN.R-project.org/package=gapminder\nKamil Slowikowski (2017). ggrepel: Repulsive Text and Label Geoms for ‘ggplot2’. R package version 0.7.0. https://CRAN.R-project.org/package=ggrepel\nMax Kuhn (2017). AmesHousing: The Ames Iowa Housing Data. R package version 0.0.3. https://CRAN.R-project.org/package=AmesHousing\nPeter Kampstra (2008). Beanplot: A Boxplot Alternative for Visual Comparison of Distributions, Journal of Statistical Software, 2008. https://www.jstatsoft.org/article/view/v028c01\nR Core Team (2017). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.\nWinston Chang, (2014). extrafont: Tools for using fonts. R package version 0.17. https://CRAN.R-project.org/package=extrafont\nYihui Xie (2018). knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version 1.19.\n\n\nsessionInfo()\n\n\nR version 4.0.3 (2020-10-10)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS High Sierra 10.13.6\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRblas.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.0/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n [1] AmesHousing_0.0.4     gghighlight_0.3.0     fivethirtyeight_0.6.1\n [4] waffle_0.7.0          ggridges_0.5.2        gapminder_0.3.0      \n [7] urbnmapr_0.0.0.9002   extrafont_0.17        ggrepel_0.8.2        \n[10] urbnthemes_0.0.1      forcats_0.5.0         stringr_1.4.0        \n[13] dplyr_1.0.2           purrr_0.3.4           readr_1.4.0          \n[16] tidyr_1.1.2           tibble_3.0.4          ggplot2_3.3.2        \n[19] tidyverse_1.3.0       knitr_1.30           \n\nloaded via a namespace (and not attached):\n [1] httr_1.4.2         jsonlite_1.7.1     splines_4.0.3     \n [4] modelr_0.1.8       assertthat_0.2.1   blob_1.2.1        \n [7] cellranger_1.1.0   yaml_2.2.1         Rttf2pt1_1.3.8    \n[10] pillar_1.4.6       backports_1.1.10   lattice_0.20-41   \n[13] glue_1.4.2         extrafontdb_1.0    digest_0.6.27     \n[16] RColorBrewer_1.1-2 rvest_0.3.6        colorspace_1.4-1  \n[19] Matrix_1.2-18      htmltools_0.5.0    plyr_1.8.6        \n[22] pkgconfig_2.0.3    broom_0.7.2        haven_2.3.1       \n[25] scales_1.1.1       distill_1.0        downlit_0.2.0     \n[28] mgcv_1.8-33        generics_0.0.2     farver_2.0.3      \n[31] ellipsis_0.3.1     withr_2.3.0        hexbin_1.28.1     \n[34] cli_2.1.0          magrittr_1.5       crayon_1.3.4      \n[37] readxl_1.3.1       evaluate_0.14      fs_1.5.0          \n[40] fansi_0.4.1        nlme_3.1-149       xml2_1.3.2        \n[43] tools_4.0.3        hms_0.5.3          lifecycle_0.2.0   \n[46] munsell_0.5.0      reprex_0.3.0       compiler_4.0.3    \n[49] rlang_0.4.8        grid_4.0.3         rstudioapi_0.11   \n[52] labeling_0.4.2     rmarkdown_2.5      gtable_0.3.0      \n[55] DBI_1.1.0          R6_2.4.1           gridExtra_2.3     \n[58] lubridate_1.7.9    stringi_1.5.3      Rcpp_1.0.5        \n[61] vctrs_0.3.4        dbplyr_1.4.4       tidyselect_1.1.0  \n[64] xfun_0.18         \n\n\n\n\n",
    "preview": "posts/2020-05-31-grafieken/grafieken_files/figure-html5/staafgrafiek-1.png",
    "last_modified": "2020-11-02T20:56:55+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-05-31-dslabs/",
    "title": "dslabs",
    "description": "Een aantal mooie grafieken uit het goede data-analyseboek van Rafa Irizarri (Harvard University)",
    "author": [
      {
        "name": "Rafa Irizarri en Amy Hill, bewerking Harrie Jonkman",
        "url": "www.harriejonkman.nl"
      }
    ],
    "date": "2020-04-22",
    "categories": [],
    "contents": "\ndslabs-pakket om van de werkelijkheid te leren\nRafael Irizarri schreef een prachtig boek over moderne data analyse Introduction to Data Science. Data Analysis and Prediction Algorithms with R dat je gratis in boekdown formaat kunt inzien hier. Daar heb ik het al eens over gehad en daar zal ik vast op een later tijdstip nog wel weer eens op terugkomen. Dat boek komt voort uit diverse colleges die hij over dit onderwerp op de Harvard University heeft gegeven. Irizarri wil je van echte data laten leren en leert je zo hele verschillende technieken. Voor het gebruik van die data heeft hij een speciaal pakket gemaakt dat dslabs heet. Daarover heeft hijzelf eerder en Amy Hill later een post geschreven op Simply Statistics. Ik heb deze posts bewerkt om te laten zien hoe levensechte datasets eruit zien en hoe je deze data aantrekkelijk zichtbaar kunt maken. Misschien vooral ook wel om op zijn goede boek te wijzen.\nEen veelbesproken onderwerp in het statistiekonderwijs is dat informatica-computatie een prominentere rol zou moeten spelen in het curriculum. Izigarri (en in alle bescheidenheid ik ook) is het daar volledig mee eens, maar hij denkt dat de belangrijkste verbetering zal komen van het op de voorgrond brengen van toepassingen en het zo goed mogelijk nabootsen van de uitdagingen waarmee de toegepaste statistici in het echte leven worden geconfronteerd. Izigarri probeert daarom het gebruik van veelgebruikte voorbeelden, zoals de -mtcarsdataset die in R zo vaak worden gebruikt, te vermijden wanneer hij les geeft in datawetenschap. Volgens hem is het niet zo eenvoudig om voorbeelden te vinden die zowel realistisch en interessant zijn als geschikt voor beginners. Na een paar jaar lesgeven heeft hij een aantal datasets verzameld die volgens hem wel aan deze criteria voldoen. Om het gebruik ervan in introductielessen te vergemakkelijken, heeft hij ze in het dslabs-pakket opgenomen. Dat pakket heb ikzelf al geinstalleerd (en daarom staat er een # voor). Als jij het wilt gebruiken, moet je het #-teken weghalen.\n\n\n# install.packages(\"dslabs\")\n\nHieronder laat hij wat voorbeelden zien. Je kunt in ieder geval zien welke datasets in het pakket zitten:\n\n\nlibrary(\"dslabs\")\ndata(package=\"dslabs\")\n\nMerk op dat het pakket ook enkele van de scripts bevat die worden gebruikt om de gegevens uit hun oorspronkelijke bron te halen:\n\n\nlist.files(system.file(\"script\", package = \"dslabs\"))\n\n [1] \"make-admissions.R\"                   \n [2] \"make-brca.R\"                         \n [3] \"make-brexit_polls.R\"                 \n [4] \"make-death_prob.R\"                   \n [5] \"make-divorce_margarine.R\"            \n [6] \"make-gapminder-rdas.R\"               \n [7] \"make-greenhouse_gases.R\"             \n [8] \"make-historic_co2.R\"                 \n [9] \"make-mnist_27.R\"                     \n[10] \"make-movielens.R\"                    \n[11] \"make-murders-rda.R\"                  \n[12] \"make-na_example-rda.R\"               \n[13] \"make-nyc_regents_scores.R\"           \n[14] \"make-olive.R\"                        \n[15] \"make-outlier_example.R\"              \n[16] \"make-polls_2008.R\"                   \n[17] \"make-polls_us_election_2016.R\"       \n[18] \"make-reported_heights-rda.R\"         \n[19] \"make-research_funding_rates.R\"       \n[20] \"make-stars.R\"                        \n[21] \"make-temp_carbon.R\"                  \n[22] \"make-tissue-gene-expression.R\"       \n[23] \"make-trump_tweets.R\"                 \n[24] \"make-weekly_us_contagious_diseases.R\"\n[25] \"save-gapminder-example-csv.R\"        \n\nIn het boek Introduction to Data Science kun je zien hoe de datasets worden gebruikt. Hier volgt een kort inkijkje op het geheel.\nUS murders\nDeze dataset bevat gegevens over moorden met wapens in de Verenigde Staten in 2012. Hij gebruikt deze dataset om de basis van het R-programma te introduceren.\n\n\ndata(\"murders\")\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(ggrepel)\n\nr <- murders %>%\n  summarize(pop=sum(population), tot=sum(total)) %>%\n  mutate(rate = tot/pop*10^6) %>% .$rate\n\nds_theme_set()\nmurders %>% ggplot(aes(x = population/10^6, y = total, label = abb)) +\n  geom_abline(intercept = log10(r), lty=2, col=\"darkgrey\") +\n  geom_point(aes(color=region), size = 3) +\n  geom_text_repel() +\n  scale_x_log10() +\n  scale_y_log10() +\n  xlab(\"Populations in millions (log scale)\") +\n  ylab(\"Total number of murders (log scale)\") +\n  ggtitle(\"US Gun Murders in 2010\") +\n  scale_color_discrete(name=\"Region\") \n\n\nGapminder\nOver deze dataset heb ikzelf ook vaker geschreven. Deze dataset omvat de gezondheids- en inkomensresultaten van 184 landen van 1960 tot 2016. Het bevat ook twee karaktervectoren, de OESO en de OPEC, met de namen van de OESO- en OPEC-landen vanaf 2016. Hij gebruikt deze dataset om data visualisatie en ggplot2 te onderwijzen.\n\n\ndata(\"gapminder\")\n\nwest <- c(\"Western Europe\",\"Northern Europe\",\"Southern Europe\",\n          \"Northern America\",\"Australia and New Zealand\")\n\ngapminder <- gapminder %>%\n  mutate(group = case_when(\n    region %in% west ~ \"The West\",\n    region %in% c(\"Eastern Asia\", \"South-Eastern Asia\") ~ \"East Asia\",\n    region %in% c(\"Caribbean\", \"Central America\", \"South America\") ~ \"Latin America\",\n    continent == \"Africa\" & region != \"Northern Africa\" ~ \"Sub-Saharan Africa\",\n    TRUE ~ \"Others\"))\ngapminder <- gapminder %>%\n  mutate(group = factor(group, levels = rev(c(\"Others\", \"Latin America\", \"East Asia\",\"Sub-Saharan Africa\", \"The West\"))))\n\nfilter(gapminder, year%in%c(1962, 2013) & !is.na(group) &\n         !is.na(fertility) & !is.na(life_expectancy)) %>%\n  mutate(population_in_millions = population/10^6) %>%\n  ggplot( aes(fertility, y=life_expectancy, col = group, size = population_in_millions)) +\n  geom_point(alpha = 0.8) +\n  guides(size=FALSE) +\n  theme(plot.title = element_blank(), legend.title = element_blank()) +\n  coord_cartesian(ylim = c(30, 85)) +\n  xlab(\"Fertility rate (births per woman)\") +\n  ylab(\"Life Expectancy\") +\n  geom_text(aes(x=7, y=82, label=year), cex=12, color=\"grey\") +\n  facet_grid(. ~ year) +\n  theme(strip.background = element_blank(),\n        strip.text.x = element_blank(),\n        strip.text.y = element_blank(),\n   legend.position = \"top\")\n\n\nGegevens over besmettelijke ziekten in de Verenigde Staten\nDeze dataset bevat jaarlijkse tellingen voor Hepatitis A, mazelen, bof, pertussis, polio, rodehond en pokken voor de Amerikaanse staten. Originele gegevens met dank aan Tycho Project. Hij gebruikt het om te laten zien hoe men meer dan 2 dimensies kan plotten.\n\n\nlibrary(RColorBrewer)\ndata(\"us_contagious_diseases\")\nthe_disease <- \"Measles\"\nus_contagious_diseases %>%\n  filter(!state%in%c(\"Hawaii\",\"Alaska\") & disease ==  the_disease) %>%\n  mutate(rate = count / population * 10000 * 52 / weeks_reporting) %>%\n  mutate(state = reorder(state, rate)) %>%\n  ggplot(aes(year, state,  fill = rate)) +\n  geom_tile(color = \"grey50\") +\n  scale_x_continuous(expand=c(0,0)) +\n  scale_fill_gradientn(colors = brewer.pal(9, \"Reds\"), trans = \"sqrt\") +\n  geom_vline(xintercept=1963, col = \"blue\") +\n  theme_minimal() +  theme(panel.grid = element_blank()) +\n  ggtitle(the_disease) +\n  ylab(\"\") +\n  xlab(\"\")\n\n\nFivethirtyeight Data van de verkiezingen van 2016\nDeze gegevens omvatten de resultaten van de Amerikaanse presidentsverkiezingen van 2016, geaggregeerd door HuffPost Pollster, RealClearPolitics, stembureaus en nieuwsberichten. De dataset bevat ook de verkiezingsresultaten (volksstemming) en de stemmen van de kiescolleges in results_us_election_2016. Hij gebruikt deze dataset om les te geven over inferenties.\n\n\ndata(polls_us_election_2016)\npolls_us_election_2016 %>%\n  filter(state == \"U.S.\" & enddate>=\"2016-07-01\") %>%\n  select(enddate, pollster, rawpoll_clinton, rawpoll_trump) %>%\n  rename(Clinton = rawpoll_clinton, Trump = rawpoll_trump) %>%\n  gather(candidate, percentage, -enddate, -pollster) %>% \n  mutate(candidate = factor(candidate, levels = c(\"Trump\",\"Clinton\")))%>%\n  group_by(pollster) %>%\n  filter(n()>=10) %>%\n  ungroup() %>%\n  ggplot(aes(enddate, percentage, color = candidate)) +  \n  geom_point(show.legend = FALSE, alpha=0.4)  + \n  geom_smooth(method = \"loess\", span = 0.15) +\n  scale_y_continuous(limits = c(30,50))\n\n\nStudenten rapporteren lengte\nDit zijn zelfgerapporteerde lengtes in inches voor mannen en vrouwen die de afgelopen jaren aan de cursus dataanalyse hebben meegedaan. Hij gebruikt ze voor het onderwijzen van distributies en samenvattende statistieken.\n\n\ndata(\"heights\")\nheights %>% \n  ggplot(aes(height, fill=sex)) + \n  geom_density(alpha = 0.2)\n\n\nDeze data zijn behoorlijk aangepast omdat studenten lengte vaak in andere waarden dan inches rapporteren. De originele vormen staan hier:\n\n\ndata(\"reported_heights\")\nreported_heights %>% filter(is.na(as.numeric(height))) %>% select(height) %>% .$height\n\n [1] \"5' 4\\\"\"                 \"165cm\"                 \n [3] \"5'7\"                    \">9000\"                 \n [5] \"5'7\\\"\"                  \"5'3\\\"\"                 \n [7] \"5 feet and 8.11 inches\" \"5'11\"                  \n [9] \"5'9''\"                  \"5'10''\"                \n[11] \"5,3\"                    \"6'\"                    \n[13] \"6,8\"                    \"5' 10\"                 \n[15] \"Five foot eight inches\" \"5'5\\\"\"                 \n[17] \"5'2\\\"\"                  \"5,4\"                   \n[19] \"5'3\"                    \"5'10''\"                \n[21] \"5'3''\"                  \"5'7''\"                 \n[23] \"5'12\"                   \"2'33\"                  \n[25] \"5'11\"                   \"5'3\\\"\"                 \n[27] \"5,8\"                    \"5'6''\"                 \n[29] \"5'4\"                    \"1,70\"                  \n[31] \"5'7.5''\"                \"5'7.5''\"               \n[33] \"5'2\\\"\"                  \"5' 7.78\\\"\"             \n[35] \"yyy\"                    \"5'5\"                   \n[37] \"5'8\"                    \"5'6\"                   \n[39] \"5 feet 7inches\"         \"6*12\"                  \n[41] \"5 .11\"                  \"5 11\"                  \n[43] \"5'4\"                    \"5'8\\\"\"                 \n[45] \"5'5\"                    \"5'7\"                   \n[47] \"5'6\"                    \"5'11\\\"\"                \n[49] \"5'7\\\"\"                  \"5'7\"                   \n[51] \"5'8\"                    \"5' 11\\\"\"               \n[53] \"6'1\\\"\"                  \"69\\\"\"                  \n[55] \"5' 7\\\"\"                 \"5'10''\"                \n[57] \"5'10\"                   \"5'10\"                  \n[59] \"5ft 9 inches\"           \"5 ft 9 inches\"         \n[61] \"5'2\"                    \"5'11\"                  \n[63] \"5'11''\"                 \"5'8\\\"\"                 \n[65] \"708,661\"                \"5 feet 6 inches\"       \n[67] \"5'10''\"                 \"5'8\"                   \n[69] \"6'3\\\"\"                  \"649,606\"               \n[71] \"728,346\"                \"6 04\"                  \n[73] \"5'9\"                    \"5'5''\"                 \n[75] \"5'7\\\"\"                  \"6'4\\\"\"                 \n[77] \"5'4\"                    \"170 cm\"                \n[79] \"7,283,465\"              \"5'6\"                   \n[81] \"5'6\"                   \n\nZe gebruiken het vaak om het string proces en regex uit te leggen.\nMargarine en het niveau van scheiden\nTot slot is hier een gek voorbeeld van de website Spurious Correlations dat hij gebruikt als hij wil uitleggen dat correlatie niet te verwarren is met oorzaak.\n\n\nthe_title <- paste(\"Correlation =\",\n                round(with(divorce_margarine,\n                           cor(margarine_consumption_per_capita, divorce_rate_maine)),2))\ndata(divorce_margarine)\ndivorce_margarine %>%\n  ggplot(aes(margarine_consumption_per_capita, divorce_rate_maine)) +\n  geom_point(cex=3) +\n  geom_smooth(method = \"lm\") +\n  ggtitle(the_title) +\n  xlab(\"Margarine Consumption per Capita (lbs)\") +\n  ylab(\"Divorce rate in Maine (per 1000)\")\n\n\nUitbreiding in 2019\nZe hebben het dslabs-pakket, dat ze eerder introduceerden als een pakket met realistische, interessante en toegankelijke datasets die gebruikt kunnen worden in inleidende datawetenschappelijke cursussen, in 2019 uitgebreid. Deze nieuwe uitgave heeft nog eens zeven nieuwe datasets toegevoegd, met data over klimaatverandering, astronomie, levensverwachting en borstkankerdiagnose. Ze worden gebruikt in verbeterde probleemsets en nieuwe projecten binnen het HarvardX Data Science Professional Certificate Program, dat beginners R-programmering aanleert, maar ook laat werken met datavisualisatie, dataverwerking, statistiek en machine learning zonder dat ze een coderings- of programmeringsachtergrond hebben.\nHet dslabs-pakket is al geinstalleerd. Om verder te gaan is het ook nodig om de volgende pakketten en opties te installeren.\n\n\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif(!require(\"ggrepel\")) install.packages(\"ggrepel\")\nif(!require(\"matrixStats\")) install.packages(\"matrixStats\")\n\nEn daarna actief te maken:\n\n\n# load libraries\nlibrary(tidyverse)\nlibrary(ggrepel)\nlibrary(matrixStats)\n\nHaal ook een kleurenpakket binnen als je wilt:\n\n\n# set colorblind-friendly color palette\ncolorblind_palette <- c(\"black\", \"#E69F00\", \"#56B4E9\", \"#009E73\",\n                        \"#CC79A7\", \"#F0E442\", \"#0072B2\", \"#D55E00\")\n\nKlimaatverandering\nDrie datasets met betrekking tot klimaatverandering worden gebruikt om datavisualisatie en dataverwerking te leren. Deze gegevens leveren duidelijke plots op die een toename van de temperatuur, het broeikasgasniveau en de koolstofuitstoot van 800.000 jaar geleden tot de moderne tijd laten zien. Studenten kunnen hun eigen impactvolle visualisaties maken met echte atmosferische en ijskernmetingen.\nModerne temperatuur anomalie en koolstofdioxide data: temp_carbon\nDe temp_carbon dataset bevat jaarlijkse globale temperatuurafwijkingen in graden Celsius ten opzichte van de 20e eeuwse gemiddelde temperatuur van 1880-2018. De temperatuurafwijkingen boven land en boven de oceaan worden ook gerapporteerd. Daarnaast zijn de jaarlijkse koolstofemissies (in miljoenen tonnen) van 1751-2014 opgenomen. De temperatuurafwijkingen zijn afkomstig van NOAA en de koolstofuitstoot van Boden et al., 2017 via CDIAC.\n\n\ndata(temp_carbon)\n\n\n\n# line plot of annual global, land and ocean temperature anomalies since 1880\ntemp_carbon %>%\n    select(Year = year, Global = temp_anomaly, Land = land_anomaly, Ocean = ocean_anomaly) %>%\n    gather(Region, Temp_anomaly, Global:Ocean) %>%\n    ggplot(aes(Year, Temp_anomaly, col = Region)) +\n    geom_line(size = 1) +\n    geom_hline(aes(yintercept = 0), col = colorblind_palette[8], lty = 2) +\n    geom_label(aes(x = 2005, y = -.08), col = colorblind_palette[8], \n               label = \"20th century mean\", size = 4) +\n    ylab(\"Temperature anomaly (degrees C)\") +\n    xlim(c(1880, 2018)) +\n    scale_color_manual(values = colorblind_palette) +\n    ggtitle(\"Temperature anomaly relative to 20th century mean, 1880-2018\")\n\n\nBroeikasgasconcentraties over 2000 jaar: broeikasgassen\nHet gegevensframe voor broeikasgassen bevat vanaf 0-2000 CE elke 20 jaar kooldioxide (CO2, ppm), methaan (CO2, ppb) en lachgas (N2O, ppb) concentraties. De gegevens zijn een subset van ijskernmetingen van MacFarling Meure et al., 2006 via NOAA. Er is een duidelijke toename van alle 3 de gassen vanaf het Industriële Revolutietijdperk.\n\n\ndata(greenhouse_gases)\n\n# line plots of atmospheric concentrations of the three major greenhouse gases since 0 CE\ngreenhouse_gases %>%\n    ggplot(aes(year, concentration)) +\n    geom_line() +\n    facet_grid(gas ~ ., scales = \"free\") +\n    xlab(\"Year\") +\n    ylab(\"Concentration (CH4/N2O ppb, CO2 ppm)\") +\n    ggtitle(\"Atmospheric greenhouse gas concentration by year, 0-2000 CE\")\n\n\nVergelijk dit patroon met de door de mens veroorzaakte koolstofuitstoot sinds 1751 uit temp_carbon, die op vergelijkbare wijze is gestegen:\n\n\n# line plot of anthropogenic carbon emissions over 250+ years\ntemp_carbon %>%\n    ggplot(aes(year, carbon_emissions)) +\n    geom_line() +\n    xlab(\"Year\") +\n    ylab(\"Carbon emissions (metric tons)\") +\n    ggtitle(\"Annual global carbon emissions, 1751-2014\")\n\n\nCarbon dioxide niveaus over de laatste 800,000 jaren, historic_co2\nEen veelvoorkomend argument tegen het bestaan van antropogene klimaatveranderingen is dat de aarde van nature cycli van opwarming en afkoeling ondergaat die worden beheerst door natuurlijke veranderingen die buiten de macht van de mens liggen. CO2-niveaus van ijskernen en moderne atmosferische metingen in het Mauna Loa-observatorium tonen aan dat de snelheid en de omvang van natuurlijke variaties in broeikasgassen verbleken in vergelijking met de snelle veranderingen in de moderne industriële tijd. Terwijl de planeet in het verre verleden warmer was en hogere CO2-niveaus had (gegevens niet getoond), laat de huidige ongekende snelheid van verandering weinig tijd voor planetaire systemen om zich aan te passen.\n\n\ndata(historic_co2)\n\n# line plot of atmospheric CO2 concentration over 800K years, colored by data source\nhistoric_co2 %>%\n    ggplot(aes(year, co2, col = source)) +\n    geom_line() +\n    ylab(\"CO2 (ppm)\") +\n    scale_color_manual(values = colorblind_palette[7:8]) +\n    ggtitle(\"Atmospheric CO2 concentration, -800,000 BCE to today\")\n\n\nEigenschappen van sterren voor het maken van een H-R-diagram: sterren\nIn de sterrenkunde worden sterren ingedeeld naar verschillende belangrijke kenmerken, waaronder temperatuur, spectrale klasse (kleur) en lichtkracht (helderheid). Een gemeenschappelijke plot voor het demonstreren van de verschillende groepen sterren en hun interpretaties is het Hertzsprung-Russell-diagram, of H-R-diagram. Het gegevensframe van de sterren verzamelt informatie voor het maken van een H-R-diagram met ongeveer 100 genoemde sterren, inclusief hun temperatuur, spectrale klasse en magnitude (die omgekeerd evenredig is met de lichtkracht).\nHet H-R-diagram heeft de heetste, helderste sterren linksboven en de koudste, zwakste sterren rechtsonder. Hoofdreekssterren staan langs de hoofddiagonaal, terwijl reuzen rechtsboven staan en dwergen linksonder. Met deze gegevens kunnen verschillende aspecten van de datavisualisatie geoefend worden.\n\n\ndata(stars)\n\n# H-R diagram color-coded by spectral class\nstars %>%\n    mutate(type = factor(type, levels = c(\"O\", \"B\", \"DB\", \"A\", \"DA\", \"DF\", \"F\", \"G\", \"K\", \"M\")),\n           star = ifelse(star %in% c(\"Sun\", \"Polaris\", \"Betelgeuse\", \"Deneb\",\n                                     \"Regulus\", \"*SiriusB\", \"Alnitak\", \"*ProximaCentauri\"),\n                         as.character(star), NA)) %>%\n    ggplot(aes(log10(temp), magnitude, col = type)) +\n    geom_point() +\n    geom_label_repel(aes(label = star)) +\n    scale_x_reverse() +\n    scale_y_reverse() +\n    xlab(\"Temperature (log10 degrees K)\") +\n    ylab(\"Magnitude\") +\n    labs(color = \"Spectral class\") +\n    ggtitle(\"H-R diagram of selected stars\")\n\n\nLevenstabellen van de Verenigde Staten: death_prob\nDe levenstabel voor de periode 2015, die is verkregen van het Amerikaanse Ministerie Sociale zekerheid, vermeldt de kans op overlijden binnen een jaar op elke leeftijd en voor beide geslachten. Deze waarden worden vaak gebruikt om levensverzekeringspremies te berekenen. Ze kunnen worden gebruikt voor oefeningen over waarschijnlijkheid en willekeurige variabelen. De premies kunnen bijvoorbeeld worden berekend met een soortgelijke benadering als die welke wordt gebruikt voor de rentevoeten in de casestudie over The Big Short in Rafael Irizarry’s Introduction to Data Science-boek.\nBrexit stemdata: brexit_polls\nbrexit_polls bevat stempercentages en verdelingen van de zes maanden voorafgaand aan het Brexit EU-lidmaatschapsreferendum in 2016 samengesteld uit Wikipedia. Deze kunnen worden gebruikt om een verscheidenheid aan inferentie- en modelleringsconcepten te oefenen, waaronder betrouwbaarheidsintervallen, p-waarden, hiërarchische modellen en voorspellingen.\n\n\ndata(brexit_polls)\n\n# plot of Brexit referendum polling spread between \"Remain\" and \"Leave\" over time\nbrexit_polls %>%\n    ggplot(aes(enddate, spread, color = poll_type)) +\n    geom_hline(aes(yintercept = -.038, color = \"Actual spread\")) +\n    geom_smooth(method = \"loess\", span = 0.4) +\n    geom_point() +\n    scale_color_manual(values = colorblind_palette[1:3]) +\n    xlab(\"Poll end date (2016)\") +\n    ylab(\"Spread (Proportion Remain - Proportion Leave)\") +\n    labs(color = \"Poll type\") +\n    ggtitle(\"Spread of Brexit referendum online and telephone polls\")\n\n\n# Borstkanker diagnose voorspelling: brca\nDit is de Breast Cancer Wisconsin (Diagnostic) Dataset, een klassieke dataset voor machine learning die classificatie mogelijk maakt van borstlaesie biopsies als kwaadaardig of goedaardig op basis van celkernkenmerken geëxtraheerd uit gedigitaliseerde beelden van fijne naald aspiratie cytologie dia’s. De gegevens zijn geschikt voor de analyse van de belangrijkste componenten en een verscheidenheid aan algoritmen voor machinaal leren. De modellen kunnen worden getraind tot een voorspellende nauwkeurigheid van meer dan 95%.\n\n\n# scale x values\nx_centered <- sweep(brca$x, 2, colMeans(brca$x))\nx_scaled <- sweep(x_centered, 2, colSds(brca$x), FUN = \"/\")\n\n# principal component analysis\npca <- prcomp(x_scaled) \n\n# scatterplot of PC2 versus PC1 with an ellipse to show the cluster regions\ndata.frame(pca$x[,1:2], type = ifelse(brca$y == \"B\", \"Benign\", \"Malignant\")) %>%\n    ggplot(aes(PC1, PC2, color = type)) +\n    geom_point() +\n    stat_ellipse() +\n    ggtitle(\"PCA separates breast biospies into benign and malignant clusters\")\n\n\nTot slot\nDe datasets in het dslabs-pkket maken data science onderwijs bruikbaarder door echte en wereldse casestudies en met motiverende voorbeelden.\nIs programmeren in R nieuw voor jou en wil je dit leren? Check dan het Data Science Professional Certificaat Programma van Harvard University, onder leiding van Rafael Irizarry!\n\n\n",
    "preview": "posts/2020-05-31-dslabs/dslabs_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2020-08-21T10:38:38+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-12-15-beste-boeken-2019/",
    "title": "Beste Boeken 2019",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "www.harriejonkman.nl"
      }
    ],
    "date": "2019-12-15",
    "categories": [],
    "contents": "\nBeste boeken van 2019\nDecember is de maand van de lijstjes. Net als vele anderen maak ik dan jaarlijstje met, in mijn geval, de beste films, de beste cd’s en de beste boeken. Hier nu ook zo’n lijstje met beste boeken van 2019, maar dan statistische boeken. Op dit terrein verschijnt bijzonder veel en ik wil niet zeggen dat ik op dit terrein alles overzie. Maar hieronder de vijf boeken die mij afgelopen jaar het meeste aanspraken en die ik er nog steeds regelmatig op na sla.\nKieran Healey, Data visualization: a practical introduction\nDit is een boek over data visualisatie en met name het R-pakket ggplot. Hier (https://kieranhealy.org/publications/dataviz/,) vind je er informatie over. Het is bijzonder duidelijk geschreven. Aan de hand van het maken van figuren, grafieken en kaarten leer je heel goed met R en RStudio om te gaan. Lees dit boek aandachtig en maak de kaarten zelf die Kieran Healey in dit boek maakt. Het boek is met plezier en gemak geschreven. Ik schreef er eerder een kort stukje over ().\nRobin Lovelace, Jakob Novosad en Jannes Muenchow, Geocomputation with R\nDat boek vind je hier (https://kieranhealy.org/publications/dataviz/). Deze drie wetenschappers hebben in twee, drie jaar dit boek geschreven over het maken van geografische kaarten. De theorie en de praktijk worden heel goed door deze drie jonge wetenschappers besproken. Op internet kun je hun syntaxen vinden en verschillende presentaties die ze hierover hebben gegeven. R is een gratis programma dat tegenwoordig de concurrentie aan kan met dure GIS-programma’s. Net als het boek van Healey is ook dit boek met plezier en gemak geschreven en het boek duwt je in de tijd vooruit. Ook over dit boek schreef ik eerder een blog ().\nRafael Irizarry, Introduction to Data Science. Data Analysis and Prediction Algorithms with R\nRafael Irizarry is epidemioloog van de Harvard University en geeft uitgebreide collegereeksen over moderne data-analyse. Als ik begin 20 zou zijn en waar ook een cursus mocht uitzoeken zou ik misschien wel zijn cursus uit hebben gekozen. Op basis van de introductiecursus gaf hij dit open-source boek uit (https://kieranhealy.org/publications/dataviz/). Hij laat zien hoe je data analyse uitvoert en wat er bij komt kijken aan voorbereiden, opschonen, visualiseren, modelleren en communiceren. Hij heeft aandacht voor standaard statistiek maar ook voor waarschijnlijkheidsleer en machinelearning. Daarnaast besteedt hij aandacht aan allerlei aanvullende zaken die hierbij om de hoek komen kijken als het gebruik van GitHUB, basis computerkennis en reproduceerbaar onderzoek. Een heel goed studieboek over moderne data-analyse in deze tijd.\nDanielle Navarro, Learning Statistics with R\nDanielle Navarro geeft al jarenlang statistiek aan studenten psychologie van de Universiteit van Adeleide (Australië). Zij heeft haar boek dat ze hierbij gebruikt ook toegankelijk gemaakt voor anderen en je vindt het hier (https://learningstatisticswithr.com/). Dat las ik tegen het einde van het jaar. Verschillende aspecten van de statistiek worden uitgelegd en als je dit boek hebt doorgewerkt, kun je goed artikelen begrijpen en ook analyses uitvoeren. Daarnaast besteedt zij aandacht aan Bayesiaanse statistiek. Deze vorm van statistiek ligt haar beter en aan het einde legt zij heel goed uit hoe dit werkt. Dat hoofdstuk heb ik hier vertaald ()\nRussel Poldrock, Statistical Thinking for the 21st Century\nDit is het boek dat ik recent las en dat mij bijzonder aanspreekt. Het is een heerlijk tegendraads boek van iemand die les geeft over statistiek en niet tevreden was met het studieboek dat hij gebruikte. Dat kon hij beter. Hij is een Bayesiaan en bouwt zijn boek vanuit dit raamwerk op. Voor hem is deze vorm van statistiek de statistiek van deze eeuw. Verschillende aspecten die je in gewone statistiek boeken tegenkomt, bespreek hij ook maar dan vanuit dat 21eeuwse perspectief. Net als de andere boeken laat hij ook zien hoe je dit het allemaal uitvoert (zowel het schrijven met wiskundige tekens, tabellen maken als de modellen maken). Bijzonder leerzaam en dit boek zal ik er nog vaak op naslaan. Dit boek vind je hier (http://statsthinking21.org/\n\n\n",
    "preview": {},
    "last_modified": "2020-03-13T23:28:58+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-12-15-r-als-een-gis/",
    "title": "R als een Gis",
    "description": "Over ruimtelijke data en het gebruik van R als een GIS",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "www.harriejonkman.nl"
      }
    ],
    "date": "2019-12-15",
    "categories": [],
    "contents": "\nIntroduction to Spatial Data & Using R as a GIS\nDit is een bewerking en verkorte versie van de tutorial die Nick Bearman eerder schreef (Introduction to Spatial Data & Using R as a GIS) en die vrij toegankelijk is hier. De data die in deze tutorial worden gebruikt zijn eigen data of open data ook om de techniek goed onder de knie te krijgen. Dank je wel Nick Bearman.\nEindtermen: R Functies & Bibliotheken:\nR gebruiken om CSV data in te lezen: read.csv()\nR gebruiken om ruimtelijke gegevens in te lezen: st_read()\nWeten hoe ruimtelijke gegevens te plotten met behulp van R: qtm() & tm_shape()\nWeten hoe je kleuren en classificaties moet aanpassen: style\nBegrijpen hoe je loops moet gebruiken om meerdere kaarten te maken: for(){}\nWeten hoe je ruimtelijke gegevens opnieuw geprojecteerd krijgt: st_transform()\nIn staat zijn om punten te gebruiken in veelhoekanalyse: poly.counts()\nWeten hoe je shapefiles moet opslaan: st_write()\nIntro op R & GIS\nR Basis\nR begon als een statistisch programma en wordt nog steeds door veel gebruikers als een programma gebruikt. We gaan een programma gebruiken dat RStudio heet, dat bovenop R werkt en een goede gebruikersinterface biedt. Ik zal het in de presentatie even hebben over RStudio, en de belangrijkste gebieden van het venster zijn op de achterzijde gemarkeerd.\nOpen RStudio (klik op Start en typ RStudio in of dubbelklik op het icoontje op het bureaublad). R kan in eerste instantie als rekenmachine worden gebruikt - voer het volgende in de linkerkant van het venster in - het gedeelte met de titel Console:\n\n\n[1] 14\n\nMaak je voorlopig geen zorgen over de [1] - let wel dat R 14 heeft afgedrukt, want dit is het antwoord op de som die je hebt ingetikt. In deze werkbladen laat ik soms de resultaten zien van wat je hebt ingetypt, zoals hieronder:\n\n\n[1] 20\n\nMerk ook op dat * hier het symbool voor vermenigvuldiging is - in het laatste commando vroeg R om de berekening 5 maal 4 uit te voeren. Andere symbolen zijn - voor aftrekken en / voor delen:\n\n\n[1] -2\n\n\n\n[1] 0.3529412\n\nJe kunt de antwoorden van de berekeningen ook toewijzen aan variabelen en gebruiken in berekeningen.\n\n\n\nHier wordt de waarde 300 opgeslagen in de variabele prijs. Het <- symbool betekent dat de waarde rechts in de variabele links in de variabele wordt gezet, deze wordt getypt met een << gevolgd door een -. De variabelen worden getoond in het venster met de naam Environment, rechtsboven in het venster. Variabelen kunnen gebruikt worden in volgende berekeningen. Om bijvoorbeeld een korting van 20% op deze prijs toe te passen, kunt je het volgende invoeren:\n#\n\n\n[1] 240\n\nof gebruik tussenvariabelen:\n\n\n[1] 240\n\nR kan ook werken met lijsten met nummers, maar ook met individuele nummers. Lijsten worden gespecificeerd met behulp van de c-functie. Stel dat je een lijst hebt met huizenprijzen in duizenden euro’s. Je zou ze kunnen opslaan in een variabele die house.prices genoemd wordt, zoals hieronder:\n\n\n[1] 120 150 212  99 199 299 159\n\nMerk op dat er geen probleem is met punten in het midden van variabelenamen. U kunt dan functies toepassen op deze lijsten.\n\n\n[1] 176.8571\n\nAls de huizenprijzen in duizenden euro’s zijn, dan zegt dit ons dat de gemiddelde huizenprijs 176.900 EURO bedraagt. Merk op dat het antwoord op jouw scherm meer cijfers kan weergegeven. Dus je kunt iets als 176.8571429 voor gemiddelde waarde hebben.\nHet Dataframe\nR heeft een manier om gegevens op te slaan in een object dat een dataframe wordt genoemd. Dit lijkt op een interne spreadsheet.\n\n\n[1] 240\n\nWaar alle relevante gegevenselementen samen als een set kolommen worden opgeslagen.\nWe hebben een CSV-bestand van huizenprijzen en inbraakcijfers, dat we in R kunnen laden. We kunnen gebruik maken van een functie genaamd read.csv die, zoals je misschien wel kunt bedenken, CSV-bestanden leest. Voer de onderstaande coderegel uit, die het CSV-bestand in een variabele met de naam hp.data laadt.\nRotterdam\n\n\n\nAls we de gegevens inlezen, is het altijd een goed idee om te controleren of ze goed zijn binnengekomen. Om dit te doen, kunnen we een voorbeeld van de dataset bekijken. Het head-commando toont de eerste 6 rijen van de data.\nHieronder lezen we dan ons databestand in.\n\n\n# A tibble: 6 x 2\n  buurtenrotterdam            NNGB\n  <chr>                      <dbl>\n1 Afrikaanderwijk             0.86\n2 Agniesebuurt               NA   \n3 Bedrijvenpark Noord_West   NA   \n4 Bedrijventerrein Schieveen NA   \n5 Bergpolder                 NA   \n6 Beverwaard                  0.78\n\nJe kunt ook op de variabele in het venster Environment klikken, die de gegevens in een nieuw tabblad zal tonen. Je kunt ook zelf invoeren en een tabblad openen met de gegevens:\n\n#Probeer onderstaande, hier niet afgedrukt want dit wordt te lang\nView(RotterdamStaat)\nJe kunt ook elke kolom in de dataset beschrijven met behulp van de summary-functie:\nItem Beschrijving\nMin. De kleinste waarde in de kolom 1st. Qu. Het eerste kwartiel (de waarde 1/4 van de variabele) Median De mediaan (de waarde 1/2 van de variabele) Mean Het gemiddelde van de kolom 3rd. Qu. Het derde kwartiel (de waarde 3/4 van de variabele) Max. De hoogste waarde in de kolom\n\n\n buurtenrotterdam        NNGB       \n Length:93          Min.   :0.6500  \n Class :character   1st Qu.:0.7650  \n Mode  :character   Median :0.8000  \n                    Mean   :0.8006  \n                    3rd Qu.:0.8400  \n                    Max.   :0.9000  \n                    NA's   :30      \n\nVoor elke kolom wordt een aantal waarden genoemd:\nOp basis van deze getallen kan een indruk worden verkregen van de spreiding van de waarden van elke variabele. Met name kan worden vastgesteld dat de mediaan van de huizenprijs in St. Helens per wijk varieert van 65.000 EURO tot 260.000 EURO en dat de helft van de prijzen tussen 152.500 EURO en 210.000 EURO ligt. Ook kan worden vastgesteld dat, aangezien de mediaan van het gemeten inbraakpercentage nul is, ten minste de helft van de gebieden geen inbraken had in de maand waarin de tellingen werden samengesteld..\nWe kunnen vierkante haken gebruiken om specifieke delen van het dataframe te bekijken, bijvoorbeeld hp.data[1,] of hp.data[,1]. We kunnen ook kolommen verwijderen en nieuwe kolommen aanmaken met behulp van de onderstaande code. Vergeet niet om het head() commando te gebruiken zoals we eerder deden om naar het dataframe te kijken.\n\n\n# A tibble: 6 x 3\n  buurtenrotterdam            NNGB counciltax\n  <chr>                      <dbl> <lgl>     \n1 Afrikaanderwijk             0.86 NA        \n2 Agniesebuurt               NA    NA        \n3 Bedrijvenpark Noord_West   NA    NA        \n4 Bedrijventerrein Schieveen NA    NA        \n5 Bergpolder                 NA    NA        \n6 Beverwaard                  0.78 NA        \n\n\n\n# A tibble: 6 x 3\n  buurtenrotterdam            NNGB `Price-thousands`\n  <chr>                      <dbl> <lgl>            \n1 Afrikaanderwijk             0.86 NA               \n2 Agniesebuurt               NA    NA               \n3 Bedrijvenpark Noord_West   NA    NA               \n4 Bedrijventerrein Schieveen NA    NA               \n5 Bergpolder                 NA    NA               \n6 Beverwaard                  0.78 NA               \n\nGeograpfische Informatie\nR heeft zich ontwikkeld tot een GIS waar gebruikers aan hebben bijgedragen met pakketten, of ‘libraries’ zoals R ze noemt. We zullen in de tutorial verschillende van dit soort ‘libraries’ gebruiken en zullen ze laden als dat nodig is.\nAls u uw computer gebruikt, moet u de R-libraries installeren en ze ook laden. Om dit te doen, start u install.packages (“library_name”).\nOm met ruimtelijke gegevens te kunnen werken, moeten we na dat installeren een aantal ‘libraries’ laden>\n\n\n\nOm met ruimtelijke gegevens te werken, moeten we enkele libraries laden. Daarmee is R echter alleen maar in staat om geografische data te verwerken. Het laadt nog geen specifieke data sets. Om dit te doen, moeten we enkele gegevens inlezen. Hiervoor gaan we shapefiles gebruiken - een bekend GIS-dataformat. We gaan LSOA(Lower layer Super Output Areas)-data gebruiken voor St. Helens in Merseyside.\nR gebruikt werkmappen om informatie op te slaan die relevant is voor het huidige project waaraan je werkt. Ik stel voor dat je een map een bepaalde naam geeft die het R-werk ergens zinvol maakt. Dan moeten we R vertellen waar deze map staat, dus klik op Session > Set Working Directory > Choose Directory. . en selecteer de map die je hebt aangemaakt.\nZoals met de meeste programma’s, zijn er meerdere manieren om dingen te doen. Om bijvoorbeeld de werkmap in te stellen, kunnen we het volgende typen: setwd(“M:/R_werk”). Jouw versie kan een langere titel hebben, afhankelijk van hoe je de map noemt. Merk ook op dat schuine streepjes worden aangegeven met een ‘/’ en niet ’'.\nEr is een set van shapefiles voor de St. Helens-wijken op dezelfde locatie als de dataset die je eerder hebt gelezen. Omdat er meerdere bestanden nodig zijn, heb ik deze in één zip-bestand gebundeld. Deze download je naar jouw lokale map en pakt deze vervolgens uit. Dit doe je met de volgende R-functies:\n\n\n\nDe eerste functie downloadt het zip-bestand daadwerkelijk in uw werkmap. De tweede functie pakt het zip-bestand uit. Nu kunnen we het bestand in R lezen.\n\n\nReading layer `wijkindeling' from data source `C:\\HARRIE\\Tijdelijk\\Git\\HarriesHoekje\\_posts\\2019-12-15-r-als-een-gis\\wijkindeling.shp' using driver `ESRI Shapefile'\nSimple feature collection with 85 features and 14 fields\ngeometry type:  POLYGON\ndimension:      XY\nbbox:           xmin: 55500 ymin: 428647.4 xmax: 101032.6 ymax: 447000\nepsg (SRID):    NA\nproj4string:    +proj=sterea +lat_0=52.15616055555555 +lon_0=5.38763888888889 +k=0.9999079 +x_0=155000 +y_0=463000 +ellps=bessel +units=m +no_defs\n\nDe st_read functie doet dit en slaat ze op als een Simple Features (of sf) object. Je kunt de qtm-functie gebruiken om de polygonen (d.w.z. de kaart van de LSOA) te tekenen.\n\n\n\nWe kunnen ook het head()-commando gebruiken om de eerste zes rijen te tonen, precies hetzelfde als bij een data frame.\n\n\nSimple feature collection with 6 features and 14 fields\ngeometry type:  POLYGON\ndimension:      XY\nbbox:           xmin: 87699.71 ymin: 433848.1 xmax: 96594.48 ymax: 440401.8\nepsg (SRID):    NA\nproj4string:    +proj=sterea +lat_0=52.15616055555555 +lon_0=5.38763888888889 +k=0.9999079 +x_0=155000 +y_0=463000 +ellps=bessel +units=m +no_defs\n  GROEP CODE GEMEENTE GEBIED BUURT SUBBUURT SBTDEEL BLOK TEKST\n1   TIR   15      599     10    79       -1      -1   -1  1079\n2   TIR   15      599      3    27       -1      -1   -1  0327\n3   TIR   15      599     19    26       -1      -1   -1  1926\n4   TIR   15      599      3    22       -1      -1   -1  0322\n5   TIR   15      599      8    42       -1      -1   -1  0842\n6   TIR   15      599     10    82       -1      -1   -1  1082\n             GEBDNAAM                    BUURTNAAM wijknr Shape_Leng\n1          Feijenoord      Kop van Zuid - Entrepot     79   4134.193\n2          Delfshaven    Oud Mathenesse/Witte Dorp   3004   4828.106\n3    Nieuw Mathenesse             Nieuw Mathenesse      0   6494.563\n4          Delfshaven                 Tussendijken     22   2853.695\n5 Kralingen-Crooswijk Kralingen Oost/Kralingse Bos   3005  12400.652\n6          Feijenoord                   Hillesluis     82   4786.269\n  Shape_Area                       geometry\n1   671847.0 POLYGON ((93530.4 436049.7,...\n2   880203.0 POLYGON ((89592.66 437446.3...\n3  2071912.9 POLYGON ((87705.53 436406.7...\n4   399761.8 POLYGON ((89484.61 436419.5...\n5  6521950.1 POLYGON ((95980.45 440392, ...\n6   902617.3 POLYGON ((94062.15 434700.2...\n\nVoor degene die met GIS werkt: Dit is hetzelfde als de attribuutentententabel in programma’s als ArcGIS, QGIS of MapInfo. Als u het shapefile in QGIS of ArcGIS wilt openen om vast te stellen hoe het er zo’n beetje uit ziet, kunt u dat doen.\nJe kunt zien dat er veel informatie beschikbaar is, inclusief de geometrie. Voor ons is het ID-veld belangrijk, en zien dat dit overeenkomt met het ID-veld in het hp.data bestand. We kunnen dit gebruiken om de twee datasets samen te voegen om de inbraakgegevens op de kaart te tonen.\nHet idee is dat er in elke dataset een veld is dat we kunnen gebruiken om de twee samen te voegen; in dit geval hebben we het ID-veld in sthelens en het ID-veld in hp.data.\n\n\n\n\n\n\nGebruik de head-functie om te controleren of de gegevens correct zijn samengevoegd.\n\n\nSimple feature collection with 6 features and 16 fields\ngeometry type:  POLYGON\ndimension:      XY\nbbox:           xmin: 88174.91 ymin: 432495.7 xmax: 99128.64 ymax: 442409.5\nepsg (SRID):    NA\nproj4string:    +proj=sterea +lat_0=52.15616055555555 +lon_0=5.38763888888889 +k=0.9999079 +x_0=155000 +y_0=463000 +ellps=bessel +units=m +no_defs\n                   BUURTNAAM GROEP CODE GEMEENTE GEBIED BUURT\n1            Afrikaanderwijk   TIR   15      599     10    86\n2               Agniesebuurt   TIR   15      599      5    15\n3 Bedrijventerrein Schieveen   TIR   15      599     26    50\n4                 Bergpolder   TIR   15      599      5    31\n5                 Beverwaard   TIR   15      599     12    90\n6                   Bloemhof   TIR   15      599     10    81\n  SUBBUURT SBTDEEL BLOK TEKST                   GEBDNAAM wijknr\n1       -1      -1   -1  1086                 Feijenoord     86\n2       -1      -1   -1  0515                      Noord     15\n3       -1      -1   -1  2650 Bedrijventerrein Schieveen      0\n4       -1      -1   -1  0531                      Noord     31\n5       -1      -1   -1  1290                IJsselmonde     90\n6       -1      -1   -1  1081                 Feijenoord     81\n  Shape_Leng Shape_Area NNGB Price-thousands\n1   3461.527   621148.9 0.86              NA\n2   2809.173   384570.5   NA              NA\n3   5036.165  1518931.1   NA              NA\n4   3253.672   454195.3   NA              NA\n5   6592.857  1514886.2 0.78              NA\n6   3613.866   788457.4 0.81              NA\n                        geometry\n1 POLYGON ((93595.07 434773.5...\n2 POLYGON ((91881.82 438449.9...\n3 POLYGON ((88174.91 441742.2...\n4 POLYGON ((91216.07 439018.2...\n5 POLYGON ((97775.88 434798.3...\n6 POLYGON ((93506.9 434355.2,...\n\nNu we de gegevens hebben samengevoegd, kunnen we een kaart maken van deze huizen-prijzen.\n\n\n\nDit is een zeer snelle manier om een kaart met R te maken. Om de kaart te gebruiken, klikt u op de Export-knop en kiest u vervolgens voor Copy naar Clipboard. . . . Kies vervolgens Copy Plot. Als je ook Word hebt, kun je de kaart in je document plakken. Je kunt de kaart ook opslaan als Afbeelding of PDF.\nEen Kaart maken Census Data\nWerken met R vereist vaak meerdere coderegels code om een output te krijgen. In plaats van de code in de Console in te typen, kunnen we in plaats daarvan een script gebruiken. Daar kunnen we altijd naar teruggaan en de code zeer eenvoudig te bewerken, om fouten te corrigeren!\nMaak een nieuw script aan (File > New File > R-script) en voer de code daar in. Vervolgens kunt je de regels die je wilt uitvoeren selecteren door ze te markeren en vervolgens op Ctrl+Enter te drukken, of door de Run knop bovenaan te gebruiken.\nNu gaan we hetzelfde principe gebruiken als voorheen om een kaart te maken van enkele gegevens uit 2018. We moeten de gegevens eerst downloaden.\nGa naar https://www.cbs.nl/nl-nl/dossier/nederland-regionaal/wijk-en-buurtstatistieken/kerncijfers-wijken-en-buurten-2004-2019.\nOpen vervolgens https://www.cbs.nl/nl-nl/maatwerk/2018/30/kerncijfers-wijken-en-buurten-2018.\nsla het bestand kwb-2018.xls op in map Buurtexcel.\nOmdat de bestanden te zwaar zijn, vind je de informatie hierover niet hier. In het pdf bestand kun je hier wel over lezen (zie in deze map WerkdocumentNEDoriginal.pdf).\nReferenties\nNick Bearman. Introduction to Spatial Data & Using R as a GIS. https://github.com/nickbearman/intro-r-spatial-analysis/blob/master/workbook.pdf\nLovelace, R., Nowosad, J. and Muenchow, J. Geocomputation with R. https://geocompr.github.io/\nDeze ‘practical’ is geschreven met R 3.5.1 (2018-07-02) en RStudio 1.1.463 door Dr. Nick Bearman (nick@ geospatialtrainingsolutions.co.uk).\nHet werk is gelicenseerd onder Creative Commons Attribution-ShareAlike 4.0 International License. Om een kopie van deze licentie te zien, ga dan naar http://creativecommons.org/licenses/by-sa/4.0/deed.en. De laatste PDF-versie kun je hier https://github.com/nickbearman/intro-r-spatial-analysis vinden. Deze versie is op 18 May 2019 gemaakt.\n\n\n",
    "preview": "posts/2019-12-15-r-als-een-gis/r-als-een-gis_files/figure-html5/unnamed-chunk-19-1.png",
    "last_modified": "2020-03-13T23:28:59+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-12-04-bayesbasis/",
    "title": "Bayes'basis",
    "description": "Over statistiek en waarschijnlijkheid op de eenvoudige manier.",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "www.harriejonkman.nl"
      }
    ],
    "date": "2019-12-04",
    "categories": [],
    "contents": "\nBayes’ basis\nOns leven zit vol onzekerheid, schrijft Will Kurt in zijn nieuwe boek Bayesian statistics the fun away. Understanding statistics and probability with Star Wars, Lego, and Rubber Ducks. Veel gebeurt zoals we ons van te voren hebben bedacht, andere zaken lopen toch weer net een beetje anders. Om met de werkelijkheid om te gaan moeten we wel een gevoel hebben bij het omgaan met onzekerheid in bepaalde situaties. Als we dat systematische willen doen, kan de Bayesiaanse statistiek ons een handje helpen. Het leert ons logisch denken en we kunnen het ook nog mathematisch uitdrukken. Will Kurt helpt mensen bij het omgaan met onzekerheid. De boeken die over Bayesiaanse statistiek zijn geschreven zijn vaak moeilijk, zeker als het een nieuw onderwerp is voor je. De standaardboeken over dit onderwerp, van Gelman, McElreath of Spiegelhalter bijvoorbeeld, zijn vooral weggelegd voor een kleine groep statistisch onderlegde lezers. Kurt heeft een boek voor iedereen voor ogen. Bayesiaanse statistiek zet hij tegenover frequentistische statistiek. Bij frequentische statistiek heeft waarschijnlijkheid met frequentie te maken, bij Bayesiaanse statistiek gaat het om de vraag hoe onzeker we zijn over de informatie waarover we beschikken. Bij Bayesiaanse statistiek beschrijven we zo nauwkeurig mogelijk hoe onzeker we over zaken zijn en gebruiken daarbij specifieke gereedschappen. Hier kijken naar een probleem, lossen we het logisch op, gebruiken bepaalde regels, drukken het mathematisch uit en kijken we vervolgens weer opnieuw naar het probleem. Het boek deelt hij in vier stukken op.\nIn het eerste deel laat Kurt ons Bayesiaans denken en alledaags redeneren. Hier introduceert hij Bayesiaans denken en geeft er een overzicht van. Hij laat zien dat Bayesiaans denken heel dicht bij gewoon denken en de dagelijkse problemen ligt. Het is niet anders dan het observeren van data, het formuleren van een hypothese en dan weer het updaten van wat je denkt op basis van de data. Om de wereld te begrijpen, kun je vertrouwen op wat je ziet en de hypothesen die je erover hebt. Met nieuwe gegevens kun je de hypothese aanpassen, waarna je vervolgens beter met de data om kunt gaan. Onzekerheid is te meten en in getallen om te zetten. Iets is weinig of zeer waarschijnlijk en dat kunnen we in mathematische modellen omzetten. Iets wat dichter bij 0 ligt, is waarschijnlijk niet waar en iets dat dicht bij 1 ligt is dat wel. Iets kan niet waar en onwaar tegelijk zijn. \\(P(X) + P_n(X)=1\\) Om te rekenen met waarschijnlijkheid waarin we geloven, moeten we kunnen zeggen hoeveel keer meer we in een bepaalde hypothese geloven dan in een andere hypothese. Daarvoor moeten we met logica kunnen werken en dan zijn drie operators belangrijk: EN, OF, NIET, zoals in de volgende stelling: Ik heb een paraplu nodig als het regent EN ik naar buiten ga of Als het niet regent OF als ik NIET naar buiten ga, heb ik een paraplu NIET nodig. Wat ook belangrijk is, hoe je verschillende waarschijnlijkheden combineert. Als de uitkomsten niet met elkaar te maken hebben (bv. de kans op een kop van een munt en een zes van de dobbelstaan) is \\(P(A,B)=P(A)*P(B)\\). In dat voorbeeld gaat het wel om zaken die elkaar uitsluiten en dat is niet altijd het geval. Je kunt geen kop en een munt gooien EN ook niet een zes en een twee met de dobbelsteen. Maar het wordt anders bij een kop van de munt gooien OF een zes met de dobbelsteen. Dan moeten we alle waarschijnlijkheden optellen en dan de waarschijnlijkheid dat beide tegelijk gebeuren, eraf trekken of te wel: \\(P(A) OR P(B)=P(A)+P(B)-P(A,B)\\) of, zoals in dit voorbeeld: \\[P(kop) OR P(zes)=P(kop,zes)=1/2+1/6-1/12=7/12\\] Het eerste voorbeeld van een mathematisch model dat Kurt geeft, is een binomiale verdeling, waarmee je bepaalde successen in de uitkomst kunt onderzoeken. Bij deze uitkomstmaat heeft iets succes of niet (twee uitkomsten, vandaar: bi).k Is het aantal uitkomsten waar het ons om gaat (bv. k=2, 2 keer kop van de munt)n is het totale aantal (bv. n=3, het aantal keren dat een munt wordt opgegeooid)p is de waarschijnlijkheid dat iets gebeurt.(p=1/2, de waarschijnlijkheid op het gooien van een munt)\nUitgedrukt als \\(B(2; 3;1/2)\\), waarbij de B de binomiale verdeling is.\nDe tweede verdeling die hij ons geeft is de Beta verdeling, een continue verdeling. Deze gebruik je als je een aantal keren iets hebt geobserveerd en het aantal succesvolle uitkomsten weet, bv. je hebt 41 keer gegooid en je hebt 14x kop gegooid en 27 keer munt. Hier gaat het om \\(alpha,beta,p\\). Hierbij isp de waarschijnlijkheid dat iets gebeurta de keren dat het ook echt gebeurtb de keren dat het niet gebeurt\nBayes is vooral interessant bij conditionele waarschijnlijkheid, het ene heeft wel degelijk invloed op het andere, daar gaat het om in het tweede deel. Juist hier is Bayes interessant. Conditionele waarschijnlijkheid drukken we uit als \\(P(A|B)\\), de waarschijnlijkheid van A gegeven B. Dit is belangrijk omdat hier de aanvullende informatie ons geloof in iets beinvloedt. Hij geeft het voorbeeld van kleurenblindheid:\nP(kleurenblindheid)=0.00425\nP(kleurenblindheid|vrouw)=0.0005\nP(kleurenblindheid|man)=0.08\nAls je de kans wilt weten op kleurenblindheid van een man, dan is dat: P(man| kleurenblindheid)=P(man) x P(kleurenblindheid|man) 0.5X0.08=0.04.Dit kunnen we ook omdraaien. Ingewikkelder is de vraag: wat is de kans dat de persoon man is gegeven dat je weet dat er sprake is van kleurenblindheid (P(man|kleurenblindheid)=?)\n\\[\nP(man|kleurenblind)={\\frac{P(man)P(kleurenblind|man)}{P(kleurenblind)}}=0.5X0.08/0.0425=0.941\n\\]\nHiervoor is de Bayesiaanse wet geintroduceerd die zo goed en makkelijk te gebruiken is:\n\\[\nP(A,B)={\\frac{P(A)P(B|A)}{P(B)}}\n\\] Met deze Bayesiaanse wet nemen we onze kijk op de werkelijkheid en ons geloof daarin als uitgangspunt. Hier combineren data en transformeren dit in een schatting van onze nieuwe kijk gegeven hetgeen we hebben geobserveerd. Met Bayes nemen we deze overtuiging en kwantificeren exact hoe sterk dit bewijs ons denken verandert. Met lego stukje laat Kurt heel concreet zien hoe dit werkt, zoals hij steeds in het boek eenvoudige voorbeelden neemt om duidelijk te maken wat hij bedoelt. Met Bayes heb je drie delen: de prior P(H), de hypothes oftewel hoe de manier waarop we naar de werkelijkheid kijken; de likelihood P(D|H), de data gegeven onze hypothese en de posterior, wat we eigenlijk willen weten: P(H|D), de hypothese gegeven de data die we hebben, de theorie die we op basis van de gegevens opstellen. Deze delen maken tezamen vormen het theorema van Bayes. De prior is natuurlijk het meest controversiele deel van deze wet. Hier gebruiken we de informatie die we al hebben in het schatten van een onzekere situatie. Ook bij de prior gebruik je niet één bepaalde waarde maar vaak ook een verdeling van waarden. Juist om verschillende mogelijkheden mee te kunnen nemen in jouw berekening.\nIn het derde deel van het boek gaat hij in op het schatten van parameters. Hij begint met het gemiddelde en dat we dat meten door alle observaties te wegen door de waarschijnlijkheid dat deze observaties voorkomen. Hier is niet veel nieuws onder de zon, behalve dat je wel goed moet weten waar je het over hebt. Datzelfde geldt voor het meten van de spreiding van de data waar hij ook over schrijft. Hij laat zien dat er drie manieren zijn om de spreiding in kaart te brengen: MAD (Mean Absolute Deviation), Variantie en de Standaard Deviatie waar het meest mee gewerkt wordt. Het is goed om niet alleen gemiddelden te kennen maar ook uitspraken te doen over de spreiding omdat deze meestal aangeven hoe zeker of onzeker we over iets zijn. Hoe groter de spreiding, hoe onzekerder we zijn en andersom. In de normale verdeling is de spreiding ook uitgedrukt in standaard deviatie. Hieronder zien we een normale verdeling met een standaard deviatie van 15, relatief veel zekerheid.\n\n\n\nEn hier onder een normaal verdeling met standaard deviatie 40, relatief veel onzekerheid.\n\n\n\n65 procent van de mogelijke waarden vallen binnen ongeveer een standaardeviatie, 95 procent binnen twee standaarddeviaties en 99.7 procent binnen drie standaard deviatie. Interessanter hier zijn de gereedschappen om de parameters te schatten op een Bayesiaanse manier. Kurt presenteert drie functies en ik hou even de Engelse termen aan: de Probability Density Function (PDF), die ons laat zien hoeveel iets voorkomt. Kurt geeft een voorbeeld. Hij start een email lijst waar je je voor kunt inschrijven. Hij stuurt 40.000 mails uit en 300 mensen geven zich op. Dit kan in een beta-functie worden uitgedrukt\n\n\n\nDe Cumulative Distribution Function (CDF) die ons helpt om de waarschijnlijkheid van bepaalde waardes vast te stellen. R geeft ons daar ook het instrumentarium voor. Als we in R de waarschijnlijkheid van Beta(300, 39700) vast willen stellen, uitgaande van kleiner dan 0.0065, kunnen we dat met de volgende formule uitdrukken.\n\n\n[1] 0.007978686\n\nEn om vast te stellen dat de bevestigingsgraad meer is dan 0.0085 is, schrijven we het volgende:\n\n\n[1] 0.01248151\n\nHet maakt niet uit of het continu is zoals hierboven of discreet zoals hieronder:\n\n\n[1] 0.8125\n\nOm bepaalde waarden vast te stellen kunnen we de Kwantielen Functie gebruiken, bv. de mediaan en het interval dat er bij hoort. Of als we de waarde van 99,9 procent en minder willen hebben kunnen we de volgende functie gebruiken. We weten hier voor 99,9 procent zeker dat minder dan 0.0089 zich opgeven voor onze lijst.\n\n\n[1] 0.008903462\n\nEn ook, weten we voor 95 procent zeker dat tussen de 0.67 procent en 0.84 procent van de mensen zich opgeeft voor de lijst. Dat onderzoeken we op de volgende wijze:\n\n\n[1] 0.006678074\n\n[1] 0.008368562\n\nOp deze manier kunnen we parameters en intervallen berekenen die met onze waarden samenhangen. Die schattingen kunnen we ook maken door informatie toe te voegen. We hebben dan niet alleen de data (300, 39700) maar gebruiken ook kennis die we hebben. Dan kunnen we verschillende prior gebruiken die onze kennis uitdrukken. Bijvoorbeeld als we pessimistisch zijn kunnen we de Beta(1,41) gebruiken en die veronderstelt vooral lage waarden of Beta(5,200) als we iets meer geloof in verandering hebben. Of als we niks weten de Beta(1,1). Maar ook als is jouw prior verkeerd, deze wordt door data overruled en zeker als je veel data hebt. Het is jammer dat Kurt niet eenvoudig laat zien hoe je dat doet. Kun je er bij optellen.\nHet laatste, vierde deel gaat over de kern van statistiek: het testen van hypothesen. Hij begint met een simpele A/B test, een test om vast te stellen of het ene beter werkt dan het andere. Ook hier laat hij zien hoe je dat kunt doen en hoe je daar simulaties bij kunt gebruiken. De ene groep van 150 ontvangt een email met een grote illustratie erop en de andere niet. Hij wil weten welke groep zich op basis van de email vaker opgeeft\nVariant\nGeklikt\nNiet geklikt\nOpgegeven\nVariantA\n36\n114\n0.24\nVariant B\n50\n100\n0.33\nWe weten uit eerder onderzoek dat 3 van 10 mensen zich opgeven en die kennis verwerken we in de prior.\n\n\n[1] 0.95954\n\nBij 100.000 pogingen was variant B 96 procent beter.\nIn dit hoofdstuk presenteert hij ook twee maten: de Bayes Factor and Posterior Odds. Als we twee hypothesen met elkaar willen vergelijken dan vergelijken we in elk geval de prior maal de likelihood en dan krijgen we de ratio van de posteriors als volgt:\n\\[\n{\\frac{P(H_1)*(P(D|H_1)}{P(H_2)*(P(D|H_2)}}\n\\] Dit vergelijkt hoe elke hypothese de data verklaart die we observeren. Stel dat we uitgaan van dezelfde prior houden we het volgende over: \\[\n{\\frac{(P(D|H_1)}{(P(D|H_2)}}\n\\] Dit is de Bayes factor die we dan overhouden, de ratio tussen de likelihoods van twee hypothesen. Is het meer dan 1, dan is hypothese_1 (H_1) beter, is het kleiner dan 1 dan is H_2 beter. De Prior Odds is de ratio van de waarschijnlijkheid voordat we naar de data hebben gekeken \\({\\frac{P(H_1)}{P(H_2)}}\\). Als we de Bayes Factor en de Prior Odds samen nemen krijven we de posterior odds: \\[posterior odds=O(H_1){\\frac{P(D|H_1)}{P(D|H_2)}}\\]\nEn dan is het handig om de volgende regels in het achterhoofd te houden:\nPosterior odds\nSterkte van het bewijs\n1-3\nInteressant, geen conclusies\n3-20\nHet echt ergens op lijken\n20-150\nSterk bewijs ten faveure H1\n>150\nOverweldigend bewijs\nHij geeft enkele voorbeelden waaronder deze: Stel je wordt op een ochtend wakker en je hebt problemen met horen en een ringtoon (tinnitus) in je oor. Er zijn twee mogelijkheden, twee hypothesen: - Vestibular swannoma en je hebt een tumor die tot oorproblemen leidt (H_1). Hoorproblemen zijn hier 94% en tinnitus is hier 89%: \\(P(D|H1)=0.94*0.89=0.78\\) - Oorsmeer en je oor moet worden uitgespoten (H_2). Hier geldt voor de complicaties: \\(P(D|H1)=0.63*0.55=0.35\\) en de Bayes Factor wordt dan: \\[\n{\\frac{(P(D|H_1)}{(P(D|H_2)}}={\\frac{0.78}{0.35}}=2.23\n\\] Het lijkt de kans op te gaan van vestibular swannoma maar dat kan niet echt geconcludeerd worden. Maar hier kunnen we er niet vanuit gaan dat de prior in beide geval even groot is. De kans op de tumor is \\[P(H_1)={\\frac{11}{1,000,000}}\\]\nEn de kans op gewoon oorsmeer is \\[P(H_2)={\\frac{37,000}{1,000,000}}\\]\nDe Odds Priors is dan \\[O(H_1)={\\frac{P(H_1)}{P(H_2)}}={\\frac{11}{37,000}}\\]\nAls we nu rekening houden met deze voorkennis en dit met de Bayes Factor vermenigvuldigen krijgen we het volgende resultaat:\n\\[\nO(H_1*{\\frac{(P(D|H_1)}{(P(D|H_2)}}={\\frac{11}{37,000}}*2.23={\\frac{223}{370,000}}\n\\]\nHet is 1,659 meer waarschijnlijk dat het met jouw oorsmeer te maken heeft en dat je het best naar de huisarts kunt gaan. En zo werkt hij dit concept in dit hoofdstuk verder uit en krijg je een goed idee wat die Bayes’ basis jou op kan leveren. Soms is het goed om weer terug te grijpen op de basis van een heel goed instrumentarium en Kurt kan jou als lezer heel goed vertellen hoe die basis eruit ziet.\nKurt, W. (2019). Bayesian statistics the fun way. Understanding statistics and probability with Star Wars, Lego, and Rubber Ducks. San Francisco: No Starch Press.\n\n\n",
    "preview": "posts/2019-12-04-bayesbasis/bayesbasis_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2020-03-13T23:28:58+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-09-25-sf-ggplot-en-tmap/",
    "title": "Sf, ggplot en tmap",
    "description": "Over het maken van kaarten van Nederland met nieuwe pakketten en mogelijkheden.",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "www.harriejonkman.nl"
      }
    ],
    "date": "2019-09-25",
    "categories": [],
    "contents": "\nDoel\nIn deze blog wil ik laten zien hoe je tegenwoordig geografische kaarten in R kunt maken. Het pakket sf (Simple Features) is de moderne standaard daarvoor en is de natuurlijke opvolger van sp. Dit sf pakket kan goed met het databewerkingspakket tidyverse werken en met onderdelen daarvan zoals dplyr en het visualisatiepakket ggplot. Een pakket waar je, zeker in combinatie met sf, goed kaarten mee kunt maken is tmap (statische en interactieve kaarten). Ik had mij bij het maken van deze blog de taak gesteld om te laten zien hoe je Nederland geografisch af kunt beelden. Dat laat ik hieronder zien en daar gebruik ik die moderne R-pakketten voor. Ik kwam er bij het maken ook achter dat je van de open data van het CBS ook makkelijk kaarten van Nederland kunt maken. Dat laat ik je hieronder ook zien. In een latere fase kom ik hier nog eens op terug.\nPakketten\nEerst maar eens enkele moderne R-pakketten binnenhalen.\n\n\n\nGeographic Data Science (GDS) en Geographic Information Science (GISc)\nIn een mooie blog laat Katie Jolly het verschil zien tussen Geografische Datawetenschap en Geografische Informatiewetenschap Blog van Katie Jolly. Bij R en Python praat je meer over geografische datawetenschap of geocomputation, zoals Lovelace e.a. dat noemen Geocomputatie. Daarnaast heb je Geografische Informatie Systemen zoals QGIS en ArcMap as GISc (vaak GIS-programma’s genoemd). Een van de grote verschillen, en daar heeft ze gelijk in, is de de reproduceerbaarheid van Geografische Datawetenschap ten opzichte van GIS-programma’s. Met het volgende overzicht, dat zijn ontleent aan Lovelace e.a., maakt zij dat duidelijk:  Deze tabel komt uit Robin Lovelace’s blog-post Can Geographic Data Save the World?.\nMet sf in combinatie met tidyverse en enkele specifieke pakketten kom je een heel eind tegenwoordig en kun je dat doen wat dure GIS-programma’s ook kunnen en soms nog meer.\nR’s ruimtelijke ecosysteem\nEr is tegenwoordig een hele variëteit aan ruimtelijke data-analyse pakketten beschikbaar.\nsp: Classes and Methods for Spatial Data\nsf: Simple Features for R (bouwt voort op sp)\nspdep: Spatial Dependence: Weighting Schemes, Statistics, and Models\nlwgeom: Binding to the liblwgeom library\n…\nDe nieuwste en de beste op dit moment is sf. Over ruimtelijk analyseren is een hele serie tutorials beschikbaar op internet, ook al lopen ze in de tijd wat achter, is te vinden op de r-spatial site. Daarnaast is er een hele serie specieke pakketten voor data-visualisatie waar je mee kunt werken en die de gebruiker veel flexibiliteit bieden. Terwijl sf zelf heel veel kan, werkt het heel goed in combinatie met deze twee pakketten.\nggplot2: De Grammar of Graphics in R.\ntmap: R pakket voor thematic maps.\nDe laatste maand heb ik wat gewerkt met sf in combinatie met ggplot en tmap. Ik wilde mij dit wat beter eigen maken en het gebruiken voor het maken van kaarten van Nederland en delen van Nederland. Hieronder vind je achtereenvolgens wat informatie over sf en ggplot en daarna iets over tamp.\nStructure of sf data\n\nEen figuur uit Geocomputation with R, zie ook mijn vorig blog Geocomputation with R.\nSimple Features is een hierarchisch data model dat een behoorlijk breed palet aan geometrische types representeert. Het komt erop neer dat simple features een dataframe is met in elke rij ruimtelijke gegevens (een bevolkingsgegeven, een punt, een stad, …) met een list-kolom met coördinaten waarmee de geografische vorm gemaakt kan worden. Hieronder leer je hoe het werkt.\nProjecties\nHet is goed om te weten dat projecties een 3D oppervlakte naar een 2D-oppervlakte omvormen. Verschillende projecties laten de geografische vorm er heel anders uit zien.\n\nOpen data file binnen tmap\nLaten we eerst eens een databestand van Nederland binnenhalen dat in het pakket tmap opgeslagen zit. Dit is een databestand met de steden van Nederland en enkele gegevens van deze steden. Onderstaande werkt dus alleen als je de library(tmap) hebt geopend.\n\n\n\nLaten we met sf eens kijken wat we in huis hebben gehaald. We tonen enkele plot mogelijkheden van sf. Eerst maar eens de grove kaart van ons land.\n\n\n\nWelke data zitten er in het bestand, in combinatie met de geografische kaders? In het databestand zitten de codes, de namen, de provincies, de populatie, populatie_man, populatie_vrouw, populaties over verschillende leeftijdsgroepen etc.\n\n\n\nStel dat we maximaal 15 variabelen willen afdrukken.\n\n\n\nOf alleen maar een, de eerste.\n\n\n\nWe kunnen de gegevens nu ook goed vanuit sf met ggplot afdrukken. Stel bijvoorbeeld dat we de populatie van 0-14 jaar (een variabele die in het databestand zit) willen laten zien en hoe dat percentage in de verschillende steden in Nederland eruit ziet. We breken de percentages op in van 14 tot 40 procent (je kunt zien dat de percentages daar tussen liggen).\n\n\n\nOmdat het met tidyverse, en dus met dplyr werkt, kunnen we ook de standaard databewerkingscodes gebruiken. Dat maakt het werken met geografische data een stuk makkelijker. Als we alleen Zuid-Holland willen laten zien, maken we het databestand Zuid-Holland.\n\n\n\nDan kunnen we hier de eerste variabele laten zien en dan zien we de gemeenten van deze provincie afgebeeld.\n\n\n\nEn hetzelfde als hierboven. Hoe zit het met de jonge bevolking in de ZuidHollandse steden? We zien het hieronder met inzet van het pakket ggplot.\n\n\n\nOverstappen naar tmap\nDaar waar sf goed is voor het binnenhalen en bewerken van de data op allerlei manieren, daar is tmap heel goed in het maken van de kaarten op een eenvoudige manier. tmapis gemaakt om met grote flexibiliteit kaarten te kunnen maken. Het heeft dezelfde gelaagde structuur als ggplot. Je vindt op de website een document om er makkelijk mee te kunnen beginnentmap: get started. En de ontwikkelaar (Martijn Tennekes van het CBS ) heeft er een inzichtelijk artikel over geschreven in Journal of Statistical Software artike.\nStel dat je er een percentage bevolking aan wilt toevoegen en dit percentage per provincie wilt afbeelden. Met de volgende code doe je dat.\n\n\n\nStel dat we twee grafieken naast elkaar willen zetten. Dat doe je zo.\n\n\n\nMet tmap kun je ook en net zo makkelijk interactieve kaarten maken.\nOpen data via CBS\nIk kwam er ook achter dat je kaarten ook kunt maken via de open-data mogelijkheden van het CBS (Het kan, maar het vraagt nog wel wat oefening de komnende tijd). Open hiervoor het pakket cbsodataR\n\n\n\nZoek vervolgens op welke data beschikbaar zijn:\n\n\n  [1] \"WijkenEnBuurten\"                         \n  [2] \"\"                                        \n  [3] \"Gemeentenaam_1\"                          \n  [4] \"SoortRegio_2\"                            \n  [5] \"Codering_3\"                              \n  [6] \"IndelingswijzigingWijkenEnBuurten_4\"     \n  [7] \"\"                                        \n  [8] \"AantalInwoners_5\"                        \n  [9] \"\"                                        \n [10] \"Mannen_6\"                                \n [11] \"Vrouwen_7\"                               \n [12] \"\"                                        \n [13] \"k_0Tot15Jaar_8\"                          \n [14] \"k_15Tot25Jaar_9\"                         \n [15] \"k_25Tot45Jaar_10\"                        \n [16] \"k_45Tot65Jaar_11\"                        \n [17] \"k_65JaarOfOuder_12\"                      \n [18] \"\"                                        \n [19] \"Ongehuwd_13\"                             \n [20] \"Gehuwd_14\"                               \n [21] \"Gescheiden_15\"                           \n [22] \"Verweduwd_16\"                            \n [23] \"\"                                        \n [24] \"WestersTotaal_17\"                        \n [25] \"\"                                        \n [26] \"NietWestersTotaal_18\"                    \n [27] \"Marokko_19\"                              \n [28] \"NederlandseAntillenEnAruba_20\"           \n [29] \"Suriname_21\"                             \n [30] \"Turkije_22\"                              \n [31] \"OverigNietWesters_23\"                    \n [32] \"\"                                        \n [33] \"GeboorteTotaal_24\"                       \n [34] \"GeboorteRelatief_25\"                     \n [35] \"SterfteTotaal_26\"                        \n [36] \"SterfteRelatief_27\"                      \n [37] \"\"                                        \n [38] \"HuishoudensTotaal_28\"                    \n [39] \"Eenpersoonshuishoudens_29\"               \n [40] \"HuishoudensZonderKinderen_30\"            \n [41] \"HuishoudensMetKinderen_31\"               \n [42] \"GemiddeldeHuishoudensgrootte_32\"         \n [43] \"Bevolkingsdichtheid_33\"                  \n [44] \"\"                                        \n [45] \"Woningvoorraad_34\"                       \n [46] \"GemiddeldeWoningwaarde_35\"               \n [47] \"\"                                        \n [48] \"PercentageEengezinswoning_36\"            \n [49] \"PercentageMeergezinswoning_37\"           \n [50] \"\"                                        \n [51] \"PercentageBewoond_38\"                    \n [52] \"PercentageOnbewoond_39\"                  \n [53] \"\"                                        \n [54] \"Koopwoningen_40\"                         \n [55] \"\"                                        \n [56] \"HuurwoningenTotaal_41\"                   \n [57] \"InBezitWoningcorporatie_42\"              \n [58] \"InBezitOverigeVerhuurders_43\"            \n [59] \"EigendomOnbekend_44\"                     \n [60] \"\"                                        \n [61] \"BouwjaarVoor2000_45\"                     \n [62] \"BouwjaarVanaf2000_46\"                    \n [63] \"\"                                        \n [64] \"\"                                        \n [65] \"GemiddeldElektriciteitsverbruikTotaal_47\"\n [66] \"\"                                        \n [67] \"Appartement_48\"                          \n [68] \"Tussenwoning_49\"                         \n [69] \"Hoekwoning_50\"                           \n [70] \"TweeOnderEenKapWoning_51\"                \n [71] \"VrijstaandeWoning_52\"                    \n [72] \"\"                                        \n [73] \"Huurwoning_53\"                           \n [74] \"EigenWoning_54\"                          \n [75] \"\"                                        \n [76] \"GemiddeldAardgasverbruikTotaal_55\"       \n [77] \"\"                                        \n [78] \"Appartement_56\"                          \n [79] \"Tussenwoning_57\"                         \n [80] \"Hoekwoning_58\"                           \n [81] \"TweeOnderEenKapWoning_59\"                \n [82] \"VrijstaandeWoning_60\"                    \n [83] \"\"                                        \n [84] \"Huurwoning_61\"                           \n [85] \"EigenWoning_62\"                          \n [86] \"PercentageWoningenMetStadsverwarming_63\" \n [87] \"\"                                        \n [88] \"\"                                        \n [89] \"AantalInkomensontvangers_64\"             \n [90] \"GemiddeldInkomenPerInkomensontvanger_65\" \n [91] \"GemiddeldInkomenPerInwoner_66\"           \n [92] \"k_40PersonenMetLaagsteInkomen_67\"        \n [93] \"k_20PersonenMetHoogsteInkomen_68\"        \n [94] \"Actieven1575Jaar_69\"                     \n [95] \"\"                                        \n [96] \"k_40HuishoudensMetLaagsteInkomen_70\"     \n [97] \"k_20HuishoudensMetHoogsteInkomen_71\"     \n [98] \"HuishoudensMetEenLaagInkomen_72\"         \n [99] \"HuishOnderOfRondSociaalMinimum_73\"       \n[100] \"\"                                        \n[101] \"PersonenPerSoortUitkeringBijstand_74\"    \n[102] \"PersonenPerSoortUitkeringAO_75\"          \n[103] \"PersonenPerSoortUitkeringWW_76\"          \n[104] \"PersonenPerSoortUitkeringAOW_77\"         \n[105] \"\"                                        \n[106] \"BedrijfsvestigingenTotaal_78\"            \n[107] \"\"                                        \n[108] \"ALandbouwBosbouwEnVisserij_79\"           \n[109] \"BFNijverheidEnEnergie_80\"                \n[110] \"GIHandelEnHoreca_81\"                     \n[111] \"HJVervoerInformatieEnCommunicatie_82\"    \n[112] \"KLFinancieleDienstenOnroerendGoed_83\"    \n[113] \"MNZakelijkeDienstverlening_84\"           \n[114] \"RUCultuurRecreatieOverigeDiensten_85\"    \n[115] \"\"                                        \n[116] \"\"                                        \n[117] \"PersonenautoSTotaal_86\"                  \n[118] \"\"                                        \n[119] \"PersonenautoSJongerDan6Jaar_87\"          \n[120] \"PersonenautoS6JaarEnOuder_88\"            \n[121] \"\"                                        \n[122] \"PersonenautoSBrandstofBenzine_89\"        \n[123] \"PersonenautoSOverigeBrandstof_90\"        \n[124] \"PersonenautoSPerHuishouden_91\"           \n[125] \"PersonenautoSNaarOppervlakte_92\"         \n[126] \"Motorfietsen_93\"                         \n[127] \"\"                                        \n[128] \"AfstandTotHuisartsenpraktijk_94\"         \n[129] \"AfstandTotGroteSupermarkt_95\"            \n[130] \"AfstandTotKinderdagverblijf_96\"          \n[131] \"\"                                        \n[132] \"AfstandTotSchool_97\"                     \n[133] \"ScholenBinnen3Km_98\"                     \n[134] \"\"                                        \n[135] \"OppervlakteTotaal_99\"                    \n[136] \"OppervlakteLand_100\"                     \n[137] \"OppervlakteWater_101\"                    \n[138] \"\"                                        \n[139] \"MeestVoorkomendePostcode_102\"            \n[140] \"Dekkingspercentage_103\"                  \n[141] \"\"                                        \n[142] \"MateVanStedelijkheid_104\"                \n[143] \"Omgevingsadressendichtheid_105\"          \n[144] \"\"                                        \n[145] \"TotaalDiefstalUitWoningSchuurED_106\"     \n[146] \"VernielingMisdrijfTegenOpenbareOrde_107\" \n[147] \"GeweldsEnSeksueleMisdrijven_108\"         \n\nGebruik de WijkenenBuurten-data en de GeboorteRelatief_25 en verwijder spaties uit regiocodes.\n\n\n\nHaal de kaart met gemeentegrenzen op van PDOK\n\n\nReading layer `OGRGeoJSON' from data source `https://geodata.nationaalgeoregister.nl/cbsgebiedsindelingen/wfs?request=GetFeature&service=WFS&version=2.0.0&typeName=cbs_gemeente_2017_gegeneraliseerd&outputFormat=json' using driver `GeoJSON'\nSimple feature collection with 388 features and 5 fields\ngeometry type:  MULTIPOLYGON\ndimension:      XY\nbbox:           xmin: 13565.4 ymin: 306846.9 xmax: 277992.8 ymax: 619291\nepsg (SRID):    28992\nproj4string:    +proj=sterea +lat_0=52.15616055555555 +lon_0=5.38763888888889 +k=0.9999079 +x_0=155000 +y_0=463000 +ellps=bessel +towgs84=565.2369,50.0087,465.658,-0.406857,0.350733,-1.87035,4.0812 +units=m +no_defs\n\nKoppel CBS-data aan geodata met regiocodes\n\n\n\nMaak een thematische kaart\n\n\n\nIk zal kijken of ik de komende maanden een eenvoudige tutorial over dit onderwerp kan maken. Als ik deze klaar heb, kom ik terug op dit onderwerp.\n\n\n",
    "preview": "posts/2019-09-25-sf-ggplot-en-tmap/sf-ggplot-en-tmap_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2020-03-13T23:28:58+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-08-31-geocomputation/",
    "title": "Geocomputation",
    "description": "Bespreking van het fantastische boek Geocomputation with R",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "www.harriejonkman.nl"
      }
    ],
    "date": "2019-08-31",
    "categories": [],
    "contents": "\n\nEverything is related to everything else, but near things are more related than distant things (blz. 259)\n\nIntroductie\n\nBegin dit jaar verscheen Geocomputation with R, een boek van Robin Lovelace, Jakub Nowosad en Jannes Muenchow. Het is een standaarwerk over wat vandaag de dag mogelijk is met geografische data analyse, visualisatie en modellering. Aan het boek hebben ze met z’n drieën meer dan twee jaar gewerkt en dat proces konden we gedurende die periode volgen. Ze hebben het boek open-source gemaakt en mensen uitgenodigd om te reageren op eerdere versie. Alles kon je binnenhalen en je kon als het ware meedoen aan het maken ervan. Ze hebben het met Bookdown gemaakt, ze hebben het op GitHub geplaatst en je kunt het nu nog als Git Hub-boek lezen link en alle syntaxen gebruiken. Daar wordt het ook up-to-date gehouden. Het boek en onderdelen ervan zijn reproduceerbaar, openbaar en wereldwijd toegankelijk. Recent verscheen ook een uitgave bij CRC link. Daarnaast is er ook een volle map met aanvullend materiaal link; je moet je best doen om niet om te komen in al dit materiaal.\nGeocomputation with R wil iedereen helpen bij het analysere, visualiseren en modelleren van open-source geografische data. Met R heb je gereedschap in handen dat jou daartoe goed in staat stelt. Dat is eigenlijk iets van de laatste jaren. Voorheen werd dit geografische of ruimtelijke data-analyse werk met dure GIS-apparatuur uitgevoerd. Dat was enkel weggelegd voor enkele geografen die specialistische cursussen hadden gevolgd en met dure kliksytemen konden werken en voor anderen was het zeker niet te reproduceren. Maar tegenwoordig kan dit specialistische werk met een standaard laptop-computer thuis gedaan worden en Lovelace en companen laten jou heel goed zien hoe je dat moet doen. R is heel goed in moderne data-analyse en wordt door steeds meer mensen wereldwijd gebruikt. Er zijn ook andere talen waar veel mee gewerkt wordt zoals Python, Java en C++ maar de statistische capaciteiten van R kunnen hier tegenwoordig heel goed mee concurreren. R is niet alleen open-source en kosteloos, het is ook geschikt voor andere disciplines, het werkt met commando’s en syntaxen en het werk kan juist heel goed door anderen worden gereproduceerd. Anderen kunnen de resultaten van de analyses zelf genereren door gebruik te maken van toegankelijke codes. Met de term Geocomputatie onderscheiden zij zich van GIS-programma’s omdat het werk echt computerwerk is geworden, de focus ligt op het schrijven en gebruiken van codes en reproduceerbaarheid staat centraal.De laatste jaren is er niet alleen veel ontwikkeling geweest op het gebied van reproduceerbaarheid maar zijn er ook een groot aantal programma’s gekomen die het werken met ruimtelijke data mogelijk maken. R is een waar data-ecosysteem geworden met een hele sterke community van gebruikers. Lovelace et al. gaan in op de geschiedenis van die spatial pakketten waarin rgdal en sp lang de agenda hebben bepaald.Er zijn programma’s gekomen om data beter te bewerken (zoals dplyr en tidyverse) maar ook specifieke programma’s die daarop afgestemd zijn zaols sf en via dat kun je weer goed met ggplot2, plotly, raster, leaflet, sp en tmap werken. In het eerste deel van het boek leggen ze de basis, die bereiden ze in het tweede deel verder uit en in het derde deel laten ze drie concrete toepassing zien en kijken ze vooruit.\nDe basis\nIn het eerste deel van dit boek gaat in op twee fundamentele manieren om met geografische data om te gaan: vector en raster modellen. Dit zijn de twee modellen die het vak beheersen. Vector data modellen representeren de wereld door punten, lijnen en polygonen te gebruiken. Het raster model deelt de werkelijkheid in cellen van gelijke omvang in. Vector modellen worden vooral in sociale wetenschappen gebruikt en raster modellen komen we vooral in omgevingswetenschappen tegen. Tegenwoordig is sf het pakket waar verctor modellen in bewerkt worden. Het pakket is betrekkelijk nieuw en incorporeert eerdere pakketten die hiervoor gebruikelijk waren (sp, rgeos en rgdal).\n\n\n\nsf leest en schrijft data makkelijk, het kan goede figuren maken, het behandelt de hele dataset (inclusief geografische data) als een data frame. Daarom werkt het goed met het databewerkingspakket tidyverse en de functies waarmee gewerkt worden zijn consistent en intuïtief om mee te werken. Op eenvoudige manier kun je eruit krijgen wat je wil.\n\n\n\nRaster modellen is de andere manier om kaarten te maken.\n\n\n\nVoor geografische data-analyse zijn Coördinaten Referentie Systemen belangrijk waarmee 3D in 2D wordt omgezet. Kaarten kunnen er, afhankelijk van het systeem dat wordt gebruikt, anders uitzien. Lovelace en companen leggen verschillende systemen uit en ook hoe deze om te zetten zijn. Vervolgens gaan ze in op Attribute data operaties, waarbij het gaat om niet-geografische informatie die wel wel met de geografische data te maken hebben. Een bepaalde hoogte hoort bij een bepaalde ruimte, bv.. In sf zijn deze gegevens goed te verwerken en je kunt er makkelijk een subset van een dataset mee maken (uit de gegevens van de wereld kun je bijvoorbeeld de gegevens voor Europa halen of je wil alleen maar de landen afbeelden waar de gemiddelde levensverwachting groter dan 82 jaar is). Je kunt ook gegevens uit andere datasets aan een geografische dataset koppelen met sf. Een voorbeeld: de wereldgegevens gekoppeld aan koffie data:\n\n\n[1] \"sf\"         \"data.frame\"\n\n\nUitbreiding\nHalverwege het boek laten Lovelace et al. zien hoe je vandaag de dag goed kaarten kunt maken. Ze openen een aantal standaardpakketten\n\n\n\nen enkele pakketten om verder te visualiseren:\n\n\n\nZe werken vooral met tmap omdat dat een goed visualisatiepakket is (gemaakt door de Nederlander M. Tennekes), het werkt net als sf goed met tidyverse en je kunt er statische en interactieve kaarten mee maken, in een handomdraai. Het werkt laag voor laag. Het zo opgebouwd, als we bijvoorbeeld de kaart van Nieuw Zeeland willen laten zien:\n\n# Eerst vul je de vorm aan\ntm_shape(nz) +\n  tm_fill() \n# Dan de grenzen\ntm_shape(nz) +\n  tm_borders() \n# Dan vorm, vulling en grenzen\ntm_shape(nz) +\n  tm_fill() +\n  tm_borders() \n\nZe laten heel veel verschillende toepassingen zien.\n\nma1 = tm_shape(nz) + tm_fill(col = \"red\")\nma2 = tm_shape(nz) + tm_fill(col = \"red\", alpha = 0.3)\nma3 = tm_shape(nz) + tm_borders(col = \"blue\")\nma4 = tm_shape(nz) + tm_borders(lwd = 3)\nma5 = tm_shape(nz) + tm_borders(lty = 2)\nma6 = tm_shape(nz) + tm_fill(col = \"red\", alpha = 0.3) +\n  tm_borders(col = \"blue\", lwd = 3, lty = 2)\ntmap_arrange(ma1, ma2, ma3, ma4, ma5, ma6)\n\nZe gaan in dit hoofdstuk in op allerlei aspecten van het kaarten maken: de objecten zelf, de esthetica, de kleur, de lay-out en ook om verschillende kaarten naast elkaar te kunnen plaatsen (bv van 1970, 1990, 2010 en 2030).\n\nurb_1970_2030 = urban_agglomerations %>% \n  filter(year %in% c(1970, 1990, 2010, 2030))\n\ntm_shape(world) +\n  tm_polygons() +\n  tm_shape(urb_1970_2030) +\n  tm_symbols(col = \"black\", border.col = \"white\", size = \"population_millions\") +\n  tm_facets(by = \"year\", nrow = 2, free.coords = FALSE)\n\nZe laten ook zien hoe je animaties kunt maken, interactieve kaarten en hoe je met andere pakketten kaarten kunt maken (mapview, leaflet, plotly, ggplot). Dit is een belangrijk hoofdstuk om zelf door te nemen met jouw computer op jouw schoot zoals ik dat nu heb. Een apart hoofdstuk besteden ze aan de relaties met andere GIS-pakketten. Omdat ik daar niet mee werk, laat ik dit voor hier liggen. Je hoeft niet alleen anderen te volgen in wat zij hebben gedaan maar je kunt ook jouw eigen scripts en algoritmes schrijven en de Lovelace et al. nodigen je daarvoor uit. Hoofstuk 11 gaat in op statistisch leren en machine learning. Ze geven een voorbeeld van supervised en unsupervised leren, verdelen de dataset in een training en een testset en volgen met het pakket mlr een aantal standaard stappen om te kunnen voorspellen. Het boek laat je de toekomst proeven.\nTot slot\nIn de laatste hoofdstukken hebben de drie schrijvers elk een hoofdstuk genomen om te laten zien hoe geocomputation op hun vakgebied is te gebruiken. Lovelace heeft het hoofdstuk over Transport geschreven en werkt een case study van Bristol uit.\n\n\n\nNovosad werkt het onderwerp Geomarketing uit aan de hand van een studie over fietswinkels in Duitsland.\n\n\n\nMuenchov werkt tot slot een ecologisch studie onderwerp in Peru uit\n\n\n\nHet laatste hoofdstuk is een concluderend hoofdstuk waarin ze vaststellen dat de manier waarop ze met sf en tidyverse in dit boek de ruimtelijke data benaderen een manier is om met de werkelijkheid om te gaan. Veel van wat ze hier presenteren kan ook met sp en rdal bereikt worden zoals de afgelopen jaren gebruikelijk. Deze nieuwe manier, hun manier, zal zich de komende jaren verder ontwikkelen. De keus van de pakketten is aan de gebruiker. Dit boek is de basis van geocomputation en veel wordt ook niet in het boek besproken. Daarvan zijn ze zich bewust, of het nu om big data gaat of andere analysetechnieken. Hierover is elders meer te vinden. Maar met hun kennis in de achterzak kom je wel verder in het eigen maken van deze andere technieken, kun je meer ontdekken en ook meer daarvan leren. Hun boek, en daarom vind ik het ook zo goed, toont vooral hoe een open-source benadering kan werken. Het leert je creatief met data omgaan, je leert echte problemen op te lossen, gebruik te maken van goede wetenschappelijke tools en het leert je kennis te delen en te reproduceren. Het daagt je uit verder te kijken, voort te bouwen op wat anderen hebben gedaan en alles praktisch toe te passen. Het daagt je uit hier zelf mee aan de slag te gaan en daarin samen te werken met anderen.\nEerder had ik de html-versie gelezen en afgelopen weken heb ik met veel plezier dit boek gelezen, een groot aantal presentaties van Lovelace op YouTube bekeken en enkele powerpoint presentaties bekeken. Ik heb verschillende syntaxen geprobeerd en ik kan je zeggen: deze schrijvers helpen je echt om bij de tijd te blijven en achgterstand in te halen. Het plezier en het gemak straalt er vanaf. Het is stimulerend en het zijn geen nerds. Ze hebben wat te vertellen en willen dat deze ontwikkelingen ons verder helpen. Geocomputation with R; het is een fantastisch boek.\nBoek: Lovelace,R., Nowosad, J.& Muenchow, J. (2019). Geocomputation with R. London: Chapman & Hall. 353 p.\n\n\n",
    "preview": "posts/2019-08-31-geocomputation/geocomputation_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2020-03-13T23:28:58+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-08-17-xaringan/",
    "title": "Xaringan",
    "description": "Een mooie presentatie geven met het pakket Xaringan",
    "author": [
      {
        "name": "Yuhui Xie, overgezet Harrie Jonkman",
        "url": "www.harriejonkman.nl"
      }
    ],
    "date": "2019-08-17",
    "categories": [],
    "contents": "\nPRACHTIGE HTML-SLIDES MAKEN MET XARINGAN\nOm mij dit goed eigen te maken heb ik de post van Fabio Votta (7 augustus 2018) en de presentatie van Yihui Xie vertaald (zie: Prachtigepresentaties.rmd). Het lijkt simpel, maar voor mij is dit een goede manier om het eigen te maken. Nadat ik dit heb gedaan, heb ik er zelf enkele zaken aan toegevoegd die ik handig vind of die nieuw zijn.\nIntroductie\nEr zijn vele manieren om presentaties te maken met R Markdown. Het pakket xaringan, gebouwd door Rstudio medewerker Yihui Xie, is een van de mogelijkheden om attractractief vormgegeven HTML5 slides te maken die echt opvallen. De mooie layout en de beknopte en nette codeerstructuur maakt het aangenaam om mee te werken. Het pakket is gebaseerd op de remark.js bibliotheek en als je er meer over wilt weten kun je dit hier (https://remark.js.org/) doen. Hier zijn enkele voorbeelden van dia’s van de maker die ook als introductie dienen:\nhttps://slides.yihui.name/xaringan/\nBen je klaar om een presentatie ninja te worden?\nInstallatie\nEr zijn verschillende manieren om pakketten in R te installeren. Een veilige manier is het om het xaringan pakket van CRAn te installeren, en wel op de volgende manier\ninstall.packages(\"xaringan\")\nSoms zijn de versies op CRAN iets ouder. Als je geïnteresseerd bent in de nieuwste versie, is het altijd een optie om te zoeken naar de gerelateerde (GitHub) repository. Om het vanaf hier te installeren, moet je eerst het devtools- pakket installeren voor het geval je het niet hebt en vervolgens install_github(\"yihui/xaringan\") gebruiken om het pakket vanuit GitHub te installeren. ## installeer alleen devtools indien nodig ## devtools::installeer_github(‘yihui/xaringan’)\nOm een xaringan-presentatie te maken, maak je een RMarkdown document met het xaringan::moon_reader outoutformat. Het is makkelijk om binnen RStudio te beginnen. Klik eert op het New File icoon en dan op RMarkdown zoals hieronder\n\nSelecteer dan vervolgens “From Template” de Ninja Presentation en je zult een RMarkdown voorbeeld zien:\n\nDruk op de Knit-knop en voeg de boel samen.\nBasis\nZoals bij elk R Markdown document, is er een yaml header aan het begin die enkele meta data specificeert. Hier kun je de titel van de presentatie, auteurs en meer opgeven. Dit configureert de titeldia. Hier kan nog veel meer gedaan worden om je dia’s aan te passen en vast te stellen hoe ze eruit zullen zien, maar voorlopig houden we ons aan de basis. Hier is hoe uw yaml header eruit zou kunnen zien:\n\nDat geeft het volgende resultaat:\n\nZo worden slides met xaringan gemaakt. Makkelijk!\nOver het algemeen gelden de gewone R Markdown-regels, zodat je deze cursief kunt gebruiken en twee sterren voor bijvoorbeeld vetgedrukte of zelfs je meest ingewikkelde LaTeX-vergelijkingen.\nHoe meer hashtags je toevoegt, hoe kleiner de header.\nDe dia’s worden dan gescheiden door drie streepje— Voor de eerste dia hoeft u dit niet te doen omdat die na de yaml header verschijnt.\nAls we elementen op de dia’s met een klik willen laten verschijnen, scheiden we ze met twee streepje, – zoals deze.\nTenslotte, als we meer ruimte tussen de elementen op een dia willen hebben, kunnen we de html-tag  gebruiken.\nJe zou met zoiets kunnen beginnen:\n\n# Slide 1\n\nDit is slide 1\n\n* Item 1\n* Item 2\n    + Item 2a\n    + Item 2b\n  \n\n---\n\n# Slide 2\n\nDit is slide 2\n\nHier een moeilijke vergelijking:\n\n$$S (ω)=1.466\\, H_s^2 \\,  \\frac{ω_0^5}{ω^6 }  \\, e^[-3^ { ω/(ω_0  )]^2}$$\n\n---\n\n# Slide 3\n\nDit is slide 3\n\n--\n\nDit verschijnt met een klik\n\n--\n\n<br>\n<br>\n\nDit verschijnt ook met een klik maar wat later\n\n\n\nDit ziet er al goed uit.\nHet opmaken van dia’s\nLaten we zeggen dat we een beetje met het formaat van onze dia’s willen spelen. Er zijn enkele ingebouwde functies waarmee we precies dat kunnen doen. We definiëren de volgende code aan het begin van een dia:\n\n---\nclass: inverse, center, middle\n# Statistische Analyse\n\nHierdoor wordt de kleur (hier: zwart) omgekeerd door de elementen horizontaal (midden) en verticaal (midden) op die dia te centreren. Dit zorgt voor een aantal mooie koele overgangsplaten.\nDat geeft de volgende slide:\nEen GIF of een afbeelding toevoegen\nU kunt ook GIF’s of afbeeldingen aan uw dia’s toevoegen. Dit gebeurt op precies dezelfde manier als bij een normaal R Markdown document. Hier is een voorbeeld:\n\n---\n\nclass: inverse, center, middle\n\n![](https://www.ukcophumour.co.uk/wp-content/uploads/f2w/1526816_674334732588821_1244473478_n.jpg)\n\n ## Inzet van thema’s en xaringanthemer\nWe hebben al een lange weg afgelegd om mooie xaringan dia’s te maken. Als we het thema van de presentatie wilden aanpassen, dan wordt xaringan geleverd met een aantal ingebouwde kleurenschema’s die je kunt uitproberen. Hier is er een van. Voeg gewoon de volgende regel toe in je YAML header en je bent klaar om te gaan:\n\noutput:\n  xaringan::moon_reader:\n    css: [metropolis]\nHier is een rijtje thema’s: - metropolis - hygge - rladies Als u uw eigen thema’s wilt creëren, kan ik u het xaringanthemer pakket aanbevelen: https://github.com/gadenbuie/xaringanthemer\nDe presentatie exporteren naar .pdf\nDe output van xaringan is html-format. Als u echter een .pdf-bestand wilt, kunt u de dia’s gewoon openen in uw favoriete webbrowser en ze afdrukken naar .pdf. Dit lijkt de gemakkelijkste versie om dit te doen.\nWat nu?\nIk hoop dat je genoten hebt van deze kleine tutorial!\nHier zijn nog enkele voorbeelden van mooie xaringan dia’s die als inspiratiebron kunnen dienen: https://github.com/favstats/xaringan_slides/\nWilt u animatieovergangen toevoegen? Nou, hier is een implementatie daarvan: https://www.garrickadenbuie.com/blog/2018/12/03/animate-xaringan-slide-transitions/\nTot slot helpt een wiki om meer geavanceerde opties te implementeren. Zorg ervoor dat je het op een gegeven moment bekijkt: https://github.com/yihui/xaringan/wiki\n\n\n",
    "preview": {},
    "last_modified": "2020-03-13T23:28:58+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-06-24-bookdown/",
    "title": "Bookdown",
    "description": "Hoe maak je een boek. Bookdown is het pakket van R waar dat mee kan. Hier enkele tips om dat te doen",
    "author": [
      {
        "name": "Jack Dougherty en Ilya Ilyankou, bewerking Harrie Jonkman",
        "url": "www.harriejonkman.nl"
      }
    ],
    "date": "2019-06-24",
    "categories": [],
    "contents": "\nIntroductie\nHier zie je een klein open-source boekje dat met R/RStudio, het pakket Bookdown en het programma GitHub is gemaakt. Voor deze leidraad heb ik gebruik gemaakt van een stukje stukje tekst dat Jack Dougherty and Ilya Ilyankou hebben gemaakt voor hun open-source boek link.\nOok hun boek is gemaakt met open-source-gereedschappen, het pakket Bookdown en de programma’s RStudio en GitHub. Als je in Bookdown werkt ben je als auteur in staat om met Markdown te werken (een makkelijke computer syntaxtaal om op verschillende platforms mee te lezen en te schrijven). Met die taal kun je ook publiceren in verschillende formats (HTML web-taal, PDF, ePUB en Microsoft Word documenten). Als je het boek host in een openbaar toegankelijke GitHub repository en het met GitHub Pages publiceert, wordt de tekst van het boek en het product toegankelijk op het web. Over Markdown en RMarkdown heb ik in eerdere blogs van Harrie’s hoekje geschreven. Voor de technische basisgids om met Bookdown te publiceren kunnen we verwijzen naar Yihui Xie, Bookdown: Authoring Books and Technical Documents with R Markdown, 2018, link.\nEen alternatief platform voor het uitgeven van boeken is Pressbooks. Deze open-source bron van WordPress ondersteunt ook meerdere publicatie formaten (HMTL, PDF, ePUB). Auteurs kunnen gebruik maken van de betaalde hosting service http://Pressbooks.com. Gebruikers met geavanceerde WordPress-expertise en wat systeembeheervaardigheden kunnen de code ook downloaden van [link] (http://github.com/pressbooks) en hun eigen zelf gehoste boekenuitgeverij runnen.\nSetup RStudio, Bookdown en TinyTeX instellen\nHieronder volgen enkele stappen die je moet zetten om een boek op te zetten zoals hierboven geformuleerd. Het zijn algemene principes die van toepassing zijn op verschillende computersytemen. Je hebt geen speciale kennis nog, maar het is allemaal makkelijker als je avontuurlijk bent of al bekend bent met R Studio, GitHub en/of computercodes.\nInstalleer het programma R link om jouw boek te bouwen met Bookdown. (Ja, boeken kun je ook met een statistisch pakket maken en publiceren, en het werkt!). Zie link\nIs R de motor, RStudio is het dashboard voor jou. Installeer daarom de gratis RStudio Desktop versie om R gemakkelijker te gebruiken met een visuele editor link. Zie link\nSelecteer in RStudio het tabblad Pakketten en selecteer Installeren. Zie link\nIn RStudio installeer je het ‘bookdown’-pakket om jouw boek te bouwen en gebruik je het pakket (library(bookdown). Zie link\nBookdown zou nu met succes geïnstalleerd moeten zijn in RStudio. Zie link\nIn RStudio installeer je ook het ‘tinytex’-pakket voor Bookdown. Daarmee kun je ook een PDF-editie van het boek te maken. Zie link\nVergeet niet: in RStudio console tinytex::install_tinytex() te typen en druk op return om de installatie te voltooien. Zie link\nIk hoop dat dit met tinytext lukt. Ikzelf werk niet met tinytex maar heb een latex-programma op mijn computer staan die de pdf-versie maakt.\nDownload en bouw een voorbeeldboekje\nMaak een gratis GitHub-account link aan om de codes te delen en online boekuitgaven te publiceren.\nIn jouw webbrowser, log je in op jouw GitHub account, vervolgens ga je naar de minimale repo link van de software ontwikkelaar en maak je een kopie voor jouw GitHub account. Hoe je dat doet daarover je in dit hoofdstuk link van het dataviz-boek.\nInstalleer GitHub Desktop link om bestanden over te brengen tussen jouw online GitHub repo en uw lokale computer.\nGa in je webbrowser naar het exemplaar van bookdown-minimal en klik op de groene knop Clone of Download het en selecteer Open in Desktop. Dit opent automatisch de GitHub Desktop applicatie. Zoek de code en kopieer deze naar een map op uw lokale computer.\nIn RStudio in de rechterbovenhoek selecteert u Project > Open Project om de bookdown-minimal map op uw lokale computer te openen. Zie link\nOpen in RStudio het bestand index.rmd en maak enkele eenvoudige bewerkingen op de tekst van dit minimale boek. Verwijder bijvoorbeeld het hashtag # commentaarsymbool in regel 8 om commentaar te verwijderen en activeer de optie PDF-boek. Bewaar uw bewerkingen. Zie link\nOptioneel: Gebruik jouw favoriete teksteditor, bv. Atom editor link, om de tekst te wijzigen maar het kan ook goed met RStudio.\nIn RStudio, rechtsboven in de hoek, selecteer je de Build Book tab, selecteer je Build Book, en kies je All Formats om zowel de gitbook-stijl statische webeditie als de PDF-editie te bouwen.\nAls RStudio met succes beide edities van je minimale boek bouwt, zal de output worden opgeslagen in je bookdown-minimal map. Dit wordt in een submap met de naam: _book geplaatst, omdat dit voorbeeld zo is geconfigureerd. Open ook de submap en bekijk de PDF-editie. Als RStudio fouten heeft gevonden, zullen deze in de Build viewer verschijnen. Zie link\nTip: In toekomstige sessies met RStudio moet u mogelijk het tabblad Pakketten selecteren en op Update klikken om uw softwarepakketten up-to-date te houden. Zie link\nSluit RStudio.\nPubliceer je boek met GitHub Pages…\nOpen het GitHub-bureaublad en navigeer naar de map bookdown-minimal op uw lokale computer. Schrijf een samenvatting om de wijzigingen die u hierboven gemaakt hebt vast te leggen (op te slaan) aan uw master branch, en duw (pull) deze versie naar uw online GitHub repo. Over GitHub zal ik binnenkort nog eens een blog schrijven.\nGa in jouw webbrowser naar jouw online GitHub repo, met een webadres gelijkaardig aan https://github.com/USERNAME/bookdown-minimal (vul je GitHub gebruikersnaam in).\nIn je GitHub repo, selecteer Settings, scroll naar beneden naar de GitHub Pages sectie (dat is een gratis web hosting service om je code en boekuitgaven op het publieke web te publiceren). Selecteer Master Branch als jouw bron, en sla het op.\nScroll weer naar beneden naar deze sectie, en het webadres van uw gepubliceerde site zou moeten verschijnen. Kopieer dit adres.\nPlak het webadres van bovenaf in een nieuw browsertabblad en voeg uiteindelijk _book/index.html toe, omdat dit voorbeeld is geconfigureerd om de webeditie van uw boek in deze submap op te slaan. Uw webadres moet vergelijkbaar zijn met: https://USERNAME.github.io/bookdown-minimal/_book/index.html.\nUw Bookdown en GitHub instellingen aanpassen\nOm de aangepaste instellingen voor dit boek te bekijken, ga naar de online repository https://github.com/datavizforall/dataviz-bookdown\nIn het _bookdown.yml bestand is de uitvoermap ingesteld om alle boekformaten in de docs map te bouwen.\nDe GitHub Pages Settings voor deze repo (die je niet kunt bekijken) is ingesteld om te publiceren vanuit de master/docs map, zodat ze overeenkomen met de uitvoer map hierboven. Dit vereenvoudigt het gepubliceerde webadres naar dit formaat: https://USERNAME.github.com/REPONAME\nDe meeste van de Bookdown configuratie-instellingen verschijnen in het bestand index.Rmd. Lees meer over deze opties in de technische handleiding van de software ontwikkelaar, (https://bookdown.org/yihui/bookdown/).\nDaarnaast is deze GitHub Pages repo gepubliceerd met een aangepaste domeinnaam https://DataVizForAll.org. Meer informatie over de aangepaste domeinnamen vind je op https://help.github.com/articles/using-a-custom-domain-with-github-pages/, waarvoor je een domeinnaam moet kopen bij een webhostingdienst (zoals http://ReclaimHosting.com). Door het toevoegen van een GitHub Pages aangepaste domeinnaam creëer je een extra CNAME-bestand in de submap docs. Wees voorzichtig om het niet te verwijderen (of plaats een kopie in een submap voor de bewaring).\nDit boek bevat ook een aangepast 404.html-bestand dat handmatig werd overgezet naar de submap van de documenten, aangezien het niet automatisch wordt gebouwd door Bookdown.\nDit boek bevat ook een aangepaste google-analytics-datavizforall.html bestand in het root-niveau van repo (waar bookdown naar zoekt) en wordt ook handmatig overgebracht naar de docs submap (aangezien bookdown het niet lijkt te kopiëren naar daar bij elke build). Deze volgt het webverkeer met Google Analytics.\nNog enkele tips\nLinken met gehoekte en ronde haakjes.\nGebruik gehoekte én ronde haakjes voor link in de tekst.\nGebruik alleen ronde haakjes voor een niet-ingesloten link (http://example.com).\nOok, geef URL met ronde haakjes weer: (http://example.com)\nGebruik indien nodig HTML om een link te maken die in een nieuwe pagina wordt geopend.\nDit Den Haag-boekje, bestaat uit acht hoofdstukken. Elk hoofdstuk is een apart Rmd-bestand. De numerieke volgorde van de bestandsnamen (index.Rmd, 0.1-AntilliaanseNederlandersinDenHaag.Rmd, 0.2-opleidingsniveauouders.Rmd, 03-werkeninkomen.Rmd, 04-zorggebruikalgemeen.Rmd, 05-zorggebruijeugd.Rmd, 06-gezondheid.Rmd, 07-criminaliteit.Rmd, 08-conclusies.Rmd) bepaalt de volgorde waarin ze in het gebouwde boek verschijnen.\nIn het index.Rmd bestand staat in de instellingen voor het webboek dat elk hoofdstuk en sectie wordt opgesplitst in een eigen HTML pagina, zonder automatische nummering. split_by: section number_sections: false\nHoofdstukken beginnen met een eerste niveau koptekst (één hashtag: #) en secties beginnen met een tweede niveau koptekst (twee hashtags: ##). Zowel de hoofdstuk- als sectietitels worden onmiddellijk gevolgd door een korte ID tussen gehoekte haakjes om kruisverwijzingen in het boek mogelijk te maken. De korte ID’s MOETEN uniek zijn, en moeten idealiter overeenkomen met de .Rmd-bestandsnaam. Hoewel elke hashtag hoofdstuk/sectie titel de standaard ID is (zoals #Inleiding in index.Rmd), is het veiliger om voor elk hoofdstuk/sectie een unieke korte ID te maken om verwarring te voorkomen.\nDe cursief gedrukte lijnen staan bovenaan elk hoofdstuk of sectie, met een kruisverwijzing naar de auteurspagina, gevolgd door de laatste bijgewerkte datum. Voorbeelden van hoofdstukkop, met korte ID’s, bylines en data: # Hoofdstuk Titel {#Hoofdstuk} *by[Esther Horrevorts, Hans Bellaarts](authors), last updated: June 18, 2019*\nSubrubrieken binnen de hoofdstukken en secties beginnen met drie hashtags en eindigen met een symbool zonder nummering {-}.\nOm een interne kruisverwijzing naar een hoofdstuk of sectie in het boek in te voegen, kun je een link invoegen met haakjes [voor de tekst] en haakjes (met de korte ID). Als verwijzing in de tekst verschijnt, voeg dan de Furfase “in dit boek” toe aan het einde van de zin, zodat we in de toekomst alle kruisverwijzingen kunnen zoeken/vinden als dat nodig is:\nZie de [GitHub tutorial](github) in het Dataviz-boek\nOPMERKING: de bovenstaande verwijzingen zijn ontworpen voor het HOSTEN van WEB BOOEKEN, en werken mogelijk niet hetzelfde in lokale repo versies, of in ebook/PDF versies.\n\n\n",
    "preview": {},
    "last_modified": "2020-03-13T23:28:58+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-05-19-website-maken/",
    "title": "Websites maken in R",
    "description": "Met R kun je ook website maken. Maar hoe doe je dat? Emily Zabor schreef hierover een leerzaam blog dat ik hier licht heb bewerkt en aangevuld.",
    "author": [
      {
        "name": "Emily C. Zabor, bewerking Harrie Jonkman",
        "url": "www.harriejonkman.nl"
      }
    ],
    "date": "2019-06-09",
    "categories": [],
    "contents": "\nDeze tutorial is van Emily C. Zabor die ik heb bewerkt, vooral ook om te zien of ik mijn eigen website kan maken. Deze tutorial laat je zien hoe je een website maakt met gebruik van R, RMarkdown en GitHub.\nDeze tutorial presenteerde zij voor het eerst op R Gebruikers Groep Bijeenkomst op 23 Januari 2018 op het Memorial Sloan Kettering Cancer Center Department van Epidemiologie and Biostatistiek.\nDeze versie ververste en presenteerde zij op de R Dames NYC Bijeenkomst op 15 Februarie 2018.\nTypen websites\nDe belangrijkste typen websites die je zou willen maken zijn:\nPersoonlijke websites\nWebsites om een pakket te presenteren\nProject websites\nBlogs\nDe basis van R Markdown website\nWat jij minimaal nodig hebt voor een R Markdown website zijn:\nindex.Rmd: bevat de inhoud van de homepage van de website\n_site.yml: bevat de metadata voor de website\nEen basis voorbeeld voor een _site.yml voor een website met twee pagina’s kan er zo uitzien:\n\n\n\nAnd a basic index.Rmd to create the Home page:\n\n\n\nHier vind je een overzicht van de basis van R Markdown website hier.\nGitHub\nDeze tutorial left de nadruk op het hosten van websites via GitHub. Hosten van websites via GitHub is gratis.\nAls je nog geen GitHub account hebt, teken dan op via https://github.com/join?source=header-home met username YOUR_GH_NAME. Ik zal naar deze username, YOUR_GH_NAME, als “jouw GitHub username” refereren in deze hele tutorial.\nEr zijn andere mogelijkheden om jouw website gratis te hosten. Een ander populaire keuze is Netlify.\nPersoonlijke websites\nEen voorbeeld van een homepage van Emily Zobore’s website is:\n\nEr zijn twee belangrijke stappen om een persoonlijke website te maken die op GitHub wordt gehost:\nGitHub setup\nLokale setup\nGitHub setup\nCreëer een GitHub repository (“repo”) genaamd YOUR_GH_NAME.github.io, waar YOUR_GH_NAME jouw GitHub username is.\nInitialiseer het met een README\nVoor hen die met GitHub weinig ervaring hebben: dit kan het proces van klonen van repository en daarmee het afstemmen met de ‘master branch’ vergemakkelijken.\n\nLokale setup\nKloon deze remote repository op een locale directory met dezelfde naam, YOUR_GH_NAME.github.io\nVoeg een R Project toe aan deze directory\nMaak een _site.yml en een index.Rmd file in jouw nieuwe directory\nWaarom heb ik een R Project nodig?\nHet R Project is gemakkelijk omdat RStudio jouw project als een website zal herkennen en zorgt voor de goede gereedschappen die je nodig hebt.\nOpgelet: Nadat je een R Project met de goede files hebt gemaakt, kan het zijn dat je het project moet sluiten en heropenen voordat R het herkent als een website en de goede gereedschappen toont.\nCreëer inhoud\nPas de _site.yml file aan door de metadata te veranderen en het thema van jouw website. Kijk maar eens naar de Jekyll thema’s hier en speel wat met de verschillende opties. Thema’s zijn makkelijk te veranderen, zelfs nadat je de inhoud hebt toegevoegd.\nBijvoorbeeld de _site.yml voor de persoonlijke website van Emily ziet er zo uit:\n\n\n\nPas aan en creëer .Rmd files met de inhoud van jouw website, die er html-pagina’s voor jouw website van maken als jij ze ‘knit’.\nDe index.Rmd file voor de homepage van Emily’s persoonlijke website ziet er zo uit:\n\n\n\nAls je een keer de inhoud hebt geschreven en de lay-out hebt opgezet, zoek dan de Build tab in RStudio op en selecteer “Build Website”:\n\nNu heeft jouw local directory alle files die nodig zijn om jouw website te maken:\n\nDe website uitzetten\nBasis benadering:\nSelecteer “Upload files” van de hoofpagina pagina van jouw GitHub repository:\n\nEn sleep eenvoudig of selecteer de files van jouw locale repository:\n\nGeavanceerde benadering (aangeraden):\ngebruik Git als cliënt of van binnenuit RStudio (een andere goede reden om een R Project! te gebruiken)\n\nMaar dit is geen Git/GitHub tutorial. Als je meer wilt leren over Git/GitHub, ik raad jou aan dit te doen, dan is dit een goede bron om mee te beginnen: http://happygitwithr.com/\nAangepaste domeinen\nHet standaardadres om jouw wite te hosten is http://YOUR_GH_NAME.github.io, maar je kunt jouw domeinnaam ook aanpassen. Dan zijn er twee stappen te zetten:\nIn jouw GitHub repository YOUR_GH_NAME.github.io, ga je naar Settings > GitHub pages. Typ jouw domeinnaam in de box onder Custom domain en sla het op (Save).\n\nVoeg een CNAME file toe aan jouw GitHub repository YOUR_GH_NAME.github.io.\nHet zal als volgt in jouw repository verschijnen:\n\nAnd inside the file you will simply have your domain name:\n\nPakket websites\nEen voorbeeld hiervan is deze website van Emily’s R-pakket ezfun:\n\nGebruik Hadley Wickham’s goede pakket pkgdown om makkelijk een website van jouw pakket te maken die op GitHub wordt gehost. Details over pkgdown kun je hier vinden de pkgdown website, die ook met inzet van pkgdown is gemaakt.\nDit veronderstelt wel dat je al een R-pakket met een locale directory hebt en een GitHub repository.\nFrom within your package directory run:\n\n\n\nDit zal een folder toevoegen met de naam docs binnen de locale directory voor jouw pakket\nUpload/push deze veranderingen in de GitHub repository voor jouw pakket\nIn the GitHub repository voor jouw pakket ga je naar Settings > GitHub pages. Selecteer “master branch/docs folder” als de bron en sla op (Save)\n\nDe persoonlijke pagina zal worden toegevoegd aan jouw persoonlijke website en aan YOUR_GH_NAME.github.io/repo_name\nDe homepage kun je via README file op jouw repository binnenhalen\nDe referentiepagina van de site omvat alle functies met hun beschrijving\nElke functie klikt door naar de hulppagina ervan,\nEn in bepaalde gevallen ook naar vignettes met goede informatie\nEn dan ben je klaar, zo makkelijk als dat.\nProject websites\nOok als je geen pakket maakt kun je nog wel een repository maken. Emily Zabore heeft bijvoorbeeld een pagina op haar website die linkt naar de repository waarin deze tutorial is opgeslagen.\n\nLokale setup\nVanuit de lokale directory van het project waar het jou om te doen is:\nCreëer een _site.yml en index.Rmd file in jouw nieuwe directory\nPas deze files met jouw inhoud en layout, net zoals bij persoonlijke websites\nGitHub setup\nUpload/push deze nieuwe files in de GitHub repository voor jouw project\nGa naar GitHub pagina’s voor de repository en ga naar Settings > GitHub Pages, waar je de “master branch” folder selecteert en je drukt op Save\n\nBlogs\nR Markdown websites zijn makkelijk te maken en uit te zetten, maar het wordt lastiger als je het voortdurend moet verversen of veranderingen moet aanbrengen, zoals dat het geval is bij een blog. Gelukkig, het R-pakket blogdown bestaat juist voor dit doel. blogdown is een R pakket dat jou in staat stelt statistische websites te makenthat allows you to create static websites, wat betekent dat de uitgezette versie van de website alleen bestaat uit JavaScript, HTML, CSS en plaatjes. Gelukkig is het blogdown pakket zo opgezet dat je over al die zaken niets af hoeft te weten om toch nog een mooie website te maken voor jouw blog, met de ondersteuning van Hugo.\nVoor de beste referentie van blogdown website, kijk naar dun blogdown boekje.\nEmily Zabore heeft geen persoonlijk blog, maar wel een website/blog gebouwd rond deze bijeenkomsten in New York R-Ladies NYC en dat is hier als voorbeeld toegevoegd.\n\nSetup\nDe eerste drie stappen zijn hetzelfde als het maken van een basis R Markdown website:\nCreëer een GitHub repository met de naam YOUR_GH_NAME.github.io, waar YOUR_GH_NAME jouw GitHub gebruikersnaam is, geïnistialiseerd met een README file\nKloon deze GitHub repo op een lokale directory met dezelfde naam\nVoeg een R Project aan jouw lokale directory toe\nDan beginnen we met blogdown.\nInstalleer blogdown en Hugo\n\n\n\nKies een thema en vind de link naar de thema’s van de GitHub repository. In dit geval zijn de thema’s niet zo makkelijk te wisselen als binnen de basis R Markdown website, dus kies het thema zorgvuldig.\nGenereer een nieuwe site binnen jouw project sessie. De optie theme_example = TRUE zal voor de files van een voorbeeldsite zorgen die je op basis van wat je nodig hebt kunt aanpassen.. “user/repo” refereert naar de GitHub gebruikers naam (user) en de GitHub repository (repo) voor jouw geselecteerde thema.\n\n\n\nDit zal alles van de filestructuur van jouw nieuwe blog genereren.\n\nNadat je dit hebt afgerond, moet je sluiten en dan het project weer heropenen. Als je heropent, zal RStudio het project als een website herkennen.\nHet aanpassen van het beeld\nVeranderingen voer je in de config.toml file door (hetzelfde als de _site.yml die we bij de R Markdown websites tegenkwamen); zo verander je de layout en het beeld van jouw website. De beschikbare kenmerken in de config.toml zullen verschillend zijn afhankelijk van jouw thema en de meeste themavoorbeelden hebben een eigen config.toml die je als template kunt gebruiken.\nAls je een keer de kenmerken van jouw website hebt aangepast, klik dan op RStudio’s “Serve Site” om de site lokaal al te bekijken.\n\nEen nieuwe blog post schrijven\nEr zijn verschillende manieren om een nieuwe blogpost op jouw site te schrijven, maar het is het makkelijkste om dat via “New Post” in RStudio te doen:\n\nDit opent een pop-up waar je de meta-data voor a nieuwe post kunt plaatsen:\n\nIn aanvulling op Titel, Auteur en Datum van de post, kun je aanvullend ook categorieën creëren, die jouw post in folders organiseren en kun je tags aan de posts toevoegen, waarmee ze te zoeken zijn binnen de inhoud van jouw website. Wees er wel van bewust dat het functioneren van deze kenmerken varieert per thema. Bepaalde blogs kunnen ook in toekomst worden geplaatst.\nZie onderaan dat je ook kunt kiezen tussen een regulier markdown (.md) of R markdown (.Rmd) file. .Rmd files moeten worden gerenderd voordat ze html pagina’s genereren. Dus het is het beste dit te gebruiken waar ook een R code in zit.\nEen file naam en een ‘slug’ worden automatisch genereerd gebaseerd op de andere metadata. De ‘slug’ is een URL-vriendelijke titel van jouw post.\n\nPresenteren\nEen blogdown site is een beetje lastig te bouwen en te presenteren via GitHub vergeleken met een reguliere R Markdown website en vergeleken met wat hierboven is beschreven.\nProbleem 1: Omdat het een statistische site betreft, de files genereren automatisch on line in een aparte subdirectory onder de naam public binnen jouw lokale directory. Echter, dit veroorzaakt problemen met het hosten (presenteren) van GitHub omdat de files dan in de lokale YOUR_GH_NAME.github.io directory moeten zitten.\nDe oplossing:\nHou aparte directories voor de bron files (deze directory bijvoorbeeld “source”, bron noemen) en voor de statische files (de directory YOUR_GH_NAME.github.io) die gegenereerd worden. De “source” folder is waar your R project en config.toml files zich bevinden.\n\nGebruik in jouw config.toml de optie publishDir = om blogdown te publiceren via de YOUR_GH_NAME.github.io folder. Dat is eenvoudiger dan de standaard manier op jouw lokale locatie.\n\nProbleem 2: GitHub gebruikt standaard Jekyll met website inhoud en dat moet ongedaan worden gemaakt omdat blogdown sites met Hugo worden gebouwd.\nOm dit op te lossne moet je een lege file toevoegen met de naam .nojekyll in jouw GitHub repo YOUR_GH_NAME.github.io, voordat je het publiseert.\n\nHet pakket Distill\nMijn eigen blog Harrie’s hoekje maak ik met het pakket Distill. Om een blog te maken installeer je Distill. Dan maak je een new blog en gebruik je Distill Blog.\nDan worden er een project gemaakt met een aantal documenten: _site.yml\nindex.rmd\nabout.rmd\n_posts/welcome/welcome.rmd\nVervolgens pas je de *_site.yml* aan op basis van hoe jouw blog eruit moet zien.\nals je een post wilt creëren open je het programma (library(distill)) en je tikt in `create_post(“Hier de naam van de post”). Hier Distill vind je veel meer informatie hierover.Hier [Tensorflow)(https://blogs.rstudio.com/tensorflow/) vind je een ander voorbeeld hoe de blog er dan uit kan zien.\nMet Distill kun je op dezelfde eenvoudige manier een website maken Distill-website. Nadat je het Distill hebt geinstalleerd en geopend (library(Distll)). Krijg je de volgende documenten _site.yml (om de website te configureren)\nindex.rmd (voor de homepage)\nabout.rmd (waar de website over gaat)\nVerder werkt dit hetzelfde.\nAanvullende bronnen\nHieronder vind je de aanvullende bronnen en linken waar in deze tutorial naar wordt verwezen:\nhttp://rmarkdown.rstudio.com/rmarkdown_websites.html: an overview of R Markdown website basics\nhttp://jekyllthemes.org/: Jekyll themes for use with your R Markdown website\nhttp://happygitwithr.com/: an introduction to Git/GitHub\nhttp://pkgdown.r-lib.org/: Hadley Wickham’s pkgdown website\nhttps://bookdown.org/yihui/blogdown/: Yihui Xie’s blogdown book\nhttps://themes.gohugo.io/: Hugo themes for use with your blogdown website\n",
    "preview": {},
    "last_modified": "2020-03-13T23:28:58+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-04-23-shiny_files/",
    "title": "Shiny lespakket",
    "description": "Met Shiny kun je in R apps maken. Maar hoe doe je dat? Julia Wrobel gaf hierop vorig jaar een interessante inleiding die ik hier licht heb bewerkt.",
    "author": [
      {
        "name": "Julia Wrobel, bewerking Harrie Jonkman",
        "url": "www.harriejonkman.nl"
      }
    ],
    "date": "2019-04-23",
    "categories": [],
    "contents": "\nDeze Shiny-tutorial maakte Julia Wrobel voor R Ladies-bijeenkomst in NYC op 8 mei, 2018. Hartelijke dank, Julia, en ik hoop dat je het goed vindt dat ik deze naar het Nederlands heb vertaald en iets heb bewerkt!! Voor mij was het een goede manier om mij Shiny eigen te maken. Naast het vertalen en bewerken van deze tutorial heb ik ook de herziene versie van Chris Beeley gelezen Web Application Development with R Using Shiny en hier en daar informatie hiervan in deze blog verwerkt.\nShiny is het raamwerk van RStudio om interactieve grafieken en webapplicaties te maken in R. Aan het einde van deze tutorial weet je hoe Shiny werkt en kun jij ook een Shiny app maken. Wij maken in deze tutorial gebruik van data uit de Amerikaanse basketbalcompetitie (NBA) en laten we ons inspireren door Todd Schneider’s ballR app.\n\n\n\nHet begin\nVoordat je begint, moet je ervoor zorgen dat je de shiny, plotly, tidyverse en rsconnect pakketten hebt geïnstalleerd. Julia heeft een template gemaakt voor de app template voor onze app en ook een volledige versie. Die kun je downloaden en uitpakken.\n\ninstall.packages(c(\"shiny\", \"plotly\", \"tidyverse\", \"rsconnect\"))\nIn ieder geval moet je onderstaande pakketten steeds binnenhalen, ook al heb je ze geïnstalleerd.\n\n\n\nProbeer onderstaand voorbeeld eens van een eenvoudige shiny app om er zeker van te zijn dat dit pakket goed is geïnstalleerd.\n\n\n\nDe schuifknop (‘slider bar’) stelt jou in staat om het aantal bins in de histogram te veranderen.\nDe basis van Shiny\nElke Shiny-app heeft een ui- en een server-file, die moet je beide definiëren. De ui definieert een webpagina waarmee de gebruiker interacteert. Het controleert de layout en hoe het op beeld verschijnt. De server file is een set van instructions die jouw computer nodig heeft om een app te bouwen. R code wordt op de achtergrond uitgevoerd en de output hangt af van de input van de gebruiker en deze R code.\n\n\nBeeld uit https://deanattali.com/blog/building-shiny-apps-tutorial/\n\n\nHet raamwerk van Shiny\nAlle Shiny apps hebben eenzelfde overall-structuur. fluidPage() controlleert de paginalayout voor de ui. De server is een functie met de argumenten input en output.\n\n\n\nDeze template zelf is een minimale Shiny app. Probeer de code eens te runnen. Kopieer deze template in een nieuwe file die je app.R noemt en bewaar het in een nieuwe folder. Nadat je de file hebt opgeslagen, zie je een Run App knop bovenaan, dat herkent RStudio R Studio als een Shiny app.\n\nEr zijn twee manieren om een Shiny app te maken:\nPlaats zowel de UI als de server code in één file die je app.R noemt en dat is het makkelijkste voor eenvoudige apps. Als je een enkele file gebruikt, moet je de file app.R noemen om de app te runnen.\nCreëer aparte ui.R en server.R files en dat is meer geschikt voor complexere apps. Deze moeten ui.R en server.R worden genoemd. De NBA app die we gaan maken, gebruikt deze benadering.\nJe kunt beide benaderingen rechtstreeks van R Studio halen:\n\nSelecteer Shiny Web App… en het volgende komt naar boven:\n\nSelecteer Multiple File om een app te genereren met aparte ui.R en server.R files. De gegenereerde app zal worden opgeslagen in een aparte folder op het bureaublad die “new_app” heet. Om deze app te draaien kun je het volgende doen:\nOpen de server.R of ui.R file en klik op de Run App knop.\nEnter shiny::runApp(\"~/Desktop/new_app/\") in jouw R-console\nDe input en output argumenten voor de server-functie zijn eigenlijk als ‘lists’ van objecten gedefinieerd in de UI. Deze opties voor input en voor output en de code in de server-file worden renderstatements genoemd. Dit zijn de belangrijkste onderdelen van de meeste Shiny apps en worden hieronder gedefinieerd.\nInput options\nInput opties gaan (meestal) in het ui.R bestand. Input wordt gedefinieerd door middel van functies, die widgets worden genoemd. Dit zijn tekstelementen waarmee een gebruiker kan interacteren, zoals schuifbalken of knoppen. Hieronder staan drie widget-opties en de code die gebruikt wordt om ze te genereren.\n\nAlle input functies hebben een inputId en een label als de eerste twee argumenten. De inputId is een string die aan de server-kant zal worden gebruikt om toegang te krijgen tot de waarde van de input van de gebruiker. Bijvoorbeeld, als inputId = \"slider_widget\" dan zal de server de input$slider_widget gebruiken voor zijn waarde.label is de titel van de widget die in the UI wordt getoond.\nDe hr(), br() en h2() in de voorbeeldcode hierboven zijn ‘wrappers’ voor de html-tags <hr> <br> en <h2>. Er is een hele serie van prachtige html wrapperfunctiesom jou te ondersteunen de interfact aan te passen.\nOutput opties\nOutput opties gaan (ook gebruikelijk) in de ui.R file. Zij definiëren zaken als grafieken en tabellen en geven Shiny aanwijzingen waar ze deze items in de UI moeten plaatsen. Voorbeelden zijn bijvoorbeeld plotOutput(), textOutput(), tableOutput().\nTip om fouten op te sporen: Wees er zeker van dat je een komma hebt geplaatst tussen elke input en output oproep! Komma’s zijn nodig tussen elementen van de UI maar niet van de server, die veel meer als een reguliere R code werkt.\nrender* oproepen\nRender oproepen worden in de server.R file gezet. Zij halen de input weg van de widgets en bouwen reactieve output richting UI. Voorbeelden zijn renderTable() om tabellen te maken, renderText() voor tekst, and renderPlot() voor bepaalde plots.\nInput, output en render oproepen zijn de simpelste voorbeelden van het paradigma reactief programmeren dat Shiny gebruikt. Op het reactieve element gaan we later in meer detail in.\nData van NBA Schoten\nHaal de shiny_nba-folder binnen en pak deze uit. Onderzoek wat er in de folder zit:\neen R project dat shiny_nba.Rproj heet\nde nba_shots.RData met data van schoten van LeBron James, Kevin Durant, Russell Westbrook, Stephen Curry en Carmelo Anthony\neen helper.R file met extra R code voor ons app gebruik\na ui.R file\na server.R file\nR project\nDubbel klik op shiny_nba.Rproj om RStudio te openen. Dit stelt automatisch je werkmap in op de shiny_nba map. Dit maakt het makkelijker de gegevens te laden en de bron van de helpfuncties te vinden. Dit is een van de vele redenen waarom ik altijd R-projecten gebruik als onderdeel van mijn workflow.\nThe data\nDe nba_shots gegevens bevatten 81.383 basketbalschoten genomen van vijf sterren uit de NBA00.\n\n\n# A tibble: 5 x 2\n  player_name       `n()`\n  <fct>             <int>\n1 LeBron James      22381\n2 Kevin Durant      14476\n3 Russell Westbrook 13778\n4 Stephen Curry     10508\n5 Carmelo Anthony   20240\n\nDe gegevens hebben 19 variabelen, met informatie o.a. over schotafstand, nauwkeurigheid, seizoen en locatie op het veld.\nHelper file\nDe helper.R code is afgeleid van Todd Schneider’s ballR app. Hoewel de code er ingewikkeld uitziet, wordt hij alleen gebruikt om de lijnen van een basketbalveld te trekken. Het lege basketbalveld (hieronder) is een ggplot object waar je andere ggplot lagen op kunt plaatsen.\n\n\n\nDeze code is opgeslagen in een apart bestand om de code binnen de Shiny app leesbaarder te maken en omdat dit deel van de code niet verandert met de input van de gebruiker. Het hoeft echter niet als een apart bestand te worden opgenomen - alle code kan in plaats daarvan in het server.R bestand worden geplaatst. Waar statische code als deze te plaatsen, is een keuze die je moet maken bij het bouwen van een Shiny-app.\nPlot van het basketbalveld\nLaten we vervolgens een plot toevoegen. We gaan de locaties van de schotpogingen van de geselecteerde NBA-ster in een bepaald seizoen in kaart brengen, zoals die van LeBron James in zijn eerste seizoen 2003-2004 (zie hieronder).\n\n\n\nPlotly\nSommige van de plots voor onze app gebruiken Plotly, dat is een kader voor het maken van interactieve grafieken die een verscheidenheid aan implementaties heeft, waaronder de plotly bibliotheek in R (zie ook vorige blog). Plotly heeft een aantal aardige voordelen:\nHoogwaardige plots gemaakt met een paar regels code;\nOmdat de interactiviteit (in tegenstelling tot Shiny) geen server nodig heeft, kunnen plots in R Markdown documenten geplaatst worden die op GitHub gehost worden (zoals deze tutorial)!;\nCompatibel met Shiny framework, wat extra interactiviteit mogelijk maakt.\n\n\n{\"x\":{\"visdat\":{\"27644cfd1944\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"27644cfd1944\",\"attrs\":{\"27644cfd1944\":{\"y\":{},\"color\":{},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"box\"}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"legend\":{\"x\":0.2,\"y\":1},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"shot_distance\"},\"xaxis\":{\"domain\":[0,1],\"automargin\":true},\"hovermode\":\"closest\",\"showlegend\":true},\"source\":\"A\",\"config\":{\"modeBarButtonsToAdd\":[{\"name\":\"Collaborate\",\"icon\":{\"width\":1000,\"ascent\":500,\"descent\":-50,\"path\":\"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z\"},\"click\":\"function(gd) { \\n        // is this being viewed in RStudio?\\n        if (location.search == '?viewer_pane=1') {\\n          alert('To learn about plotly for collaboration, visit:\\\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\\n        } else {\\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\\n        }\\n      }\"}],\"cloud\":false},\"data\":[{\"y\":[15,13,16,14,5,19,0,11,3,49,9,7,4,11,0,12,21,17,0,24,18,0,0,23,25,17,17,0,10,9,24,7,0,6,19,23,25,0,0,22,0,15,0,18,22,21,24,22,20,16,0,20,0,0,0,17,22,25,1,0,2,1,9,12,2,3,9,26,2,1,6,26,17,1,9,8,3,2,21,19,1,1,0,14,1,13,18,3,2,0,0,1,0,1,0,0,0,17,16,24,24,15,16,15,13,21,17,24,3,19,15,23,0,13,18,18,24,25,0,24,0,5,0,25,0,23,15,1,24,18,4,24,25,14,15,2,4,9,1,20,0,17,6,49,1,1,0,1,14,19,5,16,21,4,12,0,9,1,0,24,23,6,2,0,13,0,16,4,1,14,2,21,0,0,0,16,23,0,1,17,6,3,25,2,19,25,25,17,14,1,1,1,13,18,3,22,1,19,2,16,24,1,7,3,7,1,25,6,0,12,14,15,15,2,22,19,19,0,0,0,16,24,0,0,0,21,0,23,19,13,0,25,13,17,18,25,18,23,21,17,24,0,0,0,21,17,27,0,26,6,11,1,1,0,15,3,2,1,5,2,0,14,1,24,20,3,22,24,5,8,3,0,2,18,24,24,25,0,24,13,17,0,15,0,19,11,0,0,17,3,7,23,5,3,0,25,0,17,25,26,0,23,0,0,19,16,6,24,0,8,25,19,0,18,0,20,14,13,0,18,25,2,15,15,16,11,11,13,15,18,19,14,14,15,20,1,5,0,12,0,5,3,8,0,6,23,38,18,15,13,17,2,14,16,15,17,16,24,0,12,15,22,20,0,1,0,23,16,14,10,22,24,24,15,16,24,0,25,9,4,12,1,23,23,2,19,1,18,1,1,22,9,1,25,12,6,16,2,12,7,17,24,5,12,12,16,24,17,13,10,6,27,26,22,18,0,11,10,25,5,0,0,19,0,0,24,25,0,14,27,26,26,10,23,6,13,14,19,7,9,23,0,0,11,15,17,26,20,0,25,25,9,0,18,23,0,17,15,19,1,20,6,1,11,5,23,2,1,11,24,5,26,14,22,8,17,19,3,24,23,0,8,23,18,18,24,22,0,22,23,25,0,21,20,11,0,47,21,8,12,20,18,25,5,0,13,7,25,27,17,10,25,3,2,2,13,12,10,13,13,1,25,8,9,25,17,3,1,3,3,1,23,2,2,23,19,17,2,11,0,0,0,1,14,11,3,2,12,14,1,1,1,0,0,0,24,23,0,15,2,21,21,10,12,24,3,15,16,11,3,16,2,8,23,0,24,18,13,0,9,7,0,17,16,0,22,20,23,12,23,32,26,0,13,0,14,22,4,0,3,0,4,22,5,1,2,0,1,22,21,21,2,8,17,17,0,21,0,6,6,14,7,0,21,10,19,18,24,19,6,24,10,11,24,12,14,5,24,3,20,18,8,14,1,21,4,3,3,13,15,26,26,18,6,6,9,8,10,5,15,0,12,15,25,16,14,19,15,19,21,18,26,5,27,3,10,6,2,0,0,9,6,0,4,25,0,11,0,18,22,14,0,3,4,0,10,16,3,0,22,21,4,0,11,18,12,19,24,3,22,10,3,13,17,24,0,0,1,25,25,0,0,0,0,21,9,20,0,9,20,9,25,0,21,7,0,21,0,0,24,0,23,0,11,25,0,20,0,0,18,14,1,18,18,15,4,25,16,0,0,0,2,22,23,0,17,16,6,16,17,17,6,0,4,0,12,18,17,15,23,2,3,21,0,4,0,5,2,4,0,19,2,1,0,15,23,8,14,0,0,0,25,20,21,23,26,0,6,0,16,5,23,0,16,12,18,20,26,1,24,15,17,16,4,4,17,19,7,1,4,1,19,20,1,17,3,11,22,23,7,17,12,18,1,0,0,18,13,13,6,0,0,19,7,21,15,9,12,21,17,42,13,0,8,17,13,1,6,22,21,23,17,16,20,1,20,15,1,10,3,17,1,16,17,20,24,0,12,25,25,16,8,11,18,8,9,10,7,12,11,0,0,13,22,0,24,21,25,18,1,1,18,10,9,19,2,1,3,14,4,11,3,1,3,1,1,0,11,0,23,11,0,17,0,8,38,5,12,12,4,1,24,0,24,17,25,9,5,24,2,9,1,6,4,13,14,12,0,25,2,13,21,0,14,4,1,7,22,9,14,0,19,4,0,1,17,10,2,16,2,13,17,4,16,0,0,13,9,0,1,1,0,1,18,1,16,9,0,10,21,16,0,20,20,12,13,5,14,0,20,0,2,0,0,4,4,9,14,2,14,24,21,24,17,19,10,0,2,16,24,24,1,25,11,17,0,0,0,16,0,21,17,12,0,6,0,24,0,9,0,25,11,4,8,5,0,0,15,0,0,3,0,25,12,14,0,0,15,7,0,0,24,0,21,11,0,0,18,36,0,8,0,0,15,13,19,0,7,0,1,10,6,0,0,0,15,24,12,19,13,4,20,16,24,21,18,0,0,17,6,0,0,16,0,0,24,0,15,5,22,0,3,8,16,1,8,1,0,0,0,0,22,1,0,22,10,23,4,19,2,0,11,25,15,7,12,0,0,5,16,0,18,27,0,26,25,4,0,15,0,0,0,0,21,23,0,17,3,18,9,6,15,0,0,0,0,19,14,5,8,0,3,10,1,21,21,16,0,1,2,23,0,0,20,8,20,20,15,15,25,19,1,32,0,21,0,0,9,1,14,23,0,0,17,0,1,0,22,25,5,2,25,21,0,9,5,1,20,0,16,2,25,14,15,1,21,9,7,14,0,4,23,1,20,22,18,1,24,23,17,13,2,18,4,24,20,0,25,6,15,25,25,0,11,14,3,11,26,0,8,14,12,0,8,13,6,25,14,29,17,0,0,0,0,14,7,4,13,19,0,24,18,17,9,1,24,1,1,0,1,2,2,0,2,6,1,22,2,18,14,0,0,0,8,1,16,7,15,12,23,4,19,20,1,15,22,5,18,3,7,7,19,19,0,11,22,0,22,0,0,16,20,0,0,0,0,4,18,0,20,4,24,21,0,4,0,0,0,14,0,17,0,17,4,13,20,6,20,0,12,13,18,15,21,9,8,11,2,0,4,2,3,1,4,2,0,13,25,1,22,15,11,2,16,1,0,24,24,6,25,24,1,13,14,15,6,15,6,5,2,0,1,0,6,0,22,4,6,8,0,12,22,19,16,0,19,14,19,20,9,7,10,0,18,6,19,18,0,16,12,0,0,24,14,10,16,7,0,16,24,3,11,1,1,23,11,1,1,0,0,25,22,24,0,0,1,19,11,23,14,0,24,2,2,0,0,0,13,0,5,0,5,24,6,4,25,18,0,0,25,0,0,17,2,15,1,16,15,22,12,24,25,0,23,21,24,13,1,21,3,25,19,0,0,0,22,26,12,19,0,0,8,20,0,9,0,7,19,5,7,23,0,26,0,6,5,25,26,0,0,0,6,0,7,0,0,0,0,19,25,16,12,5,0,5,0,22,19,14,17,0,26,0,0,24,26,5,21,26,17,9,0,0,0,13,0,5,2,18,14,18,12,0,18,17,14,22,25,13,2,17,23,0,1,13,23,25,1,12,4,0,1,2,15,5,1,23,21,15,4,14,15,13,0,0,15,18,8,15,0,1,13,25,24,18,25,14,2,6,24,1,18,24,1,0,15,0,0,0,0,15,9,25,18,24,0,7,25,14,19,8,4,0,23,3,14,2,2,5,1,1,0,2,2,0,17,1,25,2,0,16,0,0,26,0,0,13,14,0,12,23,18,10,7,25,14,0,0,3,1,5,15,1,14,1,15,14,2,6,1,5,17,19,24,24,22,16,10,25,5,20,7,22,17,21,18,2,2,24,1,2,3,17,2,0,0,18,18,15,21,6,24,2,0,0,15,24,20,0,0,26,18,0,5,25,0,15,0,19,0,10,0,12,23,17,4,0,0,9,17,0,8,13,14,0,10,19,6,25,0,11,6,22,15,18,22,19,6,15,21,21,16,6,0,0,0,18,20,16,18,20,17,1,24,0,15,17,5,24,17,0,0,6,1,23,5,18,5,0,18,2,2,17,19,17,16,16,12,0,0,20,0,21,7,21,9,24,2,1,1,0,16,20,10,18,25,0,19,25,0,19,21,15,22,10,0,5,6,1,5,17,14,1,16,22,18,7,23,5,0,3,43,17,4,24,3,0,20,20,20,19,15,17,0,0,22,19,21,0,21,18,0,24,0,23,0,0,7,11,3,0,1,2,16,13,13,25,4,5,25,3,24,22,0,0,22,25,0,9,11,20,20,8,19,15,13,16,25,17,16,0,20,8,14,5,8,4,22,28,0,0,3,6,0,0,0,26,15,0,24,0,24,19,19,11,4,6,24,4,17,9,1,1,8,0,19,18,21,1,7,18,20,25,14,18,16,19,10,6,20,19,0,14,0,16,0,7,16,18,16,18,8,26,17,0,0,12,22,0,6,18,1,19,1,14,23,0,0,21,19,0,4,3,7,0,9,0,14,18,17,0,16,0,0,0,19,26,0,24,24,16,0,17,0,14,18,24,24,2,24,11,14,2,0,6,17,23,0,0,0,0,0,21,0,25,0,18,15,0,15,0,25,19,1,24,12,0,1,1,0,25,7,0,24,24,25,0,2,5,1,0,0,0,10,1,7,19,19,22,0,5,19,0,20,16,2,24,16,3,0,21,25,3,15,1,24,2,6,3,24,1,24,0,16,20,0,0,0,8,0,25,14,0,18,15,0,7,10,0,25,7,0,0,18,21,0,18,21,25,24,8,18,0,22,20,7,4,0,0,6,20,11,24,9,19,19,18,11,10,22,22,15,0,0,13,0,0,23,12,5,0,23,0,25,16,0,22,21,14,20,2,3,25,2,25,4,6,0,24,25,20,5,0,0,9,21,0,0,22,23,22,0,0,0,0,23,5,11,20,0,7,18,0,0,16,5,14,5,9,15,10,15,18,9,24,1,18,6,5,20,19,4,3,25,1,10,21,7,16,3,17,20,3,2,2,17,0,10,15,18,6,18,12,18,0,14,21,15,0,19,0,0,1,1,15,2,0,15,0,0,14,8,1,24,21,22,1,20,0,16,25,24,24,24,17,18,1,19,0,2,23,15,0,20,8,0,4,11,6,6,9,19,10,25,0,0,25,10,8,9,25,7,21,9,6,9,6,11,7,11,25,0,0,19,0,0,18,20,4,8,10,11,24,26,0,24,0,14,17,16,1,15,0,1,3,22,0,20,20,16,0,2,0,18,22,15,0,0,18,21,24,21,17,2,25,0,19,24,16,20,0,15,21,0,2,1,1,17,2,0,1,24,18,11,24,4,3,16,24,5,15,1,24,7,21,1,26,29,7,3,0,19,17,1,20,0,21,20,0,5,9,20,0,3,5,0,8,0,17,6,19,21,19,0,1,0,1,3,0,19,2,0,16,0,16,1,24,24,21,0,11,19,0,0,6,8,0,20,0,24,0,20,9,20,14,25,0,1,7,0,1,17,16,0,18,13,12,1,4,2,15,1,24,8,19,18,14,19,19,4,5,0,2,1,4,18,2,15,2,19,5,15,3,2,15,2,8,6,24,16,0,0,0,0,0,20,0,4,0,0,0,0,0,0,7,11,6,28,25,24,19,11,5,7,17,1,25,15,1,3,12,4,1,11,1,15,0,9,23,2,23,1,20,1,7,4,0,0,21,0,4,22,24,13,24,24,23,0,24,1,24,16,0,0,8,11,12,0,6,21,4,13,6,0,7,20,0,0,19,0,0,26,19,4,21,20,16,2,16,13,0,7,18,24,18,6,14,3,22,2,16,2,0,0,3,21,22,24,2,25,18,19,3,25,3,25,14,1,0,24,24,20,21,16,15,19,14,16,21,2,3,21,20,24,20,18,0,24,24,17,0,24,25,0,21,21,19,20,21,24,0,37,16,1,1,25,25,24,0,1,0,17,16,12,16,6,0,19,25,20,25,26,25,0,26,21,0,0,15,0,17,27,0,0,24,26,26,18,15,22,7,4,19,11,16,24,2,14,25,8,18,17,19,8,23,15,24,17,0,22,16,5,17,0,16,24,5,11,0,0,24,24,24,11,0,26,23,23,21,6,18,24,21,14,0,18,0,14,8,24,25,26,25,21,19,21,7,10,14,24,20,2,24,18,15,11,25,21,9,25,24,17,3,24,25,1,20,0,19,25,11,21,17,21,24,22,14,0,22,18,11,22,21,17,0,20,13,0,26,8,24,19,4,21,23,5,11,13,8,6,0,10,21,0,20,19,15,0,23,14,22,0,16,14,24,0,0,0,28,22,17,17,0,24,27,0,0,23,23,25,0,26,22,26,0,14,17,1,17,24,1,19,24,2,24,1,19,22,16,0,24,8,24,24,24,21,20,20,0,17,25,5,0,0,0,0,12,0,12,21,17,0,0,18,0,0,0,15,0,25,0,25,30,1,1,5,7,4,1,17,0,1,25,0,19,3,22,24,12,2,1,3,25,25,1,24,1,24,19,24,1,21,14,18,24,23,23,1,18,1,14,14,5,3,24,6,23,23,21,26,2,22,14,9,1,23,0,22,0,25,0,1,6,17,22,2,16,24,1,0,1,24,2,2,17,19,10,25,21,18,6,2,10,19,15,5,18,20,0,0,0,14,0,0,0,0,24,6,0,25,19,18,11,25,5,0,25,20,25,23,6,6,25,0,8,2,16,14,1,2,13,0,0,24,22,0,24,22,24,25,24,24,18,21,24,14,24,24,22,25,17,19,20,0,0,24,0,21,25,26,19,5,0,0,23,0,25,0,0,24,18,24,0,22,17,25,18,0,23,1,24,1,24,15,25,0,24,20,17,25,24,1,12,25,24,25,25,24,10,11,16,2,13,14,2,16,4,0,19,16,2,0,1,0,2,17,26,34,0,19,0,14,0,25,2,2,26,0,27,13,26,20,20,0,0,0,28,16,0,21,11,19,24,26,20,0,0,0,24,0,21,16,5,19,26,14,0,0,11,8,0,15,22,25,15,25,0,14,17,0,1,15,15,1,23,21,1,0,13,17,18,17,1,19,21,1,24,7,24,14,12,18,2,0,25,14,17,0,7,0,23,25,0,14,24,0,23,23,0,23,20,18,18,15,29,15,0,0,0,3,23,25,24,25,25,22,0,17,25,25,1,14,14,5,13,2,6,1,17,1,24,10,20,25,1,0,0,24,0,22,25,0,24,17,0,14,19,0,0,5,20,18,10,21,16,0,25,0,0,25,27,23,25,0,0,23,25,19,0,16,14,0,0,15,22,0,22,14,24,8,11,13,20,0,2,1,2,5,0,12,18,25,5,13,0,25,1,20,15,14,0,0,16,19,1,0,19,22,3,17,24,0,0,0,17,0,0,0,21,6,20,22,19,24,0,24,0,20,26,0,17,24,0,15,12,21,1,10,0,0,23,22,19,3,4,4,9,1,2,17,7,2,15,1,0,23,13,0,8,1,13,9,1,9,2,9,17,24,1,23,10,24,1,4,0,14,9,0,0,0,0,0,0,0,0,0,26,0,28,0,18,0,0,0,0,0,0,1,1,3,13,25,0,0,22,13,1,14,1,12,21,3,9,25,6,0,2,4,20,2,24,3,2,23,6,3,22,21,10,11,26,2,22,0,8,25,16,8,11,14,0,2,23,1,23,5,24,25,12,2,1,27,10,25,1,24,2,21,0,17,23,23,1,1,27,43,1,25,1,5,20,18,2,20,17,1,20,2,25,1,19,4,2,8,23,2,19,20,0,14,0,21,19,21,25,21,0,25,21,25,22,18,0,22,0,18,17,26,18,24,0,0,0,0,11,0,0,18,0,21,0,16,0,0,0,20,17,14,29,26,24,0,14,0,26,0,0,24,24,22,20,17,24,0,0,18,0,6,0,22,21,24,18,29,9,4,0,2,0,24,18,16,2,2,10,0,0,25,0,17,0,18,23,18,0,0,0,0,20,0,24,0,0,3,0,0,26,19,0,21,25,23,18,0,26,0,0,0,0,19,0,18,24,3,23,20,24,0,16,13,29,18,0,0,27,27,1,22,0,15,24,14,24,22,25,0,2,1,0,0,1,24,22,0,23,21,2,0,0,1,17,1,1,12,17,24,1,21,1,10,24,20,17,22,0,0,24,16,16,17,25,18,1,19,18,0,0,15,23,19,2,20,15,24,20,25,20,0,15,18,3,6,0,10,17,1,2,24,0,0,16,15,0,19,26,25,15,0,13,26,18,12,26,13,0,0,8,22,24,0,0,28,19,1,4,1,19,5,24,17,13,4,5,17,17,0,1,11,0,0,16,15,17,0,0,0,25,0,25,19,20,0,0,0,0,19,69,25,18,16,19,19,20,25,15,22,16,23,0,0,20,15,21,12,23,0,14,0,25,8,0,0,0,25,27,21,21,14,1,1,0,0,23,33,24,2,22,16,4,0,2,25,4,1,8,25,24,13,26,0,16,5,0,0,0,26,0,23,0,0,12,0,26,13,17,18,0,0,25,26,27,27,0,0,16,20,24,23,0,1,12,25,3,14,8,16,15,19,13,17,25,24,15,25,24,1,0,0,0,3,0,40,0,0,2,6,24,17,25,1,24,24,24,24,2,18,20,25,13,21,17,10,2,16,15,5,0,34,20,2,16,23,15,0,11,2,20,16,20,24,20,23,17,2,25,1,25,9,21,21,18,0,0,0,25,0,0,17,0,0,19,22,0,26,20,25,17,0,0,18,21,20,17,10,0,10,0,17,17,8,16,0,0,25,25,21,25,25,0,17,20,0,24,25,26,0,0,22,0,6,4,4,21,20,0,0,20,21,8,0,20,16,0,0,23,24,24,20,0,8,23,23,16,24,15,24,17,0,12,19,19,0,23,0,23,23,26,21,15,26,0,0,20,0,4,2,2,17,4,14,4,0,4,4,4,27,12,4,3,13,27,3,2,3,5,25,19,15,18,0,0,0,0,18,0,0,22,21,19,24,15,22,0,21,3,21,19,20,7,0,0,24,26,24,0,25,0,25,25,18,25,1,1,16,24,9,6,20,0,18,2,1,11,2,25,25,17,11,8,25,24,18,21,8,18,10,0,0,22,3,25,14,0,0,23,0,0,24,0,0,0,22,0,24,17,21,20,25,4,2,10,18,16,1,3,1,21,13,25,20,18,22,19,6,5,9,22,2,20,16,0,3,19,2,18,1,16,19,2,1,18,25,4,1,24,1,1,1,25,24,19,24,2,8,1,14,19,25,25,4,0,27,0,22,24,21,26,19,0,22,0,21,19,22,26,0,0,23,0,16,14,16,17,17,16,12,22,24,1,1,2,3,18,1,1,1,2,0,2,22,21,0,0,0,0,18,18,0,22,7,17,7,19,0,0,15,24,0,25,0,25,7,8,0,18,0,7,24,24,0,2,24,23,24,1,1,20,17,11,25,18,25,1,15,25,23,19,1,1,0,2,4,23,3,25,21,18,59,24,24,21,24,25,18,0,1,23,21,24,13,0,0,6,0,24,0,0,21,22,12,21,8,20,0,6,19,21,0,20,0,22,24,0,0,8,26,26,10,5,0,21,22,0,21,2,24,22,2,12,10,4,12,13,19,12,23,20,14,1,24,16,21,20,1,11,20,1,5,25,23,0,14,4,1,26,25,22,18,4,24,12,38,16,16,20,20,2,4,4,1,1,1,1,16,27,1,0,17,21,26,18,16,1,1,1,0,8,1,0,0,2,0,0,9,18,18,4,19,20,17,2,20,22,2,2,23,20,21,30,21,25,0,22,24,1,0,22,1,2,0,4,18,1,16,1,17,0,0,7,21,25,1,15,10,2,20,14,0,59,11,24,17,24,2,0,0,12,19,32,0,10,19,21,0,0,10,25,0,21,0,0,25,20,0,0,26,21,20,24,20,0,0,0,17,17,3,10,25,9,23,21,2,25,25,25,0,0,20,20,7,1,23,22,24,0,19,0,18,16,9,0,17,0,0,0,8,0,25,0,25,18,26,0,14,8,18,1,14,13,22,23,14,2,11,17,16,14,1,2,20,18,21,19,7,25,24,4,17,0,1,22,0,5,23,14,26,23,18,18,1,21,20,24,25,0,1,24,19,7,0,10,21,16,18,0,8,11,12,22,26,0,26,39,16,14,17,16,13,19,16,19,18,18,16,12,2,15,21,1,14,0,19,52,18,1,23,15,1,12,4,21,16,11,1,4,1,1,15,25,3,20,28,24,18,18,11,2,11,24,17,0,22,25,22,39,1,16,1,22,0,22,27,24,15,6,0,13,0,13,19,16,17,16,0,0,25,0,0,0,17,0,6,0,9,0,0,24,0,27,0,20,18,17,25,0,26,24,20,0,7,0,9,25,22,0,0,25,11,24,18,0,15,9,0,6,0,24,18,0,9,25,7,17,16,22,21,0,0,22,0,24,30,0,11,17,17,0,7,7,5,0,17,25,0,19,0,10,22,19,24,20,17,7,0,0,0,0,0,0,20,2,19,24,16,26,0,14,24,0,10,0,0,1,1,24,1,23,18,0,0,22,10,12,21,13,24,18,20,18,0,0,1,21,0,0,0,16,25,19,1,18,16,6,18,24,25,1,24,1,24,24,20,0,1,0,23,3,21,2,25,1,2,25,20,17,1,20,24,11,0,12,18,26,22,25,18,19,19,0,41,1,4,23,20,22,6,0,19,23,1,21,1,25,16,21,11,15,0,21,5,25,19,20,0,0,14,0,0,12,14,17,19,9,14,16,27,0,0,6,17,8,0,25,11,0,82,0,1,15,1,16,1,4,0,0,25,23,24,25,1,25,0,24,26,7,0,15,0,0,20,0,0,24,14,18,0,25,6,1,1,1,1,23,1,0,17,0,0,2,24,24,25,21,1,8,15,0,25,24,18,0,23,0,0,0,0,0,24,17,28,21,18,20,4,13,14,0,21,17,1,1,24,0,3,24,1,24,1,1,25,0,4,18,23,25,1,0,25,23,31,25,24,0,20,0,19,22,24,15,24,17,8,0,0,0,0,0,24,24,25,24,0,0,0,0,13,21,22,5,0,5,13,26,11,0,15,6,0,25,0,23,0,0,8,4,0,0,0,0,0,0,10,0,0,0,24,0,0,0,0,24,19,17,0,21,0,24,0,18,17,15,20,5,0,22,7,0,0,26,0,0,16,38,0,0,0,0,20,23,0,17,0,0,7,17,1,0,21,0,4,22,0,14,21,17,6,6,20,0,18,6,22,14,20,23,18,1,15,12,16,21,0,18,1,1,0,19,3,25,24,0,1,1,18,2,1,21,11,2,1,21,14,22,20,1,25,19,2,17,2,2,82,1,1,8,2,2,16,1,25,18,18,0,13,17,20,20,23,0,8,25,25,20,24,2,9,13,3,20,21,0,1,7,22,16,4,24,0,22,8,0,24,0,24,1,26,1,25,17,6,0,20,24,2,17,20,0,9,22,13,5,5,17,0,23,0,2,9,24,9,24,0,1,0,23,1,2,0,19,19,0,18,15,0,16,0,25,18,20,21,6,0,24,26,25,7,0,24,19,22,25,23,7,8,24,1,1,40,24,0,1,8,1,20,2,1,0,24,0,16,2,16,0,25,17,7,24,24,21,7,14,14,0,25,20,15,17,2,1,21,5,17,13,13,2,0,17,19,12,10,7,9,11,1,15,24,24,2,19,0,35,3,0,2,6,25,23,0,4,16,12,20,2,4,24,1,3,23,24,21,6,5,10,5,22,7,25,0,19,21,0,0,21,0,24,0,23,6,6,25,24,19,15,1,16,12,24,17,16,17,13,18,5,8,24,1,19,13,20,2,1,0,0,4,1,23,23,0,1,25,1,1,4,24,2,22,1,24,1,0,1,2,10,0,25,19,15,9,25,24,24,1,18,19,1,0,3,0,4,0,15,0,17,0,0,16,0,20,19,27,27,0,19,0,0,17,20,0,26,21,18,17,11,15,20,0,0,0,15,24,0,26,19,16,20,23,12,10,25,0,25,25,3,9,18,1,16,21,5,0,26,0,8,2,19,14,0,24,0,0,1,0,0,0,0,0,23,1,25,1,0,1,24,0,20,19,0,24,0,15,0,0,16,19,0,21,7,19,24,11,9,21,0,24,1,0,1,7,14,8,8,8,14,18,14,2,18,25,25,1,6,0,1,1,1,24,10,3,1,16,25,20,23,1,21,0,23,14,17,21,10,24,10,0,0,25,23,0,17,11,0,25,0,0,0,0,0,0,0,20,16,0,0,9,0,29,0,24,11,0,0,9,10,11,0,23,14,15,13,16,0,22,19,17,9,0,22,0,10,2,22,8,0,3,25,0,15,1,14,0,15,5,1,17,23,2,22,16,16,20,1,23,20,15,4,15,26,26,0,18,13,20,18,0,21,0,18,0,19,17,21,21,0,17,19,0,0,0,9,28,0,0,3,23,2,0,25,18,6,20,1,1,25,1,23,6,0,20,39,17,1,0,24,16,17,0,21,0,20,0,14,0,0,19,0,8,0,0,25,26,17,17,7,0,0,0,23,0,24,2,1,0,21,9,15,16,24,12,7,13,18,1,0,16,21,8,15,17,14,0,2,19,9,18,1,16,18,19,24,14,82,8,25,13,4,17,26,18,3,53,12,16,0,15,24,18,30,14,20,25,20,16,25,0,18,18,17,20,7,8,0,21,18,7,11,0,26,4,0,21,17,25,15,6,4,12,5,7,0,14,0,15,19,22,0,23,22,26,26,27,26,10,0,18,29,25,21,9,24,25,0,21,24,19,7,19,13,24,21,53,25,20,22,11,0,21,17,0,20,25,26,22,20,18,0,18,0,0,7,0,0,0,7,0,0,0,5,4,27,14,16,0,20,19,21,7,20,5,19,0,5,5,0,0,21,0,5,6,44,25,0,25,25,14,23,21,25,26,12,22,12,7,22,24,6,21,0,3,6,20,21,21,22,22,0,24,3,0,0,6,6,0,17,0,6,0,17,0,24,11,7,0,18,0,20,0,9,20,0,0,0,0,21,7,5,0,19,16,3,3,0,1,25,25,2,2,22,24,0,25,2,25,25,21,0,25,0,18,24,0,14,8,21,24,14,1,2,0,15,0,14,3,1,42,1,23,10,15,17,1,5,0,0,8,26,0,22,24,4,0,25,0,24,24,37,0,10,2,0,0,6,25,20,0,23,24,24,21,17,15,1,1,19,11,0,24,24,9,9,20,11,0,0,24,7,25,16,21,6,24,27,13,23,25,25,0,16,1,19,35,0,4,1,15,0,20,0,6,10,17,0,5,1,6,5,5,4,0,22,4,24,25,3,24,4,0,0,1,0,24,0,9,1,2,10,1,0,0,3,19,23,24,2,1,23,0,15,24,3,9,5,14,0,18,7,0,23,24,18,10,0,2,16,14,0,17,9,2,14,4,13,4,0,0,24,9,1,24,0,17,0,19,13,20,19,0,19,25,21,0,19,5,5,0,20,19,24,0,22,22,21,24,15,6,5,0,0,0,0,0,0,0,19,5,25,13,0,0,0,0,0,15,13,19,17,5,0,0,19,5,0,0,9,14,18,17,15,15,12,23,13,0,20,0,9,9,19,18,1,1,0,15,0,20,25,10,16,10,14,1,1,16,16,12,16,19,25,14,21,2,20,6,25,0,25,13,0,9,18,9,20,12,0,16,0,20,19,15,21,23,0,25,6,26,26,0,26,0,18,17,24,0,0,20,15,15,1,18,2,24,16,15,1,18,0,21,15,24,25,0,16,0,12,0,10,21,0,0,0,22,20,0,0,17,22,0,15,25,5,0,7,0,24,6,25,25,0,0,14,16,17,3,0,20,25,10,1,16,16,16,19,0,25,0,27,19,24,13,24,0,17,9,2,14,1,19,24,16,14,25,3,24,3,24,4,1,17,22,18,0,0,0,0,19,0,20,25,0,0,11,0,17,0,19,20,7,0,19,0,17,0,0,6,25,15,0,16,17,0,0,31,0,8,11,0,0,0,17,26,14,12,25,16,27,14,0,14,26,28,27,0,0,0,22,24,22,24,3,22,0,21,2,17,0,0,1,24,0,24,17,0,0,17,2,0,18,0,1,19,5,0,23,0,27,17,21,22,25,25,27,25,26,24,18,4,11,1,12,2,3,24,23,40,0,0,0,0,0,1,13,22,25,16,18,19,0,14,25,26,6,18,0,25,29,6,0,0,16,0,25,25,0,14,0,18,0,0,0,25,0,0,28,0,35,5,2,24,2,17,14,16,12,12,3,15,2,16,2,25,9,18,24,2,1,24,5,0,0,0,24,0,24,24,19,1,0,0,19,19,16,21,3,9,1,21,1,1,3,23,4,1,15,1,0,24,0,0,0,24,23,0,4,0,26,15,21,24,14,16,21,17,1,20,2,2,2,16,26,25,17,8,24,1,1,1,19,14,23,7,0,0,20,0,13,11,0,0,24,24,24,0,0,17,0,24,14,16,0,0,0,20,13,19,20,13,20,21,22,22,15,19,22,19,25,24,17,0,27,10,9,0,16,0,11,14,20,19,0,17,0,24,15,18,25,0,14,18,17,0,0,0,0,22,20,15,15,15,0,13,20,2,3,1,1,19,17,16,25,17,0,25,25,25,10,23,3,3,20,5,0,1,17,2,5,11,24,23,24,4,17,0,21,1,2,15,14,0,17,18,17,23,6,0,0,9,19,16,14,19,25,21,0,26,25,1,0,15,15,15,19,18,0,6,16,24,5,1,19,0,9,17,10,1,17,1,6,15,14,18,0,0,25,0,26,26,21,0,24,23,7,6,8,16,11,9,1,30,24,0,18,0,0,0,47,0,19,0,0,20,0,0,14,22,24,9,0,16,3,6,9,25,0,18,1,24,17,0,13,0,1,2,11,0,1,14,9,20,24,10,25,0,10,8,10,24,4,6,11,24,0,13,14,1,0,10,13,24,22,1,25,21,19,15,3,17,1,21,18,16,8,16,0,0,16,14,0,19,0,21,7,17,20,20,0,14,0,25,26,0,25,0,0,24,25,0,16,3,16,4,19,22,0,10,18,2,0,16,2,21,25,27,18,22,20,0,8,20,0,19,0,0,9,0,19,16,0,24,8,5,0,24,25,0,21,0,0,0,23,0,20,21,0,17,0,17,26,22,22,17,21,0,0,21,21,0,12,0,0,13,0,0,0,26,16,0,0,20,0,19,26,6,17,0,0,0,0,0,0,21,20,0,22,19,14,18,0,26,24,19,21,22,7,26,20,0,21,0,1,26,3,16,5,0,16,19,7,7,24,14,1,0,15,1,2,25,0,2,26,2,0,16,0,22,13,16,15,21,0,0,6,1,1,0,0,1,1,2,1,11,8,19,0,3,0,24,1,22,25,8,26,25,3,0,24,11,26,0,4,2,2,20,26,0,0,25,0,0,0,0,26,0,23,6,0,0,24,24,20,24,0,12,0,7,25,25,0,24,25,24,8,16,19,0,27,27,0,18,37,3,1,25,1,0,0,0,0,0,0,0,9,25,24,25,19,0,31,11,14,26,1,0,24,14,0,16,0,2,26,15,25,1,2,4,21,25,8,24,4,15,13,25,2,20,14,6,28,7,4,1,25,26,0,16,10,24,1,0,22,23,25,22,25,20,22,24,0,9,20,0,25,26,10,25,0,21,0,0,21,0,20,16,26,8,6,3,21,1,0,17,22,31,19,24,0,24,25,0,0,9,0,20,25,0,22,0,0,16,17,15,23,23,24,24,22,0,25,17,0,0,1,1,0,0,0,16,21,22,0,21,17,2,29,16,0,14,10,0,25,0,0,0,0,21,18,25,11,1,21,0,19,12,4,17,26,24,5,25,0,0,0,0,0,18,6,25,15,23,20,0,10,18,0,0,24,0,0,25,4,25,25,19,0,25,18,9,19,0,24,19,0,14,17,1,25,2,0,21,0,20,19,9,16,7,17,1,0,16,21,3,17,0,0,1,39,11,0,25,4,0,16,18,1,11,22,1,1,1,0,0,0,1,20,14,24,15,1,22,0,0,24,12,5,8,25,15,19,15,0,17,0,0,7,10,0,12,0,24,24,22,0,13,7,24,19,0,21,11,0,25,16,0,0,20,14,20,17,15,17,10,0,11,0,0,0,0,25,23,0,27,24,1,18,0,16,7,0,6,1,0,4,21,21,21,9,24,26,23,1,16,20,21,20,1,0,14,18,22,26,0,0,26,25,17,21,25,0,3,16,27,24,8,18,5,0,0,24,0,0,21,10,18,16,0,24,24,0,22,0,0,0,24,24,22,13,0,25,25,7,0,0,0,0,0,5,0,0,9,0,0,15,0,0,25,26,0,0,25,0,0,0,0,26,28,0,0,0,0,0,0,0,26,0,0,0,2,27,0,0,0,0,19,0,0,0,0,6,18,0,24,25,0,0,25,24,24,12,29,22,24,0,0,25,25,26,26,6,0,6,25,24,25,18,25,0,19,5,25,18,0,24,0,25,0,0,18,26,19,21,18,17,19,22,23,2,18,20,24,2,26,4,20,19,15,10,18,15,2,25,2,2,25,0,0,12,0,25,25,19,0,10,0,0,0,0,25,18,19,14,0,23,18,21,0,17,0,15,8,0,17,11,23,26,1,2,25,18,19,0,0,0,17,0,0,1,9,17,4,0,21,1,21,0,9,16,22,1,21,20,19,1,11,20,19,22,3,0,19,24,0,1,21,0,0,0,24,0,13,18,0,22,0,0,23,0,0,26,0,19,6,5,20,22,0,26,0,27,25,0,14,20,0,21,8,0,24,0,19,3,22,5,0,0,15,23,21,10,17,0,0,17,7,0,0,15,0,24,24,24,26,0,0,2,0,1,10,24,25,23,20,2,22,1,0,24,0,2,0,2,1,18,18,25,1,27,25,28,0,0,26,24,18,16,0,21,21,25,22,26,8,13,8,22,0,0,0,19,18,0,9,25,8,25,8,16,0,19,25,14,25,7,0,0,17,26,19,21,0,0,0,10,25,0,0,0,0,26,0,18,17,26,0,0,17,18,13,20,0,18,17,0,0,1,0,9,20,10,18,12,3,24,24,0,25,20,19,20,2,25,23,15,0,0,9,11,21,19,26,27,0,23,14,0,22,0,0,20,16,20,22,18,9,0,1,24,0,1,11,2,22,0,25,18,22,0,0,17,22,3,21,12,24,1,26,1,24,7,0,2,14,2,21,20,0,20,19,26,24,0,1,24,24,1,25,26,24,0,6,14,0,0,0,26,4,10,20,6,14,21,19,17,0,29,0,26,16,27,26,25,25,18,28,0,5,13,1,1,1,17,24,19,0,0,22,0,1,2,24,27,0,0,25,25,1,0,0,20,11,24,10,7,0,11,0,0,17,0,0,19,0,3,0,4,9,19,13,0,0,25,0,0,26,0,21,0,25,0,0,0,22,27,1,1,2,8,24,21,3,21,18,25,17,23,2,1,0,23,2,27,4,9,2,4,19,17,25,9,0,0,9,26,15,0,5,0,0,0,19,24,0,1,16,0,26,0,16,9,16,6,20,24,25,0,21,20,9,10,1,2,16,16,15,0,25,1,0,21,24,8,0,0,0,20,25,21,0,10,0,24,0,0,24,6,0,23,33,24,26,21,21,0,21,0,22,28,26,22,5,20,23,25,26,23,24,0,16,17,25,12,25,0,10,34,0,11,24,27,0,8,18,12,24,25,9,0,22,18,9,0,15,25,18,0,12,16,0,1,4,10,1,23,20,25,26,24,27,0,1,25,3,15,18,25,23,23,1,12,1,0,0,1,6,0,31,1,22,15,0,19,16,25,24,25,0,22,0,18,0,0,0,0,0,26,0,0,24,0,0,24,0,25,15,25,17,0,25,19,20,11,1,0,0,1,24,1,24,0,21,22,18,18,22,25,18,25,1,25,26,0,0,15,2,19,0,19,20,19,19,0,0,0,1,22,24,12,24,15,25,7,5,21,0,24,0,0,0,0,0,21,9,25,20,20,25,22,22,6,25,0,19,0,0,20,15,18,22,17,0,14,23,9,1,19,1,21,25,0,24,0,25,0,25,21,4,2,0,12,8,22,5,20,24,0,20,14,19,22,17,0,26,0,0,23,21,20,20,18,21,20,10,0,0,7,47,20,10,22,0,25,19,11,16,25,25,25,25,0,0,26,29,2,3,1,21,1,17,1,9,19,27,0,0,0,60,17,25,18,19,0,0,15,25,0,16,9,26,21,0,26,19,40,10,20,9,1,3,0,0,1,23,0,0,0,19,0,24,17,11,4,0,0,6,0,25,28,0,0,0,0,15,26,25,0,12,0,0,0,14,4,0,18,7,1,25,18,18,19,0,24,27,21,24,7,0,20,1,24,9,23,20,26,0,24,0,9,19,7,0,8,0,0,16,7,21,21,10,25,23,17,0,1,25,24,25,2,16,21,13,25,28,0,0,7,24,1,0,1,24,18,2,0,21,8,15,0,0,0,12,0,17,0,0,23,21,23,25,21,0,24,0,21,0,0,0,24,24,4,1,0,21,2,0,24,7,1,0,16,0,0,0,15,19,16,0,19,30,0,21,21,0,28,0,0,19,25,1,23,1,1,20,0,22,0,16,0,0,0,22,9,0,24,0,24,0,0,15,17,0,0,0,15,13,27,13,0,24,18,0,1,14,16,14,0,21,11,19,10,26,0,11,0,25,0,22,0,0,8,6,33,20,0,25,0,0,7,6,6,16,0,0,11,23,22,0,0,0,0,0,9,4,8,1,24,1,20,18,0,14,0,0,14,24,0,1,14,0,0,26,12,0,24,0,18,17,0,26,19,20,0,1,0,20,20,16,1,25,20,1,20,24,24,17,0,24,0,0,0,0,0,0,0,21,0,0,18,0,18,0,0,18,26,24,24,0,25,0,14,0,25,23,0,12,21,0,24,25,20,14,0,1,24,0,22,21,22,11,0,18,22,1,19,18,0,27,0,21,1,0,21,0,1,0,20,1,16,19,19,18,1,20,19,19,14,23,1,24,6,1,0,1,24,27,1,18,1,5,19,0,24,25,0,25,10,16,24,20,20,0,25,0,21,14,15,18,18,0,28,24,26,10,0,0,9,19,0,20,0,17,17,19,18,0,0,18,0,20,7,27,19,19,26,17,27,9,0,28,22,24,21,16,1,27,0,1,1,5,21,2,22,8,16,21,0,24,0,26,23,26,13,17,15,26,24,23,17,27,0,20,26,1,1,0,0,19,6,0,2,12,25,21,0,6,1,1,0,0,1,1,25,0,9,25,24,25,16,6,28,16,19,0,19,0,24,0,15,1,0,19,14,26,0,0,17,19,0,0,25,15,20,26,0,1,24,25,24,1,1,21,25,21,3,20,22,21,23,1,11,24,1,23,10,2,22,0,20,0,0,0,19,0,0,26,66,21,0,24,25,16,21,21,0,0,14,23,1,1,8,1,24,0,24,27,1,0,24,21,21,22,6,0,0,0,25,23,0,27,0,25,26,0,25,21,0,19,0,0,27,11,15,22,15,0,1,1,0,0,1,17,1,0,4,24,22,27,21,26,26,0,5,4,21,17,26,2,20,26,0,4,26,5,28,1,18,3,0,28,27,3,28,2,6,4,0,0,0,51,24,13,21,25,0,11,3,25,17,0,23,22,25,17,19,0,0,0,15,0,25,16,19,17,25,0,0,23,0,16,18,15,15,8,0,0,0,0,25,24,24,6,0,11,17,0,0,0,0,0,10,25,0,18,8,0,16,18,20,0,22,1,16,3,15,24,17,1,1,19,1,6,0,24,20,1,24,1,12,1,1,22,0,21,1,4,0,25,0,3,14,0,24,7,22,2,22,7,20,17,16,1,23,24,23,23,22,19,0,21,8,0,26,6,0,13,0,19,14,24,0,0,8,0,0,0,5,24,25,18,24,23,13,24,0,26,24,0,1,7,27,13,18,25,25,1,25,25,25,26,20,3,19,25,25,20,1,1,26,5,2,26,0,18,1,0,27,0,0,27,21,0,2,0,0,0,22,11,16,4,20,10,12,18,18,0,0,20,0,0,2,25,26,24,20,22,0,0,0,26,15,26,29,22,1,20,0,24,0,5,25,27,25,0,0,0,0,27,0,22,0,26,0,0,17,0,0,19,19,0,0,19,24,7,0,0,21,16,0,25,25,20,25,0,0,0,0,19,20,26,27,21,18,1,1,1,1,0,1,5,1,18,1,0,25,21,20,14,16,25,0,20,19,18,22,0,25,17,0,21,20,0,0,24,21,8,18,22,20,0,25,0,21,0,0,27,0,0,26,10,4,21,19,18,17,20,0,18,0,0,20,48,20,18,14,17,19,23,24,16,20,20,24,0,21,0,24,24,17,0,0,2,19,4,1,25,7,4,16,22,2,26,25,25,20,9,19,28,1,9,2,6,3,18,0,0,1,0,0,17,0,15,25,29,22,23,24,7,0,0,31,0,7,20,0,0,0,22,20,26,0,18,14,0,0,1,0,28,22,69,0,19,21,6,0,0,21,0,0,10,5,18,20,8,22,19,8,8,0,9,25,0,21,8,21,19,25,19,1,1,1,0,25,22,19,24,11,17,24,0,0,0,24,21,18,0,22,0,12,0,10,61,16,27,18,0,40,21,0,27,13,24,20,7,26,0,27,25,0,19,1,0,0,25,0,25,23,21,1,1,6,27,0,17,14,7,23,25,8,18,6,24,0,20,25,20,24,24,21,19,14,0,0,17,10,22,20,12,21,20,25,20,0,21,0,27,14,0,23,14,0,9,0,20,22,0,0,32,23,18,24,23,0,1,0,2,25,23,0,10,0,25,24,23,0,10,22,25,25,26,0,15,0,27,1,20,7,16,1,1,40,25,2,25,1,0,19,0,16,21,25,0,25,11,20,25,23,2,1,20,1,24,25,14,0,20,0,0,7,0,25,0,21,22,14,26,21,0,21,6,6,17,26,86,14,0,0,15,17,28,32,15,26,26,26,25,14,19,15,25,27,20,0,7,31,16,12,20,1,0,14,24,8,24,0,1,20,17,18,24,1,1,1,16,1,0,9,8,24,25,1,22,22,0,22,0,18,19,7,24,24,25,25,0,20,25,10,22,0,23,21,0,19,17,19,2,4,25,2,18,1,17,1,4,1,0,16,1,23,25,24,23,20,24,0,6,13,3,7,1,20,23,20,4,27,27,0,29,19,14,25,28,26,3,1,23,25,0,18,0,3,20,17,0,24,21,0,24,24,17,0,10,0,23,23,0,24,6,7,0,25,2,1,25,23,25,0,1,21,1,25,1,22,23,20,1,24,25,21,21,24,24,20,0,6,10,0,20,25,23,0,0,21,10,1,21,25,25,19,6,0,0,0,0,0,15,25,20,0,0,0,25,27,26,8,26,0,0,0,18,19,21,0,7,14,24,0,0,25,32,24,25,0,19,0,10,25,24,0,25,20,19,10,21,5,18,18,1,19,22,25,25,22,19,2,1,3,24,2,25,25,2,25,0,5,23,4,3,14,2,27,6,26,25,0,9,25,20,0,22,5,24,22,21,22,21,0,24,20,20,20,17,22,22,8,0,2,25,15,0,27,26,18,22,7,20,21,28,21,21,0,24,21,0,14,6,25,26,22,22,17,24,22,7,9,6,0,2,19,23,11,14,0,8,20,13,0,22,4,7,27,0,0,7,25,4,15,0,25,25,22,22,21,23,10,0,3,22,16,21,21,9,21,0,0,0,23,20,24,24,19,0,0,19,0,0,28,0,1,0,7,18,8,0,0,25,0,0,21,8,1,19,0,8,0,28,7,16,25,25,1,20,14,1,0,13,15,5,17,2,0,6,10,1,15,12,20,23,20,1,20,22,27,21,0,25,25,25,9,14,0,7,27,21,14,14,0,6,19,22,10,21,20,0,18,16,18,27,0,0,0,20,0,25,19,2,16,16,16,6,2,24,25,0,2,25,25,26,29,32,21,16,14,19,9,36,10,7,8,15,0,0,10,26,7,19,23,17,25,27,20,18,7,1,18,18,14,21,24,3,21,0,0,21,24,0,29,23,21,28,23,0,9,0,21,24,18,20,17,9,15,0,20,18,0,26,0,0,10,0,0,0,0,26,14,26,10,21,26,28,17,20,22,25,25,26,3,5,7,1,24,1,1,22,25,26,28,22,26,5,0,20,7,8,24,0,25,27,8,1,26,22,20,20,53,9,24,11,1,1,23,1,24,33,0,20,2,1,28,20,22,25,23,1,1,17,1,26,25,1,24,25,21,0,0,16,0,8,24,0,19,0,0,0,25,13,23,7,0,24,26,20,21,0,22,25,17,0,20,24,0,0,19,0,28,21,21,20,18,19,1,20,26,20,0,21,20,17,0,22,20,1,8,23,10,50,0,1,25,25,23,25,6,1,23,20,25,0,18,0,1,0,1,0,37,22,18,18,22,20,21,25,2,21,21,26,16,19,16,21,20,25,23,18,24,0,22,18,19,9,20,20,0,0,0,25,0,20,19,0,0,4,21,25,26,12,21,0,13,21,20,17,19,20,24,19,0,0,0,0,22,12,0,0,0,25,19,19,26,6,21,25,0,18,21,14,17,26,20,25,0,7,16,0,2,25,21,0,1,6,1,10,25,26,5,21,20,24,18,0,17,0,1,25,11,19,24,1,1,1,1,20,22,25,24,24,19,26,25,1,0,25,16,7,1,1,24,19,2,16,19,22,3,21,21,3,9,19,13,26,12,10,11,1,25,20,5,6,2,7,11,25,21,0,25,24,22,16,8,1,23,24,0,8,1,1,0,0,2,0,0,0,27,0,0,19,19,0,21,10,9,17,25,19,0,21,26,18,0,0,26,18,0,19,0,16,0,16,4,0,19,26,0,25,0,0,0,26,26,25,29,5,0,0,25,16,0,2,23,1,1,0,1,3,25,0,1,10,1,20,1,16,1,16,20,1,8,0,23,20,23,25,1,12,6,19,18,0,11,26,1,1,23,19,1,24,0,1,1,0,7,0,29,0,22,20,23,14,7,0,17,19,0,0,9,0,12,0,0,0,28,0,0,25,0,25,29,22,24,26,21,0,9,0,24,0,0,27,2,0,24,7,22,25,0,29,17,23,0,20,22,21,25,0,0,7,0,22,7,0,22,0,24,0,25,0,0,16,17,0,1,13,14,16,2,11,24,0,20,18,14,12,0,19,24,0,14,14,3,1,15,24,18,0,0,0,0,14,0,5,6,0,8,25,0,19,19,24,0,20,20,0,24,25,26,36,24,18,22,22,14,0,3,0,27,47,16,25,15,24,24,1,7,1,2,8,20,1,1,19,0,25,0,15,10,45,0,22,21,0,6,9,0,21,4,25,21,0,0,26,22,0,0,19,28,0,14,26,14,17,23,14,20,29,18,14,17,25,18,22,18,7,0,24,0,24,18,0,24,24,0,17,22,6,18,21,17,0,0,24,0,18,0,7,18,0,0,14,9,13,25,0,21,25,15,17,21,13,17,25,15,0,19,0,19,20,0,0,8,22,20,20,20,18,26,24,21,18,4,21,19,0,24,6,0,0,22,22,0,0,0,24,19,26,25,0,18,0,0,24,9,0,30,19,24,20,29,0,25,0,10,19,14,24,0,1,25,1,1,15,0,16,11,26,26,26,18,1,18,6,9,19,0,25,0,0,2,2,0,25,18,18,16,1,1,25,19,24,3,26,18,0,0,1,7,18,0,7,1,25,25,2,0,6,20,0,24,24,25,14,25,1,1,0,0,0,0,0,0,0,0,23,0,21,25,23,24,0,0,0,0,24,23,25,1,22,1,0,25,0,0,20,21,4,21,22,25,23,0,1,23,18,0,0,26,24,3,0,0,17,21,24,26,18,24,0,0,0,20,24,16,20,0,25,19,0,20,24,19,19,21,26,0,20,0,12,0,11,23,27,26,0,8,0,0,0,22,25,19,0,10,25,12,21,22,0,24,4,16,22,22,25,4,1,2,1,0,3,21,13,14,20,15,26,5,9,1,28,12,1,19,3,26,20,24,20,24,0,0,0,0,6,19,20,29,0,0,8,24,0,0,24,24,0,0,0,0,0,23,25,0,0,25,25,0,0,0,0,0,25,0,9,25,0,0,24,1,2,1,34,25,0,24,0,24,14,19,25,0,24,11,25,1,16,0,25,0,18,18,22,24,0,21,23,1,25,23,22,25,20,24,20,25,18,24,0,24,1,26,20,17,1,28,25,24,24,15,24,28,2,19,26,25,21,0,21,25,10,10,0,0,0,0,19,24,25,21,25,26,18,20,0,13,24,43,24,25,7,1,1,25,1,14,5,25,28,5,22,1,29,5,2,23,19,21,27,26,19,26,22,1,1,20,1,0,0,0,26,25,26,29,26,28,17,7,1,19,14,16,8,0,13,16,17,8,6,19,0,1,22,27,0,1,25,23,4,25,0,25,27,19,11,22,26,1,0,30,18,18,16,11,1,23,22,0,1,0,1,26,0,25,20,25,32,31,0,18,25,20,20,29,0,20,1,24,17,0,25,39,24,23,22,18,20,19,0,0,1,26,13,0,9,27,1,2,1,27,19,25,0,16,1,18,17,9,19,25,25,9,22,17,0,24,18,0,1,27,18,0,23,7,14,29,0,22,23,18,22,8,6,15,8,0,12,0,11,19,5,24,0,1,19,1,1,1,0,25,17,0,1,24,1,24,79,12,25,24,25,8,27,18,0,18,0,24,21,8,20,23,22,25,25,29,8,7,0,0,0,23,24,0,24,17,0,21,0,6,20,0,0,21,23,17,8,21,0,25,26,0,1,1,18,7,1,25,24,24,2,0,0,20,25,0,25,29,8,13,20,20,0,24,17,11,18,24,2,2,1,25,0,26,24,2,25,11,22,1,20,0,24,0,1,12,1,1,27,21,0,20,25,1,1,1,25,22,18,18,20,0,25,24,1,19,0,21,27,1,28,17,17,11,0,8,26,20,0,0,18,54,16,0,21,24,0,6,0,0,30,0,19,22,1,0,18,22,1,25,9,0,1,16,1,1,0,26,13,1,18,24,25,24,17,10,26,0,25,14,0,28,28,26,0,5,16,0,25,26,0,23,26,26,0,0,27,26,0,10,25,19,18,7,6,21,2,6,21,13,18,0,12,25,19,0,1,18,25,10,0,21,0,14,0,26,18,0,0,0,0,0,15,19,0,0,0,8,25,18,25,16,9,1,1,7,7,2,2,21,24,9,23,21,25,19,9,24,0,20,1,8,18,8,10,21,0,0,0,19,20,20,22,25,0,20,0,0,21,22,21,0,8,25,27,19,21,1,25,8,16,14,20,23,24,17,24,8,0,0,17,26,0,0,6,0,10,25,22,0,25,21,11,17,8,9,26,25,1,18,1,5,24,1,0,17,19,18,15,7,24,0,1,24,1,1,27,45,0,26,18,18,0,24,1,1,23,21,22,24,18,20,22,18,0,19,19,27,25,25,1,1,1,0,17,0,10,0,0,20,20,15,0,21,23,0,0,6,0,0,1,8,0,1,17,0,0,18,21,17,1,3,25,24,22,0,28,24,3,2,18,24,22,2,0,20,2,22,24,21,13,20,24,1,3,2,3,24,1,3,2,1,22,2,20,0,25,25,3,4,7,1,0,23,19,1,22,0,26,21,2,19,18,19,21,1,7,21,21,0,24,23,0,1,24,21,0,24,22,24,25,1,0,16,3,41,19,1,1,5,17,25,4,0,26,22,19,18,6,26,16,24,12,9,21,18,22,2,27,2,14,1,2,1,22,25,2,10,26,20,23,12,2,2,1,20,17,24,8,22,19,22,2,24,14,10,16,2,21,1,19,16,18,9,9,25,3,25,19,3,1,2,1,21,20,1,2,2,1,25,24,26,18,2,20,21,19,11,7,20,3,21,1,3,3,2,2,25,1,19,3,2,17,8,2,24,3,8,12,25,3,21,4,25,2,21,24,2,1,23,2,20,13,7,3,13,4,1,25,15,3,4,3,3,19,17,10,17,19,18,0,1,4,23,20,20,3,20,26,9,10,3,20,21,8,15,12,2,2,19,25,21,21,21,3,7,20,4,11,5,23,20,24,17,24,20,12,25,15,9,2,21,2,7,20,20,7,1,4,25,11,16,0,0,19,26,0,7,22,0,2,24,3,16,2,1,25,17,25,21,2,0,21,25,25,24,18,26,23,25,8,23,19,9,2,3,21,25,1,18,24,12,17,2,20,22,2,0,18,2,25,9,25,24,26,13,5,4,25,2,8,22,18,18,18,6,2,9,1,1,1,24,18,2,22,22,24,18,4,2,24,4,10,20,14,3,15,2,2,3,20,2,19,10,18,13,20,20,23,19,3,2,12,5,3,23,21,21,22,21,14,3,21,22,22,16,26,20,1,19,17,1,11,24,22,24,1,19,2,0,21,22,1,25,0,26,30,18,15,15,21,20,23,21,20,16,22,5,2,15,2,1,0,1,19,5,9,23,10,0,14,25,1,7,25,14,16,21,24,21,21,22,1,23,22,24,3,20,2,1,1,22,18,23,2,1,1,2,2,0,12,17,13,4,2,3,1,0,13,10,1,26,25,25,26,1,23,5,19,18,2,3,5,2,16,18,3,18,25,24,21,20,2,1,3,2,19,2,18,22,2,2,24,12,23,7,21,11,4,23,23,17,4,1,11,24,3,7,23,3,17,1,10,23,10,25,17,25,25,10,18,16,18,6,2,7,24,16,18,2,19,7,20,26,5,25,2,6,22,23,15,0,6,0,7,26,25,25,7,18,19,1,20,19,24,25,0,19,3,12,3,2,23,9,24,26,22,21,27,20,13,2,2,6,0,23,5,0,16,9,0,0,20,0,0,21,5,0,24,21,22,25,27,22,19,18,1,19,23,26,17,19,2,23,26,1,25,25,1,18,1,2,26,16,18,26,9,8,3,13,19,18,8,22,11,20,3,23,20,1,8,3,17,7,25,0,17,18,26,24,20,4,2,8,5,2,7,2,14,2,25,26,25,2,2,1,3,14,10,12,8,4,10,1,21,2,1,18,1,25,25,18,20,20,18,25,33,0,10,19,9,19,16,2,2,21,17,19,12,5,8,2,2,2,13,21,1,14,18,24,8,1,16,26,14,3,6,13,25,0,26,4,19,0,11,2,13,1,19,16,18,14,1,17,1,2,25,1,25,0,3,18,13,18,1,2,1,1,20,1,22,16,6,3,24,25,25,17,20,10,1,5,65,27,2,2,24,22,4,4,19,14,3,10,4,23,23,3,4,2,31,16,20,23,21,19,24,11,7,8,4,5,20,2,2,25,3,3,22,2,16,17,23,2,27,3,24,2,24,26,23,5,10,5,2,7,2,2,17,4,24,2,20,2,19,9,19,3,1,5,2,24,68,12,2,1,12,2,20,20,15,1,1,1,15,18,28,2,2,4,2,0,32,1,5,6,5,2,7,3,18,1,2,2,2,18,11,13,4,14,19,17,3,2,2,23,1,3,2,24,20,2,9,25,25,13,9,2,25,24,4,2,17,2,13,0,1,24,24,12,24,61,2,19,8,1,7,3,1,17,9,2,21,19,1,4,2,14,16,18,21,18,9,1,17,22,3,1,20,14,4,6,24,21,24,27,24,2,9,21,20,25,17,18,1,1,14,17,18,2,39,26,2,1,31,6,21,3,2,2,2,3,8,39,3,24,1,19,4,2,2,2,1,1,18,23,2,17,26,1,1,2,25,20,1,16,24,1,4,6,27,16,17,0,0,0,7,15,11,19,19,1,16,20,26,17,7,19,0,24,5,0,7,0,0,1,22,24,25,3,0,1,0,0,2,1,6,9,22,19,0,2,2,1,13,36,9,19,11,7,14,1,18,18,20,26,1,25,0,0,8,20,1,22,21,2,2,24,21,8,2,23,20,0,22,1,1,2,10,17,2,2,3,16,3,15,10,6,2,11,16,17,2,3,25,45,15,24,25,25,11,20,16,3,24,17,1,1,0,7,6,22,24,17,2,22,1,1,20,24,2,5,5,23,7,24,13,11,8,21,17,2,19,21,18,21,25,25,2,16,19,2,2,23,15,28,19,5,18,2,12,25,19,3,2,2,24,19,2,14,15,21,20,21,14,16,2,8,22,15,18,25,25,21,1,19,6,2,6,2,2,1,6,2,24,15,25,17,17,1,2,23,2,2,20,3,5,18,19,25,5,19,25,20,11,13,2,17,10,3,14,23,2,2,21,24,2,5,2,6,2,2,21,25,9,16,2,3,23,3,23,25,16,1,9,9,2,25,4,2,19,1,2,19,1,3,2,2,19,19,25,25,2,18,16,24,1,10,2,25,3,18,17,2,20,20,11,4,25,18,14,3,2,14,19,4,2,20,21,10,6,11,3,20,2,2,11,18,12,8,19,18,2,2,1,17,6,23,19,42,14,24,2,24,14,1,19,0,11,19,23,25,18,25,15,22,22,24,25,3,19,3,21,22,21,25,24,2,1,15,18,20,6,9,19,21,13,18,21,21,20,1,16,1,24,0,24,19,9,0,18,0,1,9,0,0,27,23,1,0,3,6,0,0,24,2,9,25,15,24,2,8,1,26,24,1,1,1,16,21,1,19,2,16,15,1,1,4,1,2,2,19,4,0,18,24,2,8,2,11,1,19,11,19,15,19,4,3,7,17,1,20,5,2,0,5,53,24,29,23,1,23,0,15,0,5,4,6,2,15,26,0,1,25,0,1,24,1,20,21,23,19,17,12,10,1,0,0,1,0,18,20,11,22,9,9,1,1,0,0,1,3,2,14,2,16,25,1,2,2,27,6,2,2,15,5,10,1,1,1,25,10,2,1,14,2,18,2,25,25,17,18,24,11,25,0,25,14,0,27,5,14,17,3,1,2,2,24,17,0,1,9,7,6,2,2,7,1,1,2,2,3,16,1,15,8,3,25,26,1,20,7,2,16,19,2,20,19,1,25,20,0,5,24,19,0,17,18,25,17,20,19,25,1,16,0,0,0,9,21,0,3,10,12,18,18,17,21,17,3,18,20,17,4,17,19,1,20,8,18,1,2,16,1,20,17,13,21,1,7,1,11,6,11,16,4,14,23,0,17,21,2,4,9,8,19,5,2,1,1,0,17,0,0,20,16,22,0,22,22,20,20,19,0,0,16,0,1,18,21,11,0,13,2,3,17,25,0,7,16,2,2,11,16,0,19,1,17,1,2,1,2,18,17,14,1,17,1,0,18,22,16,19,1,1,1,1,14,1,17,11,19,1,12,6,8,1,1,0,1,0,1,19,1,0,15,17,3,16,2,4,2,1,19,16,20,21,2,5,17,22,17,15,2,0,0,7,4,0,6,1,0,1,20,11,1,21,21,17,18,1,26,23,17,1,3,16,17,2,2,16,21,3,7,3,27,3,2,20,6,9,2,15,26,0,1,18,2,18,19,22,1,15,18,15,17,20,22,19,24,26,25,20,19,1,2,1,9,11,1,17,1,24,7,16,11,18,24,24,26,27,14,25,3,15,21,1,21,21,24,24,18,8,24,18,21,1,11,3,15,18,8,2,1,0,16,20,15,1,17,20,0,0,1,20,10,1,16,1,20,7,4,17,2,10,18,17,9,8,8,3,1,0,6,4,1,17,15,19,2,22,26,15,18,0,19,8,10,14,18,20,19,29,19,0,29,25,2,18,18,13,24,3,1,18,1,18,0,21,11,16,17,0,8,0,1,4,25,5,0,0,16,26,0,16,27,0,0,16,12,0,1,0,25,19,14,20,19,0,9,24,20,1,18,0,9,1,0,0,1,11,5,17,9,11,5,19,18,7,0,20,18,26,18,1,19,5,0,1,2,2,6,2,1,21,25,24,1,1,1,1,17,22,0,25,19,0,26,24,6,24,14,0,1,19,17,19,16,9,17,0,24,25,21,17,5,13,20,10,2,2,4,13,38,17,19,2,22,21,1,18,1,0,1,21,0,6,18,16,16,18,19,1,18,24,0,7,15,1,1,18,5,7,1,1,7,1,23,1,1,13,8,0,19,3,3,19,2,1,12,16,12,25,20,2,16,25,18,0,2,6,23,1,0,18,1,1,25,1,19,3,1,20,19,1,25,2,0,1,1,3,24,25,20,19,20,1,8,11,4,10,6,4,4,0,11,13,0,1,10,2,13,15,18,0,11,0,12,25,5,16,15,2,1,0,26,0,30,25,26,1,15,15,1,10,0,24,20,0,12,1,1,26,1,11,20,0,25,24,25,17,17,19,8,10,2,0,24,24,15,1,9,1,9,0,1,20,0,4,1,0,0,23,25,6,18,0,18,0,18,16,0,0,0,20,18,9,15,21,20,16,0,2,18,17,0,1,1,1,17,24,1,1,21,7,1,1,20,19,19,27,0,17,3,7,24,0,10,4,24,8,19,5,0,24,2,16,2,11,13,16,19,0,17,21,20,24,22,1,16,1,26,12,1,14,15,7,9,2,7,12,23,2,20,2,2,27,2,2,19,1,3,26,7,14,14,14,12,1,19,8,3,13,18,33,4,0,24,12,12,1,15,24,17,1,19,18,17,19,18,18,2,25,0,18,3,1,6,1,19,16,1,0,2,15,22,1,1,0,2,24,2,28,25,9,8,20,3,3,7,24,10,13,10,24,21,2,0,15,12,1,1,24,24,18,15,16,13,2,11,11,1,14,13,2,20,1,2,5,9,6,21,16,24,13,25,21,8,26,13,3,1,0,16,11,10,10,1,1,4,19,17,1,17,5,26,12,17,28,1,1,20,18,22,9,1,18,24,12,13,16,8,22,1,1,12,0,16,0,1,16,0,20,25,1,24,25,3,11,12,19,13,0,0,36,8,0,27,18,0,16,22,22,17,11,28,15,14,0,12,4,3,24,7,2,17,16,5,23,2,14,23,7,20,3,23,2,2,19,17,26,9,16,2,13,19,19,12,15,13,6,11,24,10,25,21,1,1,1,12,1,24,1,1,1,43,2,2,0,3,1,5,2,4,12,21,5,1,1,6,10,0,22,14,25,3,10,20,21,18,0,16,0,2,1,11,14,1,9,11,17,22,15,2,1,14,13,15,1,19,18,3,17,6,1,0,2,4,13,17,7,1,0,25,26,20,14,18,8,8,17,18,17,1,14,25,12,24,32,1,24,24,24,1,20,25,1,14,1,1,1,2,4,17,25,0,15,1,24,25,25,2,1,8,1,25,2,1,8,2,3,1,24,11,18,12,1,18,1,1,12,1,1,2,18,25,1,15,22,14,19,18,19,23,21,0,5,0,18,5,2,8,1,1,25,24,13,1,19,16,0,17,1,11,1,7,0,24,2,1,11,6,19,20,18,16,24,25,25,2,16,12,21,17,24,2,14,16,24,19,2,24,17,12,8,9,0,1,0,16,2,1,24,0,6,1,18,1,24,1,14,24,24,20,9,0,19,2,1,4,19,22,24,24,20,1,39,21,20,21,19,21,1,15,1,16,7,23,25,1,12,19,1,14,1,2,10,1,2,2,7,1,14,0,1,2,0,1,1,18,1,24,16,19,18,8,24,10,12,17,16,18,1,20,19,2,1,8,6,25,1,19,12,8,15,21,6,1,1,0,13,20,8,4,24,13,5,1,23,14,16,13,1,24,0,19,17,7,1,18,0,8,11,3,1,24,24,25,22,22,24,2,1,16,19,4,24,5,15,15,1,4,8,3,15,24,24,1,20,16,18,1,17,11,0,0,22,19,1,19,17,3,2,16,1,1,2,1,1,6,12,15,22,0,12,24,0,1,19,23,26,0,1,23,2,25,0,11,12,1,1,15,24,0,21,0,2,30,1,11,9,19,18,3,0,19,26,2,18,19,14,0,0,2,5,0,4,19,18,1,19,1,25,5,0,5,4,0,21,1,23,12,19,1,4,13,13,1,12,22,25,24,19,14,21,18,18,25,2,24,2,23,3,27,2,2,12,18,4,3,16,2,8,24,12,2,9,26,17,17,3,2,15,27,8,8,9,4,15,1,2,0,0,19,1,4,14,18,18,2,21,1,21,22,1,6,19,1,16,26,16,20,15,15,25,9,15,11,4,24,0,26,12,16,19,10,0,1,2,22,2,16,8,1,6,7,15,18,17,16,11,25,17,1,0,22,25,25,0,1,0,1,16,25,24,0,1,6,42,1,3,1,25,0,21,21,25,1,24,15,25,1,6,26,0,11,1,18,24,2,8,1,5,25,21,1,1,2,1,7,25,1,24,8,0,6,1,16,1,0,12,13,17,20,12,0,2,9,25,0,3,0,2,0,7,0,0,26,1,1,25,2,24,17,0,24,19,17,0,24,24,24,4,0,17,1,12,0,0,6,26,1,0,1,7,4,1,0,1,14,19,2,0,1,10,0,19,17,15,12,16,1,10,0,0,1,24,8,1,0,4,25,7,0,21,5,0,1,16,2,10,24,4,1,7,1,18,5,24,8,19,4,24,18,9,4,24,19,2,25,16,25,6,0,13,1,25,0,13,16,0,13,17,1,17,16,18,18,1,23,1,24,8,24,24,25,7,19,1,21,5,5,20,22,2,24,2,11,1,4,25,24,23,24,18,1,7,8,18,17,19,18,2,18,1,1,16,1,11,20,13,18,1,24,1,19,24,24,1,0,5,11,1,0,17,1,22,0,2,20,22,8,24,0,1,0,0,18,0,0,13,1,2,1,1,1,18,26,6,17,24,25,27,19,16,15,4,17,0,17,0,15,12,0,13,20,25,17,26,20,16,0,0,26,27,14,1,0,0,12,0,18,3,1,9,1,12,3,23,0,15,12,12,0,23,24,2,1,21,17,20,21,26,1,24,21,0,18,25,25,1,10,6,17,24,2,22,24,19,1,1,1,6,3,11,2,1,1,0,0,25,24,3,1,21,16,1,15,17,23,7,18,15,24,24,0,19,24,0,19,25,19,3,8,23,12,26,10,24,19,1,16,17,10,18,15,4,0,17,12,6,2,9,11,9,6,1,2,1,18,17,12,9,0,1,25,20,17,15,1,0,1,8,24,18,1,27,0,23,8,25,18,24,2,0,0,6,2,24,4,4,3,1,3,0,2,1,2,1,17,1,2,23,0,0,18,24,23,2,6,2,24,11,0,22,1,0,1,25,26,2,23,20,12,12,26,19,26,0,2,17,18,18,19,1,2,15,2,3,1,2,1,18,6,11,2,21,11,1,21,1,2,2,26,19,26,6,2,1,3,0,1,0,12,18,18,0,5,26,1,10,0,1,25,0,9,23,7,25,20,25,2,12,6,9,0,1,0,1,18,25,20,16,1,8,6,12,0,1,2,0,0,0,1,14,24,26,25,1,20,15,16,21,26,11,26,19,25,0,0,16,1,18,7,2,13,3,14,3,2,20,22,16,4,4,1,26,2,23,2,26,18,18,15,8,23,3,1,11,8,4,4,27,25,4,19,12,14,14,6,15,1,0,21,2,2,6,15,25,4,17,2,21,4,0,24,22,0,5,7,9,1,1,1,0,4,6,1,1,0,1,1,19,20,20,1,20,5,8,0,25,25,23,24,1,26,1,16,25,25,1,18,19,5,3,2,26,25,25,23,19,1,8,1,1,18,15,19,5,3,0,24,19,23,0,0,0,19,0,1,16,1,8,0,1,10,17,22,24,0,24,0,0,1,24,13,12,23,25,1,25,22,20,24,28,23,0,18,1,16,24,25,0,11,14,0,28,0,0,5,3,0,18,26,21,24,23,14,25,25,10,1,2,1,4,24,1,26,2,3,20,21,22,25,25,17,21,26,0,4,2,24,14,18,24,1,0,25,7,0,0,0,3,17,1,16,1,3,1,16,23,7,8,3,19,5,2,1,9,17,17,0,20,0,18,1,25,0,1,1,13,0,7,19,4,0,21,0,1,25,20,25,13,25,6,0,20,1,19,19,1,9,13,24,25,16,0,26,1,0,1,1,6,25,1,0,18,15,10,21,24,24,24,1,23,1,17,1,15,1,1,11,0,6,0,23,0,25,24,0,8,26,1,1,19,9,1,25,0,0,25,25,15,25,2,25,24,1,25,0,0,0,1,3,1,1,19,24,2,1,4,25,1,1,0,17,11,15,16,14,12,1,4,19,5,14,1,24,0,6,1,1,25,10,0,0,3,18,0,0,1,26,0,0,0,25,25,1,1,25,21,20,20,1,0,0,19,1,0,9,12,0,17,0,14,24,12,15,17,0,14,25,0,0,0,17,11,8,21,23,18,1,2,24,23,3,25,25,21,23,16,12,0,1,0,21,1,25,0,0,1,0,20,25,0,15,18,24,0,14,25,0,0,0,0,1,1,1,0,20,1,16,1,23,1,26,1,0,0,3,25,0,1,25,25,25,7,0,18,1,0,24,1,3,18,1,12,1,1,19,25,22,1,1,1,20,19,0,0,13,0,0,0,1,0,26,15,15,16,18,0,2,1,3,3,14,21,17,10,7,1,2,2,1,0,2,20,24,25,25,4,1,2,2,1,1,24,18,1,10,1,24,1,1,21,1,18,25,26,24,25,22,26,13,25,25,18,16,1,25,25,12,1,24,16,1,27,1,3,8,11,20,19,12,24,21,20,1,0,1,1,4,0,21,0,1,15,8,20,17,24,5,20,14,1,0,1,4,1,0,20,0,0,0,0,1,20,0,23,0,0,1,0,16,18,1,10,5,12,14,20,20,2,2,0,17,26,1,1,17,16,9,2,22,1,1,25,24,25,24,9,3,0,18,5,25,26,20,7,7,1,3,15,0,1,20,16,25,1,4,2,1,1,1,1,22,1,0,24,23,1,23,15,24,0,19,0,17,0,25,3,6,4,6,2,1,1,2,2,2,23,1,2,2,1,15,24,3,25,2,10,4,25,0,17,9,25,11,25,1,1,1,21,0,0,2,13,8,18,4,2,6,1,11,20,16,1,6,2,1,3,7,16,6,7,1,1,19,25,24,25,23,1,16,25,25,24,12,8,0,1,2,3,1,2,0,18,1,24,11,16,21,20,1,14,10,1,10,1,0,1,1,0,1,5,1,1,0,17,15,15,1,18,14,15,2,1,13,1,0,0,8,19,1,5,19,2,2,19,16,1,1,1,2,26,36,1,10,1,0,24,25,24,25,6,25,12,1,26,17,24,0,1,1,25,0,24,1,25,8,8,18,15,16,19,0,0,19,0,7,1,0,1,1,2,30,18,25,25,16,18,17,10,0,15,0,24,1,20,25,24,2,0,25,19,3,21,24,21,1,19,0,0,9,12,1,10,20,24,1,1,23,23,19,3,24,0,2,23,1,1,1,1,24,6,25,0,25,1,2,15,11,10,1,23,22,24,0,1,24,0,14,1,25,19,0,0,21,5,17,1,0,0,0,1,0,10,0,0,26,16,1,10,16,0,0,0,0,20,23,0,20,26,16,0,6,2,11,2,1,18,1,24,0,1,24,2,24,1,1,1,1,0,1,18,1,0,25,4,25,8,17,1,14,9,0,0,2,26,0,0,0,17,1,21,1,0,8,1,0,24,5,17,1,20,5,20,1,11,25,25,0,0,7,0,26,0,0,24,18,1,1,3,4,0,1,24,2,25,6,25,1,9,12,16,1,0,1,24,26,0,25,2,4,5,26,26,0,18,25,1,3,2,26,19,3,23,0,16,26,27,2,16,20,1,18,1,26,18,3,3,5,5,23,19,24,2,3,25,1,18,25,2,25,23,25,25,2,21,25,25,24,33,17,24,25,16,25,24,25,6,17,0,21,24,0,25,4,15,2,1,0,25,8,1,1,25,21,7,6,19,2,2,2,1,1,25,2,20,16,1,0,0,10,3,5,1,1,4,6,1,1,17,18,16,11,20,1,16,25,0,23,1,0,19,17,0,1,3,0,0,0,25,21,0,20,25,28,0,24,2,20,22,24,18,22,0,2,1,1,24,0,25,0,1,0,13,4,7,2,1,25,26,25,1,18,2,14,17,1,2,2,2,25,24,0,2,9,11,0,5,20,23,16,26,0,5,18,22,26,1,1,1,21,20,1,6,1,13,0,0,24,0,4,2,4,14,22,0,16,1,1,8,11,10,26,16,15,19,27,4,1,13,1,0,0,0,0,17,12,20,28,17,1,0,0,22,24,16,0,24,15,2,0,0,6,18,3,0,23,5,25,25,24,2,24,0,26,1,2,1,15,20,8,0,1,1,25,9,16,13,23,39,0,0,1,1,9,1,6,2,14,1,25,16,1,24,2,25,24,0,19,2,8,34,25,1,9,12,25,21,1,9,1,19,20,25,6,1,1,2,0,3,25,17,24,13,2,18,20,24,11,25,2,10,1,18,2,4,0,25,24,1,1,1,0,1,21,2,21,5,26,1,17,12,19,17,2,6,13,0,0,25,0,40,13,11,0,3,20,23,21,1,12,15,21,2,3,1,25,19,3,2,2,5,24,30,25,23,1,3,4,1,27,26,1,19,25,25,22,5,5,0,2,25,25,18,24,24,1,1,2,0,1,2,17,2,24,25,6,0,0,16,18,25,15,3,28,17,25,0,6,26,0,9,3,14,22,1,26,10,1,23,0,1,15,27,2,1,8,17,15,6,7,29,25,27,24,0,6,0,7,19,5,1,14,22,10,21,22,0,24,25,25,24,24,26,0,24,0,19,1,3,18,5,27,1,0,12,24,0,8,24,16,1,0,16,24,24,16,16,1,2,0,17,24,18,1,1,19,1,2,0,1,0,1,25,16,18,8,2,1,0,13,1,25,1,24,1,1,19,19,13,1,7,23,20,0,5,2,24,25,3,0,17,11,24,23,15,19,24,0,1,1,1,26,28,1,20,14,0,16,2,24,25,8,1,1,1,3,0,25,1,2,0,11,1,21,6,2,24,22,9,28,16,1,15,14,1,2,5,2,17,16,25,15,25,0,25,13,17,1,20,21,2,3,21,9,1,14,21,2,27,13,1,0,21,27,1,2,2,3,25,2,6,23,22,24,3,2,2,0,1,0,8,25,6,2,0,0,2,7,14,1,18,0,12,24,24,3,25,20,27,0,1,0,17,0,6,4,7,24,7,1,25,1,1,2,24,22,0,24,0,15,1,24,19,17,1,40,1,21,3,7,3,1,5,25,12,12,25,23,25,30,0,0,25,16,1,19,0,25,23,0,1,1,17,0,25,1,10,19,18,24,17,1,2,5,1,20,14,18,1,3,2,2,18,24,25,1,24,18,12,1,15,12,24,2,21,18,0,2,0,14,7,19,0,1,19,16,0,0,1,0,21,25,8,1,9,1,1,7,19,1,23,2,3,0,0,15,1,15,20,25,24,5,27,26,2,27,16,16,26,13,0,0,1,0,0,25,26,1,18,0,1,25,0,14,0,23,25,1,0,2,25,25,2,1,40,0,0,1,1,2,25,0,22,3,24,1,19,1,20,15,3,0,0,17,24,0,0,24,19,13,27,3,20,24,15,2,0,4,25,7,20,21,22,5,4,12,3,25,24,21,21,1,1,1,2,21,1,2,0,25,1,1,1,0,25,20,23,16,0,1,24,1,26,25,17,1,25,25,25,1,0,1,2,1,2,5,23,9,0,15,1,1,1,19,25,1,15,1,0,0,12,13,0,26,25,26,21,20,25,0,13,20,20,0,15,22,12,26,19,23,23,23,14,11,7,2,1,20,18,14,15,1,2,1,6,18,0,20,3,24,9,36,1,16,23,24,18,21,15,25,4,5,14,26,3,1,0,27,18,0,12,22,26,9,26,0,13,25,1,11,26,0,1,19,21,5,10,18,11,14,21,23,2,3,2,2,0,0,11,5,0,19,2,26,2,2,25,2,2,14,2,16,16,0,24,28,2,1,24,9,24,2,5,8,13,6,1,1,9,19,3,1,0,3,24,24,24,2,6,16,25,3,23,24,15,25,20,9,1,0,24,24,20,5,4,16,1,17,25,2,0,2,0,26,21,9,0,1,8,1,0,1,24,0,25,16,3,18,4,24,0,24,1,4,1,23,0,8,0,5,17,24,24,0,20,25,0,24,14,0,26,0,11,7,0,24,27,0,24,0,8,0,8,24,25,18,18,17,0,0,1,0,0,0,0,0,14,26,13,5,0,0,0,25,26,9,16,18,5,24,0,0,1,25,14,25,0,14,18,17,0,1,25,12,15,0,11,1,0,0,25,14,2,2,1,20,26,1,7,5,24,25,26,8,28,26,0,4,10,19,2,19,2,25,0,3,23,25,1,14,26,7,0,11,0,0,23,25,0,20,6,19,0,0,16,17,0,25,27,12,25,23,18,1,23,18,0,25,43,10,1,12,11,10,15,1,0,13,26,10,15,10,16,5,1,1,15,1,1,3,17,13,15,21,13,5,24,0,1,25,6,7,16,25,1,0,28,13,0,2,10,25,33,2,2,26,25,1,1,13,26,0,8,26,5,20,2,19,24,2,20,0,20,12,2,26,23,29,3,6,20,3,25,17,24,19,1,4,11,6,13,18,1,17,25,2,3,19,18,0,1,6,15,1,12,8,25,15,1,6,25,6,16,13,10,1,24,25,8,26,0,2,13,2,14,16,19,0,1,20,2,2,21,0,1,2,15,21,16,19,24,26,17,17,1,25,16,1,1,21,0,20,26,0,15,18,1,17,15,21,14,15,25,19,3,0,0,0,1,18,25,0,27,0,2,0,18,19,1,0,18,22,1,0,26,0,0,3,17,3,4,11,1,16,25,0,25,16,24,18,23,26,0,21,0,25,1,25,20,0,0,10,0,8,8,13,23,1,3,1,0,25,25,17,25,30,32,25,25,1,2,24,17,23,24,18,1,17,0,24,24,0,27,27,16,0,17,13,17,26,1,2,0,29,26,0,25,0,18,6,17,7,30,0,6,15,6,26,16,26,25,17,1,38,25,13,20,0,11,8,1,7,25,24,0,1,28,25,25,1,1,0,24,0,1,1,24,1,25,1,25,28,1,19,24,5,1,1,0,14,24,25,1,24,25,26,4,28,18,25,1,9,16,1,26,1,1,25,21,25,17,19,3,1,0,2,17,23,2,26,2,26,9,25,20,26,19,19,12,0,25,0,1,9,16,24,8,0,0,1,24,21,14,12,13,15,21,1,11,13,0,15,25,0,8,0,4,0,19,25,0,9,7,5,17,4,2,24,0,5,5,0,0,0,0,0,22,24,25,16,0,10,13,0,1,0,19,0,9,0,0,21,7,25,0,14,0,22,26,14,25,23,29,20,9,14,0,0,7,5,5,0,0,0,0,24,3,6,0,17,0,27,9,16,26,25,14,0,0,4,0,13,22,1,0,14,18,25,0,8,16,12,25,0,1,28,6,5,1,1,0,29,0,14,19,15,0,26,25,1,25,21,14,25,20,0,22,26,21,25,17,25,25,0,21,1,18,1,0,22,8,17,14,23,18,25,0,26,23,0,0,9,10,0,0,1,24,0,15,13,12,0,3,1,2,2,25,2,18,2,13,12,5,19,18,24,0,16,16,26,0,26,13,7,7,18,3,24,46,24,19,20,24,27,28,26,13,4,3,11,0,0,6,1,0,0,27,5,10,28,0,21,25,39,21,1,25,26,21,2,28,1,20,15,20,20,25,27,1,1,10,17,25,27,6,1,25,21,22,25,0,20,0,1,18,13,12,9,4,28,19,0,28,27,29,26,22,15,6,6,1,13,0,0,15,15,0,26,25,22,25,1,4,41,5,23,0,26,2,0,26,3,0,21,1,30,25,4,19,0,27,0,26,6,4,0,1,0,20,24,0,24,21,25,17,24,30,1,15,25,25,25,0,4,21,16,5,26,0,8,1,0,24,24,1,26,0,5,11,2,3,1,26,10,25,24,19,19,25,24,1,9,18,25,2,21,24,0,23,2,25,1,24,15,14,1,13,21,12,0,10,20,4,0,0,16,9,1,40,11,0,1,2,17,3,14,9,3,23,27,2,7,1,22,23,14,2,0,0,25,24,2,16,21,0,23,4,0,14,2,0,14,25,24,24,2,21,1,24,8,20,1,25,25,0,16,25,7,2,5,25,22,0,2,2,13,26,24,21,2,0,1,26,23,26,1,2,18,25,0,20,21,5,16,16,26,20,15,24,25,0,3,9,24,25,0,27,1,2,25,4,11,2,0,9,19,0,23,16,12,12,24,1,24,24,15,2,24,23,18,2,1,0,25,24,24,0,0,25,16,18,10,1,25,0,25,10,17,25,6,2,10,16,8,10,1,10,18,9,25,0,0,14,24,7,6,20,9,8,23,26,11,0,9,10,25,26,0,0,19,26,26,0,1,0,1,3,0,7,0,27,25,0,15,11,1,1,0,26,26,3,18,24,25,19,22,18,0,16,1,1,22,24,27,24,1,1,24,18,22,24,2,1,0,1,10,0,0,14,22,11,2,26,25,0,17,20,5,26,0,3,1,0,20,1,8,2,11,14,5,6,13,7,15,37,24,13,15,0,18,1,1,1,0,25,3,25,0,1,1,13,1,13,13,25,0,0,22,0,13,19,16,27,0,24,0,2,17,0,24,1,0,23,10,16,24,15,6,24,1,6,7,23,20,0,27,27,0,1,25,15,1,0,12,3,0,13,25,24,0,25,20,22,25,20,7,21,20,21,7,0,2,23,2,2,24,2,1,25,26,2,2,2,21,24,6,5,24,2,15,0,25,0,15,19,14,20,1,24,20,20,23,1,5,2,7,12,13,9,26,1,18,5,5,0,24,1,1,7,1,17,2,23,17,9,2,5,1,25,3,25,0,15,0,5,7,18,4,0,0,23,27,1,24,0,0,1,2,0,1,26,1,1,1,21,18,0,10,26,15,0,25,5,1,15,1,13,1,1,0,1,1,16,25,10,22,23,24,25,2,17,7,10,0,17,15,1,1,1,26,5,15,0,5,15,19,1,15,15,4,2,1,18,18,26,5,24,12,20,0,25,14,17,23,5,15,0,0,1,8,1,5,1,22,2,1,5,24,1,10,0,2,1,25,0,0,2,17,16,1,18,11,1,1,16,2,2,11,17,7,27,1,0,26,0,17,25,24,26,3,16,0,0,26,0,2,0,0,29,1,26,27,29,1,25,0,25,1,1,23,24,13,0,25,0,20,25,0,15,0,25,12,13,4,0,0,1,7,11,0,0,13,1,19,19,25,19,2,24,13,1,0,25,2,5,0,13,22,2,1,19,8,15,0,12,18,14,5,2,3,25,16,3,3,15,23,0,21,24,2,0,25,5,24,23,13,4,27,19,3,24,2,4,4,2,22,2,2,2,1,1,7,1,2,27,26,25,0,23,15,26,5,20,1,0,25,15,17,2,1,2,19,0,6,22,24,10,0,1,24,24,1,14,26,7,22,0,13,0,5,1,0,0,0,24,20,0,25,7,0,25,3,1,2,1,0,0,13,28,0,29,10,8,0,2,20,1,26,1,25,1,2,0,1,3,2,5,2,16,1,2,18,26,9,1,0,4,9,5,0,0,0,1,1,24,0,1,1,22,24,0,12,7,1,16,26,4,3,0,4,0,0,0,1,2,1,0,17,26,10,17,1,1,2,2,2,1,2,24,2,2,4,1,9,22,5,2,0,21,2,24,12,9,25,24,25,10,2,0,29,17,3,1,0,3,0,0,2,9,6,0,0,0,1,21,22,27,14,2,8,1,2,1,0,24,18,22,6,7,0,0,0,25,8,1,0,25,0,0,17,2,1,23,0,1,24,0,1,1,1,23,2,2,20,0,2,0,2,0,0,7,5,24,1,8,2,12,0,20,26,1,25,25,1,3,1,9,1,1,2,0,0,1,3,28,3,18,24,19,1,24,2,19,2,1,0,24,0,24,3,0,24,21,25,18,15,25,25,8,4,21,9,24,24,0,8,1,0,18,21,22,2,19,25,0,2,20,18,18,21,0,0,9,0,24,23,21,17,0,26,1,1,0,14,17,25,26,25,16,24,14,1,17,1,2,18,1,0,1,26,26,12,21,9,26,4,14,23,1,3,29,29,26,26,26,24,0,7,28,1,1,2,1,25,24,0,32,15,1,1,2,21,1,24,12,25,20,0,17,26,0,18,22,18,17,18,8,0,25,21,1,25,14,0,19,15,25,1,2,24,24,19,0,27,24,25,1,13,17,12,0,26,1,2,16,18,7,0,1,1,0,1,17,26,12,13,24,25,19,1,1,1,24,25,0,2,13,2,4,2,2,1,1,18,1,1,24,20,0,1,0,2,10,3,2,7,1,24,26,2,0,2,1,1,0,2,20,2,17,26,17,16,21,0,13,0,7,25,1,1,21,25,24,2,24,1,1,9,19,1,1,26,17,3,2,2,1,5,25,0,0,25,15,0,2,1,21,25,0,0,26,0,1,1,1,2,0,17,0,1,16,14,0,0,0,20,0,24,21,17,1,1,25,5,20,1,0,0,18,0,0,7,19,14,27,15,14,25,0,16,0,11,14,11,6,19,12,27,8,15,1,0,2,1,0,2,24,1,1,1,11,1,21,3,3,4,2,2,24,18,8,21,12,4,26,2,23,0,26,3,5,3,26,10,9,14,1,0,0,17,1,13,25,8,14,13,17,24,21,15,14,11,0,0,0,25,0,20,13,3,1,1,1,24,6,24,17,0,24,2,1,1,0,0,26,7,0,1,24,2,1,5,18,0,0,8,12,0,23,7,24,25,0,24,26,12,14,13,1,0,12,2,4,0,17,0,16,2,23,16,18,21,20,24,18,19,0,1,1,1,16,22,24,0,0,18,25,13,13,1,1,22,25,25,24,0,1,4,6,1,18,1,24,0,18,1,0,0,5,17,22,15,15,25,14,1,18,14,1,0,0,5,8,24,0,16,24,1,10,11,1,14,3,14,25,24,24,22,1,0,19,16,8,10,24,6,24,25,7,9,1,2,1,0,0,24,0,1,22,24,24,24,0,0,14,0,1,0,1,4,10,17,2,5,25,26,4,6,1,16,3,28,1,0,0,2,26,1,1,15,1,8,0,0,12,24,0,5,24,23,1,1,0,12,0,0,22,8,0,1,1,0,0,16,1,1,24,25,0,26,2,16,1,1,4,25,1,24,0,1,24,16,0,1,3,1,24,0,2,24,0,17,1,7,16,0,14,1,2,1,0,1,26,25,1,1,0,3,2,2,0,0,2,25,2,24,2,2,10,15,15,4,21,23,2,7,6,1,33,25,2,2,25,24,7,7,0,7,24,0,23,0,2,0,13,24,8,8,0,11,0,24,2,3,3,2,1,26,15,1,2,0,3,25,3,0,25,39,2,12,0,24,1,24,18,19,2,1,2,7,2,14,24,24,25,1,24,2,25,1,5,0,6,0,1,24,1,7,0,21,1,0,24,1,2,13,1,25,1,0,0,24,8,16,15,2,2,1,0,25,0,1,18,1,0,5,0,2,0,0,24,7,1,0,25,0,21,0,0,13,1,1,0,0,1,0,2,0,0,0,6,0,24,26,24,0,0,3,25,1,1,15,0,1,6,1,6,15,2,0,2,1,3,0,0,29,1,1,1,25,25,1,0,3,20,22,1,0,20,17,0,25,0,0,17,1,16,25,2,22,3,1,2,0,7,0,0,0,0,1,1,1,19,25,0,2,1,14,1,1,0,11,25,20,1,0,25,0,24,1,0,0,14,24,15,1,17,0,0,0,0,0,6,1,0,0,15,22,0,14,11,2,21,13,7,2,0,0,0,2,0,1,0,17,0,12,26,27,19,17,10,20,0,0,0,18,2,24,25,2,0,24,24,26,16,1,1,0,25,25,26,0,0,19,0,0,12,11,25,5,0,0,11,24,25,0,0,0,2,0,0,23,10,0,0,8,5,1,3,9,22,1,3,1,24,26,26,12,25,2,0,24,0,27,25,25,21,0,13,28,23,19,26,18,5,0,3,4,21,5,12,0,0,22,10,26,2,0,1,25,6,5,21,25,0,0,1,12,6,4,0,26,25,1,21,0,15,29,7,16,10,3,1,6,7,0,26,3,0,13,0,3,26,21,20,0,18,25,2,1,0,24,2,23,7,19,8,1,25,14,8,7,2,25,24,0,0,0,25,1,25,5,7,0,28,26,15,23,19,1,5,20,1,19,26,1,2,0,16,9,20,0,8,1,0,25,2,25,24,17,6,0,20,0,1,0,22,7,24,13,25,14,2,24,18,0,19,25,24,0,26,26,0,0,18,0,14,0,0,0,0,12,27,29,25,14,21,18,17,21,0,6,21,0,10,0,21,2,0,4,0,15,25,13,7,4,16,25,0,0,26,17,14,23,0,21,1,15,24,29,25,13,28,0,9,25,2,0,10,1,26,1,0,9,20,26,2,23,2,25,10,1,0,2,0,11,0,0,14,27,25,26,22,24,25,15,28,23,0,14,0,1,17,2,3,5,0,1,0,24,14,11,26,1,6,22,1,1,1,2,13,12,0,26,19,15,18,1,21,25,16,25,0,1,17,5,0,19,10,25,4,16,5,1,8,25,0,2,22,1,5,25,0,1,16,20,25,6,2,25,28,6,15,0,0,26,0,1,0,16,24,0,0,13,12,16,12,25,19,0,28,0,0,12,21,25,12,14,14,1,3,13,1,25,19,21,0,0,1,15,12,13,32,28,13,23,24,25,23,24,0,0,25,0,15,24,0,5,2,1,25,25,7,4,3,25,5,19,18,10,14,25,1,0,13,0,0,25,16,2,0,0,25,25,1,3,0,24,0,25,21,1,2,4,0,1,25,11,15,3,8,10,15,27,25,29,25,2,3,30,27,30,15,3,21,16,32,1,4,1,1,25,0,1,0,2,22,26,26,0,26,15,0,26,24,0,8,2,9,0,0,25,11,0,1,1,0,1,27,8,27,8,0,14,1,12,0,1,1,1,0,24,1,22,1,0,0,0,0,26,25,25,25,23,1,1,29,0,0,24,24,1,19,33,5,22,25,9,0,22,27,0,8,0,14,1,0,24,13,1,23,26,11,7,0,15,0,24,2,24,0,24,23,3,20,27,12,1,22,1,17,31,15,2,0,0,17,2,1,0,0,11,25,17,0,0,13,1,13,25,15,24,0,0,27,25,5,7,11,1,2,3,1,20,0,6,9,4,2,3,0,0,0,15,2,16,20,18,1,4,0,2,2,16,24,0,2,19,0,1,4,16,20,2,7,9,0,0,1,25,9,1,1,0,1,26,25,21,2,25,5,22,0,2,2,8,8,25,11,2,27,0,28,9,29,25,8,13,25,2,13,5,16,0,1,23,4,26,1,26,26,28,4,5,24,26,11,0,17,17,24,1,32,0,18,1,1,26,0,14,0,2,9,25,3,2,12,2,26,1,1,26,0,27,0,0,0,0,18,2,25,0,0,0,1,0,14,0,0,1,10,15,14,26,25,0,0,30,23,31,28,17,7,2,5,1,2,3,0,1,0,0,1,25,1,23,26,1,3,1,2,5,1,0,26,1,0,3,0,7,0,22,13,1,2,33,14,26,25,12,13,21,1,0,0,0,13,15,1,28,1,14,19,10,11,0,24,0,1,0,0,2,0,2,1,17,25,16,12,2,15,0,7,24,16,0,0,0,7,2,1,25,22,23,24,0,2,19,24,13,24,15,0,0,0,0,16,5,0,0,26,25,1,1,1,24,24,24,2,0,19,0,25,24,13,29,1,13,0,19,24,25,19,20,0,0,28,0,5,1,1,0,25,23,1,24,1,27,25,28,4,0,25,17,16,1,4,3,16,2,1,2,4,25,28,1,11,16,13,13,14,22,6,0,3,1,0,5,0,14,25,24,16,24,0,1,2,14,1,2,20,7,2,1,12,11,27,13,2,1,16,1,11,0,13,25,1,15,2,1,0,0,0,24,1,1,24,2,0,26,8,23,2,24,2,0,1,0,11,15,24,25,1,21,2,2,0,0,0,27,0,9,24,1,1,1,0,1,4,15,1,21,27,1,1,0,15,26,26,25,3,2,0,2,1,23,25,13,1,1,1,23,1,2,0,19,24,0,1,21,24,24,14,22,1,3,24,1,0,1,1,2,0,22,25,26,9,17,0,0,25,2,18,24,23,20,1,11,0,13,2,7,10,6,24,0,1,0,1,0,19,14,15,24,6,25,26,26,25,17,0,0,0,0,0,17,0,0,25,0,21,25,2,26,25,1,0,26,0,0,19,12,0,0,0,24,0,26,25,25,6,0,2,26,25,27,5,24,2,0,1,2,1,25,25,15,25,26,0,25,25,20,25,30,18,2,14,1,23,16,21,24,2,6,22,1,12,10,0,25,1,1,1,26,6,26,2,0,3,0,26,13,25,0,1,18,24,1,0,1,1,0,27,25,20,1,24,6,1,24,24,0,1,26,0,0,2,26,14,15,0,0,7,2,20,19,0,0,25,25,23,0,0,1,19,0,1,2,17,19,1,0,0,0,0,18,0,0,26,0,0,1,0,0,22,0,5,25,17,26,1,25,2,0,13,25,1,5,24,0,15,0,2,0,0,1,15,25,0,0,0,0,1,24,1,0,1,0,1,25,25,0,10,25,20,20,12,9,1,25,24,25,4,0,0,0,3,26,16,11,0,24,0,0,24,15,0,25,29,0,1,24,26,1,1,3,0,1,0,0,40,0,0,0,0,21,24,14,0,29,26,26,2,0,24,24,0,0,11,24,0,25,26,3,0,11,0,27,0,0,24,15,0,1,0,26,3,20,0,1,16,1,0,3,1,1,0,7,1,2,12,16,24,25,25,25,2,3,2,26,24,0,0,6,0,1,0,0,1,0,0,0,6,1,3,25,0,1,16,0,1,1,0,25,25,0,16,0,1,22,0,0,0,20,9,17,21,1,22,0,0,9,18,2,26,1,1,15,1,28,26,1,2,26,2,1,1,0,0,2,1,19,26,1,1,0,26,27,27,1,0,0,1,1,26,9,1,1,0,2,3,25,24,12,15,20,27,18,22,1,2,2,22,2,1,25,26,26,0,22,1,20,25,0,6,18,19,15,17,10,6,2,3,3,4,4,26,6,26,3,7,1,4,3,8,1,18,14,8,5,24,27,14,20,26,2,13,6,0,1,0,26,25,25,27,2,1,2,21,1,13,0,26,1,0,0,0,0,26,20,27,14,18,16,2,0,0,0,3,25,1,23,2,2,1,0,1,1,15,26,22,19,17,1,0,11,25,0,0,0,23,12,14,17,0,15,0,13,17,2,0,0,29,1,18,12,6,21,25,11,0,1,20,25,1,1,24,2,1,1,1,1,1,1,21,12,2,1,1,13,27,26,7,1,1,1,2,24,24,2,7,3,0,13,0,1,16,25,3,4,1,20,1,1,1,3,1,13,13,0,3,2,22,26,2,26,1,9,1,0,1,24,20,1,22,24,22,10,12,2,0,26,2,7,0,16,0,0,26,24,22,2,0,26,8,18,2,1,2,2,1,1,25,0,6,26,26,26,11,5,25,15,1,11,28,1,2,25,12,12,1,26,19,1,13,26,0,14,15,23,25,1,0,21,25,1,0,2,1,0,26,31,24,8,25,15,2,18,26,1,28,25,13,26,28,15,4,19,11,19,2,27,11,2,25,22,17,8,1,29,27,1,0,1,3,26,29,10,25,0,2,7,1,1,0,25,33,2,27,32,25,2,0,24,0,0,27,6,1,2,0,26,27,1,26,1,24,14,16,2,25,23,19,21,1,27,16,1,22,1,0,1,24,1,1,2,0,1,23,1,1,18,7,22,25,7,2,1,2,1,0,25,26,25,29,25,25,24,1,0,4,15,2,1,0,16,2,13,3,2,21,1,24,2,19,25,2,2,22,2,26,2,2,0,23,14,14,26,1,15,1,26,1,25,3,7,1,25,1,2,1,26,25,28,31,0,1,25,27,2,4,24,1,0,1,24,7,1,20,1,27,0,25,15,13,17,1,6,27,29,1,0,0,0,17,1,25,1,14,3,0,19,5,2,2,6,0,22,2,1,25,25,18,12,5,28,26,25,1,25,1,1,14,15,15,13,1,23,14,1,1,6,26,1,2,1,7,0,28,22,1,25,24,25,13,1,0,5,18,0,25,26,16,1,19,6,1,25,1,25,1,0,17,3,2,0,27,16,8,0,13,28,8,2,29,13,16,17,19,26,26,21,0,17,25,26,9,12,2,15,0,26,29,1,15,1,25,26,26,27,0,14,29,8,29,24,27,75,18,0,2,1,26,2,1,3,0,2,19,26,25,14,1,7,20,24,19,1,0,14,25,3,1,25,26,16,0,1,12,26,2,26,26,13,16,27,26,5,6,1,0,17,13,13,23,2,22,1,25,2,0,24,5,7,2,2,1,1,2,2,17,24,28,1,23,25,1,1,19,25,25,2,19,27,9,5,1,25,20,13,14,1,8,0,1,32,1,13,0,6,26,0,1,12,2,6,1,25,1,1,3,4,21,25,21,18,0,17,2,0,1,4,3,2,2,0,25,25,25,0,2,2,17,25,9,7,13,25,0,2,24,25,19,2,1,4,2,1,24,24,24,1,6,2,2,1,4,4,3,9,1,24,3,1,2,8,2,1,0,1,1,13,27,4,25,28,27,1,28,22,0,6,4,0,11,10,28,0,20,29,20,26,0,3,22,1,1,24,1,1,16,0,0,24,0,16,11,7,0,1,19,0,15,1,0,1,1,0,25,25,0,0,13,24,11,25,22,1,2,1,8,1,1,1,26,6,25,0,0,0,1,25,2,11,14,1,1,3,19,25,19,1,3,0,18,25,9,2,25,1,26,2,1,26,1,25,1,3,22,13,0,16,21,17,24,1,19,20,1,1,22,1,25,0,10,5,1,0,25,7,25,13,0,25,0,24,0,0,14,20,19,17,1,0,2,2,23,20,14,26,6,23,16,27,27,1,25,24,4,21,15,25,25,1,1,7,2,2,26,17,20,15,0,12,26,24,0,0,24,14,1,3,1,0,0,26,27,9,13,3,17,19,18,7,2,2,10,7,3,25,24,7,3,24,25,3,22,0,0,7,1,1,23,20,1,25,26,0,15,25,23,23,25,25,1,22,0,17,21,1,0,4,24,1,17,10,14,22,3,6,22,25,1,3,1,23,21,10,29,18,1,4,19,1,27,7,27,3,12,14,25,12,2,5,17,6,9,23,13,1,0,22,3,1,28,26,0,0,0,24,21,16,7,25,27,25,0,22,10,22,2,1,16,1,28,0,1,5,27,23,17,1,1,1,0,24,1,0,1,1,26,22,1,0,7,1,0,17,1,1,1,38,6,1,17,13,25,25,8,0,6,12,0,0,15,24,1,9,1,3,1,6,0,0,0,0,4,24,25,1,16,1,19,25,1,24,22,5,15,10,24,7,1,0,26,8,2,11,0,1,4,2,0,1,4,17,9,0,15,0,0,1,25,26,27,1,1,21,25,0,1,17,5,0,4,16,3,26,24,1,0,0,1,0,25,26,1,0,20,24,1,1,0,23,4,12,12,1,4,25,0,0,1,22,8,0,28,1,0,26,31,0,25,24,0,1,4,4,12,24,22,1,9,1,14,25,12,24,24,18,1,25,10,25,12,18,4,28,12,15,1,18,0,18,1,26,1,1,27,3,28,27,1,2,0,5,26,27,3,24,26,2,15,0,24,2,1,1,2,27,25,4,26,25,1,16,25,2,2,26,1,21,1,14,1,25,2,1,0,0,19,16,12,27,0,0,0,20,2,0,1,0,8,27,1,1,16,25,19,12,24,11,12,25,0,1,1,1,1,1,1,1,0,23,26,1,16,22,9,21,9,25,25,19,25,21,1,24,0,1,24,19,1,1,13,21,1,1,0,2,0,0,1,25,16,0,1,2,2,1,30,26,0,30,25,24,1,31,8,1,13,18,18,1,0,0,1,4,0,2,13,8,2,1,1,25,1,1,0,25,1,1,25,3,5,0,2,0,23,2,0,0,1,13,2,0,25,25,28,0,3,22,1,2,14,3,24,1,1,2,0,24,2,24,0,0,0,30,1,0,4,6,4,5,25,1,1,1,16,25,23,18,1,22,24,7,12,16,1,24,25,1,0,1,1,23,8,0,25,26,18,0,18,26,29,29,18,1,2,0,2,15,0,0,0,1,1,6,4,1,25,26,1,6,1,26,24,5,0,20,1,12,13,10,14,22,2,2,7,0,22,7,1,2,0,2,0,13,24,11,2,0,23,27,2,1,1,22,10,25,31,0,11,16,25,25,25,2,1,1,13,1,25,0,25,0,1,1,1,17,2,27,1,19,24,25,25,1,0,1,25,26,2,22,1,0,8,5,1,0,2,2,2,25,26,3,1,1,25,0,4,2,1,25,25,2,25,1,1,27,9,21,24,13,25,0,1,23,25,0,1,27,16,18,1,27,20,17,1,26,17,20,1,1,26],\"type\":\"box\",\"name\":\"LeBron James\",\"marker\":{\"color\":\"rgba(102,194,165,1)\",\"line\":{\"color\":\"rgba(102,194,165,1)\"}},\"line\":{\"color\":\"rgba(102,194,165,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"y\":[26,9,15,0,0,13,24,0,0,24,25,7,8,26,25,20,0,7,16,23,10,0,2,25,0,21,26,14,0,9,23,26,5,18,6,19,0,18,14,0,19,25,9,13,25,19,15,7,25,11,0,22,26,21,0,25,19,18,6,0,19,16,24,25,25,20,0,26,25,20,19,25,25,24,0,25,0,0,0,12,11,25,14,17,13,27,3,22,13,16,0,24,25,16,11,17,0,19,15,15,24,16,25,7,14,24,26,24,27,0,0,19,0,5,27,0,26,24,7,5,24,4,6,0,12,0,26,17,0,9,26,25,17,0,25,17,16,8,0,22,11,16,5,0,20,0,0,18,22,18,24,24,17,0,25,22,18,0,21,25,5,0,20,22,0,23,20,0,23,22,24,24,0,26,20,0,8,0,8,19,19,0,26,17,17,20,26,25,17,0,27,0,0,7,23,25,0,17,0,8,8,7,10,13,18,0,21,0,16,0,22,26,12,18,26,17,7,16,24,0,0,25,20,3,0,0,22,0,0,18,16,25,24,5,14,0,16,5,0,24,7,25,0,14,19,0,0,18,18,0,20,16,0,23,0,0,18,0,0,0,24,15,0,13,13,0,15,0,24,14,18,26,13,6,0,3,0,6,19,0,15,19,6,15,25,16,21,16,5,0,13,14,3,14,14,25,25,24,27,6,19,0,0,12,22,20,16,7,20,0,20,0,26,0,17,14,0,0,11,12,4,20,24,18,20,7,16,25,9,19,25,17,0,26,0,27,0,20,0,23,26,19,20,19,17,0,17,14,17,19,20,16,25,24,18,17,10,19,0,17,26,19,0,0,24,0,15,0,22,21,25,24,18,21,8,0,0,24,25,20,18,6,0,25,18,15,6,0,13,19,20,19,24,24,21,0,19,11,21,14,25,0,15,27,0,16,0,25,12,25,21,24,13,0,22,22,10,0,21,24,18,10,0,14,3,18,24,16,26,6,18,4,11,10,14,8,20,0,20,20,0,22,20,0,0,0,24,25,21,25,0,16,0,24,19,20,24,22,8,0,0,9,17,24,21,23,17,24,24,7,3,0,22,21,19,17,0,16,26,6,0,19,21,20,8,18,18,0,25,25,16,0,16,0,18,25,19,6,6,0,14,10,18,17,24,17,25,11,0,18,0,15,25,0,20,18,0,26,21,21,17,0,15,20,0,0,0,8,21,16,16,15,6,21,27,9,17,1,20,6,16,19,24,1,24,1,25,18,15,3,17,0,20,0,16,0,25,0,19,0,15,0,0,0,11,24,13,0,0,23,18,21,0,19,0,25,0,18,27,21,16,18,25,24,0,18,19,13,17,13,25,10,0,24,0,19,20,28,13,11,19,17,15,16,25,8,0,25,15,0,25,15,18,25,0,10,0,0,16,25,26,23,11,17,9,0,17,0,26,24,16,0,9,0,19,0,14,20,0,15,0,0,0,18,27,20,18,8,11,24,0,25,20,0,21,25,16,11,0,15,8,20,17,19,7,21,26,0,0,22,21,17,16,0,25,0,26,0,20,24,17,25,8,21,16,13,17,16,17,15,24,3,14,10,0,17,10,15,19,16,15,6,0,5,24,16,9,18,25,17,0,5,15,4,25,0,26,25,0,13,0,18,16,15,6,0,18,19,19,5,14,7,22,11,13,24,10,19,17,5,15,26,0,10,9,0,0,10,25,0,6,17,5,18,23,22,15,15,24,24,0,0,0,0,18,0,21,5,16,18,0,0,19,17,12,15,15,13,13,0,17,0,21,19,0,0,20,19,19,20,9,22,0,0,0,0,12,13,15,10,18,24,16,12,16,10,4,0,17,4,16,14,19,21,9,14,14,16,18,0,17,22,0,19,25,19,24,0,18,21,16,0,17,0,8,17,17,17,18,0,0,8,13,16,17,0,0,12,0,0,17,7,21,0,16,49,24,22,0,0,20,20,24,0,0,16,20,24,0,11,4,8,0,0,17,18,17,8,5,11,14,0,0,0,21,0,19,5,17,0,19,0,18,20,16,26,7,15,18,0,0,0,6,2,8,26,1,1,2,16,0,18,0,4,24,2,17,18,12,3,11,1,19,17,17,5,17,10,0,6,18,0,0,10,6,0,0,0,21,8,0,15,21,21,0,0,14,0,18,7,24,11,13,0,20,10,8,9,8,16,17,13,16,0,14,11,12,21,21,0,4,0,21,22,0,14,0,20,0,17,19,0,0,0,18,26,15,15,0,14,0,0,26,0,10,0,13,0,0,4,15,0,0,0,15,0,5,15,9,0,15,18,0,16,13,18,10,0,0,0,21,20,10,0,9,26,17,17,0,0,2,20,16,1,21,15,0,8,1,11,14,25,12,2,9,15,18,10,10,3,14,16,10,19,24,2,0,3,9,19,17,16,0,8,17,20,19,0,18,3,0,26,10,0,17,15,0,0,0,0,19,19,17,0,19,6,0,0,17,8,0,3,24,17,15,4,2,0,20,18,0,6,0,14,0,13,0,15,20,14,17,19,0,13,0,16,0,24,0,0,0,20,0,21,0,14,20,22,11,7,18,14,8,0,18,12,0,17,25,7,18,25,9,18,17,15,9,0,20,0,25,17,17,12,0,5,0,12,16,8,13,0,25,17,6,1,19,18,0,7,16,18,13,6,25,7,27,14,14,18,24,5,28,0,16,6,18,19,25,8,26,11,0,18,19,13,15,0,20,14,18,19,20,19,4,0,0,20,0,7,6,19,17,6,17,4,13,12,13,18,22,18,25,14,15,8,15,17,10,16,4,9,0,6,0,6,24,12,14,5,16,22,16,0,16,18,18,11,19,0,18,12,14,11,0,25,25,6,0,17,25,16,16,30,9,22,20,0,17,0,16,20,17,18,10,11,10,11,25,0,14,16,0,12,21,13,0,26,0,19,17,0,13,14,0,12,0,16,0,0,17,0,12,0,19,7,3,1,11,1,12,18,20,4,3,19,16,1,16,17,1,19,15,13,21,20,18,0,9,0,16,9,0,0,8,7,25,13,17,16,0,0,21,2,0,5,8,0,22,20,19,0,9,17,0,5,0,15,2,18,20,26,18,17,17,26,12,11,14,23,13,0,25,0,13,18,15,24,14,14,19,18,0,7,18,0,0,14,0,15,0,0,0,12,17,12,12,14,6,18,22,19,0,22,8,5,0,11,14,14,0,16,0,15,0,0,0,0,0,13,16,5,17,0,12,14,22,10,0,0,25,0,0,0,6,18,17,18,0,25,18,19,17,24,0,19,11,23,17,0,9,0,13,13,15,13,4,14,0,16,12,7,17,0,17,15,14,0,4,22,6,0,21,9,17,25,15,2,16,0,16,19,22,0,14,17,18,0,25,17,3,17,14,18,18,25,20,13,1,7,5,15,7,14,12,8,14,20,24,16,18,8,14,15,12,18,0,0,20,6,16,20,23,11,0,20,13,0,10,0,18,8,15,16,15,0,0,0,0,19,0,14,25,18,0,18,0,25,14,22,15,17,7,0,0,0,15,8,0,19,8,14,0,15,16,25,14,24,0,17,16,17,25,18,10,17,15,17,18,20,0,0,14,0,16,0,24,0,0,0,0,19,24,19,25,20,7,0,0,16,17,17,25,0,14,16,0,25,0,24,0,16,20,7,20,24,0,23,15,16,14,24,13,18,17,7,10,20,0,24,3,18,0,0,13,16,1,7,14,6,15,24,22,17,17,12,0,14,17,11,0,25,25,25,27,24,0,0,26,25,21,21,16,6,13,0,21,13,0,20,19,0,8,17,21,7,0,25,23,18,0,26,16,0,22,16,17,0,0,25,10,0,17,15,18,19,8,0,13,13,11,25,0,24,14,17,19,12,0,14,0,24,0,9,24,19,0,25,0,0,18,24,0,6,0,16,24,16,24,16,11,24,0,0,18,0,0,25,7,25,13,0,0,22,24,25,25,11,25,17,0,23,15,0,25,14,10,11,12,11,9,16,17,18,26,14,26,17,0,17,0,18,18,14,19,0,6,0,25,23,10,15,21,21,22,2,22,26,25,20,2,0,14,2,15,7,5,2,24,25,16,2,12,9,5,6,2,14,14,26,14,17,20,0,25,16,0,19,10,15,14,12,17,0,0,15,24,16,19,13,0,0,24,24,15,0,16,25,3,0,0,25,19,25,11,25,19,15,0,0,12,0,0,13,18,24,17,24,16,15,0,18,0,16,5,24,20,20,18,0,0,0,15,17,0,21,27,19,27,29,17,20,16,19,8,15,5,0,0,19,26,23,19,14,25,25,24,25,0,18,16,0,20,0,21,3,16,24,1,12,24,2,20,17,1,19,20,10,1,24,2,26,0,1,16,17,16,25,25,0,18,17,0,14,25,5,24,0,25,0,3,8,17,13,0,0,0,25,16,15,0,22,12,5,0,0,4,18,24,17,21,5,19,17,19,0,24,7,0,11,23,16,1,17,0,22,0,24,0,25,18,24,0,16,16,16,25,20,16,22,0,24,0,21,24,3,18,22,9,9,24,17,18,19,17,0,21,24,20,15,12,1,17,25,17,16,6,4,19,21,16,11,0,0,0,25,13,16,0,14,19,0,18,16,0,0,11,0,24,25,0,15,21,21,0,14,0,15,21,21,25,20,20,24,17,14,0,0,23,26,0,0,18,21,24,8,15,25,0,20,18,15,17,14,15,14,18,0,0,24,0,24,20,15,0,24,0,15,0,14,15,15,25,18,9,20,10,24,12,16,0,17,16,11,0,14,0,14,0,0,7,0,25,17,25,15,16,16,16,24,25,19,8,25,18,1,19,19,4,20,0,27,0,0,1,19,23,24,3,0,20,2,0,18,17,17,13,10,18,0,0,25,0,18,25,15,24,16,0,18,13,23,18,0,0,23,0,0,25,16,16,15,0,0,21,13,10,0,19,0,25,25,15,16,12,10,20,0,0,25,14,0,0,15,0,19,18,0,14,6,25,0,17,24,0,0,0,19,7,0,7,20,0,16,24,8,13,0,10,8,4,0,20,25,19,16,0,0,0,9,17,26,25,12,20,6,4,15,5,0,13,25,0,0,4,17,25,0,17,0,0,19,0,25,0,15,0,24,13,15,24,14,25,17,0,31,4,17,11,0,7,10,24,0,6,0,22,20,6,18,15,19,24,14,21,0,15,21,25,0,7,0,8,0,9,0,15,7,0,18,8,16,13,8,0,17,23,24,26,11,16,15,25,0,17,17,8,0,21,0,15,14,19,17,17,0,0,0,0,25,15,24,11,15,0,0,0,7,0,20,15,0,21,15,8,21,18,22,0,0,13,23,24,22,16,24,16,17,16,24,15,0,7,0,0,0,15,15,0,6,0,0,25,15,24,5,14,0,24,8,24,14,0,24,18,17,8,9,0,6,18,0,24,15,13,26,14,17,0,16,26,18,18,25,19,26,23,16,19,2,0,0,3,15,23,18,19,11,2,14,16,17,20,28,16,0,0,14,13,0,0,16,0,0,18,25,16,0,24,14,10,22,24,24,22,18,19,21,19,10,13,9,25,18,18,9,21,12,24,0,19,19,25,9,27,0,11,11,17,13,16,14,17,6,12,24,0,0,24,11,0,19,12,0,18,15,16,11,15,18,5,0,8,0,0,19,24,19,20,22,17,0,15,18,0,13,18,0,17,12,22,25,12,10,19,19,0,0,10,13,0,17,7,21,16,14,10,18,6,0,0,20,18,0,0,2,19,16,7,16,13,0,8,6,24,0,5,0,0,0,16,0,16,20,3,25,6,17,16,24,16,4,0,0,12,13,0,23,0,24,23,16,17,0,0,24,14,0,23,16,22,8,0,24,20,20,25,25,20,16,20,9,2,17,15,7,2,19,1,18,8,1,28,4,5,25,3,16,24,20,7,17,15,21,19,26,0,19,17,12,19,17,6,3,14,18,5,14,16,22,21,9,16,0,16,0,24,19,15,8,24,5,11,18,17,22,0,16,14,10,0,22,17,16,14,4,23,0,24,3,23,0,14,11,14,0,0,16,24,0,24,0,18,24,0,20,5,0,16,20,0,26,0,24,25,20,0,25,0,16,4,0,20,0,24,0,0,0,0,15,20,15,0,14,24,23,17,13,24,4,16,23,25,16,16,23,7,16,15,7,0,0,25,27,0,24,18,18,21,19,17,15,5,24,14,16,0,0,0,22,4,0,20,12,19,0,18,18,25,25,0,0,0,23,18,20,0,24,18,5,5,0,14,0,0,19,17,24,25,0,20,7,15,20,10,24,25,18,0,0,17,17,17,18,20,16,8,13,20,17,0,16,25,20,19,0,13,16,26,16,15,16,16,8,12,25,11,0,22,0,25,13,14,10,10,8,22,17,0,25,23,17,23,19,0,5,0,0,25,17,24,0,1,13,13,10,7,24,26,0,24,0,17,15,24,18,0,13,22,13,11,13,7,0,11,0,20,22,20,0,16,0,22,0,18,7,17,9,8,24,25,0,15,24,24,25,13,17,24,0,0,0,0,18,17,15,25,25,7,24,16,19,0,18,13,12,9,20,9,18,0,9,24,25,26,17,0,10,0,0,15,26,26,7,15,4,20,19,17,28,12,0,0,12,14,14,17,2,17,10,0,18,1,6,18,12,24,16,9,25,1,16,13,13,8,0,7,15,14,0,24,0,0,9,0,0,0,12,4,0,0,7,24,10,20,23,13,24,21,8,19,7,5,0,14,19,21,9,17,21,0,0,19,19,24,20,18,24,14,20,16,16,0,17,22,19,0,27,27,10,27,0,0,23,12,0,12,0,0,9,7,10,17,0,16,6,0,0,0,26,9,24,0,24,19,14,25,17,11,26,0,6,25,18,9,18,0,0,24,0,0,26,0,16,24,12,19,13,21,22,4,0,20,0,20,17,0,25,0,24,21,22,26,4,21,22,0,24,6,24,0,0,0,0,10,7,0,0,25,15,6,0,0,16,17,13,0,15,15,0,9,0,21,16,24,10,10,13,15,25,10,9,10,17,24,13,19,0,0,5,24,24,0,21,7,19,5,0,11,24,22,24,24,24,0,14,12,11,12,18,17,24,8,11,0,0,12,14,25,14,8,24,10,7,19,25,13,15,11,7,0,8,24,0,25,14,24,25,21,0,24,22,17,16,0,19,0,13,17,0,19,16,13,0,26,0,25,0,25,0,27,0,24,9,26,13,0,0,22,9,23,0,18,16,13,0,19,12,11,7,19,24,24,14,24,25,9,23,14,12,24,14,8,9,18,0,7,25,24,18,26,0,0,12,0,24,21,8,7,0,0,13,0,16,17,25,23,24,4,16,17,10,18,19,12,6,24,0,26,5,7,23,0,0,9,0,0,24,0,0,17,24,24,15,25,24,0,9,19,14,15,10,19,26,16,0,26,11,15,14,20,24,24,26,16,0,8,0,16,0,16,9,0,19,0,8,16,13,0,11,0,0,24,20,14,16,18,0,26,12,25,29,17,9,30,7,16,8,9,15,8,19,21,21,5,0,13,20,0,25,14,11,24,24,24,0,12,0,10,23,19,7,11,19,0,23,13,0,0,10,20,0,0,0,19,11,11,17,18,4,18,7,25,0,9,0,0,17,17,24,1,1,22,9,25,25,1,14,1,24,26,12,21,1,17,10,17,0,0,0,0,0,13,24,0,16,0,0,23,15,5,18,0,20,16,24,14,18,13,0,18,14,13,15,15,12,15,23,0,26,17,5,0,15,14,12,15,14,29,18,17,26,0,17,25,10,16,12,21,14,0,26,20,25,0,25,12,11,10,15,24,0,8,0,17,12,17,0,24,25,25,16,22,15,17,7,0,13,23,13,18,15,0,16,24,19,8,20,15,24,0,0,0,0,24,27,14,17,0,16,6,21,7,15,14,0,15,11,0,13,23,22,16,17,24,17,0,14,0,0,7,10,0,4,9,16,6,0,12,19,15,8,15,19,24,0,10,15,27,14,6,14,26,27,0,5,0,9,17,24,10,0,14,0,17,6,0,0,24,5,14,24,0,24,15,5,22,13,0,23,20,18,19,0,8,11,3,10,5,20,25,1,11,0,19,17,17,12,24,16,24,18,0,24,0,0,3,14,0,0,13,6,15,15,16,15,17,23,10,12,0,13,0,11,18,14,0,0,17,0,5,0,0,9,12,24,0,21,26,15,20,1,6,25,0,3,18,0,0,1,21,13,0,1,17,14,15,15,10,24,16,15,7,24,10,21,0,19,19,15,14,24,18,0,8,24,0,9,25,15,24,24,0,17,0,16,7,24,0,0,11,22,19,11,19,0,10,24,8,0,15,0,15,14,0,16,0,15,14,14,23,0,0,18,24,0,4,23,0,18,22,0,9,15,9,20,0,12,23,15,13,25,17,17,23,11,16,23,7,18,15,0,19,15,15,0,14,18,7,0,24,0,0,16,6,25,25,15,24,18,15,1,18,1,16,15,28,4,3,12,7,27,17,25,0,14,9,27,0,11,16,3,19,0,18,0,0,20,0,17,20,20,16,24,0,0,10,11,17,24,14,0,9,17,18,20,23,19,9,24,15,0,24,16,0,4,16,3,15,24,24,0,17,18,24,18,6,22,19,10,0,12,0,24,7,0,23,0,20,18,19,0,24,22,25,15,14,0,5,7,16,15,16,5,25,25,24,25,14,16,25,17,0,0,0,16,21,20,11,18,17,25,15,0,24,18,0,9,18,6,0,24,0,17,11,24,0,0,0,0,0,24,20,0,24,24,12,17,9,22,12,25,20,24,19,13,17,18,15,24,9,16,1,9,12,24,14,2,26,16,13,0,24,0,0,24,16,0,6,0,17,0,0,15,24,18,8,13,6,16,0,24,15,7,17,16,14,24,16,11,16,24,0,0,16,14,17,0,19,0,12,0,0,16,0,24,16,14,0,14,24,11,7,14,24,0,0,23,14,12,16,22,0,0,10,25,25,13,4,25,25,24,25,11,11,8,18,0,20,20,10,20,20,11,0,0,0,12,27,0,10,5,26,0,16,10,17,14,24,0,19,26,15,18,19,13,17,23,0,0,0,15,26,19,17,11,20,17,13,16,0,16,18,0,0,9,0,0,12,4,0,24,15,24,9,24,7,0,21,0,15,24,7,7,24,24,14,15,10,23,0,24,17,9,10,19,13,16,24,16,14,0,18,11,26,10,24,19,24,15,10,25,22,0,8,19,0,0,0,26,0,25,27,0,5,0,0,0,19,0,18,24,8,10,18,0,26,26,25,0,21,19,17,0,15,25,14,24,0,14,0,22,25,1,25,8,19,13,12,17,25,6,22,17,26,13,25,9,26,1,25,24,0,16,15,24,0,0,0,0,10,13,5,24,0,17,6,24,17,0,0,24,10,15,8,12,0,0,19,8,0,0,25,16,16,17,25,25,5,24,0,0,13,14,15,0,23,25,24,0,12,12,24,19,26,15,0,25,0,24,0,0,18,13,0,15,0,4,24,24,16,15,24,6,0,0,15,25,27,5,26,26,0,0,0,28,14,11,0,0,18,0,18,15,25,10,20,19,20,24,32,17,18,18,2,17,1,24,14,24,0,0,19,25,19,17,20,17,21,14,10,19,24,13,17,0,10,25,0,6,0,0,14,24,16,7,0,17,14,15,15,23,24,15,11,0,16,17,23,17,0,24,0,18,0,19,12,22,13,11,0,15,25,24,21,24,15,20,24,24,13,13,24,0,16,25,25,0,24,24,28,40,20,17,28,7,0,18,0,5,15,8,24,19,0,18,11,0,0,25,25,24,14,24,18,7,0,19,16,0,0,0,0,0,18,24,0,17,16,0,0,25,14,22,24,6,0,12,18,12,0,1,25,15,49,25,26,27,26,2,17,18,26,5,13,2,0,13,2,0,11,2,11,14,20,27,11,18,24,15,24,20,0,24,18,24,21,13,5,0,25,7,17,24,24,25,4,0,2,14,5,5,16,24,0,17,0,0,0,24,24,13,19,23,24,3,2,18,1,16,1,25,13,7,18,19,17,11,20,1,8,25,0,24,3,17,24,1,0,18,15,1,0,0,27,19,9,20,11,0,16,16,27,19,19,19,0,15,18,19,19,10,19,16,8,1,17,12,24,6,9,6,25,16,23,24,24,1,24,4,16,8,11,9,9,25,2,26,17,26,25,25,2,2,24,7,20,26,2,25,26,17,25,2,21,18,15,9,19,1,15,24,7,19,25,14,24,2,18,15,13,18,26,1,13,16,21,24,18,15,2,0,1,19,9,20,14,5,13,18,26,25,19,2,15,24,17,12,19,0,9,15,26,15,15,23,11,1,23,17,24,23,1,2,1,1,18,24,1,17,1,22,24,15,14,24,14,17,26,8,10,24,24,0,16,19,3,11,18,16,9,17,1,15,22,1,1,6,0,7,24,3,13,20,25,14,7,7,5,24,14,24,1,25,22,24,11,1,25,16,8,25,24,24,21,24,13,14,24,15,11,1,6,24,2,24,17,16,7,23,14,5,1,14,6,14,23,17,16,16,25,0,15,19,1,9,2,24,1,14,1,13,27,18,13,20,10,4,1,24,13,1,0,24,24,16,24,24,15,16,2,1,10,24,24,0,1,19,3,16,2,19,17,18,24,24,25,14,3,25,3,15,25,25,13,26,25,12,20,20,20,10,17,18,4,15,18,24,8,0,25,0,14,19,16,13,19,23,0,27,20,13,18,11,3,1,10,17,24,24,1,24,16,14,17,3,17,13,28,24,0,13,8,16,0,24,0,17,15,25,5,2,24,18,1,1,18,10,17,10,19,17,12,2,24,24,7,22,19,26,2,17,1,23,2,24,1,19,2,16,12,4,18,25,14,2,11,1,7,21,25,26,1,9,25,19,27,25,17,25,26,17,12,10,13,25,25,13,20,25,3,2,17,1,3,19,25,19,18,19,16,25,24,1,14,4,1,0,0,15,0,25,14,17,18,24,0,16,15,14,0,16,15,24,0,25,20,18,17,16,1,2,0,0,0,17,9,14,16,24,16,3,24,7,1,19,23,23,2,0,6,10,17,15,24,17,9,9,25,24,24,24,0,25,11,11,13,12,11,23,0,18,30,14,25,2,17,0,19,7,25,19,20,17,8,23,3,21,25,1,25,18,20,0,1,24,12,2,19,22,17,5,14,19,24,8,9,2,10,15,24,24,15,24,18,6,18,29,24,24,15,8,0,17,12,17,0,24,1,0,17,0,24,20,10,26,25,24,24,24,11,15,6,17,25,16,23,24,0,24,17,17,1,16,23,24,0,25,22,19,19,24,8,1,12,15,8,25,19,16,1,24,16,25,28,24,17,26,14,25,24,12,9,26,10,16,19,5,17,18,14,0,12,0,12,12,17,14,17,21,22,0,20,22,1,1,25,15,16,24,8,15,21,14,1,24,23,15,17,20,30,18,24,19,1,18,0,2,24,24,20,24,21,10,0,14,24,12,24,0,15,9,17,14,0,24,7,24,24,7,6,16,24,19,1,20,23,8,0,13,24,19,22,24,4,16,21,10,14,21,28,19,25,3,25,16,20,0,19,26,4,1,12,16,16,20,20,14,0,18,24,18,1,19,17,17,0,2,25,24,15,19,25,22,18,2,2,18,15,26,18,27,26,26,16,19,4,13,9,22,9,27,18,13,26,0,18,21,19,18,1,19,19,1,20,26,21,24,11,25,10,20,6,7,13,0,9,19,23,16,25,17,19,12,10,24,7,5,24,24,9,19,18,24,12,16,24,24,24,24,3,13,25,15,14,1,25,5,11,17,17,14,15,16,25,21,16,24,24,16,23,18,11,24,10,26,17,20,8,3,24,29,0,16,26,16,25,17,0,16,17,18,17,18,24,0,6,18,15,0,24,5,16,1,24,19,24,0,23,24,1,25,18,22,25,22,22,21,2,24,21,18,22,0,14,9,23,1,6,24,6,24,24,4,8,19,2,20,24,4,19,6,22,24,1,1,23,16,20,18,8,16,28,11,19,26,15,17,25,13,24,15,17,17,4,20,25,20,17,1,3,12,16,28,19,5,15,12,24,13,19,2,1,20,2,24,24,1,22,10,22,24,24,25,24,14,13,12,5,1,15,5,1,0,24,14,24,23,0,0,24,3,14,24,15,16,15,16,16,2,21,20,20,19,21,18,24,21,22,18,17,12,19,2,3,18,18,7,16,28,21,0,19,16,22,7,26,2,13,0,26,15,17,14,25,18,20,25,25,18,25,28,9,24,19,24,3,25,1,0,24,25,24,15,18,23,24,0,7,15,18,11,17,0,16,8,13,2,22,1,24,7,18,16,17,16,23,17,8,25,10,16,1,16,0,18,20,0,19,24,25,10,1,12,18,19,15,18,24,16,15,19,1,18,25,2,20,18,21,25,0,20,19,2,21,25,18,16,24,12,25,1,13,13,14,17,16,1,20,24,11,24,1,5,1,1,11,19,14,7,18,25,9,3,8,16,0,18,14,24,3,19,10,17,0,11,21,19,25,1,17,24,10,2,23,24,24,18,11,15,1,18,17,17,17,13,12,15,17,24,21,10,24,16,24,18,19,13,20,25,28,4,0,28,17,9,19,16,25,27,25,1,25,15,21,23,25,26,17,0,18,24,7,12,26,25,16,26,6,18,16,18,7,2,0,26,14,20,19,13,26,27,3,17,16,22,17,20,21,7,22,0,5,0,1,25,24,11,19,12,27,26,14,13,19,24,18,23,2,15,4,18,26,24,16,13,25,21,2,1,5,24,1,25,24,20,20,2,4,26,3,20,25,2,2,19,11,15,20,19,13,21,18,25,18,19,12,24,24,9,16,23,14,13,19,8,0,14,17,24,25,10,14,13,24,17,0,16,24,1,1,24,1,19,22,10,22,7,22,6,24,15,15,19,18,17,19,23,11,12,19,11,6,2,24,24,24,8,17,18,24,19,14,15,13,16,23,21,7,24,24,27,24,24,1,24,24,19,15,12,24,0,24,19,24,24,18,23,15,23,24,24,17,9,22,19,0,1,24,1,18,24,0,20,26,0,0,7,18,7,14,17,24,1,21,21,3,0,5,15,1,25,17,12,15,25,26,24,25,20,20,17,4,26,16,17,21,25,19,11,22,22,24,29,9,24,25,24,22,19,24,7,3,24,25,0,25,24,19,16,21,1,12,2,25,17,14,9,19,7,27,2,1,20,7,6,9,15,9,26,19,27,20,19,15,2,23,9,1,21,16,25,25,13,17,25,25,25,12,2,0,13,20,9,17,17,19,0,14,24,1,0,17,24,3,25,26,24,24,24,3,24,7,6,0,8,18,24,0,14,24,1,14,8,19,24,24,7,5,19,24,9,0,24,24,0,7,17,13,8,26,6,1,6,24,2,11,15,4,14,11,22,7,21,21,20,14,2,25,7,14,18,20,2,21,25,23,21,22,22,18,17,13,24,1,19,24,1,25,17,21,0,18,5,16,0,23,15,24,1,11,5,7,24,1,18,9,18,13,24,1,17,17,9,26,10,6,25,15,19,25,0,25,14,18,16,15,11,2,9,25,17,15,13,17,25,1,2,13,17,25,25,4,20,11,11,7,28,22,10,15,16,15,16,2,9,25,24,24,23,0,0,10,25,15,23,27,16,11,5,5,24,2,24,1,15,15,25,15,0,21,15,13,9,24,1,0,10,18,26,25,11,1,25,5,0,25,0,25,14,15,0,19,24,9,14,2,17,22,0,8,16,24,22,24,24,8,6,21,2,0,24,24,17,2,7,4,9,6,2,4,5,15,12,24,0,12,0,0,13,22,16,18,16,25,21,19,0,26,9,26,21,25,18,22,6,24,25,8,20,8,25,18,12,13,19,13,4,24,14,0,16,9,13,16,12,19,7,14,0,27,11,3,17,5,0,14,10,18,1,0,18,25,25,20,25,20,16,24,9,2,13,15,18,1,14,24,25,16,10,16,13,18,15,18,5,17,0,24,11,1,24,10,16,0,17,17,5,25,16,0,2,25,25,10,0,16,24,24,1,10,25,0,27,0,2,5,15,0,24,24,24,11,26,25,0,12,3,24,20,13,20,13,15,24,18,27,26,27,11,10,5,18,2,2,15,1,27,2,25,4,16,18,19,23,16,2,26,26,24,23,0,0,13,24,10,14,23,13,13,12,8,14,0,0,14,8,8,18,11,1,17,24,0,11,1,15,12,26,19,18,18,6,20,25,0,14,13,0,22,1,7,1,7,15,11,15,27,1,1,20,9,16,26,14,11,3,19,26,15,2,26,2,10,4,1,2,2,6,26,3,2,9,27,16,0,2,10,0,26,16,16,25,24,1,26,24,0,1,20,14,13,7,2,7,5,1,10,8,24,19,0,24,18,3,24,1,5,25,1,0,24,2,1,13,24,17,9,7,14,6,15,2,10,14,11,16,6,7,11,10,25,14,11,0,25,18,18,25,24,24,0,18,3,25,0,10,0,17,13,16,11,18,18,1,0,26,16,3,26,22,24,0,2,22,14,17,18,0,13,18,17,5,6,1,15,28,1,7,0,25,26,25,12,0,27,12,27,5,1,11,15,0,25,3,17,17,15,2,11,2,16,17,16,0,17,16,14,24,25,24,4,19,25,27,18,20,2,1,5,4,1,2,24,11,7,8,1,6,21,1,5,6,0,1,25,10,0,2,13,26,11,26,2,6,2,26,10,2,1,2,9,24,2,1,26,25,1,0,17,2,15,27,16,9,15,13,13,4,13,22,1,13,1,2,15,12,24,24,14,10,15,25,27,14,8,13,4,6,3,0,0,2,7,17,11,2,2,24,24,1,3,24,0,15,24,0,2,10,24,24,10,0,24,24,0,14,24,12,1,9,24,18,24,15,16,16,14,8,24,21,0,24,14,10,3,18,24,4,4,1,18,12,5,25,14,15,16,24,24,0,23,9,9,24,1,0,24,3,15,24,13,9,0,0,6,24,5,24,3,8,13,1,2,19,11,24,18,24,24,15,15,26,5,26,24,19,18,11,14,6,25,1,11,10,25,24,15,2,4,25,2,25,24,1,2,24,25,18,20,8,25,24,13,2,20,24,6,26,2,27,1,24,18,8,10,11,23,24,17,25,26,1,3,13,24,10,17,0,24,24,10,24,1,4,0,24,23,16,11,11,2,24,8,0,19,16,6,2,17,2,2,23,3,1,22,8,7,24,24,1,8,9,4,4,4,16,24,24,0,13,1,12,24,25,13,11,2,24,5,15,25,24,1,0,20,0,16,24,0,24,24,20,1,18,25,7,9,24,4,23,6,18,24,1,24,24,24,14,24,2,24,1,17,19,11,1,24,18,25,26,6,27,25,17,0,0,9,18,3,25,12,1,15,17,24,12,16,24,0,16,24,15,24,6,1,23,24,24,1,16,1,24,19,14,12,14,24,1,24,16,2,24,25,0,6,22,18,18,0,1,21,13,4,2,4,18,22,25,24,22,25,26,4,17,20,24,3,0,11,9,7,6,17,17,8,25,24,1,1,17,23,24,19,16,3,8,18,7,8,0,15,2,13,16,1,25,18,10,4,1,8,17,12,16,24,28,16,25,22,60,14,14,0,24,1,14,23,0,14,14,24,2,0,17,24,24,17,17,24,13,9,20,0,19,1,24,20,16,26,0,6,25,23,25,22,2,18,9,1,19,4,24,1,1,9,7,4,4,1,9,8,25,1,2,15,20,8,13,16,17,10,10,4,12,16,10,18,2,24,1,6,24,5,10,9,23,0,7,5,23,9,9,24,16,24,7,15,23,24,7,4,24,25,19,1,19,24,1,22,24,17,21,0,6,12,25,24,11,17,24,2,0,12,26,13,2,1,12,9,11,1,17,1,5,6,25,25,24,4,25,15,6,25,25,26,25,3,3,3,2,24,3,24,24,1,1,3,24,24,16,18,25,25,0,24,11,0,13,16,15,28,4,19,16,26,18,15,18,22,1,2,24,24,24,0,24,18,13,16,8,24,15,4,13,17,24,25,1,14,0,1,16,2,1,17,3,16,2,3,0,24,2,1,1,18,19,13,12,12,24,1,7,25,24,1,16,24,16,0,8,24,1,23,14,6,0,20,24,0,11,8,1,18,16,19,1,25,25,0,24,25,25,25,6,7,25,11,7,22,19,20,1,2,17,25,15,6,17,11,1,11,0,0,20,26,14,20,9,15,14,9,15,17,4,5,25,26,18,25,17,16,19,2,1,20,16,25,0,25,15,13,11,25,15,2,12,25,1,1,15,12,16,25,13,24,17,25,10,8,1,18,22,24,2,25,25,24,1,24,24,2,11,12,5,24,28,16,10,16,24,16,15,17,21,5,24,12,0,24,12,0,24,23,24,6,5,4,1,3,14,0,1,24,0,24,6,0,24,18,2,6,16,4,0,15,15,2,2,20,12,25,2,2,18,0,26,7,24,16,8,24,3,0,16,0,19,1,24,8,0,19,15,1,22,10,4,1,1,1,24,8,24,0,9,23,1,2,7,12,0,8,0,25,24,25,1,25,1,18,10,1,7,1,16,20,9,1,8,5,24,1,2,2,16,25,17,17,16,9,2,24,3,1,11,1,3,18,7,19,24,1,14,0,22,19,8,1,1,24,12,14,7,17,2,17,0,17,7,7,18,11,13,8,0,17,3,4,19,13,4,15,26,0,0,4,16,6,2,17,6,25,14,12,3,7,15,17,24,1,15,1,17,1,12,14,1,2,7,17,12,6,4,15,0,1,24,7,11,0,25,25,25,27,4,14,13,11,0,14,6,24,6,23,24,4,17,23,0,12,24,16,17,24,12,16,11,0,24,17,11,24,24,17,10,15,24,1,17,24,19,0,25,26,2,0,9,4,10,20,0,26,10,26,26,1,4,17,25,26,24,19,14,16,16,25,1,0,11,27,1,2,16,0,0,26,25,12,10,18,24,16,9,4,25,19,22,5,24,25,1,7,18,1,1,12,24,14,24,22,3,23,0,18,18,1,3,24,2,12,1,12,6,13,0,7,1,1,40,6,0,2,0,3,10,25,16,1,3,27,23,0,11,10,26,2,14,2,23,16,1,1,2,12,24,1,18,18,16,18,1,22,17,2,13,17,9,3,8,0,24,1,16,15,6,14,24,8,24,14,15,9,24,24,7,25,0,1,0,13,12,11,1,24,25,21,1,6,0,18,24,15,10,7,12,13,11,9,0,9,8,16,15,0,24,2,24,0,24,17,1,24,1,1,15,2,20,21,0,1,1,23,5,24,22,7,1,6,19,0,12,17,8,13,20,15,9,1,16,24,1,22,9,0,21,1,14,23,7,2,25,20,6,11,5,20,13,26,26,13,0,26,1,25,17,25,28,13,14,13,19,14,0,19,18,19,24,15,3,28,15,2,0,18,13,7,24,25,11,24,11,1,25,14,11,14,10,0,2,20,16,24,11,1,16,10,1,14,25,13,15,1,0,15,9,1,24,1,24,1,12,24,1,0,24,25,8,25,24,16,0,9,16,20,8,1,24,12,25,14,11,21,24,18,9,25,19,26,24,11,2,24,25,24,26,11,5,1,2,1,13,2,4,16,5,1,12,0,24,16,0,14,24,8,16,25,8,16,16,20,16,0,14,6,13,0,12,24,22,0,24,24,23,3,12,2,12,18,7,9,0,24,8,2,3,25,15,7,24,0,25,24,23,15,26,17,6,24,24,14,7,1,0,26,27,17,11,1,8,19,25,3,25,1,8,0,18,5,25,1,24,24,4,13,0,24,25,1,1,14,19,23,0,13,14,24,18,0,9,16,24,5,12,25,8,1,11,16,13,24,24,1,25,25,24,17,25,1,1,25,24,1,24,24,8,25,9,0,25,24,17,2,24,24,0,27,13,0,24,19,3,3,29,12,5,12,26,8,5,3,10,3,2,19,18,24,0,17,21,5,21,5,0,18,9,24,18,17,15,23,19,1,0,27,24,24,5,7,11,1,0,0,22,6,24,24,11,25,7,10,3,12,21,25,3,19,25,26,25,25,2,25,12,22,21,16,24,6,21,11,20,10,21,24,16,14,12,0,18,17,28,26,1,18,24,18,25,12,25,17,26,16,16,26,25,26,24,16,2,2,26,19,3,5,15,18,26,10,26,10,26,19,25,25,2,13,14,2,26,18,27,1,20,7,3,13,17,19,17,14,12,4,26,1,7,25,1,9,24,24,14,0,27,11,16,22,6,1,15,12,2,18,26,3,2,2,17,6,18,14,25,26,14,1,19,16,25,20,2,17,4,1,2,1,18,24,10,14,10,8,14,11,24,9,4,4,13,17,15,26,0,16,26,9,2,15,1,22,20,27,19,24,12,28,26,18,0,24,14,8,12,8,1,24,25,22,0,15,14,14,0,24,3,1,0,8,24,19,17,8,14,1,3,24,0,18,0,24,25,24,5,25,9,0,16,1,6,24,24,6,26,1,0,14,0,1,1,15,18,22,22,25,1,14,3,16,0,1,16,1,22,23,24,6,0,7,4,3,24,8,2,24,24,16,1,17,14,0,17,25,2,7,0,2,24,15,7,0,8,10,11,18,21,7,8,1,12,10,17,16,27,29,7,16,12,17,0,1,23,22,15,17,12,13,4,13,13,16,9,4,15,13,17,14,12,24,14,5,24,24,3,1,10,24,19,13,20,19,7,24,16,7,23,23,18,9,1,14,20,2,11,24,26,18,8,25,1,0,4,25,14,14,9,20,17,4,21,3,19,0,27,1,8,17,18,8,0,25,0,26,15,26,8,26,1,26,2,24,18,28,4,18,19,5,16,17,16,1,25,13,20,24,15,16,13,18,17,12,25,12,9,11,13,4,25,5,24,9,8,25,16,1,20,17,5,2,25,17,25,24,15,6,18,25,9,25,8,18,14,15,23,12,1,1,0,10,24,16,1,0,16,24,13,24,0,24,1,10,1,7,1,15,2,25,19,4,5,15,18,18,1,0,7,8,24,13,1,1,11,5,25,8,0,1,17,19,14,1,3,5,12,25,7,24,0,24,13,23,25,0,7,0,21,11,15,14,15,25,24,10,2,20,1,24,1,6,24,13,17,24,13,12,17,25,4,1,19,0,7,7,16,24,1,0,25,1,23,15,11,7,1,25,24,0,21,16,7,13,19,0,20,14,25,6,15,7,10,11,20,1,6,8,14,0,12,5,27,26,26,13,11,6,18,9,16,14,0,15,1,1,15,26,12,18,26,24,11,6,10,1,10,11,13,23,15,10,5,16,25,17,15,24,8,15,16,0,22,10,1,15,15,15,24,19,1,19,13,10,4,17,1,4,24,5,24,13,25,14,1,13,7,18,15,12,16,16,8,14,0,0,0,0,1,9,24,18,6,0,18,10,26,2,16,0,2,16,24,9,1,14,10,3,16,24,11,19,1,13,24,2,18,14,9,11,1,0,18,0,14,1,12,25,1,26,7,14,5,17,18,14,25,17,15,0,14,9,0,17,17,17,0,15,5,20,13,24,5,8,24,13,16,11,24,20,1,9,9,24,19,18,1,2,19,4,1,2,26,11,26,1,15,11,21,26,18,21,19,0,13,12,12,22,24,12,21,17,13,14,9,14,1,1,15,23,24,24,24,1,9,22,9,9,21,25,16,4,25,19,11,3,5,24,1,0,0,24,15,25,5,24,17,17,13,24,16,24,26,22,24,25,25,24,2,3,0,5,2,15,1,19,24,24,1,18,24,2,1,3,24,2,18,2,16,24,14,0,13,19,11,0,2,3,24,15,1,1,2,4,15,11,12,24,0,10,26,20,0,0,0,26,1,0,0,26,16,16,16,16,4,2,1,3,11,3,14,13,21,19,14,13,1,24,24,9,2,25,22,26,14,2,9,3,26,16,26,25,26,11,15,2,3,8,13,27,3,26,3,2,25,13,16,14,12,10,11,2,27,26,27,26,25,5,15,24,9,0,2,3,25,26,0,25,22,14,15,9,5,0,15,9,16,24,12,9,20,6,14,8,2,4,24,24,9,1,5,10,26,10,24,24,24,25,24,13,24,6,2,4,24,24,25,24,3,0,16,1,17,2,1,6,12,1,22,1,25,1,3,9,5,5,22,19,8,15,14,1,1,24,2,24,25,18,24,0,15,16,13,0,0,9,9,24,3,13,0,13,17,15,14,16,24,2,24,16,11,0,11,8,1,11,16,15,7,15,16,6,16,24,1,15,8,1,22,0,18,0,1,18,18,3,1,1,17,25,23,11,13,18,18,20,4,19,15,24,1,2,18,1,25,13,3,5,25,2,18,2,20,26,11,21,5,1,13,0,12,25,5,14,1,2,0,0,15,17,10,13,16,11,3,23,24,21,1,25,10,4,1,12,25,15,14,25,1,2,24,23,2,0,14,12,1,24,10,9,1,9,24,24,18,13,17,13,0,1,0,12,20,11,2,0,15,24,16,0,19,0,15,19,19,0,1,26,12,17,17,23,7,0,15,18,16,15,24,9,5,0,11,0,14,7,0,0,0,16,13,1,16,10,24,25,1,24,19,16,1,24,22,19,13,12,16,23,14,4,9,24,24,1,16,16,14,14,0,21,4,6,9,5,25,5,2,17,17,1,0,24,6,23,17,14,24,11,2,5,9,9,17,17,24,10,13,0,24,1,14,25,15,16,14,6,15,14,1,14,23,10,9,24,24,14,15,23,14,14,10,24,0,24,26,5,25,17,9,8,8,6,0,7,16,25,16,24,24,24,24,13,25,26,21,13,13,1,24,24,25,2,7,2,25,24,5,2,15,23,19,2,14,6,1,26,13,9,24,1,17,19,26,6,2,2,0,15,6,18,1,25,19,25,25,20,1,18,11,7,25,23,0,3,1,0,19,0,23,24,23,11,24,1,16,13,25,9,5,24,10,12,1,5,13,23,3,19,23,9,23,9,11,24,4,24,9,13,25,8,24,24,0,5,16,24,24,26,24,24,6,1,23,6,19,25,1,14,1,2,17,14,2,1,19,12,16,0,24,25,17,10,13,12,0,18,0,25,24,3,24,24,28,18,25,18,2,4,4,24,1,14,3,4,1,0,8,9,17,22,19,12,0,24,0,6,24,6,11,19,14,25,3,16,7,14,6,24,24,24,25,4,24,2,0,25,37,6,25,2,24,25,5,4,0,25,24,2,2,17,18,1,19,5,26,41,25,16,24,15,25,6,24,5,24,7,18,5,25,1,23,0,1,4,2,24,2,5,2,7,17,16,1,25,20,11,5,1,2,13,1,0,18,8,0,1,17,1,25,12,23,25,14,5,10,21,5,12,24,3,8,0,0,20,24,0,0,3,25,24,24,1,1,25,25,7,12,5,20,0,6,10,15,6,0,24,13,25,0,12,12,14,26,4,17,14,1,24,16,24,24,24,11,2,24,24,0,25,19,1,1,18,2,19,2,13,15,24,25,25,24,24,1,13,2,17,13,6,9,24,11,3,17,20,10,2,13,23,1,24,1,10,0,24,24,23,0,0,15,15,10,2,13,17,26,4,24,0,16,19,6,12,11,1,7,15,24,26,24,7,0,2,17,13,0,25,1,0,24,15,17,12,13,0,25,1,14,0,24,1,12,1,24,24,24,1,9,0,10,24,24,6,24,10,15,2,2,16,16,12,24,20,0,16,12,0,20,24,0,10,24,21,17,27,10,5,14,25,26,25,14,31,25,27,16,14,2,12,18,27,13,26,2,2,13,0,25,21,25,7,12,6,26,7,2,28,1,0,8,26,13,4,13,10,1,26,19,0,24,24,6,1,25,24,24,1,0,16,13,18,14,7,24,14,14,0,22,11,1,15,23,1,25,10,1,4,18,3,1,24,2,22,25,20,10,19,1,17,20,14,23,24,0,24,21,16,15,1,18,1,16,14,14,17,27,24,25,16,18,13,0,4,13,0,25,1,0,0,24,6,25,24,18,11,24,7,18,0,23,18,24,19,15,10,14,3,11,13,10,7,1,10,17,0,24,13,0,3,23,10,24,13,7,9,11,1,12,2,19,15,1,24,16,17,9,15,24,0,19,26,0,0,26,27,9,15,0,26,0,25,26,27,25,12,17,2,25,25,19,10,26,17,10,15,0,24,24,22,1,15,19,0,9,12,4,24,1,23,23,1,11,0,0,1,15,0,25,12,1,0,7,8,1,1,1,15,0,18,23,0,6,12,4,25,27,0,4,0,15,25,24,24,18,1,24,11,24,13,1,24,8,25,0,23,10,24,23,11,24,25,16,7,4,5,24,24,23,7,24,15,24,25,0,24,7,5,0,4,10,9,10,16,23,4,24,8,7,14,12,1,24,24,8,1,18,24,4,15,8,25,10,15,12,17,0,24,9,1,1,0,14,25,0,1,13,24,25,1,7,6,1,24,25,6,12,24,10,1,24,23,24,25,15,0,1,18,7,1,18,25,0,26,16,7,14,27,21,16,10,7,28,24,12,25,0,0,6,5,26,15,10,2,26,26,26,22,23,27,26,25,25,21,24,3,26,24,6,9,11,16,10,10,24,13,14,24,17,24,9,25,24,7,18,13,0,0,25,24,13,12,24,19,32,24,1,14,13,1,3,4,13,25,9,23,24,16,3,24,9,24,9,0,12,13,24,0,1,9,25,24,15,18,14,6,14,1,15,10,0,11,23,0,24,25,23,55,7,1,18,1,12,24,16,13,1,25,25,1,2,1,0,0,1,16,0,15,31,24,11,26,25,32,16,4,26,2,7,16,4,20,26,13,24,3,26,0,2,24,17,26,13,1,27,4,27,15,8,26,20,27,17,27,5,31,25,17,1,2,24,22,1,7,0,15,0,24,3,13,0,24,18,5,7,24,24,17,25,24,11,0,0,14,7,22,24,19,17,25,22,4,24,16,9,24,24,17,11,16,7,24,0,25,15,24,24,15,24,1,1,1,0,24,0,16,5,24,0,11,24,24,1,23,1,24,11,25,19,11,5,22,2,24,15,0,16,18,0,2,24,9,14,16,12,17,0,0,24,5,17,11,15,14,25,14,9,12,6,26,0,8,25,13,16,10,16,6,4,23,25,24,26,7,9,24,16,25,19,25,25,24,0,29,15,25,24,25,25,26,13,0,27,8,22,2,1,25,18,25,24,13,17,25,0,20,25,21,18,16,25,24,18,1,19,3,13,12,26,15,8,7,16,26,5,15,26,27,27,0,2,20,11,26,1,19,18,12,25,12,24,13,10,1,16,1,3,0,0,1,14,11,0,10,0,0,24,9,13,11,1,26,29,16,1,18,4,6,26,1,17,6,26,16,14,27,1,25,26,16,26,6,27,25,25,2,24,0,18,20,26,22,25,16,13,25,14,25,2,7,12,8,9,6,3,27,24,25,14,17,0,1,15,14,0,1,24,17,24,15,23,0,1,13,17,14,10,20,0,0,2,2,25,6,24,1,1,0,16,23,16,16,13,29,3,25,25,0,16,28,14,26,25,3,24,1,24,14,25,19,19,14,17,4,3,18,0,10,7,10,6,18,14,25,8,16,0,0,16,0,26,26,11,0,10,26,24,25,22,22,10,13,13,24,0,0,14,18,17,24,0,11,25,1,25,1,24,9,0,14,0,15,1,1,17,9,6,25,25,19,17,15,0,24,18,0,9,7,0,12,25,1,16,11,25,8,10,8,9,23,1,4,14,0,7,2,13,2,0,0,17,15,24,9,25,8,17,12,5,15,26,26,25,26,22,3,18,3,26,26,12,15,7,23,0,16,24,24,24,24,12,26,25,23,14,15,0,16,23,1,26,13,24,23,9,24,1,9,15,24,16,24,8,4,24,26,15,24,18,2,17,17,19,12,2,26,9,11,12,1,21,18,26,12,24,12,27,14,24,0,17,18,19,6,11,25,6,24,24,6,24,18,24,0,24,0,4,8,23,1,16,0,0,24,6,12,24,26,14,1,23,24,11,14,24,1,13,9,11,22,24,25,0,24,0,24,19,13,0,24,12,12,23,1,1,0,14,13,0,0,18,24,16,11,24,23,13,24,14,2,0,6,0,0,21,25,22,18,18,17,24,0,0,24,13,1,0,11,24,20,11,24,2,25,24,26,0,0,20,24,15,20,26,1,2,17,4,16,28,24,0,16,2,27,2,28,26,1,13,27,28,26,4,0,26,20,22,23,25,1,0,0,18,0,16,17,2,20,14,17,3,24,24,24,22,25,22,14,25,6,24,2,26,15,6,26,25,0,0,14,0,0,2,0,0,12,16,14,1,0,13,20,9,12,8,16,20,25,19,25,11,11,25,12,25,26,24,23,24,14,24,1,19,24,24,20,0,17,3,8,17,2,1,14,23,2,24,24,24,24,19,6,24,11,0,14,14,13,11,13,15,24,10,25,12,19,0,23,6,11,18,16,24,25,25,26,28,24,25,1,27,25,26,11,28,16,16,25,1,1,24,25,25,24,24,17,0,7,15,15,16,25,16,24,1,5,2,22,0,24,9,24,24,7,13,15,8,15,17,24,17,17,1,9,1,25,24,14,11,24,1,24,14,24,16,17,2,0,12,16,18,1,1,14,25,24,1,0,14,25,24,13,24,2,0,24,25,25,18,24,19,11,13,2,15,0,7,24,24,13,24,7,24,16,23,2,18,25,10,25,2,0,26,25,25,13,25,13,24,15,8,27,12,26,24,25,1,24,0,16,9,14,1,3,16,17,24,10,25,1,16,13,25,3,11,8,2,10,18,17,20,19,1,14,15,24,0,24,13,14,1,30,14,25,2,27,0,15,18,3,17,24,10,14,13,0,16,14,25,2,23,11,0,24,6,24,0,13,17,9,0,3,21,0,25,27,17,17,6,0,24,0,19,23,23,0,25,24,23,0,0,1,11,15,23,0,0,13,0,24,24,25,19,5,24,24,25,24,25,11,0,15,13,25,8,24,9,18,2,1,25,25,25,26,4,13,23,10,24,24,3,24,24,16,11,17,0,15,3,1,16,11,25,24,6,10,1,1,2,12,1,12,16,24,17,0,25,8,24,12,20,0,1,26,18,0,20,25,25,25,24,9,0,12,12,25,24,25,24,5,25,10,19,25,8,13,16,26,0,11,24,0,25,0,0,26,0,10,25,2,14,16,3,14,13,18,4,1,24,11,0,2,0,24,24,17,14,16,0,25,24,25,18,17,18,17,23,15,15,4,16,26,25,1,25,23,25,24,14,0,9,2,0,21,14,24,13,16,19,13,26,3,24,0,9,24,12,13,14,15,25,24,1,13,16,18,16,14,16,24,25,18,9,24,17,25,14,23,0,12,0,0,24,15,24,13,13,14,24,25,25,11,9,24,14,10,13,24,18,9,11,12,0,1,10,0,25,1,24,10,0,24,18,24,0,2,5,8,0,15,25,22,11,11,14,1,17,6,1,18,25,24,23,10,12,9,4,24,3,1,0,24,25,24,15,17,31,25,13,5,5,13,13,10,12,1,0,24,23,17,1,11,3,5,17,1,24,24,5,24,18,1,12,26,23,1,25,2,24,20,10,18,15,10,23,14,0,1,5,24,21,0,24,18,24,24,25,24,18,12,13,24,0,21,25,19,21,25,26,9,13,12,18,16,9,18,20,25,0,17,18,2,0,15,1,7,13,1,0,24,7,9,19,26,19,7,0,16,0,8,15,25,9,0,24,24,25,18,10,19,15,0,2,0,12,24,25,17,9,26,1,20,24,20,15,24,2,16,1,24,18,20,22,17,24,16,3,0,8,5,24,24,25,17,13,9,25,25,13,24,23,17,24,19,21,16,19,1,24,24,24,6,15,9,24,24,0,24,23,22,15,18,24,0,14,24,0,0,25,25,9,0,13,0,24,9,1,24,0,4,24,25,14,24,22,13,13,0,24,24,9,6,13,25,1,20,13,1,24,23,0,0,25,9,26,25,17,20,8,2,21,23,23,12,25,25,14,2,13,16,10,25,24,1,1,0,18,14,24,24,15,2,8,13,8,14,2,23,19,2,16,6,16,5,24,24,24,24,14,7,27,26,11,14,26,12,25,0,18,21,8,0,24,0,7,26,9,25,21,1,24,24,26,1,25,24,12,25,1,24,2,13,14,13,9,1,24,1,8,9,20,0,25,26,15,2,24,15,0,23,17,1,13,24,4,6,24,16,5,15,1,15,0,24,16,17,18,9,17,1,0,1,26,16,25,12,15,0,12,15,6,13,6,6,0,0,24,24,1,0,25,1,25,12,25,25,4,25,8,10,0,24,11,2,13,10,0,24,0,19,24,25,24,0,24,25,14,5,5,1,24,13,24,14,16,1,25,1,24,24,4,0,25,24,25,24,15,27,15,2,17,1,25,15,2,15,30,25,12,17,2,26,4,2,17,2,26,0,27,24,28,25,11,17,1,24,20,16,15,28,19,19,25,20,2,35,24,5,24,14,24,2,28,26,24,25,12,6,24,22,1,0,18,14,17,2,24,24,24,23,25,14,24,25,13,16,0,0,10,0,24,0,24,18,1,16,24,21,1,24,14,2,11,23,15,25,24,24,24,7,7,2,6,24,13,1,17,23,4,0,1,0,19,25,23,15,24,24,29,3,24,18,9,1,14,20,34,13,14,24,25,27,26,24,28,17,16,20,12,0,13,11,25,24,25,14,17,10,16,25,24,26,6,2,17,7,0,27,25,13,23,12,1,12,10,25,0,25,7,16,14,11,15,24,26,25,25,25,16,24,15,4,25,18,0,19,25,24,25,24,13,16,25,11,25,24,26,18,25,22,7,25,9,26,0,27,25,12,27,17,13,13,27,0,1,26,2,17,27,26,27,8,33,15,19,26,16,9,9,2,13,8,19,13,34,2,16,18,17,1,12,16,2,0,2,25,0,23,25,14,0,24,24,20,23,20,25,23,25,0,25,0,0,17,0,23,0,0,14,24,2,24,3,5,24,24,25,0,24,23,24,20,11,21,1,24,0,19,0,1,1,0,24,24,0,16,3,0,11,16,24,0,15,26,1,3,18,26,0,16,6,25,1,16,18,15,16,6,0,17,15,10,10,24,12,2,0,13,11,24,1,1,24,14,15,22,23,12,23,25,2,14,1,1,5,25,19,0,17,12,24,25,24,17,1,21,1,1,14,0,24,16,26,1,26,25,1,0,18,16,17,0,2,16,19,17,0,2,26,26,0,16,24,0,25,27,14,26,13,24,19,19,27,20,3,4,24,0,24,24,1,2,24,2,13,0,24,7,5,5,4,3,12,24,16,0,17,0,18,16,16,24,24,24,24,0,24,24,13,13,16,25,25,7,0,14,13,25,13,17,15,0,17,24,14,0,25,18,15,25,5,25,25,5,2,15,27,26,0,5,19,26,2,25,25,13,2,19,25,19,0,12,20,1,16,24,19,1,11,14,1,18,15,24,15,2,25,24,24,0,25,25,24,27,0,19,19,24,0,12,18,1,17,25,25,1,25,25,15,14,25,24,25,24,6,15,0,25,19,12,14,26,3,2,28,1,26,25,25,25,21,19,4,0,25,10,2,2,12,26,0,25,21,25,25,4,1,1,25,25,26,25,25,4,8,26,7,16,13,24,2,24,24,24,1,24,23,0,25,24,11,2,25,24,18,24,14,16,16,25,13,23,0,0,25,1,7,2,9,6,21,10,6,1,25,11,2,2,2,0,6,19,0,20,2,8,26,15,25,12,2,0,1,3,2,18,12,9,17,27,3,26,18,26,15,26,14,16,0,17,24,1,16,0,25,19,9,14,18,1,0,24,2,0,27,18,13,8,8,1,25,0,26,11,26,26,31,6,0,26,26,0,26,11,0,26,25,0,25,19,1,17,17,0,0,25,0,0,25,0,0,25,2,17,17,1,13,22,25,1,15,2,12,6,15,26,0,25,16,0,0,13,25,27,16,2,26,24,11,26,8,58,13,25,12,2,17,19,0,13,20,16,5,8,1,24,25,0,15,25,1,11,0,0,15,11,14,25,12,1,14,4,14,12,26,1,0,5,15,13,26,24,4,25,18,4,0,23,26,3,23,9,23,26,26,19,0,13,2,26,17,18,5,18,25,19,13,24,12,13,16,25,10,12,0,20,2,16,0,0,23,0,1,28,18,23,25,23,27,0,15,6,25,24,19,1,16,25,18,26,16,28,1,27,28,13,24,0,26,0,1,27,2,0,4,0,11,19,0,24,18,12,0,24,13,22,12,24,21,15,12,8,14,24,25,16,17,1,4,0,15,4,16,16,13,13,4,2,25,6,28,25,26,17,9,18,11,13,27,12,15,13,16,16,11,25,17,9,1,27,0,0,24,1,1,19,1,0,8,9,2,18,25,8,24,23,24,15,25,2,25,26,0,15,23,14,14,25,27,22,14,0,17,2,0,0,14,16,26,9,0,0,6,13,1,1,1,25,0,1,28,26,0,0,9,26,6,1,2,18,25,18,17,26,6,11,11,13,11,24,1,1,14,26,6,24,14,6,0,0,0,13,22,25,6,0,5,24,25,9,8,27,6,20,26,19,5,4,0,12,15,9,6,19,24,19,3,22,0,20,25,17,20,1,24,17,2,14,11,10,0,2,17,18,11,25,25,0,15,0,9,0,29,13,19,0,8,28,23,1,23,9,0,1,1,25,10,16,0,23,28,12,11,26,1,0,18,0,17,10,18,26,26,18,26,26,14,26,0,20,16,14,13,16,1,4,2,14,9,2,23,16,1,6,2,18,15,1,1,24,24,0,24,1,25,3,1,24,14,2,4,2,24,17,25,0,24,10,15,0,5,26,0,17,0,0,17,15,10,15,25,1,27,24,0,9,0,7,24,27,26,7,25,14,0,17,9,0,7,13,11,26,20,33,1,26,0,1,0,26,26,0,23,7,10,15,26,17,12,25,16,15,16,14,22,16,26,28,14,24,25,1,24,16,17,0,9,20,3,13,18,16,0,26,27,10,17,1,0,26,15,26,23,2,25,26,5,2,0,25,26,0,0,26,26,25,0,12,0,20,4,22,2,7,1,6,0,11,27,10,12,25,24,26,27,17,11,24,7,17,25,4,1,24,1,17,2,0,2,24,19,27,25,4,4,0,12,9,0,2,13,27,25,7,0,2,26,12,11,16,26,1,13,11,15,26,16,17,25,18,26,14,24,16,18,25,25,1,2,25,25,26,11,2,0,0,25,0,16,26,23,18,1,18,6,46,28,12,13,0,0,24,26,24,25,0,19,25,20,10,25,6,25,6,24,17,1,1,0,0,26,25,7,25,29,17,20,26,14,1,2,2,12,2,2,2,26,0,26,15,0,16,0,28,15,25,6,15,25,24,16,25,1,1,24,1,17,18,24,23,1,0,0,1,24,12,17,1,17,7,24,23,16,24,1,11,0,0,13,17,3,19,13,25,22,1,0,18,0,8,25,0,26,24,10,1,8,25,18,22,25,29,2,25,10,14,25,4,14,2,2,17,13,4,1,24,6,21,12,27,24,25,1,13,0,26,1,25,0,25,13,4,3,18,25,11,0,0,25,17,7,26,19,8,26,26,3,2,26,3,14,10,26,25,22,25,26,0,25,3,24,0,25,15,13,6,18,16,1,15,1,8,2,28,1,10,0,1,26,0,6,9,25,23,15,23,26,13,9,21,25,9,0,26,12,11,19,13,18,5,3,31,24,1,0,0,23,9,2,25,23,1,0,18,28,11,0,2,18,25,27,0,9,15,26,0,2,25,4,25,1,0,5,18,25,25,17,2,27,3,26,18,0,1,17,0,16,25,15,17,0,0,26,0,1,23,26,27,0,26,12,0,4,0,15,4,17,25,0,27,25,23,4,0,16,15,10,3,2,0,26,13,15,12,2,19,26,17,9,24,1,26,13,18,19,25,26,20,2,11,4,26,25,0,26,0,17,26,5,26,1,17,26,16,26,26,18,10,23,26,1,0,25,11,12,15,25,7,25,17,26,9,24,11,0,12,8,16,16,3,25,3,16,27,13,1,1,2,26,2,2,23,17,27,25,25,20,23,11,3,18,14,1,25,0,14,0,15,20,11,25,18,25,26,26,23,27,13,0,25,25,24,0,25,13,12,21,9,24,13,3,15,16,26,7,0,26,8,25,1,23,18,17,1,25,14,3,14,18,14,20,16,25,26,17,12,27,1,26,25,12,0,25,15,26,1,26,24,18,0,26,2,28,16,11,25,17,27,1,18,7,17,25,26,26,1,25,25,25,14,1,4,2,1,21,4,19,25,25,25,4,6,13,0,12,20,15,23,24,1,14,2,16,1,25,18,0,1,24,24,25,2,25,23,16,19,26,26,2,24,11,9,26,26,15,25,1,14,11,26,26,24,12,23,17,10,25,18,18,0,1,16,6,8,27,25,27,2,26,1,11,14,2,25,1,1,13,17,13,18,19,27,12,25,1,2,19,1,23,0,28,0,27,22,20,1,16,15,21,24,11,18,19,15,26,25,15,14,3,16,0,0,11,24,0,26,3,29,3,14,17,26,27,19,2,15,26,25,15,1,25,2,1,16,1,13,13,28,0,0,24,1,10,25,21,28,1,18,5,27,2,2,14,1,9,12,23,22,26,19,20,3,17,11,26,19,26,11,22,15,1,1,0,23,0,8,25,2,0,10,1,26,12,17,25,1,24,23,0,26,24,1,1,2,1,0,1,12,11,26,25,25,18,25,8,15,6,16,10,0,23,14,0,9,17,25,6,26,14,15,17,14,25,19,25,11,24,25,12,2,11,11,26,11,1,10,27,9,25,10,13,25,3,1,18,8,17,19,26,25,13,12,1,3,24,8,11,0,17,15,16,25,16,26,14,16,29,21,32,19,3,26,1,13,17,2,1,11,1,18,9,12,25,28,12,10,27,15,16,11,17,17,14,25,13,17,9,20,26,1,25,11,27,12,18,17,25,18,26,27,1,26,7,27,12,0,18,25,12,11,5,18,14,25,26,1,2,25,20,25,12,8,25,2,2,12,3,7,25,25,13,25,25,25,15,21,24,0,25,0,11,8,26,14,23,18,12,19,25,20,26,28,2,20,25,0,23,14,13,2,26,29,27,37,28,26,25,6,11,15,12,6,30,17,26,20,17,26,14,16,8,0,15,9,16,5,0,25,26,25,0,1,12,26,1,16,1,26,16,27,27,3,2,7,26,2,27,12,25,27,2,25,25,0,12,1,14,6,26,26,26,2,1,18,13,1,2,17,16,12,27,19,18,25,15,0,14,18,16,25,25,26,1,26,13,17,18,4,26,19,10,29,26,17,10,25,14,9,14,13,25,14,15,14,24,16,19,27,6,25,16,19,1,10,20,12,16,14,27,13,12,26,18,27,21,10,25,11,18,18,25,25,27,23,5,0,27,0,1,3,15,26,1,1,27,16,16,20,25,18,16,17,26,0,26,9,2,25,17,2,1,1,21,15,10,24,25,26,25,15,21,3,1,1,0,15,22,0,27,17,25,9,18,1,25,29,25,15,24,0,2,25,14,25,16,14,20,16,24,27,5,18,12,25,8,23,25,19,18,15,1,3,9,10,25,25,20,15,25,1,16,26,26,27,14,25,1,23,2,19,18,22,25,13,16,28,23,27,15,26,0,11,24,22,26,1,26,11,24,18,15,24,19,7,19,2,15,18,2,27,1,13,27,30,17,11,1,16,26,9,10,1,14,25,24,1,5,25,43,0,23,15,25,24,12,12,1,24,17,2,25,18,9,1,24,25,15,14,20,13,12,1,25,2,23,18,18,14,2,15,21,2,23,26,19,24,2,26,11,26,27,7,14,15,28,24,15,23,22,15,0,2,11,23,26,16,1,26,27,1,18,8,2,18,13,16,1,26,17,16,22,8,1,3,1,12,0,4,14,4,24,26,24,25,15,17,24,1,2,24,0,25,26,28,13,19,25,56,24,19,24,0,24,17,1,1,3,11,14,19,2,26,10,9,26,11,30,14,23,20,10,5,26,2,17,14,2,27,26,26,11,10,20,2,11,24,1,27,19,22,1,9,26,15,27,15,1,9,19,3,26,22,2,23,26,2,18,29,19,23,19,27,0,19,20,13,1,0,20,18,1,18,1,1,14,18,6,20,19,25,13,15,16,12,24,6,13,17,13,15,15,24,0,17,9,7,17,6,25,14,2,2,6,14,13,3,25,1,15,18,11,25,16,10,18,17,12,2,17,27,25,26,9,28,2,28,6,14,14,19,25,4,25,8,2,0,16,17,25,14,15,26,25,23,23,18,26,24,25,24,24,18,1,15,19,0,1,5,3,25,24,26,26,25,10,19,24,17,8,1,8,26,1,24,11,56,10,24,0,2,24,25,2,26,13,25,4,6,9,26,18,15,9,13,28,27,28,9,28,8,16,1,27,21,28,14,17,16,2,2,26,3,1,19,1,15,15,3,12,19,26,28,7,24,12,24,24,2,1,10,1,26,10,25,8,13,14,26,19,27,4,4,27,19,19,17,30,22,26,12,13,17,11,14,26,16,25,8,2,25,12,24,25,24,15,6,11,26,25,15,17,19,18,1,27,14,3,20,18,25,0,15,0,19,27,18,27,26,26,27,26,6,9,23,23,19,22,28,6,17,11,27,27,26,2,10,11,12,25,1,1,5,12,7,26,25,15,18,11,3,31,24,13,25,26,1,27,13,19,13,18,21,14,13,1,18,1,9,25,16,15,8,24,24],\"type\":\"box\",\"name\":\"Kevin Durant\",\"marker\":{\"color\":\"rgba(252,141,98,1)\",\"line\":{\"color\":\"rgba(252,141,98,1)\"}},\"line\":{\"color\":\"rgba(252,141,98,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"y\":[18,0,0,14,0,0,0,26,25,0,12,25,18,0,0,0,5,3,9,0,0,22,17,0,10,0,24,0,0,15,13,13,28,0,0,22,16,0,25,0,24,24,22,0,3,19,0,12,4,0,20,0,5,0,0,15,0,25,0,13,0,25,9,0,2,17,15,1,9,24,26,13,26,0,4,23,0,64,13,0,0,0,0,0,5,15,22,16,0,0,0,21,0,24,0,25,24,0,0,0,0,24,0,0,0,0,0,0,0,4,12,16,0,0,15,0,20,25,0,38,8,9,0,0,0,0,25,0,21,0,0,0,0,15,14,19,17,16,8,16,0,0,0,0,11,10,0,2,0,17,12,0,9,0,0,0,16,12,17,24,51,0,24,14,0,0,10,5,0,0,0,13,11,0,10,6,11,1,1,1,1,1,0,0,18,12,8,2,0,0,4,0,0,0,20,24,0,5,0,17,0,0,0,0,20,0,0,0,0,0,0,0,17,12,0,14,0,0,16,0,0,0,0,14,14,25,36,15,0,0,0,9,6,0,0,0,16,0,0,25,0,15,13,0,0,0,15,0,0,0,0,0,0,0,0,0,6,0,0,15,0,11,4,4,24,25,0,0,18,8,26,16,0,24,0,0,15,8,5,10,0,19,11,0,0,0,3,5,0,0,17,25,1,0,11,3,0,0,0,16,17,0,0,0,0,15,16,9,16,18,0,19,18,25,10,14,14,5,7,22,0,3,17,14,21,0,13,24,6,25,0,18,12,1,5,18,1,19,4,3,0,0,5,11,0,0,13,0,16,16,0,19,0,0,0,0,0,0,0,17,1,1,4,0,2,9,25,4,1,0,13,23,19,24,25,12,25,5,16,0,0,5,9,0,7,24,0,0,14,0,4,23,0,24,0,15,3,0,0,0,24,8,0,24,25,0,12,15,0,6,9,11,0,0,0,25,12,24,16,0,0,0,0,9,17,17,19,0,0,17,15,24,23,14,26,0,0,17,5,8,17,11,0,0,0,0,0,0,0,0,14,20,10,0,18,0,19,0,9,15,16,0,0,23,0,0,0,9,15,19,18,24,0,0,0,0,0,7,0,7,17,0,0,17,16,0,0,0,0,0,13,14,0,0,13,24,17,0,0,0,20,0,7,10,0,0,0,0,0,0,24,0,17,0,22,2,25,2,0,17,21,19,25,10,23,23,19,15,19,6,26,2,10,26,25,13,0,0,19,8,12,0,17,0,0,0,0,24,24,18,0,17,13,17,7,4,12,14,0,12,5,17,13,0,16,15,0,24,0,0,24,15,20,0,0,5,25,0,20,0,0,0,17,0,0,23,11,0,0,3,5,0,20,0,0,3,24,6,0,0,0,16,16,0,5,0,0,2,0,6,24,12,13,13,8,14,10,18,16,15,0,24,12,0,0,22,12,0,24,25,21,0,0,3,6,0,0,0,9,16,25,17,24,0,37,0,0,0,12,26,9,22,0,17,16,17,0,0,0,15,9,0,0,21,0,22,23,18,18,0,0,18,14,0,15,0,0,18,16,0,0,2,24,25,43,7,8,17,18,0,9,0,0,24,24,0,0,25,10,24,17,15,2,26,12,2,1,2,10,1,1,4,1,19,7,13,2,20,25,10,8,25,3,1,0,0,4,3,22,0,12,0,13,14,17,6,11,19,0,21,0,0,16,5,0,0,18,22,0,0,24,24,21,0,6,11,5,5,17,0,19,0,24,22,14,24,0,0,24,19,14,0,5,24,0,0,24,0,0,0,13,0,11,0,17,11,16,11,0,17,7,0,0,0,0,24,15,12,21,24,5,11,12,16,0,25,24,0,0,18,12,0,9,14,0,13,14,19,17,14,0,24,18,0,9,18,11,13,16,18,0,12,0,18,18,0,25,16,22,24,24,0,7,0,21,13,11,20,20,21,18,18,7,5,0,9,25,18,20,14,24,0,0,7,0,21,20,21,0,0,14,16,6,19,16,20,11,16,0,12,8,14,18,16,16,18,0,7,0,0,16,14,12,21,0,0,0,17,0,0,0,0,0,13,12,22,0,6,19,17,13,15,18,11,14,10,0,24,0,0,21,6,0,25,16,24,19,19,17,0,13,16,10,17,22,4,17,0,0,6,0,21,5,5,0,17,9,0,0,25,11,22,19,21,7,25,0,0,0,10,8,4,10,0,21,16,0,20,0,8,11,0,20,24,4,25,1,20,2,13,16,11,11,22,1,17,3,2,14,10,17,12,24,19,3,0,4,1,0,4,1,2,21,14,2,0,2,19,18,2,0,10,0,0,13,22,18,0,21,8,0,0,12,0,0,18,0,19,12,0,8,10,0,0,18,20,16,12,26,19,0,19,0,12,0,20,25,15,0,0,8,17,5,16,0,0,0,0,0,23,13,0,0,16,16,19,0,19,21,0,0,17,18,0,0,0,6,11,24,0,24,18,24,24,0,26,0,19,5,0,19,0,0,0,15,16,0,17,0,19,0,24,9,8,20,20,18,25,0,0,17,0,18,24,0,19,0,0,8,11,0,5,14,0,14,17,0,25,0,22,0,0,0,11,24,12,21,0,0,12,25,23,0,0,19,10,19,0,0,0,13,22,20,16,19,16,0,31,0,0,11,25,0,9,0,25,17,16,14,17,24,24,13,0,24,24,17,6,24,0,15,6,0,19,0,17,6,14,0,25,7,0,3,0,20,0,22,0,9,20,0,0,4,17,2,2,0,24,1,10,14,8,18,15,3,14,17,24,25,0,24,19,0,22,17,4,16,17,10,16,15,0,14,21,0,0,25,11,16,14,24,22,26,4,13,17,0,16,15,0,13,26,19,0,15,25,24,24,24,23,6,0,18,0,9,11,36,24,6,0,25,19,11,19,11,25,20,23,0,0,0,0,0,13,20,21,4,12,0,24,0,24,0,10,5,0,0,8,25,17,5,0,18,24,0,0,24,17,0,9,17,17,16,0,0,17,19,20,19,0,12,6,24,0,25,6,23,14,0,24,18,0,0,19,22,25,23,0,6,24,0,11,16,13,13,14,24,0,0,0,10,18,24,14,7,7,0,15,12,0,0,18,24,0,13,19,0,24,0,25,24,10,18,0,0,19,0,26,6,10,10,0,23,0,27,0,20,0,15,0,0,12,0,25,0,0,5,23,24,1,0,22,14,17,7,15,22,28,15,0,0,17,0,7,0,12,17,0,19,0,0,18,15,23,7,8,6,0,0,17,6,0,24,24,0,0,21,0,18,24,0,12,11,0,0,0,0,0,24,0,25,0,17,22,0,0,0,15,25,0,12,15,17,11,6,7,0,22,18,0,18,17,0,0,13,24,0,18,18,12,4,0,17,6,0,0,0,6,19,7,17,20,18,18,0,0,0,0,0,15,20,0,18,13,24,18,0,0,19,0,18,24,26,18,0,19,4,0,14,0,0,19,0,4,3,0,19,3,19,23,21,5,4,1,25,16,16,0,15,0,18,0,0,0,11,0,12,24,0,0,25,16,0,0,8,20,12,13,15,10,10,0,0,0,8,0,0,17,0,0,0,0,15,0,0,0,0,0,0,23,20,24,21,18,0,9,5,21,20,0,17,4,12,0,6,5,0,0,0,0,17,23,0,0,25,11,20,0,10,0,0,0,0,0,0,12,19,4,0,17,0,0,17,15,0,11,16,0,16,13,0,3,0,6,0,0,10,4,4,0,0,5,0,12,0,10,16,14,0,11,4,0,5,0,15,18,7,23,0,17,13,19,23,7,19,17,18,13,17,5,15,4,4,15,53,11,11,9,12,14,0,3,3,17,18,0,18,2,0,15,0,21,15,5,22,18,0,0,16,13,0,0,19,18,18,17,11,20,14,18,0,1,23,0,12,15,1,1,11,1,1,10,15,8,12,11,9,0,19,0,0,0,13,10,0,0,0,0,0,0,7,16,0,18,13,17,17,0,6,0,0,0,11,0,7,0,0,0,0,7,5,13,0,13,9,0,0,0,0,0,0,23,1,18,17,16,0,0,0,0,6,10,0,16,0,0,0,0,9,0,0,17,0,20,13,17,16,2,0,12,17,17,0,2,11,0,8,2,13,11,0,10,3,4,0,20,20,0,0,7,16,23,17,0,0,16,0,0,0,0,15,15,19,7,0,0,0,0,0,0,0,0,0,18,0,13,5,6,0,0,0,13,0,9,0,0,0,15,8,0,11,20,13,0,0,6,14,5,0,0,0,17,0,12,14,7,0,17,11,0,5,0,16,14,13,0,0,17,19,16,18,0,22,19,18,2,1,1,2,2,15,14,16,14,4,15,5,3,17,18,24,15,23,25,0,6,14,6,14,15,0,0,0,19,19,0,0,9,15,10,0,0,6,0,18,0,0,0,0,0,0,27,13,14,0,13,0,12,11,11,15,11,0,0,19,18,11,0,16,18,0,0,0,17,17,0,11,0,19,0,0,0,16,0,18,15,20,18,0,7,4,19,6,17,0,0,0,6,24,0,17,13,5,0,16,0,17,0,0,15,7,15,5,5,3,0,9,0,0,13,0,15,11,11,0,8,19,17,0,6,9,19,0,15,0,19,19,17,0,10,0,18,0,16,11,0,19,25,17,15,0,0,10,0,25,0,13,0,10,0,5,1,16,0,25,11,16,7,7,20,17,15,0,16,13,11,22,0,15,0,16,0,42,4,0,17,0,0,11,24,16,18,5,0,0,6,15,9,14,12,16,13,9,16,13,19,17,0,0,0,0,15,13,13,0,0,25,16,0,13,0,13,0,0,0,13,0,0,17,3,16,1,14,5,20,0,17,19,0,1,18,1,12,0,5,25,17,0,0,5,0,0,7,15,0,0,5,18,0,16,21,16,0,16,0,13,0,22,6,15,11,15,0,0,0,0,7,0,0,0,0,20,19,23,23,19,2,19,9,0,15,0,9,0,14,0,13,16,0,0,16,0,16,0,0,16,0,0,0,0,24,4,17,5,13,0,0,16,7,0,4,6,18,1,1,1,1,2,16,2,20,4,17,25,0,6,2,2,10,11,0,10,0,22,0,17,16,0,8,0,0,4,0,22,9,0,19,0,22,11,16,0,7,0,2,13,1,15,2,1,9,2,24,16,3,11,4,0,16,18,0,19,0,19,27,18,0,0,0,15,0,12,17,0,0,10,18,11,1,1,1,12,24,2,1,21,3,10,3,12,2,1,13,20,1,16,2,24,3,18,25,1,3,24,1,1,24,6,16,1,22,24,14,1,1,3,14,19,19,1,2,18,24,12,3,19,17,11,14,1,24,1,15,12,24,17,2,2,18,3,13,3,7,16,2,2,2,14,15,10,2,0,1,16,16,5,18,2,0,8,15,17,1,2,1,0,2,1,18,16,1,16,20,11,4,12,13,24,3,2,4,2,0,5,10,2,3,13,1,1,3,4,14,4,15,11,2,8,3,18,20,18,11,19,24,17,2,1,20,19,7,4,8,4,2,1,3,1,18,6,18,0,0,4,11,0,15,15,0,19,17,15,0,24,12,14,25,19,19,24,21,14,21,20,2,18,16,0,0,14,11,11,15,5,1,12,19,2,20,13,10,9,11,14,0,14,3,0,18,1,1,1,17,14,15,15,1,16,1,24,17,20,3,12,1,5,8,2,15,1,0,15,1,6,18,0,1,16,3,15,12,2,2,14,3,25,18,9,1,3,20,2,3,1,16,0,0,0,0,9,0,14,0,0,12,10,8,0,0,11,0,10,10,7,17,1,1,0,0,2,0,18,2,7,16,24,4,2,17,10,18,1,24,2,22,13,17,17,13,11,18,1,0,1,24,2,3,8,5,3,15,16,18,1,8,19,4,18,15,18,18,15,2,3,16,7,18,16,1,15,2,1,8,1,15,1,1,2,6,20,17,2,7,24,3,2,1,5,22,12,4,5,16,1,4,0,1,18,2,14,7,12,24,13,18,1,0,19,2,2,10,14,19,6,2,3,3,1,3,10,17,3,14,0,7,16,5,14,9,3,11,0,17,10,9,18,11,22,2,25,9,3,3,8,1,13,24,2,6,25,27,13,0,21,14,18,1,0,7,24,16,2,16,16,0,16,12,1,15,1,1,9,20,3,17,24,5,24,8,4,0,5,0,1,18,13,0,14,2,16,5,2,14,13,1,8,2,8,2,0,22,8,2,4,1,1,0,1,0,1,16,5,8,19,1,2,20,1,7,7,2,1,1,26,11,2,6,24,2,8,5,4,2,14,2,20,14,13,4,2,2,17,2,3,19,1,2,14,7,4,0,6,1,9,1,0,1,0,12,3,0,1,3,2,15,1,11,11,16,16,17,2,24,2,15,1,17,7,17,2,13,2,6,11,1,2,1,11,18,0,2,19,0,0,12,4,1,2,2,1,17,11,10,14,17,10,14,17,0,13,1,1,23,13,18,8,13,2,19,15,20,1,1,7,10,16,17,15,20,5,2,2,1,18,1,2,18,1,2,0,3,24,17,10,15,4,3,31,18,1,0,11,18,1,1,7,17,14,9,0,1,23,20,1,17,3,9,15,1,3,5,24,5,17,5,2,16,19,17,2,20,0,2,7,3,1,6,18,16,19,1,16,1,6,22,14,2,2,7,8,1,12,15,0,2,14,6,12,16,17,6,9,7,13,16,2,0,11,25,4,17,2,6,11,10,0,5,2,3,15,12,2,26,21,3,5,1,18,16,2,14,13,18,26,5,1,11,17,11,16,17,15,15,11,18,3,18,14,1,14,10,0,1,6,0,6,0,14,5,16,4,0,1,0,16,1,2,2,8,21,17,10,16,20,2,15,1,17,15,2,19,0,3,11,11,3,21,1,16,1,2,1,14,7,14,3,1,1,22,0,0,0,13,1,0,22,16,15,1,0,6,1,0,3,17,18,17,17,12,4,2,16,1,0,0,0,8,1,2,25,10,18,13,0,1,0,24,2,2,18,2,18,3,0,0,16,2,17,19,24,1,1,1,16,3,4,3,18,21,9,24,25,19,18,17,3,10,14,6,2,20,3,2,19,24,24,1,24,18,9,24,16,12,0,2,1,10,15,14,24,0,2,17,15,24,7,24,4,11,0,22,6,2,21,3,2,18,2,5,15,1,5,5,18,2,1,16,1,0,27,2,25,12,24,28,2,17,0,0,16,20,15,16,1,0,14,1,2,1,0,1,2,8,0,2,1,15,19,0,6,1,16,21,17,16,6,18,15,7,17,2,15,2,24,1,18,24,3,12,17,2,9,2,13,19,10,18,13,17,20,1,2,13,7,15,1,15,2,7,14,1,17,8,0,15,1,7,1,12,22,7,18,14,11,24,0,1,0,7,1,14,3,1,1,10,20,3,20,4,1,1,3,18,11,2,25,19,18,20,6,0,19,18,18,5,0,2,26,25,1,1,1,0,17,14,0,11,4,15,24,18,16,2,24,15,19,3,18,17,5,9,4,10,8,2,2,17,19,8,0,26,1,8,18,24,1,4,1,18,1,0,15,8,1,1,5,0,1,22,4,1,26,18,3,2,15,1,3,1,1,17,0,17,6,1,0,1,1,15,24,24,12,15,1,3,21,1,0,7,0,1,2,0,0,1,1,8,2,6,1,1,0,4,16,17,10,1,18,4,4,25,2,1,0,19,17,11,2,15,4,2,3,22,15,2,18,3,2,12,26,19,2,6,0,6,5,20,1,1,16,16,13,1,1,0,0,2,0,24,2,2,0,18,22,1,2,2,1,1,15,6,17,19,3,20,3,3,14,23,12,12,2,1,16,9,2,20,2,3,13,1,24,1,24,7,1,15,2,0,2,14,0,0,17,11,1,15,24,0,13,2,2,1,16,2,24,43,11,1,5,2,14,11,24,24,3,0,24,3,17,12,1,1,1,16,18,2,17,3,4,8,17,8,25,0,1,15,2,0,6,1,6,0,16,0,0,20,0,0,18,22,22,1,13,12,25,25,2,18,1,2,25,2,8,17,19,3,19,9,8,2,14,3,2,2,6,0,7,18,1,13,24,11,4,15,20,1,13,12,1,13,0,0,2,2,13,13,15,1,3,22,5,17,14,11,3,15,0,11,17,3,12,0,16,13,0,24,19,0,21,9,18,0,17,9,26,16,2,25,17,2,16,14,22,12,12,26,2,11,25,9,1,26,2,3,2,3,16,0,24,7,1,11,6,17,14,2,2,3,5,10,4,5,25,18,0,1,2,12,24,1,15,2,0,14,0,24,24,4,14,2,14,13,1,5,19,18,15,0,1,19,15,26,4,1,16,2,1,1,25,6,1,14,16,8,17,1,15,0,8,3,13,4,20,16,21,4,25,19,6,25,14,8,19,5,15,15,4,2,15,24,9,17,13,4,17,0,16,17,16,2,17,8,0,1,2,1,0,3,0,18,24,8,2,1,18,1,4,17,1,0,0,14,9,21,13,0,2,23,2,24,1,2,13,1,16,15,10,2,22,17,23,2,9,5,0,6,24,2,0,1,0,1,0,4,6,24,11,2,1,15,13,24,3,1,26,29,10,4,6,3,1,19,7,18,14,3,15,11,12,18,18,11,15,6,18,24,15,8,1,1,24,0,1,2,24,5,19,18,10,0,2,2,7,11,19,25,4,12,0,20,1,0,2,19,1,20,15,2,25,19,17,15,16,15,2,16,8,4,15,1,2,5,1,25,17,6,19,15,2,25,20,17,1,0,24,24,1,16,28,0,0,14,2,13,0,7,12,7,19,15,4,23,3,18,25,25,21,0,3,17,1,7,14,2,2,1,16,6,14,1,24,4,18,4,26,1,1,3,19,5,0,1,17,20,22,2,16,19,6,15,17,6,2,22,24,1,2,2,3,1,2,19,24,0,0,25,13,12,9,11,0,1,1,13,17,0,2,6,24,21,14,22,1,5,1,2,24,2,3,1,1,14,0,17,3,2,5,1,18,6,2,0,1,18,17,25,1,16,1,0,18,1,24,19,2,25,8,22,3,2,2,4,12,2,1,9,1,3,3,25,2,2,8,6,16,2,16,2,3,25,8,17,1,2,18,1,24,27,1,2,16,5,2,24,0,4,0,2,6,2,15,13,19,25,1,15,1,3,1,8,1,18,1,21,18,18,0,11,24,2,14,24,16,24,0,9,2,15,0,13,5,3,9,0,1,0,3,24,0,3,0,18,13,1,0,16,0,15,4,24,0,0,6,0,20,1,22,7,26,20,19,15,0,1,19,19,20,16,1,25,17,26,17,13,15,2,0,26,17,26,12,10,19,17,2,1,15,13,2,2,6,2,25,18,18,19,15,16,5,24,2,15,15,1,10,3,22,18,3,25,17,24,2,1,5,1,17,25,13,16,19,19,1,6,19,25,0,19,19,11,1,0,23,6,0,16,18,10,13,24,11,15,2,15,3,16,2,2,25,13,2,69,13,0,3,1,21,2,16,0,16,0,25,2,77,15,24,10,1,0,14,0,1,24,1,11,24,14,24,0,13,17,18,2,0,24,18,0,20,10,2,20,13,24,24,24,1,14,1,14,15,15,25,16,17,15,1,22,18,16,2,14,2,17,1,17,1,18,13,8,12,24,24,12,24,14,10,18,15,0,1,0,16,24,1,6,18,15,1,15,24,12,4,19,17,3,5,17,10,2,16,7,2,19,15,24,24,24,20,0,11,17,16,24,11,15,25,4,3,26,1,15,2,1,6,2,4,1,1,24,1,20,16,25,13,25,16,12,9,16,2,1,17,12,19,3,5,10,16,21,12,14,3,2,17,20,12,21,15,17,25,26,13,26,15,3,26,24,16,19,20,26,17,20,25,18,14,7,24,21,15,15,1,3,23,2,22,18,16,4,23,17,14,17,19,23,17,24,25,8,24,1,1,16,13,2,0,1,22,2,12,2,15,25,17,0,24,0,24,24,24,14,4,24,0,1,1,8,7,1,11,2,2,1,20,12,0,23,2,8,7,11,7,12,15,9,25,18,2,17,10,1,5,2,1,1,19,14,14,24,20,17,19,12,23,2,24,4,10,21,19,17,17,1,0,6,3,18,13,24,26,21,26,19,8,0,2,3,12,1,18,0,1,12,14,24,24,0,12,18,14,17,16,10,1,24,1,0,8,10,20,23,1,2,24,15,15,15,13,7,15,0,14,25,1,12,14,17,24,15,0,10,24,5,8,2,1,15,14,19,13,24,10,26,11,25,24,19,24,16,24,5,14,3,1,8,1,24,3,5,16,24,12,1,0,1,15,13,13,1,0,2,11,21,8,24,0,17,7,1,6,0,7,24,2,6,24,24,5,6,4,1,0,1,1,15,17,24,18,14,1,9,13,2,0,24,15,17,24,16,1,19,1,0,11,19,0,22,24,24,0,2,0,19,10,15,14,0,4,18,13,14,9,12,18,17,18,15,1,16,24,7,17,26,1,2,2,16,1,11,17,18,25,12,19,19,27,18,0,2,15,2,10,17,2,17,24,18,15,12,18,8,0,8,1,9,19,12,24,10,16,0,23,15,6,24,14,10,1,10,20,16,8,1,3,12,7,24,26,13,0,3,11,3,7,8,8,2,1,15,1,24,2,11,18,24,2,6,1,16,9,26,2,7,4,0,11,4,2,16,14,2,2,15,24,3,1,15,9,4,3,25,6,25,0,25,26,4,15,24,1,2,2,1,4,2,2,17,5,18,0,26,0,0,15,25,13,12,7,12,11,13,25,7,17,22,22,0,13,16,0,24,18,24,14,4,41,24,0,14,18,9,24,5,8,15,14,0,1,1,14,17,0,2,13,8,1,16,17,15,3,2,16,22,10,14,19,15,2,2,19,26,3,19,19,24,12,1,21,1,11,16,25,15,19,24,1,14,25,2,26,19,1,8,5,9,19,1,25,16,13,25,25,20,20,1,6,10,11,16,13,6,13,15,22,5,19,0,0,0,10,24,15,16,2,15,2,19,15,2,19,1,6,5,16,24,1,6,20,2,2,20,17,14,2,16,12,24,21,16,16,7,2,25,24,1,17,17,0,14,2,1,22,0,24,19,1,16,14,7,14,16,25,17,0,6,1,14,16,15,0,19,16,23,1,0,19,13,25,19,16,13,2,6,1,4,11,26,14,16,2,16,26,4,11,2,2,16,25,2,0,2,15,1,15,9,1,2,8,1,11,17,2,16,24,11,1,22,0,23,2,18,17,19,16,8,0,23,5,1,14,19,18,2,24,25,14,24,14,2,23,24,1,3,2,25,24,24,23,0,16,18,9,19,24,24,1,0,1,2,25,25,1,1,2,24,25,15,16,24,20,1,2,1,15,1,18,1,2,2,11,17,1,24,0,25,19,6,1,17,1,1,23,0,16,7,24,10,31,24,24,24,0,10,0,0,0,16,0,0,0,0,27,12,0,19,25,0,0,0,26,1,0,12,0,17,0,18,2,11,22,2,24,16,7,17,1,1,13,1,24,2,24,2,24,24,8,25,14,2,0,27,25,16,9,25,3,3,24,1,10,16,1,15,13,1,6,1,11,1,2,17,16,18,1,24,17,2,19,2,23,7,1,23,0,17,24,11,17,8,14,2,19,24,7,23,2,9,13,1,24,1,24,15,20,7,25,0,0,1,4,24,15,24,1,1,16,0,26,49,1,18,5,20,1,24,0,16,2,25,17,2,1,2,16,13,19,10,26,1,2,26,26,24,7,2,11,0,1,24,0,1,13,16,2,16,0,24,10,13,17,24,7,22,14,1,24,18,13,8,1,0,8,1,24,19,1,4,24,13,22,12,1,24,9,18,19,2,3,7,2,4,2,24,3,16,9,3,25,10,0,27,28,18,2,2,2,18,6,17,24,19,23,16,18,18,18,26,25,23,24,17,0,23,6,1,9,26,19,1,17,17,23,24,24,7,2,12,2,24,1,6,1,17,16,14,19,7,1,3,24,1,19,20,16,12,19,24,13,13,17,24,4,5,2,0,4,5,1,7,1,1,18,1,16,24,24,1,3,22,16,1,17,20,1,24,24,14,16,15,10,14,1,2,14,24,18,5,13,16,24,1,18,2,1,1,18,18,0,4,24,13,18,16,12,19,17,0,10,20,24,21,25,25,1,0,3,1,17,18,17,1,17,0,23,0,1,0,13,17,26,24,0,0,5,25,4,1,1,14,9,25,9,0,1,28,25,6,0,11,0,4,0,13,11,14,13,25,23,10,1,20,8,1,24,24,4,0,16,17,1,14,24,24,3,1,16,3,2,8,7,19,1,16,1,1,24,25,25,10,1,18,11,5,1,1,0,2,2,0,21,25,24,9,15,4,2,5,3,2,15,14,13,4,19,24,11,8,24,18,1,15,6,11,8,21,17,2,1,9,24,1,1,0,25,1,16,15,13,18,16,2,13,14,24,5,24,2,16,1,14,2,16,9,25,2,1,1,24,20,12,1,14,5,24,24,2,1,2,1,16,1,8,2,5,24,25,2,17,0,13,18,24,0,0,21,13,24,29,3,26,25,21,6,10,23,5,2,19,1,1,24,24,3,6,4,24,24,17,18,16,25,8,19,1,24,25,26,13,12,2,16,1,16,2,1,2,0,8,9,1,24,1,19,7,17,4,3,0,1,1,25,28,2,19,25,18,24,14,18,18,18,14,6,1,14,8,13,15,17,9,14,17,25,2,15,0,17,16,2,25,24,1,9,0,5,15,19,14,23,9,15,3,16,16,0,24,13,18,5,15,16,1,8,1,16,1,7,16,1,16,19,4,25,1,1,16,2,24,1,18,0,26,5,16,4,12,21,20,22,1,0,1,2,2,0,1,7,17,18,2,24,2,2,0,25,17,0,23,1,2,26,16,25,25,16,9,10,13,17,13,16,25,2,27,28,2,17,26,18,12,26,4,10,2,0,2,25,27,17,17,25,13,17,7,14,26,14,29,6,13,3,2,16,14,15,5,15,7,1,12,25,25,13,7,25,39,24,3,2,11,11,27,19,13,21,26,19,2,2,26,1,16,3,16,1,5,8,25,19,24,17,17,8,1,9,1,7,9,24,9,6,17,18,25,23,17,0,25,1,11,12,17,0,15,16,1,1,25,0,18,0,0,0,1,10,13,1,2,4,24,17,3,1,0,1,1,19,13,17,1,2,1,1,19,2,9,12,0,2,24,1,4,2,16,2,22,1,0,17,25,1,3,1,8,2,1,23,1,13,2,2,0,24,0,4,1,4,19,25,16,18,24,0,0,30,1,14,19,0,0,18,24,24,13,18,2,3,12,2,7,13,24,17,1,13,1,14,16,18,9,16,1,2,17,15,0,16,0,13,8,16,15,25,3,19,17,3,4,2,10,2,26,1,24,2,26,3,1,10,1,25,11,2,26,25,10,17,1,17,17,8,6,1,2,1,1,24,13,18,2,0,13,12,0,18,1,23,13,9,16,2,11,14,0,1,24,18,16,9,0,1,14,1,23,0,14,1,1,15,17,18,10,2,24,1,2,2,0,1,22,23,6,1,18,2,1,0,19,21,19,1,0,35,1,2,0,24,13,2,0,25,17,6,1,13,0,19,2,0,15,13,1,27,12,13,11,1,29,17,18,0,1,16,11,2,11,0,17,1,2,0,9,14,8,11,5,2,1,1,10,1,15,16,2,2,14,12,18,16,24,24,1,0,19,1,10,0,24,24,15,4,19,18,9,1,14,19,1,1,25,19,15,26,14,25,5,6,10,17,0,14,12,3,26,23,1,18,17,24,1,18,18,23,8,1,2,12,18,18,1,25,0,1,16,16,4,2,13,24,16,0,1,16,31,11,2,1,3,6,0,3,1,1,9,1,9,0,9,1,0,24,7,1,17,1,25,24,13,1,16,16,14,1,25,15,2,10,1,1,24,2,23,24,19,1,14,0,0,20,1,24,0,24,21,24,21,0,15,0,11,17,19,19,24,1,16,1,2,24,1,16,1,19,1,0,1,1,26,1,17,23,1,0,2,16,24,17,1,2,1,25,14,25,21,2,1,14,13,14,0,5,24,18,0,19,3,24,20,14,30,2,51,17,23,25,13,9,7,1,2,18,18,1,0,2,16,1,0,16,16,9,2,14,1,1,8,12,1,12,24,16,19,25,15,0,2,0,13,17,15,15,13,1,22,1,12,2,13,16,0,0,18,18,23,10,16,0,1,18,6,20,24,15,9,24,9,1,17,16,13,25,0,16,41,14,26,4,3,12,0,0,17,0,12,14,8,23,25,7,10,19,25,25,24,10,2,10,11,12,24,18,9,13,1,7,1,0,0,17,0,2,1,26,0,19,1,28,19,8,18,1,1,18,26,16,2,13,18,5,5,8,0,2,1,1,4,2,24,1,3,4,17,18,0,25,17,0,9,13,24,1,27,1,1,6,0,0,2,2,20,0,2,2,18,5,16,24,24,25,21,24,24,0,12,20,23,25,4,0,1,8,2,27,25,1,19,13,26,1,19,17,1,23,11,0,1,24,15,21,1,0,14,19,19,17,24,6,26,9,15,1,24,0,1,0,2,24,1,8,24,5,6,0,19,22,24,1,2,5,1,4,7,24,2,0,0,2,14,13,16,10,0,1,2,23,1,5,1,14,1,15,9,0,24,0,14,9,1,15,9,0,2,5,1,13,2,0,2,1,0,0,18,0,0,25,0,0,26,0,5,24,1,0,0,0,8,13,17,24,22,23,2,14,12,24,18,24,1,24,1,19,2,24,12,3,16,1,25,26,3,18,2,26,13,7,3,3,2,26,2,7,14,2,21,1,27,13,25,21,21,25,9,21,15,12,11,16,18,26,4,28,16,3,4,11,0,26,3,1,15,0,10,26,18,15,25,7,15,17,28,14,17,1,16,2,18,2,2,19,1,1,1,24,2,18,1,19,15,24,2,24,24,14,3,16,1,24,4,17,24,1,24,0,24,11,24,16,24,1,18,24,1,13,24,3,18,24,22,8,27,18,7,1,18,1,0,18,19,13,13,0,1,2,17,2,11,16,2,2,1,2,25,2,25,20,1,11,24,24,22,10,3,24,24,15,18,2,24,2,24,0,24,1,25,24,15,17,2,19,7,20,2,3,3,25,25,25,25,1,2,18,23,1,4,21,25,21,16,22,25,25,6,11,16,18,19,1,14,24,0,1,2,24,1,1,25,2,27,19,26,24,26,2,13,6,25,25,24,13,25,17,22,3,25,9,6,12,10,24,24,1,1,10,13,19,3,0,20,15,24,2,24,4,0,1,26,1,12,25,18,24,1,24,3,0,13,20,25,25,0,23,25,25,2,0,0,14,24,25,24,0,18,15,17,16,0,0,1,0,20,1,14,15,25,1,18,0,12,6,18,24,13,1,17,14,2,19,25,1,25,24,17,19,0,16,2,24,8,19,2,1,0,13,1,25,50,17,26,1,1,24,17,1,20,16,17,14,10,1,11,0,2,0,15,0,1,18,25,18,2,25,22,17,1,6,40,16,18,1,1,24,15,19,25,10,3,19,18,1,14,1,15,17,2,1,9,10,24,17,24,16,0,21,18,2,16,15,1,14,14,2,23,24,2,18,3,2,24,8,12,0,7,1,23,1,1,3,4,24,1,25,19,2,13,22,12,1,1,15,1,24,25,17,24,16,17,17,0,0,15,24,11,25,1,25,23,2,17,4,24,25,24,19,4,14,2,9,24,13,14,1,4,18,24,25,4,1,15,14,24,1,24,1,1,23,3,25,20,1,29,6,2,1,24,0,0,1,0,17,6,1,25,25,1,24,27,14,1,24,10,25,19,25,24,19,23,27,25,18,0,0,25,2,1,25,1,0,1,15,28,17,17,23,2,23,3,26,25,33,5,17,0,24,1,2,24,24,27,2,13,19,18,12,0,24,24,4,17,1,0,16,24,4,1,0,1,24,24,24,24,2,18,8,25,12,12,24,1,25,25,25,25,25,2,18,15,1,24,3,1,14,25,4,25,27,17,2,24,1,14,4,24,25,1,0,0,15,20,18,13,25,24,2,19,24,9,24,10,0,16,14,2,24,0,16,0,0,8,11,1,25,1,11,0,0,2,1,24,18,2,10,1,2,7,1,17,11,24,25,1,25,14,2,0,0,0,24,0,1,24,16,2,25,2,4,1,15,1,9,0,1,29,24,1,0,9,27,24,25,2,27,3,0,27,1,2,1,17,0,18,18,0,1,2,18,1,2,16,25,1,19,26,17,3,16,17,25,1,15,17,2,12,1,1,2,25,20,2,1,15,24,0,6,1,20,19,15,0,20,2,1,17,26,1,27,27,24,26,17,1,26,2,27,0,12,27,27,27,11,24,14,16,24,1,2,24,15,16,2,15,15,0,2,1,1,2,2,15,15,17,3,1,4,12,1,0,16,2,18,15,24,16,1,18,0,11,18,24,2,2,26,2,0,16,0,23,23,14,17,17,2,2,25,17,28,1,2,16,11,7,14,4,25,2,17,1,1,2,2,25,1,13,19,25,2,2,2,2,25,16,18,17,1,18,49,1,14,0,16,1,1,26,27,10,0,0,27,0,0,19,6,26,1,0,18,0,0,26,2,0,18,16,1,0,18,14,12,18,24,0,0,0,0,24,19,2,15,0,2,17,17,15,24,1,0,19,0,15,5,1,2,25,0,1,5,9,1,0,15,2,14,19,0,19,2,17,1,23,0,12,25,24,1,1,1,1,19,9,18,0,1,8,16,1,9,24,0,1,16,24,13,1,0,24,17,26,10,17,18,0,13,16,8,4,25,4,1,1,11,26,4,13,16,0,0,26,16,0,1,18,2,26,1,25,12,5,1,26,22,2,3,1,26,7,25,1,28,2,26,2,18,27,18,18,4,17,26,3,29,6,1,2,17,1,24,1,12,0,1,16,1,1,6,2,15,12,1,0,15,16,18,14,4,24,0,24,0,5,13,15,3,3,1,2,25,1,7,0,0,0,2,0,2,23,0,0,24,24,13,2,1,1,14,3,4,15,20,9,1,2,0,1,1,24,19,3,15,24,2,23,24,1,13,8,11,14,16,30,0,13,12,24,19,0,3,1,0,19,6,2,1,0,26,18,1,16,22,17,19,16,7,7,2,12,0,11,1,0,14,17,7,12,17,0,9,24,14,1,18,10,10,17,3,12,2,4,12,13,8,17,14,7,5,24,21,1,13,0,15,13,0,7,6,14,14,3,14,1,1,17,0,19,16,0,14,13,24,19,3,17,5,1,24,3,15,1,3,15,15,0,14,17,2,14,1,0,17,9,18,17,23,39,0,15,13,22,4,2,10,1,12,3,3,8,2,18,2,1,1,2,24,1,25,26,0,6,1,9,24,1,25,23,6,25,0,2,1,2,14,12,24,18,0,19,14,1,16,25,2,24,24,0,6,1,16,15,3,9,11,0,2,18,0,13,3,18,0,11,24,0,18,18,24,2,0,16,17,0,18,24,12,5,13,1,24,4,25,0,23,1,24,15,13,0,0,2,15,23,4,15,19,10,25,2,12,28,0,10,15,26,0,12,0,16,22,0,3,5,1,1,18,7,11,24,0,17,7,1,5,2,2,26,18,10,19,10,27,14,6,0,26,21,5,14,21,11,14,17,27,12,0,11,8,12,18,0,19,0,17,4,5,1,24,3,2,10,1,6,26,25,7,24,8,2,6,16,27,14,14,10,0,14,0,13,14,17,24,13,22,11,1,27,5,3,24,0,5,29,2,0,16,4,1,14,1,14,15,17,19,24,20,25,0,24,1,18,13,15,0,17,20,1,18,16,15,2,10,0,16,1,16,20,26,15,2,2,24,4,1,26,12,15,28,16,25,2,25,0,25,24,17,14,15,10,17,26,11,11,15,2,0,14,16,22,25,27,10,1,7,1,0,22,17,18,14,24,24,0,3,14,19,16,1,1,5,1,10,14,25,1,1,2,0,7,11,26,25,2,2,14,24,1,25,2,25,15,1,1,2,14,14,18,13,24,15,16,0,18,25,20,2,0,1,27,13,22,1,15,0,25,2,0,0,6,19,18,9,1,0,1,15,15,0,1,1,24,18,2,16,6,11,24,0,3,4,1,0,24,24,1,18,15,22,1,2,11,1,0,25,1,5,14,15,8,0,1,8,17,2,24,14,14,13,24,20,3,25,13,24,0,15,23,0,9,14,1,15,23,25,29,2,2,17,1,0,7,20,0,24,0,20,2,19,1,17,1,15,25,1,28,16,12,2,17,0,1,20,1,6,1,6,2,24,1,15,2,1,1,24,24,18,0,15,10,16,9,1,17,15,5,1,23,14,23,19,1,14,8,1,2,24,17,1,1,9,24,16,2,10,1,16,1,24,1,14,14,25,14,24,15,1,26,1,25,25,24,16,16,3,14,2,24,11,1,2,15,18,15,26,1,2,2,17,18,0,1,12,1,13,6,15,17,1,16,17,12,1,2,1,1,11,1,1,16,11,12,25,2,17,12,9,7,1,24,18,3,19,1,19,0,25,17,16,0,15,2,6,1,11,17,14,2,5,23,24,16,3,2,5,15,4,16,2,16,4,18,17,1,6,18,21,2,10,2,24,27,2,16,21,1,26,1,21,1,16,22,16,3,25,1,18,21,16,27,1,22,21,22,0,1,10,15,17,1,13,0,1,5,1,27,24,23,15,2,18,24,16,23,18,1,24,25,29,1,15,3,2,25,4,21,12,24,26,0,18,17,1,13,24,1,16,7,16,24,32,17,24,0,19,1,4,1,24,14,0,25,1,13,24,5,17,24,24,1,23,2,24,13,16,14,11,24,15,14,1,15,5,15,2,2,1,0,1,2,19,1,3,24,25,16,1,2,12,15,20,25,17,25,1,13,21,4,17,15,15,17,24,24,15,0,0,2,16,24,2,17,24,1,4,25,2,0,0,2,24,3,23,17,1,24,24,23,1,1,17,1,1,2,2,24,20,0,29,2,1,3,24,17,16,12,15,18,24,0,2,1,18,13,13,16,24,0,11,1,25,25,16,13,18,19,1,0,1,0,0,25,1,2,1,24,2,0,24,1,2,1,1,19,5,18,2,2,2,6,5,23,24,8,25,16,8,14,24,24,25,2,21,2,19,4,17,2,16,3,2,26,4,25,3,24,16,25,1,13,0,11,1,25,24,25,21,3,20,10,1,24,0,16,2,24,18,24,0,0,1,14,25,1,18,18,8,20,25,18,16,12,2,25,17,4,15,4,24,2,1,14,2,18,4,1,25,18,0,17,1,24,19,24,22,24,25,5,1,24,2,10,1,26,24,46,14,1,13,12,1,0,17,25,0,0,0,0,2,10,13,1,25,24,1,6,17,1,11,0,1,24,15,0,0,0,25,16,0,24,2,24,2,2,24,0,1,18,25,1,24,28,27,25,33,13,17,2,16,1,2,0,25,24,16,0,0,1,14,1,0,15,2,16,18,18,18,24,0,24,23,1,11,18,18,1,1,24,13,9,21,26,1,15,0,12,25,0,25,13,1,26,1,26,18,14,1,18,16,1,25,15,27,1,1,23,11,2,0,26,1,25,12,0,13,26,11,26,2,26,4,28,26,14,22,15,1,13,24,1,1,8,13,14,6,2,24,17,1,12,15,10,0,24,21,11,1,1,24,12,1,25,1,1,18,0,1,25,4,18,1,18,5,25,12,25,18,24,20,1,25,11,24,2,25,1,14,2,24,1,13,1,13,2,24,0,1,15,1,15,2,23,14,2,7,13,1,11,1,25,1,25,6,2,9,1,18,1,11,1,1,6,1,0,2,2,1,37,16,12,2,26,2,17,2,1,17,26,1,18,2,24,1,26,1,3,0,9,0,0,25,24,11,2,1,18,24,7,17,3,27,26,4,1,1,28,24,24,25,14,24,4,24,25,0,23,1,13,16,24,19,2,2,2,25,24,27,2,1,1,5,15,1,18,20,6,2,9,1,13,1,24,1,13,17,2,25,2,0,24,18,9,0,23,11,18,10,9,17,25,1,19,4,1,0,24,0,12,1,14,1,5,1,1,25,12,31,1,27,1,20,15,0,28,0,13,1,23,24,0,1,12,3,35,17,0,2,13,9,0,9,24,24,12,13,22,2,24,24,2,23,25,0,24,26,1,18,3,24,25,1,25,13,2,25,12,15,12,26,1,0,1,25,16,27,0,15,25,27,11,0,10,25,25,26,10,5,10,25,1,1,1,2,10,0,13,25,12,1,2,25,19,1,11,14,0,2,0,18,1,2,1,14,25,16,0,7,12,2,13,22,1,24,25,14,24,14,25,5,1,25,0,1,25,13,12,0,24,30,1,23,16,14,7,3,11,3,25,13,1,2,14,24,14,24,24,2,0,12,12,18,24,13,10,2,24,1,20,9,1,19,14,7,22,16,13,2,0,24,0,24,3,0,0,18,0,3,12,16,1,18,24,1,0,0,1,1,25,2,2,25,1,16,23,12,19,1,7,1,1,17,25,9,1,1,0,14,25,18,17,23,2,15,14,24,14,14,1,12,0,19,0,14,12,17,12,19,1,10,3,24,25,25,1,13,1,24,13,15,16,1,14,0,24,0,25,24,3,13,7,7,1,24,24,11,24,18,0,9,24,12,12,14,15,1,17,14,0,5,24,7,10,20,16,25,1,24,8,20,2,5,24,12,0,13,24,7,2,17,24,23,8,6,18,1,1,11,16,15,15,3,5,10,14,2,24,1,4,15,2,14,27,13,16,6,2,2,25,10,2,5,24,0,28,16,3,1,2,2,0,16,0,0,1,16,17,15,1,12,1,16,2,24,0,25,14,26,1,10,17,0,1,15,10,0,24,18,15,11,1,17,19,10,24,1,0,2,9,23,11,23,0,24,18,1,13,17,3,11,14,0,25,11,12,10,2,0,1,1,22,0,13,0,20,12,0,2,18,0,3,1,1,10,24,2,8,12,17,1,2,24,0,18,1,6,4,24,16,25,1,0,25,13,24,24,0,17,1,4,1,7,1,24,1,3,1,25,8,26,1,0,0,6,9,1,11,1,13,13,1,2,1,0,3,2,1,3,26,1,23,2,0,17,1,0,2,1,0,24,1,17,18,15,0,23,1,0,6,0,15,24,14,1,13,0,17,1,0,1,11,1,23,2,24,15,1,1,19,25,23,24,16,12,1,2,24,24,22,18,10,2,23,24,2,26,15,0,1,23,1,23,24,0,13,1,25,1,1,0,1,15,1,0,2,1,11,6,1,25,13,12,12,1,1,0,0,1,33,2,2,1,18,14,11,2,1,24,1,8,25,24,1,0,10,4,15,0,2,24,18,15,3,16,16,16,24,0,8,23,24,14,1,15,0,3,1,15,0,25,27,0,0,24,25,18,0,1,18,0,16,6,6,14,16,0,0,24,0,1,0,1,8,2,25,10,2,0,2,22,0,10,14,30,25,17,14,25,2,17,2,25,25,0,0,12,2,1,18,6,24,2,2,25,17,0,25,0,0,13,24,0,16,12,15,24,0,24,10,0,1,9,1,1,7,3,25,1,24,6,3,2,7,2,25,1,2,9,26,19,2,1,3,22,0,1,25,1,1,17,10,2,1,10,25,8,9,1,1,1,2,1,8,2,24,2,25,1,1,0,1,15,0,25,18,1,10,1,2,14,0,2,0,1,7,2,25,1,2,2,1,13,12,2,12,17,2,23,1,18,25,16,0,24,1,25,12,0,0,1,0,22,1,24,12,18,0,12,12,1,1,0,14,10,1,2,11,9,3,4,2,1,25,16,24,14,13,0,24,11,11,14,13,11,1,0,1,13,25,1,0,10,1,0,2,17,4,0,25,5,0,2,6,7,1,9,25,17,6,16,2,15,2,18,5,8,25,1,7,26,6,4,24,0,7,2,25,8,25,25,2,25,12,2,17,7,16,0,20,23,1,13,1,5,24,0,2,24,1,22,16,24,0,0,17,1,11,24,15,12,25,14,12,2,14,8,1,24,1,36,13,0,0,2,1,12,0,1,25,24,0,11,1,10,25,16,10,1,15,2,25,2,7,22,11,2,24,16,26,3,1,24,16,2,15,12,20,16,9,2,17,2,2,16,17,28,1,25,19,25,0,1,2,28,1,2,28,0,2,1,12,9,25,11,1,12,14,4,0,24,2,17,2,25,11,15,0,1,1,32,26,1,4,8,13,25,17,26,3,24,24,14,19,2,3,12,7,24,26,2,1,4,16,18,26,1,25,2,25,23,17,4,4,1,25,17,15,12,1,1,1,0,18,27,15,25,1,26,13,25,25,16,11,25,24,16,2,24,1,2,22,13,26,25,3,19,18,14,1,12,9,12,22,0,1,1,22,14,0,14,1,1,18,1,24,23,24,8,24,1,0,0,0,2,0,16,24,23,0,2,4,1,24,2,4,4,1,24,8,2,0,26,26,2,1,2,1,12,5,0,13,1,1,12,0,11,1,8,1,2,0,25,24,18,27,2,25,5,23,1,2,25,25,3,2,5,1,0,25,0,1,0,2,3,27,25,12,2,25,2,0,19,25,3,12,2,22,25,24,1,30,25,27,20,19,22,18,26,19,32,23,5,24,25,12,22,22,17,5,2,24,0,0,14,24,0,15,0,14,4,1,0,25,24,14,24,3,1,2,1,0,16,14,19,12,16,10,1,12,7,18,23,16,18,0,23,6,0,0,2,14,7,0,2,28,25,5,2,0,24,7,0,2,2,4,2,1,4,8,10,3,5,26,11,0,8,15,0,5,0,0,27,13,25,16,4,26,13,16,26,13,0,26,27,29,1,25,24,15,24,1,0,14,24,3,25,25,0,15,2,24,18,13,10,24,5,19,25,17,1,11,0,24,1,12,9,24,1,1,22,25,14,5,24,24,2,25,5,16,0,5,18,25,11,22,19,4,0,24,4,25,51,3,2,24,1,13,14,2,25,2,25,25,25,1,17,14,6,19,2,1,15,13,18,16,1,12,16,0,16,26,11,25,16,1,17,1,11,1,24,12,8,0,0,16,18,5,6,14,14,25,24,15,9,0,8,15,24,24,24,0,0,24,52,12,12,1,1,13,18,1,15,19,16,2,24,15,2,24,0,15,11,12,24,2,13,0,22,22,0,1,8,24,18,16,2,24,0,0,25,14,18,26,2,3,19,27,8,13,2,2,1,0,2,16,1,27,7,0,15,24,10,25,26,14,2,11,1,0,25,16,13,4,28,26,1,1,16,26,4,5,1,5,2,10,25,9,14,10,14,25,24,2,8,1,0,24,2,1,24,0,13,24,0,15,0,24,17,10,23,1,13,26,24,13,12,23,24,15,2,24,24,15,10,24,1,11,25,4,10,1,24,0,24,24,1,24,26,24,1,0,7,0,1,24,26,19,1,14,14,10,19,19,0,1,13,24,13,14,12,2,0,25,24,1,16,19,7,25,0,27,1,25,6,0,9,1,2,0,1,12,0,12,25,16,18,23,12,0,16,19,19,0,6,10,0,0,12,5,15,7,4,0,2,14,1,26,15,0,0,25,9,13,0,24,15,15,1,10,18,1,16,7,15,1,14,0,24,2,22,24,19,0,2,13,1,1,12,1,11,12,0,0,14,9,25,16,6,14,2,12,16,1,2,1,2,22,1,15,1,9,0,17,1,6,14,2,23,12,24,24,14,11,12,24,1,7,2,1,1,27,24,14,1,15,12,1,29,2,2,9,1,1,5,24,19,1,0,14,1,12,12,0,15,15,25,27,3,27,23,13,23,0,12,24,25,18,30,14,1,5,17,24,18,19,24,0,2,25,13,2,23,17,1,3,26,16,15,0,17,16,2,14,17,25,21,24,20,1,25,2,27,2,12,2,10,16,10,20,13,3,16,1,22,1,4,22,10,1,1,14,1,1,24,17,1,1,1,25,17,1,1,23,0,0,24,27,1,0,10,11,10,18,1,1,3,0,24,13,16,15,2,4,18,3,3,2,1,14,16,1,2,0,0,14,24,11,1,22,15,0,11,11,2,13,0,24,1,16,25,13,1,1,1,24,14,1,16,18,26,15,24,0,11,25,0,18,16,0,23,2,12,25,2,2,0,24,15,16,24,24,1,2,24,20,23,1,18,19,0,26,25,19,13,24,24,13,25,2,1,17,26,18,16,0,24,1,0,26,17,1,8,12,1,24,0,1,25,24,7,1,0,1,24,25,12,2,10,1,1,25,15,3,14,19,14,0,0,1,0,4,1,25,22,0,0,2,24,24,2,0,11,7,25,1,17,0,0,19,0,1,25,24,2,1,24,0,13,6,1,24,3,2,24,3,16,15,19,1,14,24,24,0,2,10,24,24,1,2,25,17,11,2,12,12,1,1,10,13,2,2,18,16,19,15,13,16,15,1,24,3,15,20,0,24,13,4,17,1,24,1,17,14,24,24,14,16,24,1,1,19,13,19,16,25,17,14,1,24,18,18,24,25,18,20,15,24,11,4,1,25,15,2,21,7,16,2,25,2,1,25,1,1,24,1,25,24,17,2,25,16,16,15,19,16,20,26,16,1,25,20,8,0,0,7,0,24,25,2,26,1,9,17,23,12,16,17,18,5,0,26,27,17,0,23,23,1,24,11,8,2,0,12,15,1,0,16,14,25,17,15,11,1,17,13,1,1,19,2,16,1,1,1,18,19,7,1,18,25,3,1,16,18,1,13,19,16,1,19,11,1,18,26,13,0,1,26,0,24,15,10,5,1,24,1,21,26,26,24,14,1,2,1,18,8,0,24,10,14,27,17,0,17,6,16,15,0,28,24,27,16,1,15,15,26,1,10,29,25,0,1,3,26,16,19,27,30,24,2,1,17,9,24,16,1,0,13,24,26,24,17,1,25,18,25,0,0,2,15,25,1,4,23,0,25,24,23,1,20,20,14,17,24,24,24,27,17,24,14,15,3,1,2,24,26,25,26,13,24,24,20,24,26,17,2,18,18,14,25,15,9,25,19,12,24,16,1,16,24,24,26,24,15,25,13,24,25,24,28,25,26,14,19,21,24,9,1,25,8,25,26,21,26,10,1,25,15,15,25,13,13,13,24,13,25,12,18,1,26,31,16,16,17,25,2,24,0,17,24,1,2,25,18,2,25,15,17,20,24,1,24,19,18,24,2,15,25,25,22,0,24,24,0,18,0,1,15,11,16,25,2,19,16,1,22,26,16,24,24,3,15,2,0,1,10,1,1,11,24,0,24,1,15,22,24,25,2,15,7,17,24,1,24,17,24,16,20,19,19,7,19,24,28,27,0,12,20,0,29,27,15,1,19,0,19,1,5,20,21,15,18,11,14,14,24,25,24,20,0,19,24,24,3,15,1,24,16,16,13,4,19,27,20,21,19,16,26,18,24,26,2,14,1,1,13,1,26,27,1,27,26,2,25,14,19,1,25,0,16,17,24,13,25,16,24,15,1,2,11,1,8,22,15,1,2,13,0,1,15,24,16,10,24,21,22,25,25,0,1,12,25,25,0,15,1,2,1,24,1,25,16,14,1,0,13,2,0,16,25,0,1,5,18,12,25,16,16,26,6,22,25,25,17,2,16,2,24,26,1,16,0,8,19,17,24,1,0,24,26,1,24,24,24,2,1,2,0,0,1,13,24,1,24,24,14,25,1,26,14,11,1,0,16,16,24,17,0,0,25,24,17,0,1,15,1,3,25,1,1,25,25,14,13,0,25,15,0,20,1,12,14,18,0,17,1,10,16,16,12,14,19,14,16,24,24,18,16,15,13,15,22,2,16,13,26,12,34,16,18,11,25,16,15,13,23,25,25,26,3,4,26,27,23,24,2,13,18,15,24,14,24,14,14,10,24,2,0,15,1,1,1,2,25,1,16,8,16,16,2,26,1,17,25,21,25,0,9,20,1,0,1,16,25,0,1,16,0,7,26,27,23,25,0,25,24,25,0,15,11,17,26,17,15,2,25,17,16,0,18,0,9,15,25,28,2,12,0,0,7,18,24,0,0,24,10,19,1,13,24,6,17,1,23,11,24,14,0,19,0,24,1,24,1,18,1,25,23,19,17,18,5,23,0,14,25,16,1,1,0,24,15,23,13,2,14,12,24,4,1,1,18,25,18,16,14,25,25,24,13,26,13,0,0,0,25,16,9,24,18,26,1,16,24,18,25,28,13,11,24,1,17,16,13,1,0,15,24,14,14,0,24,3,1,17,25,12,2,19,26,0,25,15,27,1,2,23,25,25,26,13,26,2,2,1,27,26,17,28,13,17,17,19,26,0,2,22,25,25,13,9,18,1,18,1,1,6,26,2,24,29,17,24,25,1,24,24,25,1,24,0,9,1,0,25,1,26,0,25,23,2,6,2,6,0,24,1,1,24,1,10,24,13,25,16,14,24,14,24,4,24,25,25,15,14,7,23,24,22,9,13,24,11,13,0,13,16,2,13,12,5,14,1,24,16,24,1,5,1,0,9,2,1,1,15,25,1,14,15,25,28,1,18,0,18,12,14,24,16,17,17,7,0,25,24,0,14,16,23,15,25,23,0,2,24,24,24,8,25,24,24,16,25,3,25,12,9,0,24,1,14,14,20,24,11,16,2,19,0,0,26,0,4,24,26,2,19,9,19,26,16,25,18,26,25,8,0,26,1,18,17,25,16,27,17,26,13,27,28,25,17,27,18,15,24,23,1,25,18,23,1,0,0,25,15,14,25,4,14,24,24,12,24,1,24,1,1,22,2,17,9,0,1,1,26,24,0,26,24,24,17,1,1,3,0,9,2,20,9,18,1,26,18,1,0,25,24,27,1,2,25,25,1,1,1,0,18,24,30,29,10,12,24,13,1,19,1,0,24,24,0,2,15,19,20,17,24,27,18,13,25,0,6,24,24,7,24,9,19,15,1,0,2,19,11,21,25,3,17,2,25,24,1,24,17,25,25,0,2,24,24,18,25,26,2,14,2,2,24,24,19,27,2,30,2,2,18,2,26,2,13,17,2,24,24,24,24,18,2,16,2,17,1,0,28,1,20,0,25,15,1,30,15,12,25,24,2,1,14,15,24,15,1,24,16,0,24,24,1,23,1,26,0,24,24,24,1,0,24,26,1,24,25,17,24,15,0,8,1,1,24,24,25,1,0,25,19,25,24,8,0,1,1,19,25,19,25,26,1,25,25,24,25,13,25,4,15,26,11,25,0,15,2,25,2,2,25,2,0,24,24,17,1,24,24,25,24,2,25,19,25,24,24,2,1,19,27,15,15,18,17,14,1,16,6,0,13,25,3,14,19,24,26,14,30,3,24,0,26,25,2,26,1,24,25,2,35,16,7,14,3,25,23,26,13,27,29,1,24,1,1,1,1,1,1,1,1,0,1,20,19,9,2,3,22,22,17,24,25,25,1,0,13,1,3,0,25,25,1,10,1,1,25,10,1,26,10,0,26,1,25,24,1,24,16,24,13,24,14,15,24,1,1,8,0,8,25,1,0,10,1,12,0,15,27,1,25,1,1,16,1,1,1,25,0,20,2,1,9,2,12,3,13,3,17,11,2,20,2,24,26,1,0,13,14,18,28,15,0,1,27,1,16,15,4,13,1,0,11,24,26,1,0,1,26,0,15,25,15,15,0,0,24,24,25,16,16,2,1,24,7,1,15,12,26,29,11,25,2,0,31,24,14,24,8,25,16,24,0,40,20,13,1,24,19,24,2,2,2,25,26,25,1,19,17,18,25,3,1,25,25,2,3,4,18,2,3,2,4,25,2,24,17,2,24,19,16,12,2,26,2,24,24,1,0,1,0,1,19,25,1,1,21,0,12,0,25,16,0,13,14,1,4,25,1,15,1,7,0,1,1,24,24,25,25,15,18,16,0,25,3,24,0,0,0,24,24,25,1,12,17,1,1,26,9,11,24,13,1,24,1,5,14,1,18,14,3,25,8,1,1,23,18,1,19,18,20,18,25,25,26,19,20,1,26,21,25,28,17,2,32,19,1,17,14,4,0,0,0,1,19,0,13,23,25,14,25,0,19,1,0,15,0,1,15,42,1,0,16,23,0,9,0,24,15,14,25,0,5,16,24,28,22,25,1,1,18,1,6,24,15,1,24,25,28,1,26,2,17,26,19,1,18,7,25,20,17,16,26,25,1,24,25,22,0,24,9,2,4,0,14,26,26,1,26,18,26,16,10,27,27,26,5,2,26,2,0,23,26,0,17,0,25,14,11,24,12,24,0,25,16,7,0,0,24,25,15,25,3,23,1,15,17,4,22,1,19,1,17,7,17,13,0,13,11,2,12,0,1,25,1,17,17,15,0,24,14,15,25,1,18,1,25,24,34,24,24,14,1,12,19,24,24,13,17,0,18,21,17,1,25,0,12,17,19,16,1,18,0,25,17,14,2,1,24,24,9,18,2,24,14,15,24,1,1,1,25,0,25,0,11,27,25,1,0,25,0,24,8,0,26,26,15,19,0,20,26,27,15,6,25,25,0,18,16,3,9,0,18,0,8,0,0,19,17,2,1,26,20,2,2,6,0,24,1,7,17,1,13,0,17,17,0,3,0,27,25,19,17,4,23,2,16,15,13,9,19,4,14,19,19,1,27,2,0,19,19,1,0,2,25,0,1,27,25,19,27,21,1,1,19,22,11,10,13,20,1,2,2,1,20,15,11,15,1,26,16,9,14,0,15,26,26,17,17,18,0,12,16,9,2,1,10,0,13,17,1,19,0,0,19,25,10,2,26,18,19,1,1,16,24,1,17,11,18,20,19,21,0,1,1,18,1,0,15,1,21,13,16,11,1,0,16,0,16,34,0,0,0,17,26,21,14,19,13,1,2,18,17,17,11,16,24,2,16,16,17,20,25,3,12,25,0,21,18,17,17,17,14,16,0,16,5,12,2,2,22,0,15,1,0,17,1,20,1,1,19,21,20,15,11,20,0,18,0,10,1,1,17,0,0,23,0,1,15,11,2,13,19,14,11,19,24,14,14,0,1,0,2,0,24,1,16,16,0,9,25,1,0,2,19,2,20,24,1,24,20,0,1,16,25,24,0,26,68,0,10,19,19,14,0,0,13,0,0,9,2,11,20,19,25,1,21,20,10,17,1,19,20,2,26,27,26,17,13,0,15,1,2,2,2,1,1,2,2,16,2,2,0,1,16,14,1,20,2,4,2,1,0,0,29,20,18,10,17,27,1,19,16,0,19,14,1,16,8,15,10,17,13,0,7,1,0,0,1,1,12,20,1,10,16,25,1,0,16,3,11,17,17,14,17,14,13,25,13,4,0,0,0,0,24,15,25,16,0,21,20,1,1,17,1,15,14,19,1,20,14,16,16,1,1,1,0,25,24,9,1,18,19,19,1,0,2,21,13,1,14,1,25,17,0,20,26,19,13,11,2,1,26,2,18,1,16,19,6,12,1,1,16,19,19,1,2,1,1,0,1,25,16,1,1,1,14,2,24,14,0,1,1,21,1,0,22,1,25,18,9,16,0,17,0,0,19,1,1,1,1,1,17,0,1,19,1,25,0,1,1,0,19,19,11,14,1,25,10,0,2,19,0,18,25,16,24,0,0,24,17,13,1,13,14,0,17,0,1,24,2,20,0,7,2,16,19,20,1,11,1,16,22,11,0,9,0,20,0,15,0,1,19,24,19,0,0,25,13,3,17,2,1,1,7,16,11,20,1,25,4,25,22,1,10,3,26,3,2,34,25,3,0,19,0,1,5,13,12,24,13,16,1,14,13,1,0,0,24,1,3,1,1,2,0,25,0,18,1,26,1,24,14,21,2,20,14,25,3,3,2,21,0,3,24,3,25,3,3,27,2,1,2,11,2,24,18,3,3,24,1,18,25,3,2,24,11,22,1,15,13,24,11,2,14,0,19,0,27,1,16,0,20,25,27,13,25,25,21,26,18,17,10,1,14,0,0,1,0,0,0,2,9,24,16,0,2,13,18,25,1,25,1,1,26,16,14,18,2,3,17,1,26,12,25,2,2,17,18,3,18,2,26,1,14,18,18,20,28,13,28,15,13,15,9,1,0,18,13,24,18,25,17,2,9,1,17,19,27,2,18,26,19,1,0,23,20,0,15,26,1,0,24,20,2,17,1,18,25,13,20,16,9,26,26,25,24,25,17,2,28,19,23,13,4,26,26,2,17,18,2,28,19,1,18,15,0,11,27,21,1,16,17,21,12,16,4,19,4,6,10,6,13,5,5,18,15,14,11,26,5,18,25,25,26,5,26,2,20,2,14,0,13,1,24,0,0,5,11,24,0,1,1,0,12,18,0,0,0,25,0,25,12,0,0,19,13,1,15,17,18,16,26,15,1,1,0,1,1,0,13,1,1,1,24,0,16,1,18,24,3,2,21,25,29,0,14,16,9,17,13,15,0,16,2,12,1,25,0,11,17,24,1,16,6,0,25,8,2,1,25,0,1,19,10,20,0,20,8,12,9,12,1,0,1,8,1,2,12,15,18,10,0,1,1,0,26,0,1,7,1,0,14,1,0,3,13,2,5,25,17,1,20,14,24,3,26,13,0,16,1,1,0,0,1,0,17,1,1,1,1,0,12,1,15,1,1,21,14,18,16,0,0,20,0,10,11,0,14,1,1,0,1,24,20,13,1,22,1,0,1,16,8,16,10,1,20,9,1,2,12,29,10,2,0,1,1,14,1,16,2,11,4,6,18,2,11,0,26,12,1,1,16,0,0,24,16,12,26,25,28,14,0,6,1,12,20,0,13,18,25,21,1,20,25,0,11,10,17,1,11,26,1,1,25,0,13,1,0,21,0,17,0,16,10,2,2,0,1,19,1,2,25,2,24,1,1,1,1,18,2,17,27,1,24,11,1,1,1,17,25,0,1,7,0,13,22,1,0,16,0,25,25,26,26,1,25,2,13,19,20,19,0,26,13,25,2,2,3,0,16,24,0,1,1,2,0,9,15,20,16,23,1,18,24,0,16,1,25,6,1,11,24,14,14,1,0,0,10,15,13,25,25,0,17,25,18,26,0,2,0,0,0,14,11,2,1,0,19,7,19,25,1,21,1,16,15,10,21,0,17,25,0,14,0,25,0,1,14,8,14,19,24,17,11,1,1,16,24,2,27,19,24,0,6,25,4,15,1,24,18,0,8,25,1,1,14,1],\"type\":\"box\",\"name\":\"Russell Westbrook\",\"marker\":{\"color\":\"rgba(141,160,203,1)\",\"line\":{\"color\":\"rgba(141,160,203,1)\"}},\"line\":{\"color\":\"rgba(141,160,203,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"y\":[26,18,14,19,16,4,16,17,20,18,0,1,24,22,20,26,15,0,0,10,24,25,0,18,5,25,1,18,18,15,2,28,24,14,25,13,17,7,19,17,15,26,16,1,3,29,18,16,0,19,22,26,0,21,24,14,0,13,0,22,0,13,29,0,17,25,0,1,24,1,23,35,25,24,19,0,25,0,17,24,25,23,8,25,26,26,16,18,27,19,19,16,7,24,11,0,16,25,19,22,17,0,19,24,9,25,26,15,16,4,22,16,7,1,15,18,16,20,2,15,25,17,14,26,26,6,23,16,18,11,23,26,18,5,5,17,6,19,25,20,26,24,18,20,25,25,0,24,20,11,25,0,0,24,21,16,23,9,24,7,26,4,16,20,19,22,2,16,17,12,23,21,1,27,16,17,24,17,0,0,0,24,23,21,15,0,24,24,0,0,16,24,15,24,0,25,0,23,21,0,0,0,25,0,22,25,11,24,23,21,0,0,26,23,15,0,25,20,0,24,0,21,26,0,17,19,24,8,0,25,18,22,0,0,18,15,17,22,5,20,28,11,2,8,10,22,20,23,6,25,26,1,9,17,25,21,21,27,22,26,23,26,0,20,26,0,15,26,19,18,0,25,26,0,24,26,0,26,21,0,17,25,0,10,26,23,0,15,0,25,25,25,12,3,15,26,20,6,25,5,22,3,27,13,0,18,16,25,9,0,0,20,23,0,0,24,0,19,19,26,22,10,17,22,25,25,14,21,0,18,18,23,24,22,24,25,11,22,19,19,25,0,0,18,7,25,19,14,13,20,2,19,22,17,18,20,20,25,10,2,2,2,19,2,3,15,17,2,18,19,26,2,34,13,26,25,25,19,19,2,23,26,4,9,1,20,25,2,2,14,14,20,34,2,0,11,24,16,16,26,2,5,27,17,2,26,21,26,11,26,19,22,8,27,18,17,26,26,21,19,28,18,26,26,3,23,14,17,25,16,19,0,25,23,26,19,18,5,24,27,20,49,6,11,26,18,6,27,22,3,27,1,15,17,1,0,26,22,25,0,10,24,8,0,21,9,20,21,12,28,16,13,11,24,10,18,25,25,25,0,24,15,26,21,0,0,24,25,18,4,26,24,15,17,24,0,18,18,25,22,25,0,25,23,17,1,18,19,18,7,1,49,17,17,23,2,19,11,27,26,26,17,8,4,14,12,27,2,1,1,15,15,18,9,14,24,4,14,14,18,24,0,18,0,0,24,14,19,21,17,21,0,25,0,23,20,23,14,25,18,18,25,18,24,7,23,1,17,1,26,25,19,2,26,9,57,27,17,29,2,25,20,13,5,16,19,28,28,15,20,23,12,27,17,1,18,21,15,4,2,23,26,26,28,23,27,30,1,29,2,2,21,18,17,0,26,17,26,25,17,19,4,21,0,0,26,8,27,17,27,14,0,26,17,10,27,0,25,27,24,28,0,26,17,13,26,1,2,29,23,3,17,4,9,1,27,1,0,23,25,23,28,26,20,19,25,1,21,0,13,21,28,3,15,27,1,2,20,19,20,2,2,22,25,0,24,2,1,25,19,6,2,16,16,14,21,27,5,24,27,2,2,23,2,1,26,26,26,19,17,9,21,17,19,27,21,19,26,9,15,27,18,2,5,26,1,3,20,1,20,17,63,26,0,7,6,25,19,6,25,21,17,3,10,1,2,1,1,25,25,18,25,0,18,12,24,25,18,15,0,23,24,19,26,24,14,25,16,20,0,16,23,14,20,17,0,12,23,15,0,18,0,25,9,26,16,18,25,26,19,1,26,15,26,15,26,26,24,0,26,10,17,18,18,0,23,24,17,17,0,17,24,9,25,20,23,22,17,0,25,0,20,15,19,17,7,19,13,0,22,10,24,25,26,62,27,12,0,29,2,2,11,16,17,1,27,27,23,26,26,21,25,12,17,24,18,21,2,22,1,22,0,22,19,15,22,15,25,14,25,31,7,25,14,13,25,24,2,26,18,1,27,0,11,27,21,25,26,2,26,4,0,0,25,25,25,0,12,20,19,24,1,15,26,15,22,19,1,15,14,6,23,25,18,23,9,22,26,1,18,15,19,23,10,18,26,25,18,26,5,12,24,20,11,23,4,18,1,24,19,23,19,17,26,16,15,25,1,18,24,12,0,0,17,12,21,25,22,8,0,23,7,0,0,17,19,4,19,0,0,24,0,16,0,18,23,14,23,25,24,15,18,25,3,2,25,2,18,1,12,1,36,26,21,16,15,2,26,3,21,17,21,24,25,12,16,20,22,22,17,24,20,18,19,20,20,26,70,24,24,23,0,0,26,23,17,25,20,1,5,25,25,12,18,18,19,25,17,25,1,0,18,13,26,19,16,13,0,16,17,0,25,21,22,14,25,21,13,18,0,26,23,23,15,20,0,21,20,22,22,21,0,19,22,23,25,16,23,17,23,6,24,16,23,21,25,26,22,7,26,10,28,24,2,2,8,26,2,8,25,11,17,18,20,1,11,27,21,2,17,26,27,8,0,19,2,2,26,17,11,19,25,16,25,25,0,25,10,15,19,0,0,10,20,22,0,23,25,0,12,12,24,14,16,15,16,18,26,21,1,26,25,21,27,13,23,22,27,16,15,24,20,27,25,6,17,2,1,20,8,24,8,15,25,26,19,19,19,9,20,12,18,17,2,27,0,24,14,12,23,15,17,0,19,4,9,25,15,0,11,1,15,0,12,1,22,1,2,1,2,2,13,24,1,23,2,19,27,20,0,3,15,4,10,1,1,2,0,12,16,5,1,3,4,24,1,3,5,2,15,3,15,24,19,25,16,5,23,19,7,18,22,23,26,0,6,0,15,22,1,26,17,17,1,1,9,3,20,11,24,16,20,8,15,1,18,22,24,27,10,18,19,19,21,11,19,2,9,16,2,27,18,20,26,17,2,2,20,25,23,20,26,25,20,16,26,25,25,3,15,15,3,6,11,21,16,26,15,12,22,19,22,22,21,25,18,7,9,10,11,21,27,16,3,10,2,11,3,1,15,1,7,1,25,11,1,1,17,2,17,4,2,2,12,24,22,1,23,20,24,2,25,24,4,25,23,22,3,24,18,24,25,8,1,2,18,20,17,26,20,6,17,18,22,16,29,8,21,2,1,15,27,17,5,11,8,25,17,26,2,10,6,27,25,10,18,24,1,15,2,2,2,24,20,22,2,18,16,17,25,23,14,19,1,24,24,19,17,2,25,14,14,25,18,19,2,14,3,5,25,24,6,18,24,1,26,17,18,12,20,26,19,18,18,23,2,1,23,37,16,24,7,3,7,25,11,19,24,8,2,22,2,25,20,23,25,19,1,17,1,25,20,18,23,22,16,1,25,1,19,19,12,21,17,19,25,25,26,0,17,9,24,26,1,17,20,2,5,2,25,22,17,17,2,2,19,23,2,24,15,14,3,25,16,23,23,24,12,23,2,16,19,2,21,4,20,19,18,4,26,24,28,25,21,1,25,25,19,24,24,18,2,11,25,2,24,21,24,20,24,13,26,24,25,23,46,26,18,20,20,15,18,8,23,24,25,25,18,5,26,5,0,24,2,5,35,1,16,25,26,19,26,4,25,2,2,26,16,24,19,3,13,15,24,17,23,24,13,7,15,2,20,19,25,22,47,18,0,7,13,25,23,22,0,16,26,7,26,25,16,18,4,24,1,18,4,25,27,17,36,19,22,26,5,2,2,21,12,27,25,20,26,24,1,25,26,22,17,18,27,9,23,10,15,17,24,14,1,18,20,7,9,15,16,20,17,16,6,25,37,24,6,0,18,25,25,0,21,23,16,27,26,5,20,21,25,26,18,25,25,2,0,1,25,25,23,1,25,4,14,1,25,13,22,16,0,25,13,25,25,16,4,19,20,16,18,23,24,9,2,24,6,1,7,17,19,27,18,26,17,2,20,21,15,0,22,20,20,7,24,26,6,25,26,20,2,24,25,6,14,16,24,17,12,7,20,7,1,25,24,8,26,18,9,20,12,2,23,11,26,10,26,1,15,23,19,17,17,15,24,2,2,25,25,23,18,22,23,21,1,25,2,0,21,19,2,19,24,25,18,19,1,19,26,17,26,12,2,15,14,0,25,1,19,7,13,1,22,18,52,23,24,20,21,8,24,15,24,24,8,20,27,18,27,22,26,26,20,25,5,15,12,24,15,25,1,25,26,11,23,2,14,1,1,5,26,25,25,14,16,13,2,17,26,27,21,18,24,24,6,11,22,16,24,24,1,2,17,12,22,16,0,18,13,14,14,25,16,0,21,25,25,25,24,5,25,15,0,16,1,2,20,6,18,2,25,6,13,8,31,23,2,2,12,26,2,18,22,22,2,24,11,2,18,23,24,25,6,17,25,23,24,27,1,16,25,26,18,21,25,26,2,22,3,19,25,24,2,17,8,15,17,24,20,6,31,25,26,20,25,20,24,22,23,6,24,20,24,4,23,25,6,2,0,7,22,25,20,19,4,1,26,28,20,14,2,24,24,18,21,15,24,22,19,18,25,19,7,20,24,24,11,18,18,21,24,4,21,0,20,20,10,1,25,18,24,16,2,7,15,17,1,9,2,24,4,25,19,6,21,2,25,12,23,0,26,6,22,2,4,24,15,25,20,22,22,26,17,12,15,19,23,13,26,16,0,5,23,16,0,8,8,25,3,22,24,19,1,1,1,23,16,15,2,22,22,24,24,2,5,23,2,11,21,25,25,23,10,2,26,1,1,25,17,25,7,18,2,3,25,26,19,23,23,14,13,11,19,24,19,3,18,1,20,1,24,25,20,17,27,7,21,16,1,18,16,5,27,25,19,2,20,21,26,8,17,6,1,18,25,27,1,18,27,4,1,15,19,21,23,26,25,0,1,11,25,19,20,26,14,16,29,19,24,23,18,16,21,22,1,2,5,22,20,26,26,26,18,0,5,26,1,3,47,20,19,19,23,21,25,3,26,16,18,26,16,19,19,15,23,1,14,0,25,12,18,16,9,25,3,24,25,18,2,26,29,18,26,27,25,19,27,2,20,18,21,11,8,25,23,19,24,12,16,16,16,25,24,24,22,7,17,18,18,24,12,1,25,20,26,1,19,23,9,19,19,25,25,29,18,26,15,16,5,26,27,2,21,26,20,26,22,1,21,21,26,26,19,14,24,0,23,27,22,13,23,14,22,25,16,24,26,1,25,20,1,25,4,0,26,8,23,16,0,15,24,3,20,26,25,0,27,17,26,5,28,25,20,0,5,19,25,9,23,18,17,21,1,23,0,25,27,19,24,26,22,23,24,17,6,22,11,3,24,25,22,17,18,18,27,10,18,19,1,27,19,16,24,18,25,25,22,26,18,16,25,16,15,20,24,2,24,24,24,27,11,1,26,0,26,26,18,17,22,25,11,1,18,18,12,17,27,25,26,5,23,19,12,18,1,22,15,0,19,0,26,26,9,20,16,14,24,10,23,7,14,19,24,24,24,6,20,18,20,23,24,4,24,16,24,26,1,3,9,6,22,26,26,19,25,22,23,23,27,26,7,27,15,25,4,23,16,26,1,26,4,25,0,18,9,27,6,21,18,25,17,16,17,25,20,1,1,4,27,25,29,1,1,26,25,15,5,25,21,25,21,22,3,5,4,24,25,20,8,25,2,25,9,23,16,18,4,22,23,25,16,18,2,17,26,27,28,27,27,27,12,27,19,27,18,10,25,7,17,48,27,26,26,18,19,11,12,2,10,20,24,24,19,19,11,19,25,19,24,22,24,27,27,24,23,0,5,19,2,19,25,26,5,9,18,28,22,18,25,19,4,2,25,7,25,25,22,2,25,21,9,8,16,14,20,12,9,26,27,20,23,18,18,18,25,26,19,2,30,18,18,26,24,2,11,26,18,27,19,2,26,19,23,24,0,18,2,12,25,24,21,17,19,14,24,12,24,24,11,19,24,21,15,24,9,1,24,24,24,7,14,3,21,24,2,25,21,18,4,27,17,24,26,1,19,23,25,16,29,13,7,21,17,17,16,17,18,28,25,27,16,27,20,0,19,17,19,25,25,16,16,27,9,27,27,27,19,26,36,25,17,20,22,18,25,26,27,1,14,22,23,2,15,8,16,26,25,3,18,26,1,18,27,28,20,22,25,26,18,0,18,2,16,2,26,18,16,27,27,26,16,16,26,18,19,19,26,16,1,21,24,20,17,10,19,25,26,13,21,19,0,25,14,4,18,15,26,18,26,25,9,27,25,17,18,26,18,24,2,20,21,1,27,25,7,23,26,2,20,25,24,15,25,9,20,25,21,24,26,25,20,25,26,18,25,27,8,0,2,0,20,0,1,21,5,19,23,25,17,1,17,30,18,24,28,1,16,24,20,21,25,24,20,23,21,6,18,15,24,26,15,19,19,25,1,26,16,19,13,18,25,22,17,21,19,17,25,22,23,14,25,25,25,18,19,25,16,20,7,25,1,5,26,25,27,25,7,26,24,0,18,19,18,17,13,17,22,8,1,25,18,25,16,25,25,17,24,5,22,24,23,19,53,25,3,5,13,11,25,23,19,4,21,24,24,16,26,22,1,27,19,27,26,28,25,26,25,3,24,26,29,20,25,26,26,23,22,19,4,18,15,17,3,9,10,2,23,1,26,7,70,16,26,26,16,25,25,24,25,2,13,11,8,24,24,17,20,20,22,25,20,23,2,23,24,3,25,16,2,25,27,18,19,14,17,25,19,20,24,27,18,28,4,13,8,7,25,26,19,25,20,1,23,17,26,26,26,9,12,27,15,7,18,24,23,27,1,11,26,54,3,17,26,24,8,19,9,26,19,18,16,20,13,16,22,26,1,26,23,7,0,4,13,21,25,25,27,6,20,13,27,25,26,22,14,21,19,12,26,4,1,16,22,20,17,21,25,12,25,27,23,15,0,24,15,24,24,25,23,26,26,13,10,23,23,25,1,1,25,25,25,18,23,16,18,22,24,25,17,20,25,20,11,20,23,6,27,20,26,18,2,23,17,26,2,28,24,28,26,1,0,1,19,27,25,24,26,1,1,25,25,20,26,19,19,11,12,26,27,26,23,7,26,0,26,25,20,12,22,23,1,20,20,21,18,24,25,20,24,24,25,15,7,9,16,17,18,24,25,26,14,0,26,23,13,4,0,1,13,24,25,25,22,19,6,22,2,0,12,23,25,18,28,2,16,20,26,19,27,13,27,26,26,20,5,3,7,26,26,2,24,19,27,1,27,25,15,23,13,0,16,0,43,24,23,15,1,15,24,11,14,24,4,2,25,15,23,6,2,22,12,19,2,24,23,19,22,12,12,24,13,5,12,15,26,2,22,22,24,25,25,1,3,25,24,19,14,25,13,7,18,15,22,22,9,17,2,1,9,10,23,1,25,13,10,1,7,27,9,1,18,14,2,25,9,12,10,13,3,19,0,22,15,16,18,5,27,27,9,16,26,17,18,19,13,11,21,24,17,6,25,18,24,24,4,25,3,20,10,24,24,25,25,26,2,27,13,5,27,12,43,8,17,14,23,16,12,25,11,26,27,26,23,5,7,10,25,10,4,25,3,15,28,27,20,27,25,18,1,20,25,12,7,26,24,4,17,24,8,4,21,20,26,18,25,15,26,15,16,25,4,12,14,18,27,20,24,25,24,26,26,24,11,17,3,15,20,4,1,23,28,24,25,24,5,22,18,22,24,1,21,27,25,4,1,24,25,24,19,14,25,25,23,9,24,24,21,0,21,7,18,26,24,25,5,3,23,12,21,25,26,24,0,2,2,23,0,24,24,22,4,25,2,24,1,0,13,1,27,26,26,6,1,2,23,20,26,19,26,2,7,6,26,11,19,25,19,25,1,27,5,25,16,18,17,21,27,26,1,26,7,6,10,17,26,20,6,17,25,7,27,27,25,17,9,2,26,2,27,4,27,4,17,19,19,26,29,18,5,19,15,1,23,3,26,27,16,19,11,26,14,22,25,26,9,25,27,26,27,17,5,26,22,19,10,23,27,15,27,26,11,11,26,20,20,11,5,27,26,11,8,14,13,27,9,3,19,8,25,23,2,14,2,26,26,14,9,25,24,24,26,17,1,25,14,23,10,25,18,25,23,25,19,3,19,19,24,18,25,26,25,21,13,2,2,17,24,16,18,26,25,25,29,2,5,22,20,25,18,17,27,0,16,11,24,23,26,4,16,1,25,2,21,13,25,22,18,24,2,18,17,27,27,19,17,26,27,23,21,26,26,28,1,26,19,19,23,26,19,26,25,3,20,2,22,29,25,21,18,25,19,26,17,2,17,18,19,28,12,3,3,26,20,28,27,2,25,26,22,21,23,23,30,14,1,1,2,14,26,25,24,23,24,19,3,19,1,25,17,27,2,27,27,25,27,18,20,27,26,26,25,18,23,25,2,26,25,23,8,26,27,1,26,13,25,8,25,0,25,25,28,23,2,51,18,8,22,20,16,25,17,24,21,22,26,27,8,25,17,2,16,19,24,27,27,29,26,19,2,21,19,19,26,25,27,26,7,16,25,25,11,2,10,16,27,26,1,1,18,25,4,24,25,19,4,21,5,26,27,16,17,9,10,17,26,17,9,10,25,18,25,2,26,25,25,2,18,23,25,11,12,24,25,17,13,27,19,17,10,24,24,1,24,24,17,65,24,4,27,16,26,25,20,7,17,25,21,23,26,15,22,3,26,16,1,23,26,27,28,26,25,20,25,26,21,25,25,25,17,24,14,25,22,1,23,5,13,26,25,20,26,10,25,20,25,24,10,24,24,3,20,19,26,25,19,16,24,27,25,23,24,24,2,24,25,25,4,15,25,23,23,1,24,19,26,28,26,27,2,18,9,19,18,2,23,23,26,28,24,25,18,24,1,18,25,1,25,22,25,1,18,5,21,25,13,18,25,15,26,23,12,26,24,18,29,4,26,24,24,55,20,11,2,16,23,9,23,10,21,26,3,1,26,27,24,2,17,16,20,26,19,21,19,19,3,1,7,27,26,27,21,28,21,25,21,19,4,27,2,19,19,16,3,25,19,23,17,7,22,7,19,12,23,30,2,1,3,22,24,17,24,24,24,24,25,20,2,20,22,25,30,25,7,16,18,21,26,22,1,3,25,27,18,26,12,18,26,12,19,23,6,14,24,22,2,15,22,11,11,6,3,19,19,3,17,19,25,25,20,19,2,24,22,17,21,3,22,24,25,0,7,24,22,26,24,5,24,23,0,21,12,24,26,4,2,18,24,24,6,16,20,24,13,16,21,24,7,1,23,19,22,14,3,25,25,7,3,21,24,17,24,7,25,24,4,25,18,14,25,25,16,22,11,19,20,24,11,3,18,10,16,26,13,17,27,4,18,10,24,27,6,26,3,25,26,25,19,14,24,1,8,23,19,2,25,6,18,4,24,25,17,26,13,2,26,25,26,25,1,12,20,18,25,10,1,25,24,16,17,25,4,17,20,2,20,24,23,25,16,1,1,25,25,4,27,2,25,19,1,25,27,1,23,1,28,24,39,18,14,20,19,8,23,1,23,26,25,25,0,25,19,24,11,26,13,23,26,13,26,24,2,19,16,23,17,22,22,2,18,26,26,27,1,26,0,17,3,16,24,1,25,25,15,24,5,18,25,25,24,17,22,2,17,26,20,2,27,26,27,1,18,27,9,2,19,18,25,25,3,21,27,26,17,19,18,17,9,26,27,27,18,23,21,26,26,25,10,26,26,26,26,1,16,30,3,8,26,32,4,26,27,27,25,24,6,19,44,21,18,25,21,27,24,1,25,25,3,13,18,25,20,6,1,6,21,1,19,16,27,19,26,26,10,26,2,12,18,24,1,19,1,25,1,17,16,29,17,25,24,26,1,7,21,2,26,7,7,2,22,2,4,25,24,22,11,21,27,18,14,20,26,25,26,22,0,25,25,25,8,8,25,0,2,22,24,24,15,20,26,60,12,21,25,19,24,25,24,25,1,24,1,19,18,25,25,23,25,24,9,23,29,29,0,19,24,24,15,25,25,18,5,23,26,7,21,1,25,24,0,27,4,27,6,17,1,4,25,2,15,21,0,27,21,0,26,4,28,1,25,16,10,25,25,26,24,38,18,26,25,22,1,1,25,1,18,24,1,27,22,15,46,25,2,25,21,19,3,15,25,7,24,2,25,2,20,2,18,7,24,22,29,1,9,1,19,3,1,25,28,25,23,1,27,27,25,16,23,18,22,25,2,24,21,25,27,20,10,13,26,25,25,25,26,1,27,27,26,24,24,24,24,8,24,22,22,24,14,24,3,24,19,24,2,25,0,9,26,16,2,27,25,2,21,2,26,1,1,2,1,25,10,26,1,26,2,27,21,23,17,26,21,19,25,3,26,0,19,22,19,25,1,25,26,23,17,27,29,26,8,26,11,0,17,24,0,2,30,20,30,1,11,26,2,25,25,28,1,26,25,5,0,21,11,18,27,9,26,1,26,2,27,18,27,2,25,8,1,1,25,16,19,23,1,1,16,19,10,17,27,27,15,20,19,11,26,20,2,21,18,17,26,20,25,25,26,13,26,18,25,2,12,23,4,4,25,19,26,25,25,25,24,1,25,3,18,17,16,17,24,23,6,3,24,14,24,24,3,25,24,7,22,18,25,9,20,18,23,26,6,20,16,26,26,10,20,26,22,4,18,21,14,14,27,9,13,26,2,20,26,18,29,2,26,1,21,25,19,16,13,20,1,25,18,27,19,24,23,1,21,25,1,21,25,19,24,24,24,24,25,26,21,26,23,26,1,25,26,24,25,2,28,26,4,2,19,11,25,26,0,2,1,14,6,15,15,25,21,20,4,25,9,7,25,3,2,23,7,3,8,1,25,34,26,25,19,23,27,4,26,10,3,26,17,26,25,2,26,19,9,21,1,7,18,6,26,26,26,26,26,25,7,26,17,26,23,19,25,18,15,15,25,24,12,25,1,1,24,17,1,24,20,3,20,22,5,25,24,23,25,24,7,25,2,24,22,24,24,27,20,27,20,19,15,24,26,24,25,18,14,1,19,14,2,1,3,1,24,24,1,1,25,24,1,17,25,24,12,22,1,24,11,24,18,16,11,17,16,25,16,6,12,16,25,25,0,0,1,20,25,24,15,17,25,25,7,1,13,25,25,26,19,1,27,18,23,19,17,24,18,1,1,25,18,27,22,2,15,26,18,18,19,14,11,25,28,25,6,1,18,25,2,27,18,16,21,26,25,17,24,1,24,25,21,22,25,1,1,2,24,26,25,27,26,23,17,19,26,26,25,0,22,25,0,24,6,21,24,14,24,17,24,1,2,24,20,2,24,48,24,19,26,24,24,15,25,25,15,15,25,19,19,15,23,2,23,27,20,17,24,1,1,26,22,1,51,1,25,21,5,22,1,11,22,2,24,19,20,1,11,26,1,25,1,1,25,16,12,19,20,6,17,21,1,26,11,27,26,2,25,23,27,19,17,12,26,3,26,25,25,21,10,18,6,25,18,25,26,27,19,25,25,5,19,23,26,17,26,2,19,26,0,26,25,23,1,23,24,0,24,25,22,15,22,20,20,19,17,19,8,2,17,24,25,15,28,19,25,11,18,25,17,20,25,23,11,25,19,27,20,19,18,27,27,26,25,27,0,9,7,1,27,18,31,30,20,26,17,1,29,26,30,22,1,27,23,26,1,22,22,18,14,19,1,19,25,13,5,29,13,20,3,16,2,19,1,25,17,19,26,9,18,26,19,26,18,26,4,23,24,25,24,17,19,1,24,26,2,23,5,24,17,19,3,15,24,24,24,8,24,20,1,24,4,23,23,25,17,25,26,27,26,24,23,3,26,28,17,30,13,25,0,26,10,20,16,26,1,26,10,25,2,1,24,10,8,23,25,24,2,24,4,3,25,24,24,4,19,25,7,26,18,11,26,26,6,14,24,2,1,27,26,3,26,12,18,24,1,14,26,20,21,19,1,19,0,18,24,20,24,23,4,20,24,3,24,2,0,6,17,26,25,24,22,17,4,3,17,27,27,16,1,2,25,13,22,1,26,0,19,24,24,22,24,24,10,0,25,3,25,27,3,26,21,23,25,8,23,3,24,0,24,25,0,17,11,24,3,24,20,26,11,3,1,4,28,23,2,17,28,0,9,27,3,10,25,18,31,20,23,16,24,26,0,23,28,1,21,26,27,26,26,17,18,19,27,10,1,25,28,18,19,2,21,26,22,26,11,23,2,25,1,28,21,28,8,3,3,2,25,1,26,26,51,28,26,17,1,27,25,20,27,26,24,19,25,1,24,24,1,3,0,14,2,24,12,24,22,24,24,25,13,3,24,2,21,24,25,24,0,24,2,25,24,24,1,0,9,28,2,24,23,26,20,29,19,13,27,25,26,26,29,2,26,17,3,25,6,25,1,26,12,8,24,25,26,24,1,1,0,26,3,0,1,21,8,26,17,25,23,2,27,2,29,2,6,19,5,1,12,26,26,27,24,26,27,1,9,26,3,1,10,12,1,27,26,28,2,3,14,1,23,27,24,20,20,9,2,1,1,11,15,25,25,21,23,25,0,25,18,20,12,0,1,24,16,24,24,19,0,14,0,23,25,24,19,21,28,17,26,2,17,18,23,6,28,23,2,10,1,27,2,12,25,24,29,24,10,0,22,25,23,1,23,7,24,21,25,1,19,5,24,17,7,1,16,25,25,2,24,25,21,4,18,25,27,1,20,24,25,8,5,25,26,2,20,29,1,17,0,25,0,0,25,24,11,0,25,20,1,25,25,1,14,25,20,1,0,25,25,24,25,12,27,26,0,27,29,27,1,28,2,7,1,14,18,25,1,26,22,15,10,3,28,29,10,2,7,1,28,1,28,12,18,26,23,4,25,25,0,1,3,25,10,12,18,16,25,28,16,16,26,25,25,21,2,20,25,0,15,25,22,26,16,25,1,17,26,45,1,1,26,26,18,23,2,21,26,24,2,27,26,27,25,18,26,26,27,2,0,29,27,25,26,0,26,0,27,25,28,14,1,2,23,27,1,9,1,22,16,2,0,1,26,27,12,26,31,2,20,26,21,13,9,31,27,0,28,28,20,26,6,5,24,28,1,20,25,1,1,26,26,28,3,26,27,25,7,17,25,3,25,25,17,24,1,2,25,25,78,24,3,24,24,19,7,5,0,14,23,31,25,27,28,2,14,26,1,20,25,3,24,16,25,8,23,24,7,19,2,25,23,24,24,19,23,1,22,1,22,6,25,25,24,24,23,26,2,3,14,11,9,3,19,24,24,1,2,28,13,25,14,2,33,12,2,27,1,10,27,20,23,1,1,27,19,1,27,26,27,22,10,0,25,26,27,2,26,25,25,26,12,16,29,15,25,26,2,16,21,23,18,19,0,27,18,1,4,2,27,27,9,2,18,26,26,18,2,55,1,24,25,10,16,3,19,10,25,20,25,2,2,2,22,25,4,4,23,25,26,25,25,3,25,1,18,11,25,24,26,24,21,2,23,15,21,6,25,25,0,0,16,25,26,22,10,25,28,53,27,2,14,22,10,5,16,21,3,3,5,24,4,25,16,1,6,26,25,10,25,22,28,27,3,26,26,17,22,25,25,16,25,1,2,26,26,25,26,27,3,26,12,28,21,26,12,28,27,25,23,24,21,27,27,1,1,1,2,24,6,26,26,1,24,22,1,1,26,24,10,22,0,1,26,50,25,0,26,0,10,19,24,25,25,1,20,1,25,16,2,26,25,25,24,25,22,25,25,19,0,26,24,25,1,18,24,6,15,24,10,24,22,17,23,19,13,16,19,27,24,23,0,25,1,18,23,12,21,25,10,19,2,25,25,26,27,1,4,5,26,2,21,23,5,24,25,27,0,1,18,26,27,25,24,29,18,4,25,26,21,4,21,6,24,26,23,25,24,21,0,24,13,17,0,24,0,16,2,24,26,5,1,19,2,20,25,14,9,28,2,2,25,26,0,25,1,26,5,0,25,4,21,25,22,4,26,3,1,1,25,20,25,12,23,18,23,15,22,24,24,8,25,24,1,23,22,23,24,23,24,20,8,26,24,28,29,26,8,27,25,24,25,26,26,22,2,25,26,1,23,25,25,2,25,23,26,9,27,4,19,16,11,17,12,27,25,2,0,17,27,25,25,17,2,25,1,25,26,24,0,21,2,24,19,15,25,24,1,24,26,23,26,26,20,25,14,26,4,27,22,20,12,26,18,19,25,25,24,26,3,25,7,26,26,23,27,11,25,1,25,1,22,25,1,21,26,26,13,19,2,25,26,16,26,21,19,24,21,23,23,26,22,2,24,25,28,25,21,22,2,25,28,15,19,28,29,18,27,25,27,4,28,49,17,1,27,5,1,32,9,24,25,9,5,23,22,6,1,9,26,1,26,26,26,15,19,2,5,25,29,14,25,27,28,19,25,20,2,19,19,1,23,26,0,17,0,0,26,23,11,25,24,18,24,6,1,25,21,26,9,23,26,21,7,1,22,20,0,10,22,28,0,26,25,23,27,16,26,26,25,25,23,0,26,31,24,24,12,23,0,1,25,23,26,1,19,25,23,10,9,1,20,22,26,23,15,16,9,24,25,2,27,10,19,10,23,1,14,24,16,21,8,26,26,25,25,25,5,25,4,12,0,22,26,4,24,2,5,3,5,24,18,4,1,25,25,24,1,19,24,24,20,24,0,25,26,28,40,23,22,16,0,25,15,25,26,25,1,24,1,28,9,0,28,8,28,26,28,25,26,27,26,1,22,25,27,25,2,1,23,2,2,12,26,18,1,18,2,28,25,28,23,22,4,24,26,19,2,23,16,26,2,23,1,4,26,6,25,2,27,21,1,26,21,26,20,28,26,25,45,5,11,22,25,27,1,22,29,21,25,3,26,2,27,0,25,22,28,25,27,2,2,39,24,12,24,25,1,1,1,7,25,3,25,12,0,18,24,24,29,24,24,23,26,1,25,8,11,1,2,24,31,27,18,29,26,10,9,23,26,28,25,26,14,21,1,25,2,1,1,26,26,16,14,12,29,24,9,28,20,24,27,24,3,26,6,16,25,1,2,2,25,22,9,25,26,29,18,23,25,16,30,0,25,8,26,24,26,16,22,26,25,1,25,25,26,26,15,23,26,48,2,25,25,27,23,22,23,11,25,1,2,25,27,19,2,26,24,25,22,7,23,2,23,24,2,0,24,15,27,23,9,4,15,12,23,25,16,2,24,2,3,22,26,24,2,18,1,25,2,25,23,26,19,25,1,2,26,10,1,25,21,21,1,28,28,1,39,22,0,27,24,2,24,17,23,0,6,24,27,11,2,21,0,16,25,24,25,9,26,17,26,27,18,26,6,26,22,26,2,26,7,25,19,25,17,25,2,20,27,10,25,4,26,25,26,25,8,24,23,1,17,2,24,0,17,23,4,2,3,26,25,23,20,25,1,5,25,25,20,2,26,14,25,8,11,25,16,5,26,27,26,3,3,23,26,3,27,26,20,3,26,4,26,3,24,15,25,25,25,24,24,3,8,25,27,26,26,25,1,1,6,26,25,18,15,25,25,26,0,0,0,25,25,24,1,26,17,5,26,0,16,22,25,25,1,24,25,23,26,1,25,18,25,25,16,13,54,6,26,23,0,26,2,27,2,27,1,23,25,25,24,25,24,1,25,2,26,25,26,2,23,1,24,22,25,25,43,24,2,26,25,24,25,26,25,16,23,3,3,3,2,26,2,19,25,2,24,1,23,25,21,0,26,23,23,24,30,30,25,2,23,1,1,13,0,23,1,25,29,1,28,30,24,24,22,15,24,25,24,12,1,24,4,24,4,9,15,23,24,8,25,10,28,25,24,25,25,24,2,2,15,28,24,2,24,23,23,8,27,11,2,24,2,25,0,19,1,27,26,1,26,24,18,25,24,1,25,2,1,8,28,19,18,26,4,3,20,26,2,27,26,5,7,1,24,29,26,26,22,24,2,21,0,25,2,19,26,1,28,17,25,1,26,29,25,1,1,22,1,13,3,23,16,9,17,1,24,0,27,0,19,24,26,0,26,2,17,27,24,9,25,1,25,27,10,1,1,2,20,24,2,8,10,26,27,9,25,25,21,1,4,1,24,24,20,1,25,23,10,2,24,20,27,23,24,3,24,1,18,8,0,29,1,71,2,19,3,26,2,1,2,26,25,18,24,26,24,25,25,22,26,27,2,28,25,2,23,26,1,25,24,27,25,25,25,1,27,2,22,1,27,26,9,11,18,26,2,24,25,11,27,1,2,2,26,24,2,25,2,26,25,1,23,25,24,6,2,23,22,13,20,22,24,1,1,24,1,25,25,0,24,24,24,1,26,28,24,22,25,25,24,9,9,25,3,28,27,27,6,24,25,7,25,3,23,26,22,22,23,6,2,2,1,1,26,7,4,16,24,27,4,1,14,29,27,2,13,22,51,6,26,25,18,25,1,25,25,0,25,18,25,1,26,25,16,4,25,24,26,0,0,2,1,27,1,1,24,2,33,1,27,25,27,26,26,61,26,26,25,23,24,24,26,24,25,0,30,26,26,15,1,29,26,27,16,27,0,27,6,15,0,18,6,25,26,26,25,26,24,7,26,15,25,28,1,24,23,1,27,24,0,4,0,24,25,26,24,28,0,25,25,25,24,24,1,0,18,22,25,21,11,2,8,26,24,24,0,25,25,24,26,25,19,25,24,28,48,24,25,23,22,26,27,2,13,25,23,1,24,27,1,26,2,4,22,2,10,21,2,12,25,27,25,1,3,28,3,23,19,28,23,23,1,17,23,21,25,26,28,10,24,23,24,4,23,25,25,1,2,26,25,25,2,1,6,25,27,0,27,25,18,1,24,23,23,26,5,24,12,2,25,0,24,27,26,26,25,27,25,1,24,23,25,25,20,0,1,0,26,22,2,9,13,25,26,28,0,28,12,26,28,0,2,25,17,31,25,2,2,25,25,21,16,16,1,9,1,25,1,3,25,1,3,25,8,1,27,2,17,30,2,1,25,25,22,26,29,2,31,5,2,25,24,1,25,2,25,26,26,26,10,26,25,12,23,24,7,24,24,28,29,26,4,25,24,25,0,4,2,0,1,24,25,24,23,22,2,24,2,4,24,17,15,4,1,24,26,24,24,22,24,1,26,26,2,2,1,2,25,15,27,19,2,25,26,25,26,26,24,25,24,4,1,24,24,2,21,25,1,24,0,24,10,0,21,23,25,0,1,26,26,24,1,2,2,2,23,14,21,21,40,25,1,25,7,16,2,21,13,16,24,2,24,24,2,24,13,26,26,0,1,26,25,25,25,17,2,24,21,2,26,2,20,25,19,24,2,25,24,24,26,1,44,27,25,1,23,28,24,23,27,23,22,1,15,27,24,26,23,24,19,2,1,26,26,25,25,24,17,37,25,26,2,2,2,25,26,28,1,25,2,32,10,1,28,24,26,29,1,31,10,23,2,26,26,26,25,0,25,2,3,0,24,4,15,26,1,1,24,27,2,28,24,0,24,2,27,1,25,23,26,20,25,6,35,24,1,2,26,27,24,69,6,1,28,25,3,3,1,25,25,2,26,22,25,25,54,28,1,2,27,0,0,26,26,23,25,25,25,2,21,2,26,26,5,18,25,26,2,23,22,1,25,26,24,25,24,7,24,23,26,1,24,20,2,27,5,23,26,24,1,27,5,25,25,25,26,3,26,27,1,23,1,1,26,29,2,16,1,27,1,25,23,32,25,25,32,10,1,26,26,30,29,25,11,27,17,1,25,22,3,2,25,23,24,25,3,24,2,20,0,24,31,22,1,6,23,0,26,0,8,1,24,25,8,24,8,1,3,23,27,27,0,24,24,26,0,4,0,38,25,0,27,1,26,28,25,26,22,24,25,2,24,4,19,25,1,1,29,18,26,2,27,25,26,1,27,18,23,1,26,0,26,0,25,25,22,1,26,25,2,2,19,2,27,23,0,1,23,26,6,28,22,2,25,2,22,33,26,28,23,26,25,25,0,2,22,29,17,25,29,23,25,1,23,20,66,2,24,17,25,26,25,14,23,2,13,20,23,0,33,11,2,23,1,27,3,19,2,27,27,2,2,24,2,13,24,3,15,27,3,3,24,25,26,26,27,6,19,1,26,2,25,2,5,24,15,26,25,28,27,26,22,16,25,1,26,34,23,29,12,2,23,27,7,23,4,2,1,25,20,24,33,25,29,20,26,12,26,25,26,29,26,18,27,26,25,19,27,25,2,3,30,24,5,24,28,27,28,8,2,24,25,14,3,22,1,25,1,28,19,17,1,13,2,2,68,25,24,13,1,16,26,25,1,1,2,26,25,17,24,24,1,1,23,3,25,62,20,3,24,26,27,29,3,24,24,25,2,25,23,20,25,15,24,1,25,14,9,25,25,24,23,8,15,25,13,1,21,17,27,2,2,27,27,25,31,26,27,29,25,31,27,26,2,30,26,23,2,30,8,24,2,30,2,27,24,28,25,27,21,26,7,0,27,28,31,29,0,22,0,1,22,4,23,25,2,25,24,26,19,2,1,26,22,1,29,31,20,2,25,2,0,26,17,25,7,25,2,23,26,13,5,26,27,7,1,19,23,17,23,27,14,2,1,27,21,15,40,29,24,1,28,26,6,22,1,2,22,0,25,1,1,25,25,8,3,25,2,18,26,4,23,1,31,25,25,0,23,1,20,21,24,25,1,25,2,25,26,24,1,25,1,30,27,27,23,23,26,27,25,7,1,55,26,14,12,25,19,25,26,5,24,25,25,25,26,5,28,23,1,30,25,26,23,26,2,20,13,32,2,25,26,25,25,25,1,24,20,1,25,23,25,29,28,33,29,0,23,2,25,11,25,2,26,28,25,15,28,24,27,2,10,4,25,3,3,22,25,24,0,25,1,3,0,26,16,26,25,17,26,19,26,1,1,6,25,14,25,22,26,2,43,26,25,1,28,19,23,0,23,1,30,25,26,9,0,5,25,0,26,23,28,0,26,22,0,21,9,26,0,27,1,27,19,25,23,24,25,7,25,0,11,21,26,25,26,26,26,4,26,22,26,25,1,26,59,1,24,25,2,1,25,24,1,24,23,25,31,21,0,24,1,0,24,27,24,25,0,1,24,14,31,9,3,27,9,2,28,19,14,8,2,2,22,29,27,1,26,25,25,26,27,19,27,26,3,16,25,25,0,10,4,28,26,26,12,25,28,12,28,2,25,0,1,27,23,19,26,24,23,20,26,2,23,25,24,2,18,25,23,12,27,1,26,1,25,25,10,8,26,5,9,26,4,25,25,55,24,1,23,0,26,32,2,1,19,2,23,2,2,24,64,24,26,1,27,11,26,25,23,23,20,23,24,28,8,2,20,23,24,23,19,23,1,25,1,25,71,25,25,25,25,26,25,3,2,25,25,24,19,18,2,3,1,1,19,24,25,25,25,1,25,13,26,23,26,26,2,2,41,1,26,26,22,15,2,14,16,1,1,25,20,2,20,27,26,4,18,0,1,25,29,23,3,26,22,26,27,28,26,23,26,2,7,24,1,31,26,27,19,23,25,0,26,2,2,18,11,1,25,25,0,26,24,0,24,26,24,14,23,25,23,22,47,16,2,14,23,44,24,24,21,11,24,24,27,16,25,23,0,24,26,25,10,25,24,25,26,17,0,26,17,25,26,24,9,26,5,25,15,25,0,25,12,27,27,26,8,0,27,19,18,24,1,10,18,2,1,26,2,1,23,29,0,26,26,2,21,2,25,2,25,15,25,2,1,27,20,33,27,2,19,19,29,26,1,29,15,29,19,24,25,28,2,24,26,71,24,20,6,10,26,26,2,18,29,0,8,22,23,9,12,28,22,19,23,4,25,1,1,2,2,13,26,17,25,16,25,26,27,26,0,28,0,13,27,10,6,26,34,25,2,29,25,25,25,9,16,14,23,18,24,0,1,20,24,26,2,22,2,17,22,24,22,25,23,20,25,2,26,30,2,24,27,4,18,18,26,18,16,27,2,25,28,26,1,19,21,26,21,28,26,16,25,1,24,1,1,26,26,1,26,25,2,30,23,25,2,27,26,25,0,22,27,25,19,3,23,24,16,25,3,25,7,29,2,0,26,2,22,0,2,2,28,1,52,27,0,1,1,26,1,26,1,1,25,25,19,26,19,24,26,28,24,2,27,24,18,18,23,25,26,2,25,1,28,23,23,25,27,25,25,22,29,28,24,24,2,1,24,17,24,25,8,25,0,1,5,18,24,24,1,25,21,6,15,24,1,26,14,30,31,25,1,1,16,24,24,22,0,31,67,1,23,2,24,33,1,29,1,25,25,19,23,0,26,2,27,51,14,1,26,25,30,27,1,29,23,25,57,26,25,29,25,27,0,25,26,29,26,1,31,20,26,26,1,24,20,25,28,28,1,25,5,23,26,21,24,1,9,8,27,2,25,2,15,26,2,12,2,26,26,19,20,25,19,26,25,3,25,25,2,26,24,26,29,26,25,26,27,27,3,6,23,25,7,29,21,15,2,25,17,16,13,20,20,25,11,2,23,25,25,23,0,0,1,25,16,23,24,1,24,25,44,23,18,22,23,2,6,0,20,24,45,3,24,16,24,21,24,21,25,19,24,8,26,25,26,25,23,24,24,2,16,7,1,9,17,24,25,2,25,29,29,1,30,27,28,17,26,22,1,20,27,6,25,26,26,23,24,25,2,27,31,28,25,25,3,25,7,26,18,0,3,26,25,20,24,2,2,21,3,25,3,25,25,0,26,27,23,25,5,26,1,13,25,1,26,1,1,1,24,22,23,1,1,26,1,21,18,25,12,27,3,28,25,19,0,29,1,22,1,26,28,2,1,2,31,1,22,3,14,26,24,0,40,23,0,24,39,2,20,25,22,14,1,26,1,6,18,25,24,2,25,19,25,24,2,24,15,2,2,23,23,24,1,24,61,26,3,24,1,1,0,22,25,2,24,22,23,21,26,2,24,27,25,24,24,24,8,25,23,20,25,25,23,25,1,25,2,26,1,26,12,27,23,26,1,8,22,25,1,26,26,26,10,24,1,24,21,24,1,0,3,1,19,24,1,25,24,1,21,5,25,3,0,24,0,5,18,17,17,25,14,26,2,1,23,21,32,20,25,24,5,27,2,1,9,23,26,19,25,25,27,22,1,27,8,1,25,26,26,26,21,5,2,25,1,2,63,2,2,25,26,25,1,26,26,1,33,25,1,26,26,24,15,23,0,22,26,24,0,4,22,22,24,26,23,23,25,20,1,19,27,26,22,8,26,1,22,23,25,24,49,25,15,26,24,26,10,22,25,15,30,2,22,5,20,26,26,28,30,29,23,2,27,26,27,25,20,22,1,27,2,26,12,24,26,16,29,25,8,24,1,1,24,1,21,24,0,24,24,25,25,0,24,14,13,0,24,25,18,21,25,11,0,2,28,1,2,1,26,25,28,25,1,28,2,0,38,10,1,25,16,14,1,19,26,12,26,19,1,23,27,29,2,25,28,29,10,25,27,23,26,26,4,25,25,26,2,0,25,26,18,3,1,27,25,11,25,26,14,3,26,23,26,0,19,28,2,26,6,0,16,26,25,21,2,22,7,24,25,25,11,24,1,19,25,26,4,23,25,2,1,2,25,2,0,25,19,26,18,25,19,0,15,25,26,17,2,24,9,25,27,27,1,14,1,2,26,31,25,25,25,26,24,29,25,23,6,71,8,24,26,25,1,23,26,25,13,2,27,1,25,26,13,2,26,26,1,11,25,2,2,26,25,22,23,22,26,25,2,30,25,27,16,3,25,26,26,1,23,11,26,17,26,26,25,25,25,4,0,25,25,22,28,0,25,0,21,24,26,24,23,23,3,22,25,24,12,8,28,24,22,26,6,1,25,25,18,1,26,25,2,17,2,11,22,28,26,27,25,2,25,25,12,2,25,2,14,26,26,25,25,29,23,25,1,23,2,27,25,29,25,1,21,24,1,2,14,25,1,1,34,14,25,27,1,2,25,22,23,24,22,24,26,25,1,2,23,13,25,26,25,24,7,2,25,24,25,3,16,1,11,21,1,18,25,17,12,10,26,24,23,26,24,24,25,25,14,26,15,29,7,19,10,26,3,25,26,25,2,15,25,25,3,12,24,2,25,11,19,18,25,25,0,25,22,26,14,25,9,24,15,26,2,23,25,16,24,26,20,18,17,26,28,25,25,26,26,15,1,28,8,20,26,25,0,24,24,25,24,22,2,12,27,18,19,21,19,7,14,29,25,12,27,21,26,20,25,27,18,28,26,16,28,20,2,14,25,24,25,25,23,16,24,1,22,2,7,18,22,24,14,4,25,1,17,24,16,0,24,13,20,27,24,23,1,1,1,24,25,0,20,13,26,2,25,24,16,30,1,22,28,26,1,2,25,27,1,13,26,22,28,27,27,2,20,26,10,23,38,29,3,18,20,24,22,2,18,22,28,24,26,25,10,17,2,26,17,26,2,26,1,2,2,2,25,2,25,26,22,14,23,1,21,26,26,19,15,25,16,25,12,24,25,29,2,27,15,12,17,2,23,1,22,14,3,26,1,20,26,2,25,23,28,23,20,0,25,1,25,0,27,25,25,23,25,27,21,26,1,2,22,8,27,26,7,26,23,26,1,26,4,2,26,29,1,27,26,25,23,26,18,26,26,1,23,22,28,25,22,29,25,12,8,25,6,24,0,24,22,25,27,26,23,1,1,27,27,22,24,21,1,20,24,26,2,2,23,2,8,25,1,25,27,26,26,0,2,22,27,25,26,26,24,25,14,0,26,26,27,1,2,26,27,26,27,25,2,25,27,26,22,25,2,2,1,24,1,26,25,14,18,27,26,26,18,25,14,25,25,13,24,16,25,23,2,24,15,1,25,25,1,25,2,1,26,26,33,3,28,32,1,1,25,15,4,20,25,25,23,1,50,24,26,23,20,4,24,24,24,2,25,24,0,26,26,0,25,25,19,0,24,24,57,1,25,26,25,24,26,25,2,26,25,22,25,25,25,7,25,2,26,17,26,25,24,25,28,26,26,17,25,1,27,27,23,28,2,21,26,25,27,18,27,11,23,25,16,3,25,25,27,49,25,21,1,26,21,4,1,25,26,26,12,10,24,27,1,25,8,24,2,1,2,25,24,0,5,25,25,22,23,22,25,3,22,27,3,3,1,4,3,23,26,25,10,26,23,25,24,3,24,27,25,1,24,4,24,25,25,26,3,25,31,19,29,26,10,27,4,2,26,27,23,25,26,2,62,25,17,26,26,28,25,22,27,28,1,5,26,25,23,26,1,1,26,1,3,26,27,10,2,65,25,28,25,25,26,17,1,67,0,14,26,7,27,17,2,2,45,12,24,24,5,2,2,2,22,24,24,25,1,0,23,25,23,25,1,17,20,29,37,25,26,1,10,1,25,26,25,11,26,25,5,29,25,3,25,28,27,23,19,2,1,25,62,23,26,13,3,23,26,17,23,22,16,26,26,25,18,1,23,25,25,0,20,3,16,25,25,26,13,3,24,34,27,24,4,27,20,20,24,0,26,23,26,1,26,24,1,0,25,17,24,1,25,24,3,24,23,22,26,18,26,7,28,27,13,25,27,1,5,4,23,27,17,27,2,25,27,26,15,14,29,17,25,16,24,16,12,17,25,15,26,26,18,3,26,3,25],\"type\":\"box\",\"name\":\"Stephen Curry\",\"marker\":{\"color\":\"rgba(231,138,195,1)\",\"line\":{\"color\":\"rgba(231,138,195,1)\"}},\"line\":{\"color\":\"rgba(231,138,195,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"y\":[0,0,18,0,0,0,0,0,0,0,24,0,17,0,0,16,0,0,0,22,24,0,15,23,0,26,15,0,0,6,21,0,17,0,24,0,0,0,5,19,0,7,0,2,1,13,5,19,11,17,2,24,14,24,18,10,1,20,3,0,1,14,0,1,24,1,2,0,1,6,17,0,24,19,0,25,0,21,0,17,0,25,5,0,25,0,17,16,0,0,24,25,17,0,18,27,25,0,19,19,24,25,11,20,4,13,0,14,6,4,19,24,17,0,0,8,22,0,0,22,16,0,25,14,0,25,28,13,0,25,15,21,0,16,18,0,18,25,0,0,25,0,35,0,0,0,17,0,12,20,23,0,0,0,12,25,24,16,17,19,3,25,35,17,21,24,20,17,24,5,6,0,0,0,0,24,24,21,19,0,10,27,20,0,15,17,0,17,0,0,25,0,17,19,0,0,0,0,0,24,4,16,31,0,0,0,20,23,0,17,24,9,25,25,0,16,0,23,15,0,20,23,0,0,0,20,0,0,0,0,20,0,18,0,25,0,19,0,25,25,0,25,0,24,24,18,0,0,0,22,0,0,18,25,25,0,0,21,0,18,0,0,0,0,0,18,24,19,0,0,0,0,20,0,0,22,3,19,20,25,2,2,13,25,25,16,5,0,0,6,16,0,25,23,0,22,0,0,0,25,0,26,0,19,24,8,15,28,25,0,19,0,0,25,0,25,0,0,25,6,8,0,0,26,8,0,0,0,0,0,0,0,18,0,24,4,25,0,24,0,0,19,0,0,22,0,20,13,0,26,26,0,25,0,0,15,10,0,19,18,0,14,0,0,25,0,0,0,0,25,0,18,0,0,0,19,0,0,12,14,25,24,20,21,0,0,0,0,0,17,17,4,0,24,24,6,0,0,20,22,2,0,2,25,3,25,1,19,2,4,19,8,26,0,2,17,18,21,0,17,0,0,21,0,0,0,12,19,0,25,24,18,17,25,16,23,0,0,25,21,23,0,0,5,24,0,0,0,21,0,0,0,18,11,0,0,16,0,0,0,0,17,0,0,16,0,24,24,0,0,0,13,0,24,20,24,25,0,6,18,21,24,0,0,25,26,0,0,25,16,0,18,24,25,0,0,0,23,0,21,0,0,17,20,24,26,0,24,21,0,24,5,0,25,27,0,22,21,0,21,0,0,24,5,0,0,0,14,22,20,0,0,0,19,24,8,0,17,0,19,0,0,19,17,0,0,0,15,0,0,0,24,0,25,0,0,0,6,14,19,0,9,6,26,0,0,18,0,0,0,25,0,0,16,0,6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,17,0,0,18,24,0,0,17,0,0,0,0,0,0,0,0,0,0,15,17,0,24,22,14,18,4,0,0,0,0,0,0,0,20,0,17,24,0,18,21,0,14,0,0,6,0,18,18,16,19,18,0,18,0,25,0,19,25,0,25,0,10,18,0,21,0,0,0,0,0,0,0,0,18,0,16,6,22,0,0,0,25,18,17,0,21,17,0,0,0,16,0,0,19,0,23,0,19,7,0,6,0,12,20,0,5,0,20,0,0,21,0,0,17,0,15,15,9,15,19,0,21,0,0,0,18,0,24,0,0,0,24,0,21,0,24,16,17,19,16,0,24,0,0,6,20,0,0,24,21,0,0,0,0,0,0,6,16,0,20,0,0,0,17,19,25,17,0,17,0,0,24,0,0,0,0,15,0,0,0,4,25,21,0,20,0,24,0,21,0,21,19,0,22,0,0,21,18,19,0,24,0,19,0,21,0,24,24,21,19,14,19,0,0,20,0,18,0,18,14,0,0,20,0,0,19,24,16,20,0,0,24,17,24,16,4,6,24,5,0,8,0,6,17,0,12,18,0,16,0,20,0,15,24,14,0,0,5,19,0,19,22,23,0,20,31,17,0,17,19,25,0,15,25,0,20,21,0,4,19,0,0,0,12,19,0,24,21,18,0,0,0,14,19,18,0,0,12,0,0,22,24,17,14,0,20,0,19,18,14,0,0,0,0,0,0,24,0,0,0,15,23,0,0,0,0,0,21,25,6,0,0,22,0,0,0,0,0,17,0,0,25,0,0,0,24,5,21,7,4,12,5,5,0,16,28,24,11,0,23,6,9,10,24,20,0,21,22,0,22,0,24,0,0,20,18,0,0,0,0,0,0,14,0,0,0,0,0,0,0,0,0,24,0,18,18,0,0,24,17,24,19,0,20,0,0,18,0,17,0,0,0,0,0,6,14,3,2,20,1,16,25,0,13,0,2,4,1,20,1,18,21,15,0,22,18,0,25,19,17,0,24,18,0,0,16,0,25,0,0,21,19,24,0,5,20,20,16,0,19,20,0,18,0,12,25,24,19,0,0,14,20,0,0,0,21,20,18,0,25,20,18,0,16,16,18,24,0,25,0,0,15,0,0,17,18,0,0,0,18,0,0,0,25,0,23,0,18,0,0,21,19,1,2,17,1,25,18,0,1,1,18,13,0,26,21,17,14,17,8,0,19,18,20,18,0,0,0,0,0,15,18,0,0,19,0,16,8,0,18,24,19,15,16,0,0,24,18,0,14,26,14,0,18,16,15,0,0,25,12,0,0,20,15,0,0,17,24,0,23,0,0,0,17,0,19,7,0,0,24,0,0,0,0,19,17,0,21,1,3,23,0,24,12,2,23,18,1,1,1,1,2,24,1,1,1,1,15,0,0,0,0,20,18,18,17,0,20,17,0,25,0,0,18,27,17,26,0,17,0,20,19,14,25,38,7,20,23,0,18,0,31,17,19,0,18,17,6,18,18,19,0,17,15,24,0,16,0,0,8,18,0,22,0,13,19,0,0,17,25,19,21,0,19,22,24,20,19,18,5,18,0,20,19,8,0,22,19,3,17,14,7,19,21,0,5,0,4,4,0,19,25,5,22,20,17,24,20,24,19,0,41,0,21,21,16,21,16,15,20,19,5,0,0,21,17,24,0,17,17,17,18,25,25,18,0,18,20,7,0,0,0,20,22,15,0,14,16,21,0,0,19,0,0,19,26,24,24,24,18,0,0,0,25,0,18,18,16,0,17,17,9,25,0,0,18,18,0,17,0,6,0,0,0,0,0,16,25,15,22,19,24,0,18,18,14,9,17,15,7,0,8,0,24,0,25,20,17,20,21,17,0,0,0,0,24,0,0,0,17,19,17,0,20,17,17,0,0,0,20,17,0,0,16,0,0,17,16,0,0,0,5,7,16,0,8,17,23,0,16,13,20,14,21,12,0,4,3,5,24,24,2,2,15,16,0,0,16,16,0,21,16,17,0,7,17,15,7,20,24,12,8,0,18,16,17,19,0,26,16,20,0,19,21,24,19,9,0,6,0,17,22,16,25,0,17,26,0,17,18,25,24,9,0,19,19,18,70,21,16,19,18,18,19,8,0,0,20,16,24,0,4,19,13,22,12,19,20,0,15,0,0,24,0,0,18,25,22,0,17,18,25,15,25,16,5,15,0,0,25,0,0,18,20,14,0,0,0,17,19,25,0,0,18,12,0,24,22,7,18,24,19,18,6,0,24,17,12,3,0,0,17,18,2,0,19,0,0,0,24,19,0,0,24,18,16,8,0,0,17,19,0,18,7,0,17,0,18,14,0,27,16,0,17,16,17,0,17,24,25,16,24,18,22,24,0,18,0,21,24,0,22,0,0,27,0,0,0,0,19,24,24,18,0,21,17,27,21,20,67,0,12,0,17,0,19,13,16,0,0,25,0,0,16,0,3,17,0,16,0,0,15,15,7,0,17,7,17,0,0,0,2,0,0,8,21,16,24,16,0,0,0,18,19,7,0,16,0,12,7,11,16,0,25,20,18,16,0,0,0,17,0,14,0,16,0,18,25,0,24,16,25,0,8,19,0,0,15,17,19,17,18,17,20,10,9,0,0,17,0,0,0,16,0,0,0,25,0,25,18,24,1,18,19,27,17,17,1,1,17,1,17,0,0,17,0,13,0,17,14,17,21,16,0,5,21,18,0,7,0,12,17,0,14,0,19,0,25,18,17,0,0,18,14,0,7,23,0,0,0,15,16,8,0,17,18,17,0,26,11,4,25,21,0,0,0,0,20,7,18,28,0,0,14,7,19,4,2,24,16,14,17,25,1,2,24,24,24,23,4,1,0,18,8,16,18,24,20,17,20,16,0,25,24,0,0,0,14,18,19,7,20,5,0,0,25,18,0,16,0,21,16,14,16,25,24,9,25,19,17,0,15,25,5,21,0,13,4,20,0,0,16,14,25,10,20,0,4,0,16,0,6,21,7,24,0,0,0,0,24,13,0,25,24,0,18,14,24,0,24,15,24,48,1,4,17,16,22,13,17,24,4,23,1,1,0,2,25,0,0,0,25,25,19,0,0,0,0,0,18,0,15,17,25,0,27,24,25,19,19,0,24,0,21,4,24,0,22,0,19,24,8,16,18,19,0,19,16,0,22,5,25,17,0,0,17,0,18,17,24,24,0,15,0,16,0,0,5,12,0,16,16,0,0,3,1,22,24,19,20,2,16,4,24,1,2,4,3,0,3,0,2,5,15,17,0,16,18,25,24,17,0,25,0,0,0,0,0,25,25,0,17,21,0,8,16,19,22,0,14,0,0,0,15,16,12,0,16,19,24,16,13,18,26,27,29,18,18,21,16,20,0,0,0,17,17,0,0,14,0,0,10,23,0,15,0,0,26,19,0,15,19,0,0,0,15,14,2,15,1,14,16,20,1,18,2,15,9,4,6,14,0,0,18,20,17,0,16,0,0,0,14,0,25,24,24,21,0,18,0,17,20,0,22,7,24,6,24,24,0,19,17,24,6,20,19,0,24,18,24,0,0,19,20,0,20,16,25,15,0,24,0,0,21,0,24,0,18,0,22,17,0,0,19,0,0,0,0,18,17,0,20,26,17,18,19,18,16,2,19,17,1,2,1,0,0,0,20,19,23,11,0,21,0,18,0,0,14,0,0,0,13,0,0,24,0,0,17,0,0,0,26,0,24,14,0,0,19,0,17,19,18,0,0,0,0,0,0,17,0,19,0,0,9,20,7,19,9,0,0,0,18,0,25,0,25,13,0,0,0,14,0,20,0,0,22,16,0,0,17,0,22,0,0,23,22,13,0,0,19,4,5,0,0,19,16,0,23,24,0,0,9,18,0,16,14,16,0,17,0,0,14,0,23,0,19,22,0,0,0,5,14,0,0,0,0,0,0,16,15,0,12,16,19,17,0,0,0,0,0,0,16,0,0,0,0,4,0,0,15,24,0,0,16,24,0,0,21,0,16,0,1,3,4,19,23,1,3,1,17,1,4,3,2,2,3,13,2,18,17,0,0,20,0,4,0,0,0,0,17,0,24,18,0,0,24,0,0,0,0,0,0,24,24,0,0,0,0,17,0,0,24,0,0,0,0,25,0,0,0,0,21,0,0,24,0,16,0,0,0,16,0,0,24,20,17,0,0,0,16,24,0,0,0,20,0,0,0,0,0,0,20,21,19,24,26,0,0,0,24,24,0,15,0,14,0,21,20,19,0,24,0,6,19,22,21,25,0,25,0,0,0,20,20,0,0,0,0,0,21,0,0,24,20,0,24,0,18,0,0,0,0,0,7,54,0,0,0,0,0,0,20,0,0,0,18,17,0,17,18,16,19,4,17,18,0,0,0,13,14,22,17,19,23,2,21,0,25,23,0,0,16,0,0,15,0,17,0,15,3,0,15,0,0,18,0,0,16,16,6,10,0,21,0,0,20,0,18,0,19,0,0,0,0,21,0,0,0,0,0,0,19,19,20,20,19,0,18,17,0,0,25,0,14,0,0,0,0,0,0,0,0,0,0,0,0,0,75,0,19,19,0,0,24,0,0,0,0,5,0,0,0,16,0,0,0,0,0,0,0,0,0,0,19,0,17,0,0,19,0,24,16,7,0,18,0,9,0,0,0,18,0,21,22,22,0,17,0,21,17,20,0,0,18,0,21,22,19,0,19,19,19,0,0,16,30,15,0,19,24,0,16,16,0,0,0,18,0,20,25,18,17,24,0,0,0,1,16,3,1,2,2,24,1,1,1,7,1,1,13,7,0,0,0,19,5,0,16,19,15,0,21,25,0,16,4,0,17,0,0,0,0,0,18,24,0,24,15,0,15,0,17,24,16,18,11,25,0,0,0,18,15,0,17,4,0,22,19,22,0,0,0,21,0,0,23,16,17,0,24,18,4,21,0,20,20,18,0,18,6,13,18,18,24,24,21,0,15,24,0,0,0,0,0,21,17,16,17,18,0,0,0,6,17,0,0,10,0,0,21,7,0,0,21,19,24,0,16,0,15,25,0,20,9,0,18,19,18,18,16,0,0,21,7,18,18,0,0,18,8,0,0,24,0,0,20,0,18,17,0,0,14,0,15,0,0,16,0,0,0,57,5,17,0,0,17,0,16,19,0,16,0,0,24,0,0,0,0,0,0,0,0,80,17,0,18,1,18,3,0,0,19,14,2,7,24,1,40,1,25,5,1,19,8,0,0,18,0,0,24,9,5,0,16,0,0,0,0,5,17,17,0,0,0,23,18,0,0,17,0,26,0,0,0,23,17,21,17,8,19,8,19,24,16,0,25,15,0,19,0,17,0,17,0,16,17,0,23,19,0,18,17,16,7,8,17,16,16,15,17,0,19,17,0,0,17,0,16,0,15,0,0,0,0,0,8,15,14,0,0,9,0,0,0,0,17,0,19,18,0,0,0,17,0,16,19,20,17,0,0,23,19,0,17,8,25,0,0,20,0,0,0,0,0,0,0,20,0,0,24,0,0,0,0,16,16,18,0,0,7,6,0,0,0,0,18,16,24,0,21,0,20,0,1,1,64,1,18,16,1,19,17,0,18,2,0,19,0,1,2,16,16,0,16,15,0,8,24,0,16,0,15,26,25,0,22,20,0,0,10,0,0,21,25,18,0,0,18,0,0,0,18,20,0,18,15,0,0,3,0,0,14,26,0,16,0,25,0,0,16,0,16,0,0,25,6,0,0,0,19,0,0,0,25,0,0,24,6,15,14,25,19,16,25,7,0,16,0,24,0,0,16,0,14,24,0,16,20,2,21,2,19,17,1,2,14,24,3,2,25,3,19,0,0,20,0,0,0,0,16,0,0,0,0,0,0,0,0,0,17,0,20,0,0,16,25,0,26,0,0,18,7,18,0,23,0,18,0,0,21,25,0,18,0,16,0,27,21,0,0,0,0,25,18,0,0,0,0,17,22,0,0,0,0,18,19,0,0,0,0,0,20,0,0,20,0,0,0,0,22,0,17,0,0,0,0,17,0,22,18,15,20,26,0,17,25,17,21,6,0,6,0,0,16,20,12,21,0,0,6,5,16,17,0,0,0,24,16,23,0,18,18,25,22,15,19,0,0,25,17,0,19,25,0,25,18,15,17,15,25,16,0,0,15,15,16,0,25,21,25,0,14,20,18,18,21,17,16,0,0,0,18,0,0,0,0,0,18,23,15,0,26,5,0,25,18,17,16,0,0,0,5,17,17,0,14,0,18,15,0,13,19,18,19,0,8,0,19,17,24,23,0,0,0,16,0,0,0,0,0,0,0,0,24,18,0,15,23,0,0,23,0,25,17,0,0,20,19,0,26,0,0,0,18,0,13,0,10,18,0,20,17,8,0,16,18,0,0,20,0,0,0,19,0,19,19,14,20,19,25,0,15,20,16,0,0,0,13,0,0,21,7,4,5,5,11,0,0,24,0,0,17,8,1,2,0,18,3,22,3,2,18,2,44,4,15,2,16,15,17,11,17,25,0,0,17,0,0,17,0,18,0,20,0,17,0,22,0,19,0,22,0,0,0,17,0,49,0,19,0,21,21,0,17,0,24,0,24,0,0,17,17,17,0,16,0,0,0,24,0,0,19,16,16,17,3,24,16,17,0,14,0,15,0,17,0,0,17,18,16,16,0,15,0,0,0,18,25,19,17,18,14,20,24,0,18,21,3,22,5,18,0,12,0,22,24,6,0,18,0,19,20,0,10,0,18,0,21,22,17,12,19,22,11,6,0,21,17,0,15,8,17,0,0,0,0,18,0,20,17,0,0,20,20,18,0,5,0,0,2,2,25,17,2,4,2,44,21,2,3,26,16,2,3,17,15,16,2,1,2,25,1,18,0,0,0,17,20,16,0,17,0,18,18,15,14,16,16,18,0,0,0,20,0,25,0,17,0,18,18,0,21,19,20,0,18,25,15,0,17,19,0,14,0,19,0,19,17,0,0,20,0,16,0,4,0,0,17,18,19,0,3,20,18,0,18,0,24,20,18,0,0,26,19,22,18,17,5,0,0,0,0,0,0,0,19,0,0,16,19,0,0,22,0,0,25,0,0,25,17,16,0,20,8,19,0,0,12,0,21,22,23,0,0,16,25,19,19,0,17,0,0,0,25,22,25,0,16,24,22,0,21,0,0,25,0,0,16,0,23,8,6,25,0,18,0,10,18,25,0,0,18,5,0,12,16,0,0,0,16,23,0,0,0,0,24,0,0,22,0,24,19,0,13,13,0,25,17,0,0,18,26,25,17,16,19,24,28,10,20,0,19,0,0,18,0,16,16,19,9,16,0,0,0,0,17,25,20,16,20,0,0,0,17,0,14,0,24,19,0,19,0,0,20,0,0,20,0,22,18,21,0,0,0,11,0,0,23,11,0,0,0,17,0,8,25,0,19,18,0,0,11,11,18,0,20,18,19,6,21,0,18,1,1,21,18,1,0,15,1,0,30,23,17,1,2,17,25,24,0,2,20,21,0,0,0,0,11,13,16,0,14,0,0,17,13,0,12,18,0,20,11,1,18,10,20,9,2,1,1,2,12,1,8,12,15,17,1,1,2,3,1,18,2,2,1,18,17,13,0,0,0,15,15,16,18,21,25,10,19,21,18,19,16,0,0,14,0,10,22,25,25,16,23,0,0,0,0,17,28,18,0,14,16,19,17,0,26,24,0,20,21,21,20,19,0,0,17,0,15,20,13,0,16,17,15,8,9,18,17,18,20,0,19,17,0,0,0,16,0,0,16,17,0,16,16,17,0,21,0,0,0,13,18,0,16,0,16,0,0,0,19,15,15,27,20,10,5,16,8,20,19,24,19,19,17,16,20,16,17,0,23,15,0,15,0,24,19,30,16,11,0,0,22,14,0,0,0,0,17,24,24,7,0,8,24,16,16,0,0,15,24,0,0,18,15,0,17,0,0,19,17,20,16,12,0,18,0,20,9,18,0,24,6,24,25,25,0,11,0,0,16,24,0,6,17,23,9,50,0,19,0,18,24,19,26,0,19,0,0,21,18,0,25,26,19,4,22,0,18,24,0,19,0,20,24,0,25,0,0,16,0,0,24,17,17,21,15,16,14,0,24,0,17,19,19,19,18,16,0,0,17,0,0,0,15,8,0,10,18,11,0,0,15,20,20,22,0,20,0,20,0,22,0,20,21,17,17,0,20,20,0,18,0,15,15,0,8,11,0,20,0,19,0,17,20,18,11,18,17,13,19,17,12,7,17,7,18,19,0,20,17,0,16,0,12,17,0,13,0,16,20,0,17,0,15,18,8,0,21,20,0,7,0,26,0,0,25,18,0,0,0,20,0,18,7,24,11,14,0,0,19,0,0,18,0,0,0,0,5,20,26,17,23,16,24,0,25,31,15,0,0,0,0,21,16,0,0,0,0,4,6,6,15,19,0,0,20,0,0,18,0,17,8,0,0,12,0,11,12,0,0,14,0,6,0,16,0,0,17,17,17,17,20,19,17,0,17,17,0,0,0,0,0,0,0,15,0,18,14,18,0,17,0,18,0,17,0,17,0,14,20,17,0,26,0,0,18,0,18,0,14,0,20,0,20,17,15,20,40,0,0,0,17,17,17,20,0,12,22,19,0,0,0,13,12,0,19,0,16,24,17,22,25,26,0,0,0,15,0,17,0,0,0,18,0,19,0,24,24,25,25,16,16,17,0,0,0,0,24,16,34,17,15,14,20,18,23,0,0,18,24,16,1,2,20,15,2,19,12,19,0,4,0,18,0,25,0,3,16,14,16,0,26,0,0,26,14,25,0,17,0,0,0,13,0,19,0,19,15,0,15,18,16,0,0,18,0,23,17,0,20,0,0,0,14,0,0,19,17,18,18,0,25,0,15,17,12,0,0,0,19,0,0,18,45,0,0,13,17,0,0,0,0,16,0,15,5,0,24,0,17,8,0,18,18,6,18,0,18,0,0,18,0,0,0,16,22,22,0,17,24,14,0,24,16,22,0,28,5,12,18,0,13,24,19,0,0,24,15,0,0,19,0,0,23,5,16,12,0,8,18,16,17,18,0,0,24,24,0,0,0,19,6,0,0,19,0,20,0,0,0,0,0,0,0,8,7,0,18,0,0,0,21,0,0,17,0,0,0,20,0,25,25,0,15,0,0,0,6,0,18,17,16,0,0,25,0,19,0,25,19,16,0,0,15,10,21,16,18,15,6,17,16,19,4,0,21,25,18,16,0,18,24,0,21,6,0,0,17,20,0,18,0,0,0,18,17,24,0,6,0,17,0,27,18,0,23,0,0,0,1,18,0,1,2,0,0,10,1,13,15,20,0,0,16,2,0,1,1,16,18,17,18,19,6,2,0,17,1,17,1,14,17,2,19,0,4,5,15,20,19,27,20,0,17,2,13,0,10,0,0,0,29,0,15,15,6,0,15,0,19,0,19,46,13,0,0,4,13,0,0,20,20,14,0,0,20,4,6,16,0,5,15,19,0,18,0,20,0,0,0,8,19,24,19,9,7,17,0,16,5,6,16,7,7,7,0,0,27,16,0,0,4,0,15,0,24,0,0,0,0,21,0,17,0,0,18,0,20,16,0,0,0,0,25,19,4,15,0,14,12,7,17,0,0,0,0,0,24,26,0,0,0,0,20,5,0,18,15,0,13,24,19,0,19,4,0,18,14,20,19,8,18,0,0,21,0,0,0,17,44,20,7,24,0,0,0,24,18,16,0,25,11,0,24,0,0,18,5,9,24,17,15,17,17,16,4,0,0,15,0,0,0,24,0,15,18,4,0,0,14,0,14,10,0,0,24,0,24,15,14,0,19,14,30,0,14,0,17,16,0,14,1,5,0,25,25,21,17,0,0,24,24,26,0,10,0,18,6,18,16,19,19,0,4,15,16,15,24,0,0,0,0,24,17,0,13,21,7,25,24,0,25,7,0,0,3,25,7,14,14,0,6,0,19,3,14,18,0,7,16,19,17,17,0,8,6,0,24,0,19,7,0,17,27,0,17,16,0,6,19,14,16,17,0,17,5,17,2,4,19,16,25,16,11,15,12,10,20,16,14,33,19,17,1,2,18,17,5,15,16,15,0,16,15,0,15,14,24,25,0,0,16,27,0,27,0,17,4,12,18,16,0,0,18,16,12,17,0,18,20,17,17,16,0,0,0,16,0,25,21,0,0,14,0,0,0,0,15,0,24,17,0,16,16,15,0,16,18,15,18,18,28,0,26,10,11,20,14,12,0,20,18,0,0,1,0,13,0,3,7,18,13,1,16,10,11,16,6,17,0,14,0,17,15,0,16,12,10,16,0,0,0,0,24,15,0,24,8,16,0,17,0,0,15,16,14,0,15,9,0,17,0,18,0,0,14,13,15,18,0,19,0,4,0,15,0,0,0,19,0,0,17,0,0,21,16,18,15,0,0,16,28,0,0,5,0,16,0,0,0,0,24,8,0,18,7,0,0,0,23,12,15,16,22,15,14,24,0,22,25,12,16,25,18,0,17,8,0,24,19,7,15,25,0,5,0,0,0,0,6,0,24,16,17,0,6,24,0,18,16,0,13,16,16,0,0,11,8,14,0,0,0,18,0,0,16,15,0,19,0,0,0,0,17,5,6,24,9,24,0,0,4,3,17,20,5,0,0,18,0,0,23,6,9,26,0,7,5,12,18,15,0,19,17,0,17,0,19,0,0,0,0,15,0,0,17,0,16,0,0,0,0,0,0,0,0,17,0,4,19,0,0,0,27,0,0,0,0,0,52,17,17,17,16,16,0,0,0,16,0,0,0,17,19,18,0,18,0,18,0,18,17,18,0,0,0,0,20,18,17,0,0,17,0,0,0,17,0,16,12,24,12,7,23,26,0,14,12,0,17,24,12,25,0,0,0,20,15,0,19,20,17,0,20,11,15,15,0,0,0,20,0,18,0,20,0,19,23,24,9,18,21,0,17,0,0,16,0,0,0,15,3,16,2,18,0,13,1,0,20,17,0,0,0,17,16,0,16,0,0,0,18,19,26,0,23,0,0,8,0,20,22,24,16,0,0,0,0,13,0,0,0,0,0,7,0,16,0,0,0,25,18,0,0,0,0,5,0,15,27,0,17,0,17,0,0,15,16,17,25,26,0,16,0,0,15,0,0,0,3,0,0,0,20,16,18,22,0,0,19,26,18,17,15,0,15,17,0,0,15,0,2,7,0,15,0,13,0,15,0,21,15,0,0,20,0,17,23,5,0,0,0,0,24,0,0,0,13,18,17,16,15,0,0,0,17,16,0,20,18,15,0,5,0,4,16,17,0,16,6,0,5,15,0,16,10,0,18,18,5,18,18,9,5,0,24,14,19,0,0,18,0,0,0,24,17,0,0,0,18,0,0,25,0,18,24,4,0,24,0,18,7,0,0,0,24,24,0,13,24,6,20,20,18,0,0,17,19,23,7,0,8,0,13,18,15,0,0,0,24,22,16,23,0,0,0,25,24,4,24,0,0,0,26,0,0,24,25,25,25,0,25,0,13,25,19,26,21,13,12,24,0,16,17,0,0,19,19,0,0,0,17,24,21,0,0,0,0,0,0,15,17,12,0,24,25,18,26,24,24,18,0,11,26,16,2,18,0,10,0,0,13,6,14,24,0,25,0,25,14,0,15,18,24,0,25,10,0,0,17,0,0,0,21,19,7,23,21,12,0,20,0,17,17,7,0,0,18,0,0,6,26,4,0,20,15,17,15,21,0,17,0,19,0,18,0,0,18,9,0,20,15,24,0,25,22,21,6,1,16,13,18,26,24,10,19,33,1,16,19,27,4,1,20,1,22,3,0,0,15,19,24,18,19,17,18,20,24,0,18,19,15,12,2,17,2,0,11,18,18,25,24,10,1,24,24,17,2,2,25,24,10,15,15,16,8,0,17,12,0,14,25,17,24,0,19,21,0,15,21,14,24,0,11,21,17,19,11,21,19,16,24,18,12,19,21,18,0,0,0,25,0,12,0,8,20,15,25,17,10,0,19,22,17,15,0,17,0,17,17,16,15,0,0,0,24,17,9,16,13,14,20,24,30,16,18,16,0,16,13,44,19,0,13,24,11,0,24,25,26,24,19,14,25,11,0,25,19,18,20,18,0,0,17,7,7,24,15,0,20,0,12,10,8,0,11,24,19,22,24,25,8,19,15,5,20,0,26,4,11,18,25,15,19,10,16,0,0,0,15,9,15,0,71,0,18,14,16,22,14,11,14,20,15,25,15,0,0,0,17,20,19,18,19,25,0,0,18,18,15,0,14,0,4,0,0,0,0,20,0,18,16,5,0,0,16,0,0,9,62,0,18,14,0,15,15,0,18,0,21,18,0,0,0,0,24,19,0,25,17,0,19,0,0,15,18,18,16,17,24,25,11,0,14,18,0,17,0,0,0,27,0,19,15,0,5,0,0,13,11,0,8,0,0,11,17,21,70,18,0,17,0,0,4,18,18,0,0,19,5,0,6,16,0,0,14,19,0,7,48,18,5,17,25,12,24,25,0,15,0,0,17,18,23,17,11,0,4,10,0,0,0,15,0,13,0,0,0,15,25,12,17,0,0,0,0,10,13,25,1,0,15,1,11,1,2,1,0,18,24,0,0,17,0,0,0,25,0,6,25,24,25,20,0,0,25,24,0,17,7,0,11,17,16,16,0,15,23,18,0,0,8,16,6,20,18,17,0,16,0,15,20,22,0,18,0,11,0,13,0,0,0,0,0,14,17,0,19,21,16,9,16,20,0,25,0,18,18,0,14,6,5,21,5,0,11,10,0,16,13,14,0,0,15,3,5,8,20,0,24,4,21,11,0,7,26,0,16,0,0,0,18,0,21,0,0,0,20,0,0,6,0,0,15,12,16,3,12,3,25,15,1,15,16,1,2,0,15,17,8,15,25,10,3,11,18,16,0,0,7,0,6,5,0,0,0,0,15,0,21,25,4,0,9,16,0,15,16,14,0,19,19,8,0,17,12,9,0,19,0,0,14,0,0,16,18,0,19,0,17,31,13,0,16,0,10,0,16,0,16,18,0,17,17,0,17,0,0,12,15,10,17,0,0,0,15,7,0,0,0,6,0,10,18,0,9,0,5,0,6,17,0,0,0,0,9,0,8,0,12,0,16,20,0,0,24,24,0,0,20,17,17,16,0,0,0,0,19,0,0,0,0,15,12,0,0,10,0,0,0,14,0,25,0,0,0,0,19,16,0,0,20,18,0,8,0,0,0,19,0,18,18,0,24,0,0,20,19,16,17,18,0,0,0,0,7,16,10,0,0,0,17,0,6,0,8,15,12,24,17,18,0,0,0,0,25,16,19,0,6,21,0,15,0,20,9,16,19,20,18,15,0,0,0,0,4,17,20,5,0,0,0,15,17,0,0,0,15,22,25,14,0,0,21,14,13,0,20,0,17,17,0,0,0,0,14,18,18,24,4,24,0,0,2,0,0,5,0,19,0,16,10,5,0,0,11,17,0,25,0,0,0,0,6,24,26,0,22,16,19,17,18,10,0,0,0,16,18,26,21,0,0,19,22,0,21,19,0,0,13,24,20,15,0,0,25,16,16,2,9,25,0,19,24,15,14,15,26,19,3,25,13,15,0,16,20,16,20,0,17,0,10,0,4,0,0,8,24,25,0,19,9,0,8,15,17,25,21,0,20,24,0,0,0,25,17,0,10,13,0,0,25,14,10,0,25,24,22,0,0,8,17,17,12,0,0,17,22,21,17,13,24,0,14,12,19,19,0,24,0,6,17,0,17,0,0,29,11,0,14,16,23,24,12,0,0,0,12,0,14,14,0,14,7,16,27,16,26,10,0,0,0,0,18,24,11,0,0,20,17,0,15,16,0,0,16,22,0,23,19,0,24,3,8,0,10,0,2,0,0,11,0,20,0,0,17,12,0,0,22,0,13,16,0,16,0,0,0,64,0,18,0,0,15,15,8,0,17,0,19,26,14,6,0,16,18,12,6,13,16,11,25,13,0,12,22,15,0,0,11,0,0,0,0,14,0,18,17,0,17,19,0,0,0,16,18,17,7,0,19,0,0,0,11,19,14,20,17,47,0,17,19,0,16,0,0,7,0,8,7,0,0,0,5,24,18,0,24,2,16,25,14,1,17,17,18,2,2,2,18,10,3,12,14,0,12,0,18,0,21,0,0,0,0,5,0,0,0,0,17,17,0,0,0,0,41,18,0,0,0,0,0,0,18,18,0,0,0,0,0,8,0,0,0,41,0,5,19,0,0,18,20,16,17,20,0,0,16,0,25,0,24,57,0,0,15,23,10,0,25,5,0,0,25,27,0,0,17,0,20,0,0,6,0,0,9,5,19,0,11,15,19,19,18,17,11,17,0,23,19,0,16,0,19,8,18,0,0,0,8,0,16,0,0,0,0,16,20,17,19,8,17,24,18,0,15,17,18,26,0,0,8,16,20,0,17,27,26,19,20,18,20,9,19,0,0,18,6,16,15,0,0,7,30,24,0,0,0,17,0,21,0,0,0,18,18,18,20,20,0,0,0,24,0,8,14,18,19,10,0,0,18,0,14,62,16,11,0,40,18,19,19,0,7,7,0,16,11,0,18,25,18,0,17,16,6,14,0,0,0,19,0,0,16,0,0,3,16,0,25,17,19,23,0,0,0,0,25,0,24,0,9,0,19,19,0,24,24,18,0,3,0,15,0,5,15,21,24,0,26,5,0,12,19,5,9,22,14,14,20,18,26,25,16,15,0,0,13,14,0,15,17,0,15,18,19,0,14,19,17,16,2,19,6,2,18,15,17,0,18,19,19,16,16,0,14,26,16,0,23,24,20,0,0,17,0,11,23,6,17,14,18,22,19,11,5,17,17,8,0,17,0,14,0,22,0,17,0,6,7,23,24,0,25,26,19,0,19,26,15,19,0,24,17,11,11,18,0,21,25,24,23,26,9,20,3,2,17,20,18,13,60,15,23,2,16,19,21,1,15,1,26,2,15,16,3,6,2,0,17,4,26,3,12,17,24,0,17,0,7,0,21,0,18,14,13,0,0,0,11,4,18,19,19,18,0,0,0,0,17,14,0,0,6,0,0,25,0,0,17,0,0,0,0,0,16,0,16,20,25,25,19,13,18,0,0,26,20,24,0,24,1,22,20,0,23,0,2,2,21,1,9,1,14,0,0,20,1,13,22,24,18,20,6,10,12,23,20,15,0,0,0,18,25,26,0,7,0,15,0,0,18,16,10,0,0,19,12,0,0,9,0,22,17,0,15,23,0,23,15,16,10,12,19,19,12,25,24,15,15,18,20,16,28,26,0,17,0,0,0,8,0,21,14,25,12,19,0,17,14,0,0,0,24,13,0,0,24,0,17,24,0,19,0,19,0,20,0,0,19,0,20,0,0,17,0,18,10,0,0,14,0,20,17,0,18,0,0,20,16,8,60,25,0,0,19,0,9,18,16,0,13,0,0,25,16,23,0,16,0,0,13,16,7,25,0,18,24,16,12,17,23,0,0,0,18,7,21,14,0,20,17,19,19,0,26,12,24,0,25,0,18,0,20,21,17,0,15,0,0,0,0,6,8,0,0,20,5,12,8,0,25,0,14,0,7,17,17,15,0,5,24,0,22,24,18,0,0,7,17,0,0,0,24,16,0,25,0,15,23,11,22,25,25,0,17,17,0,15,25,6,24,0,18,12,4,18,0,6,0,0,0,14,0,26,25,25,0,0,18,0,24,17,0,17,0,0,0,7,10,16,23,11,18,0,22,24,19,0,16,24,25,0,0,10,25,4,0,14,26,24,8,5,0,0,25,17,17,0,4,0,25,23,17,13,26,0,18,24,16,16,17,23,14,7,17,10,8,18,16,25,10,0,20,12,16,0,0,21,0,0,12,0,17,0,0,24,17,15,12,5,19,12,0,3,4,0,17,26,19,12,14,3,28,27,8,1,3,20,25,7,10,0,15,25,0,0,0,18,18,0,9,6,18,25,0,16,0,8,5,0,14,0,0,18,20,0,0,0,0,24,12,22,0,0,4,19,23,17,17,19,0,18,18,0,0,17,18,0,15,0,16,17,0,0,24,20,0,11,18,10,5,7,0,0,0,4,17,19,24,0,0,0,0,18,20,0,20,0,25,0,21,18,13,15,3,25,23,25,18,18,19,0,0,19,18,17,6,0,16,0,15,26,24,0,10,8,16,12,5,12,24,18,0,0,16,15,0,5,0,0,0,15,16,0,19,17,15,17,1,17,20,15,25,25,15,4,0,1,26,1,1,6,6,16,0,0,19,0,19,24,26,0,8,20,23,0,16,20,0,15,25,13,0,15,0,0,20,19,0,0,20,0,0,0,0,15,14,19,16,19,16,11,14,12,16,15,16,0,10,0,0,0,0,17,0,0,25,25,10,17,0,0,20,8,20,19,0,0,20,12,18,15,17,0,12,17,6,25,10,16,25,14,26,0,0,0,0,19,0,16,15,0,0,16,0,18,14,17,24,0,13,11,13,9,0,0,25,0,16,15,0,9,14,0,0,14,0,24,0,17,12,0,49,25,10,25,18,17,20,22,16,19,27,17,17,20,18,0,0,16,16,19,15,27,0,0,20,0,0,24,0,23,19,19,17,13,2,16,9,12,15,4,0,17,15,18,18,26,19,17,2,1,2,25,1,26,19,17,20,26,18,18,0,20,15,7,18,0,18,0,17,0,0,26,0,6,20,12,0,0,16,25,19,3,15,13,6,24,22,5,3,20,2,10,0,0,0,24,13,16,17,18,17,0,0,16,19,17,7,16,18,0,16,0,0,22,7,17,17,15,23,0,23,0,24,24,25,25,17,14,21,0,9,25,16,12,0,19,9,24,0,0,0,17,0,26,17,0,8,17,17,0,0,8,0,0,0,21,0,19,0,0,0,14,19,16,12,18,0,0,19,0,0,5,0,6,4,18,8,17,20,15,16,20,0,19,16,0,10,0,0,0,17,18,9,0,18,38,19,0,9,0,22,10,6,24,7,21,19,0,0,0,0,12,0,27,25,25,25,0,0,20,0,18,0,20,0,25,0,0,0,0,0,7,18,0,19,0,16,17,14,16,8,17,0,0,18,14,24,15,24,25,17,17,18,25,26,13,0,0,18,17,16,0,25,14,0,17,0,0,0,15,25,15,16,15,18,15,15,35,24,18,0,18,15,0,0,17,0,6,11,20,8,24,0,19,0,17,20,16,23,19,5,13,7,0,10,24,18,0,22,0,0,16,0,0,0,0,0,0,17,17,25,21,0,26,25,0,6,25,26,18,20,16,19,0,25,23,20,11,0,24,18,0,22,0,19,0,0,0,0,9,21,15,7,17,0,0,18,18,0,11,0,0,0,0,0,0,24,0,0,25,0,0,0,17,16,13,0,0,0,18,18,0,17,0,18,25,0,17,15,13,0,0,15,17,0,0,8,18,10,19,25,25,15,19,6,17,20,19,0,11,0,0,0,0,0,19,8,15,14,13,19,25,22,21,15,25,0,7,16,19,18,15,17,0,0,0,21,23,0,16,15,0,0,25,23,19,0,20,0,0,13,0,25,0,0,0,21,7,18,0,0,20,0,24,17,3,0,25,0,0,0,0,24,0,18,25,0,7,15,0,0,21,20,0,21,12,20,0,22,4,18,0,0,23,17,0,17,0,13,0,0,23,0,19,10,24,16,3,0,18,18,19,24,0,0,6,0,0,0,5,0,11,14,15,11,20,0,17,25,19,0,0,0,16,0,0,0,17,24,5,17,0,13,25,17,18,8,0,21,0,14,18,26,25,11,15,0,20,16,26,17,29,4,14,9,1,1,0,0,12,0,0,0,18,12,0,0,0,0,17,7,0,0,23,0,24,21,0,19,24,13,27,0,25,18,0,0,0,19,20,40,15,17,4,18,5,7,0,0,14,17,12,18,0,0,21,19,1,1,16,20,17,1,26,2,1,3,19,16,1,20,20,22,24,25,0,13,16,17,0,18,24,0,25,16,0,0,6,17,19,0,18,14,0,0,0,14,15,0,9,14,0,15,7,0,6,25,15,5,15,6,0,5,11,0,0,14,0,0,0,0,23,16,17,0,0,0,0,18,19,0,0,0,0,20,24,17,18,0,17,0,19,17,11,0,11,0,0,17,13,0,0,16,16,0,0,11,19,0,0,20,18,19,0,17,0,17,0,0,0,25,25,0,7,0,21,25,19,0,12,26,0,8,0,8,0,16,15,0,18,0,0,19,25,6,0,11,25,0,0,18,25,11,19,0,0,0,10,16,10,8,0,0,24,0,0,16,0,11,0,18,0,0,15,15,19,24,17,16,26,24,19,0,24,0,0,5,0,19,25,19,15,0,0,11,0,15,0,0,12,18,19,18,21,26,0,26,0,9,16,24,17,18,25,24,25,0,19,0,20,16,19,0,0,0,5,6,8,18,7,22,15,0,0,25,18,5,0,0,0,18,0,16,0,0,26,0,0,13,19,0,17,0,24,8,5,13,0,0,17,0,0,12,0,24,24,18,25,20,15,25,13,1,4,5,14,14,3,0,2,13,2,16,5,13,15,13,9,24,8,15,14,1,17,12,13,16,20,19,10,0,0,0,15,0,17,17,0,0,17,19,0,0,17,5,0,16,25,19,0,19,9,0,0,23,0,25,0,0,0,0,0,11,16,12,0,0,0,0,11,0,14,0,0,0,20,25,17,0,0,17,0,15,20,17,16,0,0,0,18,25,0,0,25,11,0,0,9,8,18,9,0,0,25,18,19,11,17,0,14,22,19,12,0,18,18,19,0,7,19,17,0,0,12,25,21,22,7,16,14,19,15,17,17,6,0,17,0,8,0,16,3,17,21,18,18,20,11,4,0,9,0,0,0,16,0,0,11,17,12,0,0,25,18,11,21,18,12,0,14,15,6,0,6,15,24,20,13,10,18,17,17,19,9,0,19,22,0,0,0,0,0,0,0,0,0,0,0,24,27,0,20,26,0,26,0,25,0,12,17,0,20,22,24,0,17,8,20,17,0,0,6,18,16,0,25,18,17,0,12,18,24,0,0,18,0,0,17,0,20,18,17,17,0,0,22,21,20,4,8,0,14,24,17,0,15,19,0,0,0,0,0,0,0,0,14,0,0,20,18,9,19,5,18,0,0,20,17,0,0,21,22,17,5,8,0,16,19,0,0,6,0,6,27,18,3,0,24,20,10,20,9,7,18,16,7,12,0,23,0,0,19,0,16,25,17,27,18,21,24,25,0,19,7,19,0,17,0,0,25,25,25,25,18,0,13,0,0,25,8,15,25,17,15,0,6,25,0,0,0,11,0,0,0,0,0,17,0,17,0,0,2,17,16,11,14,1,18,18,15,11,15,13,16,1,23,2,17,21,5,5,10,14,19,15,25,0,9,24,0,18,18,18,0,24,19,0,6,0,25,25,0,19,17,6,0,0,0,25,0,14,15,0,7,0,0,17,16,18,24,24,7,23,0,21,6,9,13,19,17,19,25,25,25,0,14,25,16,22,5,16,19,0,25,19,19,19,5,25,0,13,6,0,8,32,18,25,17,5,0,22,0,15,8,1,17,0,8,7,1,18,25,25,8,12,17,25,1,2,0,2,0,24,0,15,17,1,17,15,20,12,18,20,19,7,1,18,16,2,25,10,3,3,1,25,12,3,14,5,19,23,18,18,24,14,0,25,17,7,0,0,0,4,20,24,17,1,18,26,0,1,10,4,27,13,19,20,2,3,5,1,2,4,2,17,20,0,18,24,0,0,19,10,0,0,0,11,23,0,0,22,26,0,18,18,20,7,0,25,4,20,19,15,26,13,23,17,9,16,0,18,0,0,13,21,8,0,0,17,13,22,20,14,0,24,0,14,0,0,0,0,7,24,20,25,21,23,25,0,19,21,20,18,21,17,20,17,0,0,0,21,20,18,22,12,18,19,0,24,16,25,26,19,0,18,15,0,0,0,15,10,16,0,12,26,0,19,0,16,14,25,0,17,9,14,0,16,17,16,0,0,6,24,24,0,19,18,16,21,19,0,25,0,13,25,19,16,18,4,23,0,13,21,18,4,13,24,0,18,16,0,17,23,7,0,0,14,9,16,24,0,20,21,0,0,20,19,21,0,17,0,15,18,0,0,0,0,20,7,14,0,0,0,12,20,7,0,20,20,0,25,12,21,17,18,20,0,0,0,13,8,17,0,24,18,0,0,0,0,21,0,0,24,0,0,0,24,22,22,24,11,21,17,16,0,19,13,0,24,18,18,8,0,0,0,24,12,0,0,0,19,0,0,0,6,0,27,0,0,15,6,0,0,0,0,18,19,0,26,20,20,0,0,17,25,16,0,0,17,20,8,0,15,11,0,6,18,20,0,25,0,0,16,18,23,0,0,0,7,10,0,15,15,17,18,7,7,15,17,14,0,7,9,0,0,6,8,25,0,9,20,18,0,0,4,19,25,0,19,15,0,17,18,24,21,20,22,14,22,20,0,0,6,0,24,21,1,0,20,19,2,2,2,1,20,2,2,1,1,24,16,24,17,19,25,1,15,2,15,17,2,24,2,20,15,8,12,1,24,22,0,0,17,16,24,20,13,17,21,18,10,0,12,26,13,0,0,27,20,0,0,19,0,13,25,0,26,17,0,0,15,17,18,16,27,14,0,17,3,0,15,10,19,17,18,19,24,0,0,3,0,0,22,16,17,0,30,0,26,17,0,0,17,0,0,19,21,20,0,15,0,6,15,0,5,15,0,0,0,0,0,6,0,0,15,9,18,0,0,0,0,0,7,17,0,0,12,22,0,0,0,0,19,18,0,15,23,0,0,0,11,17,4,6,11,0,18,25,24,21,0,24,20,17,26,17,0,0,22,19,0,0,0,24,17,18,11,0,16,0,22,24,23,18,0,18,16,0,25,25,6,0,23,17,0,15,25,0,0,0,6,21,26,0,0,26,17,15,0,8,20,0,0,10,7,15,18,0,0,0,5,6,7,23,13,12,17,0,7,0,24,8,0,0,14,0,2,17,2,2,1,3,19,15,2,19,21,1,11,20,24,25,16,16,21,5,1,16,18,2,4,0,18,17,15,19,3,24,15,24,18,19,0,13,25,0,21,20,21,4,13,0,26,23,14,14,15,25,1,17,17,2,0,22,4,2,16,14,23,8,22,25,25,24,20,15,15,16,16,10,2,7,1,21,26,0,1,24,22,1,2,0,1,3,0,26,1,21,21,4,0,21,2,1,16,8,16,13,1,0,18,17,1,16,27,2,19,0,17,19,22,18,19,1,19,3,1,23,14,21,2,18,1,3,24,25,25,16,19,15,13,3,19,17,18,4,14,20,14,2,3,2,1,2,2,2,26,22,2,2,2,19,15,5,16,3,26,18,20,4,18,20,0,15,26,26,24,0,3,21,18,26,2,3,0,17,17,18,3,2,2,16,1,18,20,2,2,24,3,10,18,25,23,18,5,15,13,0,7,1,14,25,14,18,6,5,0,0,2,21,18,2,1,8,1,1,15,3,2,25,1,2,13,19,1,16,1,21,18,9,2,17,13,15,1,21,16,2,25,1,1,8,13,15,3,18,2,2,17,19,16,1,0,1,18,1,1,17,0,26,4,2,18,12,5,11,18,2,2,6,11,26,17,2,5,13,5,2,2,0,18,2,14,19,26,2,26,3,22,1,18,2,18,16,26,19,18,15,22,13,0,1,19,16,2,19,25,4,2,12,5,15,23,18,27,20,19,2,20,17,12,25,16,1,25,2,4,25,25,19,1,2,20,17,1,1,1,25,8,1,1,1,16,0,17,0,19,25,0,1,1,0,18,25,10,1,2,0,18,2,1,2,17,25,18,22,16,1,14,2,19,0,14,18,25,17,8,15,17,1,17,17,18,1,1,1,23,2,18,0,17,1,2,20,19,3,1,1,16,20,13,25,18,16,14,1,14,25,24,4,25,0,26,16,1,18,2,6,2,1,23,20,1,9,1,18,3,2,1,18,2,12,24,0,18,12,1,19,12,0,0,12,18,1,17,0,25,2,16,11,2,1,1,0,18,1,3,18,1,25,2,1,21,17,1,12,0,16,1,16,12,3,19,20,19,2,3,2,19,2,2,16,9,31,25,5,18,20,2,18,18,2,1,20,2,18,20,0,16,3,14,19,1,19,22,1,20,7,18,3,1,1,20,20,18,1,0,2,0,2,13,13,18,21,2,17,3,4,3,1,26,12,3,2,20,6,3,15,4,16,8,3,2,23,1,2,17,2,1,24,10,3,13,3,3,0,18,15,17,20,5,16,4,18,20,1,0,25,0,2,0,0,15,22,15,19,19,2,23,2,21,18,19,25,17,17,19,2,12,25,25,0,23,18,18,0,16,17,0,0,18,18,10,16,20,24,0,27,2,14,18,2,2,0,2,0,25,19,21,0,2,17,18,2,1,2,15,1,17,13,13,0,1,0,7,0,1,1,18,1,26,1,20,0,12,17,17,3,2,18,2,19,20,0,17,21,18,8,4,15,23,0,1,22,1,2,0,15,25,20,2,13,0,1,25,1,7,2,0,19,2,3,1,1,1,18,2,1,1,1,1,18,17,0,63,22,18,15,1,25,26,27,24,15,14,15,26,26,23,1,20,1,18,2,24,16,1,25,18,1,16,19,15,25,2,2,21,18,5,18,16,9,15,0,19,0,0,0,26,20,0,19,5,19,0,1,26,0,2,19,6,0,22,0,0,17,18,11,5,24,1,17,24,1,1,18,0,18,0,1,25,3,26,24,14,3,3,13,3,26,26,13,2,14,23,16,24,15,17,15,2,19,24,25,17,17,18,16,2,19,25,19,1,24,2,22,17,0,1,1,2,15,16,23,1,2,24,20,0,1,19,2,0,15,1,2,14,16,17,0,25,19,17,2,1,17,17,2,1,1,25,15,19,19,12,1,24,20,0,24,1,2,24,19,6,15,16,1,25,13,18,13,1,16,2,19,4,16,17,24,5,1,2,3,1,25,0,15,1,24,18,22,28,0,5,25,16,2,1,15,16,3,1,4,2,0,13,1,25,19,7,25,1,2,13,1,2,16,24,0,17,26,1,25,14,0,1,1,25,16,12,11,23,21,0,15,1,0,18,0,13,0,11,20,19,18,1,23,19,8,12,10,0,24,17,15,1,0,9,18,3,17,1,28,26,2,15,17,16,20,14,0,16,0,10,24,16,9,1,25,16,1,18,17,0,16,0,25,0,14,1,16,25,0,4,15,21,14,1,17,2,18,0,18,2,1,1,0,25,2,1,17,0,16,12,24,15,14,1,2,0,7,17,9,20,1,1,0,25,15,0,22,16,0,25,9,17,1,6,15,16,6,23,1,25,19,16,5,2,9,21,2,2,9,25,21,2,14,2,2,2,25,23,18,3,19,2,2,16,2,25,0,3,1,1,27,18,2,1,1,4,18,15,23,26,11,2,7,24,25,0,15,25,1,2,2,14,16,13,1,19,2,14,21,2,5,17,19,23,18,0,18,9,11,23,2,4,22,14,2,2,11,17,1,25,21,2,8,18,16,25,16,18,2,23,1,0,18,19,20,0,16,1,19,24,1,11,15,24,1,24,13,19,19,24,6,13,24,23,23,20,19,21,19,21,11,2,0,1,26,17,9,19,26,10,1,27,17,3,15,8,18,20,16,9,11,3,19,10,15,1,50,2,1,24,24,15,25,2,8,9,24,1,19,3,16,25,15,2,5,23,2,10,18,1,5,24,3,2,18,25,14,26,14,0,15,23,25,21,2,4,3,8,1,14,26,0,1,0,1,22,2,1,14,4,23,1,25,23,17,23,0,16,5,23,23,19,1,26,25,0,1,17,0,18,17,0,16,0,20,16,19,0,12,11,0,13,2,16,26,14,13,23,23,21,1,25,15,20,23,13,2,15,20,17,2,14,9,17,21,25,15,1,6,1,23,2,25,25,1,15,12,16,3,0,16,17,13,8,2,1,17,14,5,24,2,1,15,17,1,20,21,2,3,1,23,8,0,26,18,0,18,17,0,17,14,16,9,26,8,25,15,21,25,25,25,25,25,25,19,15,22,1,1,20,24,2,1,2,1,24,1,1,14,1,16,1,5,24,13,17,3,15,1,10,9,2,24,2,0,18,25,19,20,4,20,14,16,24,25,19,1,10,20,10,24,25,22,23,15,18,25,29,28,21,11,21,2,25,25,1,21,22,1,1,16,14,26,26,21,25,24,25,15,0,17,21,19,1,5,15,24,13,25,28,3,11,9,12,3,2,1,20,1,27,26,14,15,25,25,26,25,0,3,26,2,2,20,18,26,24,20,25,25,17,18,25,13,25,27,15,16,17,25,25,25,4,19,17,25,1,25,25,20,14,17,20,15,24,3,21,3,26,18,25,19,16,20,6,24,24,2,9,25,17,18,10,24,24,24,18,12,25,18,19,6,15,18,18,16,25,17,22,12,15,27,27,26,26,1,3,25,25,2,3,0,16,22,2,14,1,25,16,4,2,18,14,15,4,6,25,18,8,24,16,17,25,24,24,22,17,17,3,1,1,33,2,2,12,2,11,4,1,24,25,2,18,5,6,14,18,25,13,2,2,2,24,18,14,1,18,20,11,25,5,33,21,1,17,12,6,3,0,3,20,24,1,24,1,25,26,22,1,0,8,0,5,3,21,10,0,0,17,26,1,19,19,1,17,14,26,22,1,8,20,24,19,6,16,9,5,11,18,25,23,19,6,26,16,18,5,0,18,15,17,16,26,25,1,16,25,13,2,14,25,2,18,1,3,19,12,2,1,1,2,16,24,12,20,19,22,1,9,17,14,22,14,0,0,15,10,18,20,28,24,5,25,11,0,13,12,25,18,17,1,8,6,2,2,18,12,1,14,15,25,27,2,15,18,1,2,9,11,24,8,23,23,25,18,3,0,21,18,2,13,14,24,25,20,19,8,9,21,6,0,17,3,19,1,18,26,4,1,17,5,1,12,7,2,25,22,14,2,25,24,22,20,14,24,3,1,2,0,10,14,13,21,17,18,26,8,0,2,0,24,14,14,2,0,20,1,18,20,22,11,1,24,17,25,24,19,19,24,2,0,1,22,16,25,17,13,24,21,18,6,25,25,15,16,21,17,19,25,27,1,15,14,17,11,25,10,0,6,21,0,4,24,23,15,21,1,24,1,18,12,0,11,24,19,2,19,0,24,1,21,24,11,19,11,1,1,16,2,0,15,10,23,17,10,16,4,12,0,2,13,5,32,14,22,24,17,17,26,16,24,24,10,17,1,8,14,26,1,22,17,5,2,26,17,16,16,14,2,15,20,15,5,25,24,15,15,12,22,1,3,1,1,1,1,15,2,2,22,17,1,0,0,1,2,0,22,24,25,21,7,18,2,1,18,17,0,0,20,0,9,1,24,18,0,17,16,12,18,1,19,1,1,1,2,15,2,1,18,18,0,2,12,0,14,23,1,24,24,0,1,20,0,1,2,1,2,1,1,1,1,12,16,1,1,15,17,15,21,24,0,26,15,2,0,1,0,24,0,0,18,1,25,22,24,24,16,13,16,3,6,1,12,16,26,19,24,19,1,25,25,18,2,20,17,18,13,19,26,0,1,16,1,16,18,16,15,21,8,0,0,18,24,0,24,17,4,24,1,17,4,23,1,3,18,25,18,16,18,1,1,24,1,21,2,14,2,2,2,16,19,17,25,28,1,2,14,13,5,1,23,25,24,14,1,24,25,23,0,15,1,24,1,25,2,1,23,0,19,12,1,16,26,2,25,2,12,1,24,25,15,1,1,17,10,18,0,21,24,18,4,0,0,25,3,24,20,21,3,0,16,1,29,10,11,4,0,26,15,0,2,5,16,1,15,24,1,14,3,16,18,16,1,1,24,1,13,25,2,24,20,1,0,11,25,3,14,1,1,1,13,15,6,14,13,18,24,1,11,0,1,9,20,1,22,6,2,2,1,18,15,15,12,24,12,1,13,2,3,1,22,24,16,24,21,17,24,0,20,24,1,19,0,1,5,7,2,0,0,16,0,17,12,19,1,28,19,15,24,8,25,0,12,0,24,2,23,16,2,17,0,10,2,15,17,16,21,17,1,3,11,15,2,16,24,5,4,3,9,1,26,2,16,15,4,18,16,5,11,16,3,26,18,25,25,1,18,17,11,22,25,17,26,25,19,19,25,2,1,1,15,26,14,24,11,24,11,14,0,1,0,18,19,0,2,24,13,12,11,10,19,7,11,1,1,11,2,26,7,0,1,1,25,20,21,25,19,0,18,22,15,3,17,16,3,1,1,16,1,26,1,15,16,0,6,16,26,16,17,15,0,0,16,25,14,0,17,3,26,13,29,1,4,0,2,0,13,0,21,24,1,16,15,2,5,3,2,1,13,2,3,2,13,1,2,18,2,0,13,28,0,18,21,23,1,13,19,21,9,24,1,24,3,11,1,0,12,24,24,24,24,23,12,22,1,1,1,17,18,25,14,2,10,19,1,24,2,1,3,11,18,1,17,8,15,26,15,25,25,2,17,24,22,25,7,17,18,1,25,24,2,18,8,5,12,17,25,18,14,8,17,18,17,0,25,0,1,1,25,9,1,21,1,0,20,16,16,1,1,20,20,1,17,18,0,2,29,25,17,1,1,23,20,1,25,16,20,1,1,1,22,24,16,1,1,3,14,12,17,24,11,10,2,5,20,14,23,1,19,23,12,12,1,2,1,19,2,25,16,1,26,18,18,10,12,1,2,25,2,2,15,25,2,15,24,18,24,23,2,2,15,25,25,1,0,4,2,15,15,24,2,25,24,2,0,15,1,15,14,15,18,23,1,24,18,13,2,19,2,2,15,17,17,28,25,15,12,16,3,25,25,9,13,1,25,5,26,4,1,14,1,17,16,25,24,16,25,14,1,2,1,1,0,15,25,23,17,17,24,18,1,22,12,1,2,1,2,18,1,20,19,0,23,26,1,0,15,0,6,4,17,2,2,1,0,1,14,25,24,15,15,25,25,0,1,1,22,2,16,3,1,18,0,25,17,14,0,19,15,1,1,20,26,0,0,20,16,3,24,1,17,24,7,2,25,11,2,1,1,3,16,1,20,1,2,24,1,16,2,1,24,24,24,3,10,7,13,24,1,2,17,2,13,3,16,19,25,25,13,21,20,2,25,2,15,25,19,1,14,16,21,2,24,24,15,23,21,24,25,24,1,25,23,2,0,10,8,0,24,22,25,25,19,25,57,1,9,24,25,24,18,1,25,20,24,3,17,25,0,21,13,12,1,1,3,22,4,18,28,26,23,16,2,14,19,26,0,1,6,0,16,17,0,14,20,24,16,24,24,2,2,18,23,18,2,24,2,1,19,15,0,24,25,24,1,25,0,1,23,2,2,10,14,23,1,24,20,25,10,13,25,25,9,1,16,1,15,17,25,25,17,11,20,24,15,16,25,25,1,15,7,25,25,2,17,1,2,25,3,2,24,19,2,16,17,25,3,25,25,14,13,3,1,25,14,25,1,16,3,19,25,16,19,19,10,3,1,16,23,24,20,24,26,2,20,25,24,1,9,1,25,20,2,1,13,25,24,25,20,17,16,0,2,17,25,12,19,0,25,5,17,24,24,10,24,1,9,23,19,24,10,24,11,25,25,1,16,0,0,24,12,13,5,19,15,2,24,17,2,23,1,1,9,2,1,2,1,2,9,4,1,26,2,16,27,1,24,9,0,25,25,19,18,24,0,15,26,19,9,24,24,24,0,25,2,2,24,2,10,0,24,1,17,15,27,25,26,18,2,14,17,15,26,21,19,27,27,28,15,15,27,1,2,19,1,11,10,24,24,25,25,1,16,1,15,26,24,1,0,24,24,13,9,10,14,25,24,1,25,2,17,24,25,16,0,0,22,12,23,17,25,17,1,16,0,0,16,8,10,19,16,19,25,1,28,25,23,11,17,18,18,1,12,15,26,2,3,1,25,14,25,12,24,1,1,13,28,15,19,24,24,2,26,19,24,12,1,20,1,16,6,13,24,11,24,24,24,24,26,2,24,10,15,24,18,25,14,12,2,14,14,1,5,5,9,6,1,1,1,20,8,24,0,26,24,24,24,1,16,16,1,3,1,25,25,25,17,14,2,22,24,2,13,16,13,24,20,19,1,4,15,20,25,23,24,24,13,14,19,16,23,25,27,26,0,15,26,0,0,15,27,26,0,24,26,15,18,13,11,20,18,18,18,15,1,12,19,6,16,1,23,14,1,0,16,18,11,16,13,1,14,16,25,12,1,22,1,2,9,17,15,0,20,0,20,0,19,2,24,25,0,0,25,23,4,13,12,1,2,26,11,25,1,25,20,16,18,17,25,23,2,3,2,1,4,2,3,22,16,15,3,2,2,24,18,25,16,20,19,1,19,2,1,14,26,24,16,25,2,24,17,19,3,13,24,24,25,25,24,17,17,24,24,17,25,24,13,22,21,14,18,5,1,17,17,1,24,24,1,14,16,25,19,18,26,25,1,25,14,24,24,12,19,16,0,25,19,17,1,16,16,24,24,24,27,24,2,10,2,14,25,4,24,18,25,9,26,20,10,22,18,24,19,25,1,16,16,23,18,1,3,24,24,23,1,1,15,27,14,13,6,22,25,17,1,27,17,18,16,0,17,16,26,26,2,15,26,28,0,0,26,15,3,2,17,24,17,25,20,0,22,17,26,18,15,19,24,1,24,20,23,0,13,25,8,25,0,7,19,34,24,27,18,2,24,1,21,43,6,25,18,1,11,12,21,24,12,24,21,25,5,18,25,47,24,13,1,23,25,1,12,13,2,1,12,3,24,18,18,2,2,1,1,1,16,3,24,1,24,25,23,26,0,15,22,3,13,3,21,1,2,26,23,2,15,16,14,25,0,2,1,12,25,2,24,17,3,10,4,26,14,21,25,15,26,18,16,2,24,13,6,27,1,27,3,19,15,23,22,16,1,1,24,1,1,24,18,1,19,24,3,9,24,24,1,1,24,1,17,0,4,25,18,16,18,10,17,24,19,24,25,9,18,5,16,24,16,9,0,16,27,12,23,18,3,12,15,21,17,0,3,16,25,12,25,12,15,25,1,1,0,26,25,24,16,18,24,1,19,12,18,19,23,1,1,2,16,15,1,1,3,20,19,17,22,24,0,26,3,15,17,11,5,2,1,16,26,2,8,2,23,22,25,1,18,1,14,18,26,27,37,16,2,24,24,25,2,1,16,1,14,10,24,12,2,2,24,24,3,1,1,23,1,26,20,25,21,25,2,2,14,22,4,15,25,11,13,16,26,17,16,26,10,17,13,7,12,16,19,9,15,23,24,1,17,1,0,1,11,13,22,10,11,18,24,0,1,15,15,25,0,0,0,25,24,25,0,0,19,1,2,2,2,0,11,14,25,23,14,4,0,25,17,2,1,3,24,3,15,20,18,1,24,1,1,23,11,15,10,0,11,2,0,15,19,15,11,18,19,23,25,0,11,21,9,10,10,2,1,15,1,18,1,16,0,1,0,10,25,2,1,12,11,9,1,1,24,16,26,16,1,11,4,13,23,0,14,1,1,9,1,20,25,18,18,15,24,24,16,14,25,20,28,17,25,24,15,16,15,24,23,20,23,14,23,15,16,22,21,20,19,12,15,1,13,0,3,25,16,17,0,20,13,16,7,16,18,26,19,19,22,17,17,18,1,18,1,18,24,0,23,18,1,2,23,13,0,1,2,18,26,9,2,13,22,24,11,22,4,0,46,21,14,20,19,24,14,16,3,24,5,23,16,0,17,10,1,1,18,24,21,11,1,24,1,1,23,18,25,3,3,1,2,1,1,16,24,24,24,18,8,0,20,13,13,18,2,1,1,19,18,25,13,21,21,2,18,17,19,25,1,2,1,4,10,16,11,2,17,8,3,2,18,17,0,12,15,3,1,3,14,2,44,1,2,25,2,20,2,26,12,16,16,15,25,25,10,1,12,26,6,0,0,1,17,25,25,16,14,15,0,0,14,16,11,18,21,24,17,24,12,1,25,2,25,17,51,13,1,11,12,2,0,24,2,1,24,19,13,11,2,2,14,1,25,17,9,1,1,24,1,1,4,25,21,11,1,2,21,17,3,25,13,25,24,16,17,10,25,47,14,12,17,14,14,20,26,16,2,18,1,3,11,25,1,2,15,1,16,24,25,25,15,17,23,19,14,25,10,19,25,3,3,25,18,6,28,10,12,21,19,11,14,4,16,25,10,17,3,10,1,2,10,2,1,24,8,15,24,14,12,1,0,16,12,1,15,16,30,14,15,0,1,17,15,1,18,16,13,3,7,7,2,14,14,16,3,2,0,19,25,20,16,25,28,16,20,21,23,16,10,20,25,2,15,13,1,19,18,1,25,19,16,15,11,16,3,0,16,17,0,2,1,2,1,24,24,11,8,1,14,4,0,25,2,20,2,11,8,2,0,9,23,2,1,15,13,16,3,1,2,22,2,25,2,15,2,15,1,25,2,12,25,5,17,26,13,15,0,20,25,1,0,1,0,18,17,1,15,15,0,0,17,26,18,2,2,1,17,1,13,4,24,2,2,0,1,10,2,24,25,10,9,0,1,14,2,10,2,6,9,4,25,24,16,1,14,22,2,13,19,27,14,9,25,19,17,7,2,26,22,16,17,16,1,10,14,23,27,10,0,19,18,18,2,1,24,19,25,16,24,0,14,9,10,19,21,25,17,1,1,9,17,17,0,25,1,24,3,0,22,0,11,0,14,1,0,13,17,16,12,1,20,2,1,17,25,1,17,2,1,25,18,14,1,15,25,16,8,14,16,0,9,7,3,8,16,49,10,25,1,2,25,16,15,19,25,0,25,25,24,25,22,16,16,14,2,1,18,1,0,16,19,13,16,0,16,19,0,1,2,12,2,19,9,0,13,4,0,9,2,10,2,24,17,25,15,0,15,16,18,13,0,25,0,14,25,15,13,18,24,16,3,24,19,0,0,2,25,0,15,22,73,15,25,10,15,17,13,2,1,20,2,0,13,11,7,19,17,20,12,18,25,25,6,18,3,24,2,3,2,25,22,25,15,11,23,1,10,21,3,25,12,24,0,1,3,1,1,1,1,1,24,19,18,25,19,15,15,24,14,2,17,22,18,26,0,14,18,0,1,1,24,1,1,1,14,1,18,20,25,9,24,25,0,21,17,17,20,25,0,27,26,1,14,19,1,0,18,21,26,14,25,25,0,2,18,1,15,0,18,17,25,19,24,0,23,1,25,10,15,20,40,17,20,8,25,12,1,19,0,25,1,25,13,11,25,25,19,10,25,20,17,25,18,19,16,19,5,17,24,11,17,17,24,10,14,2,1,1,21,24,20,11,16,14,2,1,22,17,9,18,20,19,24,9,23,24,13,3,9,3,19,14,4,2,21,18,25,17,10,17,16,16,14,2,0,0,21,24,24,15,15,16,3,17,12,26,18,5,1,22,0,2,20,1,2,24,5,16,23,22,24,0,2,17,16,19,9,25,15,23,15,25,25,2,15,0,14,17,16,24,0,15,1,1,15,10,18,22,24,24,1,24,25,18,14,16,1,15,15,17,3,14,21,23,0,19,15,16,13,15,15,24,24,17,18,14,9,24,27,17,16,18,1,9,25,18,24,0,24,11,24,7,14,16,14,0,12,19,16,25,20,1,6,15,1,18,16,1,15,6,25,9,26,25,17,15,11,15,1,15,16,19,17,24,1,16,18,20,26,12,0,1,2,14,7,8,3,26,13,4,1,24,11,12,24,0,9,1,6,23,25,26,22,11,2,0,3,25,3,2,4,15,12,0,0,1,17,2,25,0,1,24,18,1,25,1,18,26,16,22,19,24,18,1,22,17,24,10,16,5,13,25,18,16,1,24,6,20,19,25,26,24,25,15,26,24,25,13,17,18,25,2,15,17,19,26,18,25,15,12,16,20,15,41,15,1,10,25,26,19,2,25,21,11,2,26,17,9,18,24,13,14,16,1,19,20,22,17,25,24,15,25,14,22,20,16,15,24,25,16,1,2,1,20,6,13,1,16,15,23,24,16,13,4,50,2,2,7,12,15,24,24,20,3,19,25,24,13,0,25,18,0,19,25,0,20,2,24,11,18,3,24,18,25,0,1,19,16,25,18,22,14,2,25,13,13,14,1,15,16,24,1,25,24,25,12,14,25,16,16,1,0,24,16,15,25,0,7,11,25,10,25,26,18,2,8,1,13,25,18,17,3,0,8,38,10,27,22,4,14,2,25,25,3,15,3,14,18,9,2,3,18,13,14,24,1,0,15,24,24,25,16,14,25,24,17,26,22,24,14,16,1,15,18,2,0,17,17,15,1,14,8,25,12,23,24,24,19,13,23,10,12,13,9,23,24,12,17,20,1,24,23,4,12,17,24,24,24,2,24,24,9,17,15,0,15,8,25,26,19,25,16,11,14,23,1,19,2,2,19,23,22,11,2,18,19,23,14,15,2,25,18,1,26,13,3,17,25,18,22,1,2,14,26,18,25,17,17,15,22,25,19,14,1,2,21,20,1,11,14,21,25,19,16,22,25,22,18,22,1,12,24,21,26,23,17,28,25,21,20,0,19,1,16,1,25,25,25,16,16,26,12,15,15,1,4,25,20,16,16,1,26,24,24,19,16,20,25,20,25,24,16,19,20,16,24,5,24,10,2,1,4,1,1,24,1,15,18,17,24,24,24,11,25,14,25,1,24,1,24,25,25,19,16,24,14,18,1,20,17,25,24,24,2,25,2,3,16,23,25,20,25,17,16,24,2,19,1,24,25,10,2,1,20,1,4,10,25,24,1,1,0,23,14,20,12,12,24,18,14,25,21,15,25,12,16,1,17,16,13,12,25,25,12,20,23,25,17,13,19,26,7,19,18,3,1,0,26,16,19,16,26,25,1,21,26,16,16,0,24,25,15,16,21,25,1,18,17,18,16,25,39,23,17,13,16,26,1,24,12,24,13,25,17,22,25,20,25,0,18,25,24,24,14,12,17,16,14,26,14,11,17,16,15,24,17,1,19,18,19,24,19,23,25,16,21,20,17,28,25,28,14,15,19,30,17,17,25,14,15,7,12,26,9,17,25,2,15,24,25,2,2,2,18,25,6,19,16,24,24,24,24,2,24,17,18,14,19,16,14,14,21,15,24,14,14,24,14,18,26,15,15,22,19,19,24,11,20,24,18,18,20,0,21,24,26,17,15,19,26,13,24,15,6,13,4,25,14,14,1,13,19,17,15,4,13,15,25,8,8,2,46,14,17,16,15,27,16,26,15,18,15,25,15,25,15,20,3,23,22,24,15,12,24,18,9,16,20,25,25,12,25,25,24,24,15,12,16,14,16,15,17,15,18,1,12,19,25,13,19,25,24,11,0,16,25,11,1,18,16,15,25,1,17,25,25,22,13,4,21,25,25,24,27,26,18,18,11,15,9,14,19,19,14,17,16,14,15,7,25,10,18,25,16,24,16,17,26,12,25,25,17,16,16,2,13,21,17,1,16,27,16,9,9,10,24,18,17,25,18,28,15,24,7,23,1,20,1,14,24,24,1,18,24,17,20,24,22,25,24,16,15,2,16,1,18,18,1,18,0,16,24,19,4,13,16,17,19,3,23,25,20,11,39,18,18,16,18,24,16,17,24,15,17,17,1,24,12,20,19,18,24,16,1,17,17,24,18,16,14,3,8,19,10,25,12,8,25,2,0,26,13,11,16,12,14,17,14,15,10,23,11,13,19,25,24,4,24,19,20,1,8,1,17,23,16,14,15,25,24,10,15,17,16,0,25,16,13,26,15,5,18,22,19,0,25,11,16,15,1,25,13,24,1,11,9,1,21,13,7,18,26,8,9,13,11,1,16,15,14,22,20,2,18,18,17,18,2,2,7,25,14,7,18,21,11,20,12,21,0,2,2,25,18,11,14,14,26,6,25,25,21,12,16,3,10,5,8,9,13,0,0,9,26,17,14,24,20,24,4,19,16,24,1,2,2,2,0,15,17,17,7,38,1,25,19,4,25,0,2,6,21,18,11,17,16,2,16,1,20,17,26,26,25,14,11,19,11,2,15,22,2,24,25,12,1,2,16,21,12,19,14,25,0,21,1,15,25,12,23,19,24,8,19,0,15,8,17,25,25,17,9,21,16,19,24,9,13,14,17,16,1,2,21,17,13,21,1,1,12,0,11,1,18,16,22,1,1,2,0,1,25,14,18,6,19,3,17,1,22,19,10,14,26,16,24,25,11,25,0,15,17,15,16,18,25,16,24,19,14,16,0,0,19,21,24,17,25,0,24,24,25,17,0,16,17,24,23,26,24,10,9,22,17,23,46,12,24,12,24,9,5,15,24,17,1,24,20,1,0,0,1,2,20,25,22,20,19,1,15,9,2,1,10,0,16,20,25,2,24,11,24,11,1,1,25,14,21,28,2,22,19,24,0,22,13,1,24,1,16,3,1,23,19,14,2,2,20,19,25,24,27,1,26,20,22,0,25,17,0,25,14,20,21,2,19,15,13,1,1,25,10,0,26,3,1,21,16,17,3,15,25,9,13,25,2,6,20,25,12,26,23,19,24,1,14,2,21,11,6,19,28,21,11,14,15,25,18,19,1,25,21,25,11,24,21,25,1,18,14,18,24,14,14,16,15,17,15,17,6,15,27,17,0,25,16,2,10,13,25,4,15,17,25,31,15,1,24,2,16,17,19,25,24,17,22,21,7,0,25,24,18,18,1,19,25,25,23,17,1,1,25,7,1,11,25,12,1,19,16,10,25,1,20,10,13,16,22,16,8,18,14,25,10,19,22,3,14,20,21,27,26,3,15,10,24,15,11,20,24,24,13,6,20,2,24,13,19,1,3,15,2,14,20,7,18,24,2,1,2,14,13,19,16,19,23,19,2,1,11,17,16,0,22,26,3,1,10,17,24,1,11,1,0,25,20,19,0,1,26,18,25,1,24,17,4,3,14,20,23,6,12,18,2,25,17,24,24,16,4,2,2,17,19,14,23,3,2,22,25,15,2,18,17,5,2,19,25,14,6,2,1,2,23,1,26,13,0,15,2,1,6,13,12,17,2,16,16,18,20,15,1,26,25,0,16,17,17,21,25,17,14,16,0,25,26,14,16,25,24,25,1,10,25,11,18,17,12,1,1,25,1,1,5,25,2,1,15,0,17,1,25,26,6,26,17,1,1,24,18,1,1,23,2,24,25,0,16,13,25,13,3,25,4,0,24,24,25,4,2,2,13,2,1,24,16,7,12,2,12,2,17,16,19,0,17,2,2,17,2,9,2,2,19,24,19,21,20,2,0,6,14,15,25,16,1,18,16,25,2,20,13,26,15,16,15,24,8,13,14,11,26,15,25,15,17,22,1,8,20,5,20,7,0,2,1,14,12,16,14,25,12,11,15,25,19,18,25,25,7,12,19,2,17,6,9,24,18,2,2,12,1,1,16,25,12,18,25,2,1,25,19,1,17,2,17,27,2,2,1,24,0,24,0,11,15,17,19,24,24,17,9,14,2,23,24,24,17,25,18,12,24,9,11,20,18,19,25,19,18,16,45,6,8,14,1,1,19,28,1,2,25,2,27,25,17,24,20,3,10,15,12,2,1,2,1,4,25,2,23,25,1,15,15,15,4,1,3,12,24,23,2,24,1,1,24,1,26,18,26,14,23,17,17,20,24,30,18,0,18,17,20,16,20,17,23,21,12,18,25,2,14,7,10,16,3,14,1,11,25,22,1,25,22,17,17,14,23,0,24,13,1,3,24,16,2,3,16,24,20,25,15,3,20,17,24,24,11,14,15,13,2,1,14,3,2,24,19,11,18,2,0,8,1,24,13,8,18,10,0,25,25,24,25,14,13,14,23,19,17,2,25,16,5,15,25,14,20,26,16,24,20,14,13,20,27,19,2,18,24,15,17,25,2,6,21,1,11,15,13,1,1,25,18,17,15,24,18,16,11,16,24,26,19,24,6,24,13,25,21,16,24,16,25,14,24,2,12,16,17,0,16,25,25,25,18,17,18,5,2,2,26,0,7,15,26,13,15,25,24,2,16,2,20,15,12,19,24,1,23,1,2,1,1,15,18,15,24,25,13,2,11,12,15,12,14,16,1,23,4,9,16,16,14,25,0,13,5,26,14,9,13,13,17,17,1,17,4,2,1,25,23,1,12,15,3,24,25,16,17,9,24,3,18,29,12,25,22,2,17,17,18,20,15,15,24,8,27,26,18,11,1,2,2,25,21,15,2,4,0,25,10,25,13,19,8,22,1,17,9,7,7,1,25,18,17,22,21,6,24,10,10,1,1,24,24,23,25,24,24,11,2,16,24,24,1,19,0,16,24,14,0,14,24,5,26,1,1,8,15,27,0,0,4,24,0,25,22,9,25,11,2,1,2,18,9,17,7,8,25,24,8,18,25,24,10,25,25,16,19,21,24,8,17,24,24,17,19,24,21,17,21,3,1,13,3,1,2,1,13,21,15,18,2,2,24,2,26,2,27,16,18,26,14,12,11,2,26,1,1,24,12,17,26,13,25,13,2,2,17,18,13,15,11,24,18,15,19,0,6,12,24,3,2,14,2,12,7,18,16,15,14,24,2,1,2,26,21,14,15,10,25,24,1,13,1,2,1,25,0,2,16,0,11,10,20,2,13,25,15,22,3,17,24,2,17,1,22,22,17,25,18,14,20,14,12,25,12,0,18,18,6,0,24,15,17,24,1,15,19,20,0,2,0,0,16,0,13,1,0,14,19,1,13,0,25,20,18,0,26,25,24,0,2,25,17,17,25,1,2,0,1,25,10,0,20,19,14,3,25,25,1,0,15,16,10,13,16,24,15,1,2,0,18,15,0,19,24,25,13,2,3,2,24,11,2,2,15,17,2,26,6,27,25,2,0,19,24,0,15,24,19,0,2,17,12,2,1,1,25,7,5,15,1,10,19,36,1,19,12,19,7,21,16,1,18,17,5,16,15,25,2,15,25,1,4,0,23,16,17,25,16,2,1,16,14,14,2,23,1,24,2,1,1,24,26,9,10,2,11,25,14,25,0,19,25,8,2,12,16,15,16,14,6,1,24,11,5,11,17,2,1,26,25,15,15,7,13,2,14,24,20,2,5,26,18,13,24,10,5,16,1,0,0,20,12,1,1,18,19,12,16,13,27,4,0,2,0,3,11,24,2,14,24,16,13,16,18,24,0,13,19,24,9,19,17,14,14,1,18,15,25,18,15,24,14,15,24,15,12,15,2,16,25,2,11,6,22,26,9,22,15,10,2,0,0,19,25,15,20,18,8,19,4,11,13,24,20,24,8,14,1,18,8,9,23,24,13,11,20,25,18,25,0,16,27,12,7,0,17,20,46,0,12,0,26,17,0,25,11,22,14,0,20,16,25,16,17,16,21,13,19,16,25,4,1,25,1,21,24,8,16,0,14,15,15,25,1,16,25,24,7,14,14,21,18,2,23,24,2,24,23,15,17,19,5,19,22,18,24,2,7,23,24,18,2,2,21,1,20,15,17,1,15,25,24,19,1,18,24,21,6,22,0,14,14,19,17,18,15,24,14,0,3,13,15,17,25,21,16,1,9,18,1,19,16,25,17,15,14,18,25,19,3,2,17,17,25,14,14,15,2,15,4,1,23,13,1,1,1,17,0,24,0,19,24,17,0,16,20,10,12,10,14,24,1,0,0,7,3,14,19,12,1,18,17,2,12,1,21,9,6,16,22,25,25,2,2,19,15,1,2,3,24,19,1,8,0,10,19,2,16,2,1,14,16,2,24,17,2,24,9,27,20,26,18,16,12,19,12,26,13,25,10,1,3,18,25,4,19,27,0,17,13,26,19,18,8,27,7,24,12,16,27,21,19,9,29,25,7,17,21,5,0,11,18,13,20,21,12,24,1,17,2,0,13,26,17,24,24,18,1,17,1,2,2,19,25,18,17,1,3,11,26,12,1,20,10,1,24,24,24,19,0,6,18,25,2,10,20,25,18,19,15,2,2,1,17,1,1,1,7,17,8,1,0,10,24,16,24,14,1,12,15,24,25,20,14,26,12,18,2,15,15,24,25,12,18,12,26,15,19,16,15,14,17,0,16,18,20,26,19,18,26,26,17,1,10,27,27,25,22,9,19,1,18,3,2,9,21,20,15,4,24,24,24,10,23,24,19,22,24,18,24,23,25,11,18,25,25,24,22,1,1,24,17,20,24,23,10,13,24,23,2,13,8,2,1,16,3,1,1,25,23,24,25,24,18,21,2,24,24,21,11,26,27,20,13,25,12,24,24,26,28,30,24,17,2,22,19,16,18,16,14,26,25,10,14,18,17,24,15,11,15,17,25,26,24,23,18,10,19,11,22,14,14,20,24,23,8,19,27,3,10,10,24,10,1,0,21,25,21,1,25,12,21,2,8,15,24,20,24,25,17,25,18,12,25,9,26,10,25,12,13,15,15,14,17,24,2,24,13,12,16,25,12,17,24,19,11,15,21,1,1,1,22,6,15,18,0,5,19,0,15,23,22,0,16,5,15,25,11,18,10,14,17,10,26,25,24,22,15,1,16,0,1,1,24,24,10,17,22,0,14,17,20,24,17,20,24,12,26,26,13,9,15,26,2,10,13,26,14,1,0,25,17,5,1,24,2,2,2,20,1,24,50,20,1,1,1,1,14,12,13,17,2,11,18,20,3,22,4,25,11,25,25,24,23,9,26,22,8,21,15,1,2,15,25,25,14,25,25,1,2,20,26,1,1,20,7,4,10,0,20,16,25,25,25,33,20,25,1,19,21,13,10,16,10,24,12,6,17,2,14,2,22,14,9,17,18,6,11,18,15,23,26,20,24,20,13,18,16,17,25,20,24,24,13,25,7,24,16,0,14,2,22,13,1,13,1,0,12,17,24,25,24,25,17,19,17,0,24,17,0,7,25,25,20,19,24,25,25,18,19,25,25,17,20,26,15,18,14,26,26,19,4,1,24,17,29,26,10,26,14,13,17,24,18,21,18,25,4,18,24,0,22,26,26,26,2,0,25,17,17,18,6,4,24,16,55,25,1,24,24,16,23,0,5,17,0,25,24,24,16,10,12,24,25,25,21,14,19,14,22,20,15,25,10,25,0,24,13,9,20,18,9,12,8,19,12,25,10,21,1,0,12,25,24,1,12,25,17,16,17,2,24,24,18,0,28,11,17,12,13,0,8,19,0,17,18,2,9,8,2,1,13,10,14,12,11,10,11,1,25,16,25,19,2,25,23,25,24,18,14,20,16,18,24,17,18,25,25,19,25,15,25,24,19,1,11,7,26,16,19,23,25,1,26,1,24,1,24,18,64,18,0,10,1,25,17,26,9,24,25,14,16,12,24,25,6,18,16,12,24,25,1,10,0,24,12,24,16,23,20,12,14,2,15,7,24,24,24,19,19,25,0,16,16,10,25,3,18,25,1,10,19,25,2,14,6,19,24,16,10,25,17,23,25,17,23,19,22,15,6,10,30,26,24,18,20,17,26,2,25,24,16,1,14,19,10,14,20,26,13,20,25,19,15,17,25,22,22,20,3,15,16,18,14,17,7,25,16,19,18,0,0,25,1,16,17,17,14,5,24,25,20,11,12,26,9,12,10,21,10,7,18,20,25,25,25,25,25,16,18,20,25,25,24,14,19,18,10,1,24,16,2,16,11,12,25,18,18,2,16,25,14,11,25,15,5,24,80,2,2,18,1,24,13,1,14,20,1,4,45,6,24,1,25,1,22,20,24,17,11,12,15,1,19,15,17,12,24,25,16,15,15,19,19,16,25,17,1,26,4,2,12,1,2,13,27,18,25,1,12,1,19,2,2,17,7,25,18,25,19,12,1,16,25,11,21,22,25,14,11,15,14,12,24,12,18,22,4,19,2,25,37,20,24,15,1,0,23,9,17,24,24,24,0,23,25,18,14,26,20,12,1,26,17,9,17,9,14,11,20,27,10,24,17,1,17,28,8,26,17,2,7,2,18,16,25,13,13,25,2,25,20,27,16,10,25,12,20,23,9,9,20,15,3,8,11,21,26,17,13,17,8,1,25,17,24,23,24,21,11,1,1,16,11,17,14,19,24,16,17,25,18,22,25,9,25,12,10,17,18,25,25,17,15,2,14,17,2,25,24,8,11,25,21,25,16,22,22,2,24,3,19,21,2,19,21,12,25,20,25,15,26,24,17,20,20,7,17,18,0,26,12,18,25,27,27,25,27,16,25,2,26,13,23,4,5,2,18,24,2,24,15,18,24,44,16,23,17,22,25,21,1,6,22,22,2,2,22,0,22,17,18,1,17,24,15,24,1,22,1,23,25,14,12,16,24,11,24,21,1,18,20,25,24,19,28,19,18,17,18,13,16,22,3,14,12,17,25,28,8,9,18,18,10,17,3,7,15,10,24,19,24,19,12,24,13,22,23,25,19,19,21,9,24,20,24,1,25,25,14,0,17,20,23,19,25,14,13,14,1,24,6,3,10,9,19,14,25,20,16,15,16,16,15,24,20,27,16,14,9,18,24,16,24,24,17,15,20,18,1,15,16,15,23,18,17,20,15,24,18,24,13,10,18,18,2,12,28,29,15,17,30,18,17,18,16,15,1,22,18,13,2,0,18,17,24,14,3,16,13,27,16,24,2,3,25,2,17,24,24,10,12,1,16,14,5,25,16,24,17,0,24,16,19,24,12,25,0,14,24,8,9,14,24,9,12,19,1,1,18,19,0,24,22,17,14,9,24,19,1,18,22,11,13,17,14,24,6,24,13,1,2,24,16,14,24,25,25,23,15,19,17,24,24,20,24,24,1,20,12,24,24,24,7,24,24,1,16,14,0,15,23,18,11,24,22,25,13,14,13,14,1,1,19,0,24,11,17,23,24,11,18,9,27,24,16,21,13,11,16,16,13,25,22,14,25,10,16,13,24,0,24,0,16,24,6,1,13,1,9,25,17,25,12,0,15,19,13,24,19,18,26,25,9,24,6,15,11,12,3,1,25,13,24,24,16,15,17,24,17,25,13,23,0,15,0,17,11,12,15,24,11,16,11,19,24,9,23,2,47,11,25,1,14,22,24,13,8,24,24,25,16,17,25,21,25,23,25,14,21,11,18,25,1,15,24,11,23,21,27,20,7,10,12,16,16,26,8,28,26,2,12,24,26,17,7,10,13,11,25,2,14,24,24,5,3,21,25,22,16,17,0,0,11,13,16,0,26,25,18,24,25,27,14,26,20,27,20,27,22,22,26,16,15,14,4,17,23,17,1,24,14,16,1,2,3,14,13,25,19,19,26,16,24,18,18,21,23,11,3,24,25,0,14,19,16,18,24,24,24,1,17,25,18,13,15,11,22,14,18,12,19,7,23,15,16,16,24,10,24,24,16,24,26,25,3,2,26,15,12,20,10,16,4,2,27,27,10,13,13,23,10,12,24,24,18,12,24,21,24,23,15,12,22,12,22,9,24,15,21,23,24,15,25,21,23,25,19,24,10,14,20,11,25,11,25,16,15,25,16,14,0,19,18,24,10,14,21,22,4,19,27,24,22,18,8,4,25,24,14,18,25,19,24,18,18,23,16,24,2,1,7,3,26,13,12,18,1,16,15,25,13,11,25,10,10,9,0,18,16,0,12,11,15,19,11,25,1,24,25,0,36,18,16,25,25,24,16,1,17,13,25,6,25,25,18,24,22,20,2,4,18,4,17,22,21,25,18,14,15,25,25,20,25,14,2,24,3,22,19,25,24,15,11,24,13,13,0,11,18,25,24,14,13,24,26,0,27,1,20,23,12,16,1,24,17,24,23,13,24,24,25,26,24,18,18,1,11,1,26,25,18,20,19,21,27,12,11,20,25,13,15,25,21,12,25,11,13,1,25,25,20,24,21,24,24,25,24,25,26,17,19,1,15,15,16,19,25,20,14,14,14,27,11,16,16,16,25,7,10,15,6,24,0,1,8,0,25,22,24,22,16,20,15,16,20,20,24,24,24,24,24,0,25,20,5,25,20,24,26,25,2,15,24,7,25,20,9,25,19,24,25,4,14,24,18,26,22,24,17,26,24,23,14,25,15,0,25,1,0,22,22,27,15,25,10,15,1,24,16,15,25,14,13,13,18,11,12,25,17,9,15,13,19,23,19,17,12,26,20,25,9,10,17,19,22,15,18,6,21,25,23,23,1,1,14,20,1,24,15,24,25,24,25,26,2,24,12,22,2,11,24,15,20,23,24,16,25,25,18,17,25,13,1,15,13,15,13,1,24,25,1,13,15,24,19,18,16,9,25,16,11,25,25,0,0,24,21,25,11,21,25,13,19,24,24,25,25,23,15,10,23,23,20,9,26,26,10,26,19,26,23,16,26,24,24,17,14,16,17,25,17,26,25,24,11,18,24,25,1,2,25,15,20,11,9,5,1,22,24,13,23,24,1,1,0,13,14,0,12,17,25,9,20,25,19,18,1,15,20,2,20,19,17,14,17,25,18,13,24,9,11,16,20,21,25,26,11,9,14,25,14,26,19,25,18,0,25,9,10,25,16,19,15,15,2,20,17,26,25,3,0,27,20,10,15,18,1,18,28,12,18,12,25,14,0,9,12,15,24,7,11,15,25,26,18,22,22,28,19,18,17,20,25,12,18,7,0,18,22,3,25,1,24,15,23,17,10,13,26,0,25,23,24,1,14,25,22,19,21,19,1,25,2,16,26,24,25,6,16,23,25,18,24,19,17,24,28,18,18,24,25,24,24,25,25,24,8,23,23,17,20,24,16,18,22,14,25,25,35,17,25,25,17,14,25,24,20,25,12,1,23,11,14,20,11,24,21,13,18,0,25,4,17,25,8,14,24,14,14,10,11,22,14,1,18,24,1,18,28,18,0,25,21,4,13,24,24,11,17,24,10,24,24,23,9,1,24,8,11,16,26,27,15,2,25,1,14,25,27,14,17,22,16,1,23,21,14,23,19,23,25,18,9,24,24,12,25,21,2,19,25,24,20,11,24,25,16,20,21,11,25,25,13,18,19,25,24,23,24,24,16,14,11,15,24,24,25,23,12,24,13,20,17,22,21,0,19,25,4,25,15,18,16,24,23,23,12,19,25,25,25,15,20,8,19,17,26,9,19,25,14,17,0,17,24,25,28,24,26,24,25,26,1,24,6,13,25,6,0,23,25,24,24,11,18,13,19,25,0,7,25,17,18,18,21,18,10,0,25,15,2,0,1,25,0,22,14,28,26,26,15,18,17,7,14,25,13,25,14,6,0,23,18,9,15,1,25,17,12,2,16,22,25,0,0,25,17,2,24,19,17,17,19,18,8,17,15,32,12,25,25,15,25,18,11,16,12,16,24,25,11,16,1,1,25,26,25,14,26,25,25,26,26,27,26,2,25,26,1,24,0,26,12,25,1,24,17,1,3,25,14,23,25,10,5,24,12,4,22,1,1,12,24,22,14,19,3,7,0,26,0,0,26,0,0,20,16,17,11,25,20,2,24,11,0,16,1,1,21,25,1,24,25,17,22,25,17,3,24,20,0,24,0,0,10,8,25,25,13,25,14,26,25,13,0,25,14,15,20,26,15,26,23,26,25,20,25,23,20,18,27,17,13,26,14,20,26,26,22,25,25,25,18,21,25,25,25,25,23,22,23,20,13,25,17,7,26,17,15,24,22,1,18,20,11,27,25,11,25,0,5,22,14,25,12,25,5,0,0,13,24,15,0,5,5,22,9,25,14,24,24,13,12,3,17,4,25,25,23,25,1,9,7,10,2,25,19,6,0,1,11,23,16,25,1,0,0,3,0,25,22,24,24,24,9,14,0,24,0,0,25,26,25,26,3,17,25,15,25,25,25,25,22,25,23,22,15,25,24,7,17,0,24,24,1,24,24,24,25,9,13,13,13,0,25,14,25,25,15,25,25,25,1,13,24,13,22,25,24,24,25,21,25,25,12,24,1,0,25,23,25,25,1,22,25,25,16,19,17,27,16,25,20,26,12,26,25,26,3,13,11,16,19,25,12,16,25,24,25,22,16,14,7,20,24,22,20,19,2,0,2,26,0,0,21,24,25,24,24,20,0,25,24,24,21,9,25,24,1,0,10,13,24,11,24,24,7,5,23,8,25,16,25,25,25,12,26,24,1,26,13,11,14,14,22,24,26,25,10,25,13,17,25,24,1,13,24,24,23,26,25,1,20,15,16,25,24,26,16,22,25,16,25,1,0,24,15,24,24,17,16,23,21,13,0,21,11,24,11,25,25,24,25,12,25,25,12,17,24,25,25,0,18],\"type\":\"box\",\"name\":\"Carmelo Anthony\",\"marker\":{\"color\":\"rgba(166,216,84,1)\",\"line\":{\"color\":\"rgba(166,216,84,1)\"}},\"line\":{\"color\":\"rgba(166,216,84,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"base_url\":\"https://plot.ly\"},\"evals\":[\"config.modeBarButtonsToAdd.0.click\"],\"jsHooks\":[]}\nLet op. Er is ook een Plotly wrapper, ggplotly, voor ggplot2-objecten. De onderstaande code maakt een box plot met behulp van ggplot() en vertaalt deze vervolgens naar Plotly. Dit kan handig zijn om snel plots te kunnen maken als je de Plotly functionaliteit wilt en je meer gewend bent aan de ggplot2-syntax. Uit eigen ervaring weet ik dat de functie plot_ly() beter werkt dan de functie ggplotly(), dus ik zou meestal aanraden om de functie plot_ly te gebruiken of gewoon bij ggplot() te blijven als je de extra interactiviteit niet nodig hebt. Ik gebruik ggplotly() om snel uitschieters te identificeren bij het uitvoeren van verkennende analyses op een nieuwe dataset.\n\n\n{\"x\":{\"data\":[{\"x\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"y\":[15,13,16,5,0,3,9,7,4,0,21,0,18,23,17,0,9,0,6,19,0,22,21,0,0,0,1,2,9,26,2,1,17,1,0,14,1,13,0,0,0,0,17,16,24,24,21,17,24,19,23,0,18,24,0,0,23,24,1,20,0,1,1,0,1,14,19,16,0,1,0,0,0,1,0,0,23,0,1,3,2,25,17,14,13,3,2,7,3,7,1,6,2,16,0,0,17,18,17,24,0,0,6,11,1,1,0,1,5,0,22,24,5,0,2,24,0,24,0,0,23,0,0,0,0,0,20,0,2,15,11,18,14,20,5,0,5,8,6,23,15,16,15,0,22,20,0,0,14,10,22,24,0,9,4,1,2,19,1,18,1,1,1,12,12,17,24,17,10,27,22,18,0,25,0,0,0,24,0,27,26,10,23,6,7,0,0,11,15,0,0,0,17,15,19,1,1,14,8,17,23,0,8,23,18,18,22,22,23,0,21,0,21,8,20,10,2,13,25,8,9,3,2,17,2,0,0,1,14,3,2,14,1,1,0,0,0,0,15,2,21,21,10,3,15,2,0,9,0,22,12,0,4,0,22,1,21,2,17,0,21,6,6,0,21,10,10,12,5,20,14,1,3,18,6,9,8,10,16,19,19,10,0,0,0,4,0,18,0,4,0,0,0,12,24,3,0,0,25,0,0,0,9,0,21,24,0,23,0,11,0,0,1,4,25,0,0,0,0,16,0,0,18,17,0,0,4,2,1,8,0,25,20,0,0,5,1,24,15,16,4,4,1,4,1,19,1,11,22,7,13,0,0,19,21,13,1,6,22,23,20,1,1,17,24,25,25,18,10,7,11,0,24,18,1,1,19,1,1,1,0,0,0,8,12,4,0,25,1,0,0,14,1,9,0,0,17,2,2,13,17,4,0,0,9,0,1,1,18,1,10,16,20,12,5,20,2,14,24,17,10,0,2,1,16,0,21,17,0,4,8,5,0,0,0,0,25,0,7,0,0,0,0,18,0,13,0,0,1,6,0,0,12,4,16,24,21,0,0,0,24,0,22,1,1,0,0,0,1,0,22,0,11,15,12,0,0,5,0,15,0,0,0,23,3,0,0,19,0,3,21,21,0,1,2,23,0,0,20,20,15,0,9,0,22,25,5,0,20,14,1,4,18,1,24,24,0,15,25,0,11,11,0,8,13,6,29,0,13,19,0,24,18,1,0,0,2,1,2,18,0,0,0,1,15,19,1,22,5,18,3,7,0,11,0,16,20,0,0,0,0,4,21,0,4,0,0,0,0,4,20,6,0,12,9,0,2,1,0,25,22,15,0,24,24,1,13,6,0,0,8,12,16,0,14,10,6,0,16,0,24,0,16,1,1,0,0,25,22,0,11,14,0,0,0,0,0,5,4,0,0,0,2,15,1,24,0,24,1,21,0,0,0,22,26,0,0,5,26,0,0,0,0,0,0,0,19,0,0,22,19,0,0,0,24,0,13,0,2,18,14,18,22,13,2,23,0,1,1,12,0,1,1,21,4,14,15,0,0,18,15,1,13,25,24,25,14,2,6,0,0,0,0,0,9,25,24,0,4,0,14,2,5,0,2,0,1,2,0,0,0,13,0,23,0,0,3,1,15,1,14,1,2,6,1,5,19,24,24,10,20,7,17,21,18,2,2,1,2,17,2,0,0,18,18,24,0,0,15,0,0,18,0,0,15,0,0,0,0,0,0,6,11,6,19,6,15,0,0,0,18,16,1,24,0,5,0,0,6,1,5,0,18,2,17,17,16,0,0,20,0,1,0,16,10,18,15,0,5,1,17,1,22,5,3,17,4,24,3,0,20,20,15,17,0,0,22,19,0,0,0,0,3,0,2,13,25,4,5,25,0,0,0,11,16,5,28,0,0,0,0,26,0,24,0,11,4,17,1,0,19,18,1,18,14,18,19,10,19,0,16,8,26,17,0,22,0,1,1,0,0,0,7,0,9,14,18,0,0,0,24,17,14,24,24,0,17,0,0,0,0,21,0,0,15,0,0,1,1,0,0,24,24,0,2,5,1,0,0,0,10,19,0,19,0,20,2,24,0,3,1,24,2,16,0,0,14,0,0,7,0,7,0,18,21,25,0,0,0,6,9,19,11,10,22,22,0,0,23,5,0,23,0,0,22,20,3,25,0,0,0,9,0,0,22,0,0,0,0,23,5,11,0,7,0,0,10,15,9,1,18,5,20,3,1,10,7,3,2,2,17,10,18,18,0,1,1,15,0,0,8,1,24,21,22,20,0,16,25,24,24,17,1,0,0,0,6,6,19,10,0,0,10,9,21,6,6,11,11,0,0,0,4,11,24,0,0,0,0,20,20,0,2,0,0,0,18,21,2,0,20,0,15,0,1,0,24,4,3,16,1,21,0,20,0,9,20,0,0,0,17,6,21,1,1,0,0,0,1,24,24,0,0,8,0,20,0,0,9,25,0,1,0,17,0,1,2,1,24,19,14,5,0,2,19,3,2,6,0,0,0,4,0,0,0,7,28,5,17,1,1,0,23,1,1,0,0,0,22,24,24,24,23,0,0,0,12,0,4,6,0,7,20,0,0,0,19,21,20,16,16,13,0,3,22,2,0,22,24,18,3,25,0,21,16,19,14,2,3,21,20,24,17,0,21,21,20,21,24,0,16,1,1,25,0,1,0,16,12,16,19,20,25,26,0,0,27,0,0,24,19,2,17,22,16,0,24,0,0,24,24,23,6,18,21,0,0,14,8,24,25,19,10,20,2,18,21,9,24,24,1,20,0,21,17,0,18,22,0,19,21,5,11,6,10,20,15,0,22,0,0,0,0,22,0,0,0,23,23,25,0,26,14,17,2,19,0,24,24,0,5,0,0,0,12,0,0,0,25,1,1,5,17,0,1,0,3,24,12,2,3,25,1,24,24,24,1,1,1,3,6,23,21,26,2,14,23,22,0,1,2,1,0,1,2,2,21,6,2,10,0,0,0,0,24,19,18,25,0,20,6,0,8,1,2,0,0,24,0,24,22,24,25,18,24,14,25,20,0,0,21,25,19,0,0,0,0,0,0,22,25,0,25,0,25,24,10,16,2,13,0,19,0,1,2,0,19,0,0,25,2,0,20,0,0,28,0,11,20,0,0,0,0,25,0,17,0,1,15,1,23,1,0,17,1,24,2,0,0,7,0,0,23,23,0,20,18,0,0,23,25,24,25,25,25,1,14,2,1,1,20,0,0,0,22,25,0,0,19,20,10,21,0,25,0,27,23,0,16,0,0,22,8,11,20,0,2,2,0,18,5,0,1,15,0,0,1,0,19,0,0,17,0,0,0,21,0,24,0,0,12,1,0,0,23,22,4,9,1,2,17,7,2,1,0,0,1,9,9,1,1,4,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,3,0,0,1,1,21,3,9,25,6,0,3,22,21,2,0,8,8,11,0,23,24,25,1,27,10,1,23,1,27,20,18,2,1,2,2,2,20,0,14,0,21,19,21,0,0,22,0,18,18,0,0,21,0,29,26,24,0,0,17,0,0,6,0,22,4,0,0,24,0,0,17,18,0,0,0,20,0,0,0,19,0,21,25,23,0,0,0,0,19,0,24,3,23,20,0,0,0,27,1,24,22,0,2,1,0,1,24,22,0,0,0,1,1,1,12,24,1,1,10,20,0,0,16,16,19,18,0,0,23,2,20,15,24,20,20,0,6,0,17,1,2,24,16,15,0,26,25,0,13,26,26,0,0,0,1,17,0,0,0,0,0,19,0,0,0,19,18,16,19,20,15,0,0,20,15,12,0,0,0,0,0,25,21,1,1,0,0,23,24,4,0,4,25,0,16,0,0,0,18,0,25,27,0,16,20,24,0,1,12,14,8,16,15,13,17,25,24,15,0,0,3,0,0,0,2,17,25,1,24,24,2,18,13,2,0,2,16,15,0,11,2,20,16,20,20,9,21,0,0,0,0,0,0,19,26,25,0,21,10,0,0,17,8,16,0,25,25,21,25,20,0,24,25,0,0,0,6,4,21,0,0,21,0,0,24,0,12,0,23,26,0,0,2,17,4,2,5,19,0,0,18,0,22,21,19,22,0,20,0,0,24,24,0,25,0,25,1,0,18,25,11,8,25,0,0,22,3,0,0,0,0,0,0,22,17,20,25,4,2,1,3,21,20,22,6,9,2,16,1,2,1,18,1,24,1,1,25,2,8,1,19,25,4,0,27,24,21,22,0,21,19,0,0,0,16,17,1,18,1,1,1,2,0,0,0,0,15,24,0,25,7,0,0,7,0,23,1,1,20,17,11,18,25,15,1,0,3,25,21,18,21,24,1,23,0,0,0,6,19,0,22,8,26,0,0,2,12,12,14,1,1,1,25,23,14,1,26,25,22,4,12,16,20,1,1,1,16,1,21,26,16,1,1,0,8,0,0,0,0,9,18,4,20,17,2,20,20,21,0,22,0,2,0,4,1,0,0,2,0,24,2,0,0,12,19,21,0,25,0,0,0,0,0,21,20,0,0,0,17,10,25,9,2,25,25,0,0,0,0,0,25,26,0,14,8,22,11,16,1,2,20,18,7,0,22,0,5,26,18,1,0,16,0,8,11,22,0,16,17,16,19,2,14,0,18,1,23,1,12,11,1,1,1,3,20,28,24,18,22,1,22,22,27,6,0,13,0,16,17,0,0,0,0,0,0,0,24,20,0,26,0,7,0,0,25,0,0,0,9,7,22,0,0,0,11,0,7,0,17,0,0,10,19,20,0,0,0,0,20,19,16,0,0,0,1,24,0,13,24,18,0,0,0,16,25,1,25,1,1,24,0,23,2,20,17,1,20,0,18,22,25,18,0,23,0,23,1,21,1,25,21,11,0,21,5,0,0,14,0,0,6,0,0,0,1,15,16,1,0,25,24,1,25,0,24,7,0,0,0,20,0,0,0,6,1,1,1,0,0,0,2,24,24,21,1,8,15,24,18,0,0,0,0,0,24,17,18,4,0,1,1,0,1,1,1,25,18,23,25,1,0,25,0,22,24,0,0,0,24,24,25,0,0,0,21,5,0,5,13,0,0,4,0,0,0,0,0,0,0,0,0,0,21,0,0,17,5,22,0,0,0,0,0,0,0,0,7,0,0,22,0,21,20,0,18,6,22,20,1,0,18,0,25,0,1,18,2,1,11,2,1,22,1,25,19,2,2,1,1,0,20,25,20,2,9,0,1,22,16,24,0,0,24,0,1,6,0,2,17,9,22,0,0,9,24,0,1,0,2,0,19,0,0,25,21,6,0,24,26,0,24,22,25,7,8,24,1,0,1,8,1,2,1,0,16,2,0,7,24,0,25,15,1,13,13,2,0,19,10,15,0,35,0,2,0,20,3,6,5,5,22,25,19,0,0,0,6,6,15,1,12,24,18,24,0,0,1,0,1,25,1,1,4,2,1,1,0,1,10,0,15,1,1,0,0,4,0,0,0,16,27,0,0,0,20,0,11,0,0,16,25,0,25,3,18,1,21,26,0,8,2,0,0,0,1,0,0,0,0,0,1,25,1,0,0,0,0,15,0,16,21,7,1,0,1,8,8,1,0,1,1,1,24,10,25,1,21,0,23,14,21,0,23,17,11,0,0,0,0,0,9,0,0,10,11,0,23,14,15,16,0,22,19,0,0,22,1,0,5,1,2,16,16,1,23,20,15,4,26,26,18,20,0,0,19,21,19,0,0,3,23,0,1,1,1,0,17,1,0,24,16,0,20,0,14,0,19,0,8,0,0,25,26,0,23,1,0,21,16,7,0,16,17,14,2,19,18,1,16,18,24,82,8,3,12,14,25,0,18,0,18,7,0,4,0,6,4,5,0,14,19,0,23,22,26,26,27,10,0,21,24,0,19,7,19,21,20,11,21,17,0,20,25,22,0,0,7,0,0,5,16,0,20,20,5,5,0,5,6,0,25,14,22,21,0,6,0,3,0,0,6,0,17,11,7,0,0,0,0,21,7,5,0,16,3,0,1,2,2,22,0,2,25,0,0,18,0,21,2,0,1,1,10,15,1,0,0,4,0,25,0,24,24,0,2,0,6,25,0,23,15,1,19,0,24,0,24,7,16,13,25,16,0,4,1,0,20,0,10,0,1,6,0,25,0,0,0,9,1,0,23,1,0,3,5,14,18,0,23,0,2,16,0,9,4,0,0,0,19,20,19,0,0,19,0,21,0,0,0,0,0,0,0,0,0,0,15,19,0,0,9,14,18,15,15,13,0,9,9,18,1,1,0,0,10,1,1,16,12,19,25,0,13,0,0,0,20,15,25,6,26,0,0,18,17,0,0,1,18,2,24,16,18,0,24,25,0,0,12,0,0,22,20,0,0,17,0,15,0,7,0,24,6,0,16,3,0,20,1,16,16,0,25,24,0,9,14,1,19,24,16,25,3,22,0,0,0,0,0,20,25,0,0,19,0,0,0,0,0,16,0,0,0,0,17,26,12,25,14,26,28,0,0,0,22,22,0,2,0,1,0,24,0,0,0,1,0,23,0,17,21,22,25,25,27,11,1,2,0,0,0,0,1,0,25,0,6,0,0,25,0,0,0,0,0,2,3,2,9,18,24,2,1,0,0,0,0,19,1,19,3,1,1,1,3,1,0,0,0,0,24,0,0,21,17,1,2,2,2,1,1,23,0,0,0,24,0,0,0,0,20,19,13,20,21,22,15,22,0,10,9,0,16,0,11,20,0,14,18,0,0,0,0,22,15,0,13,3,1,1,19,17,0,25,23,0,1,11,0,1,15,0,0,0,16,25,0,0,15,15,19,0,5,0,10,1,17,1,6,15,0,0,25,0,26,0,23,7,11,1,0,0,0,0,0,0,14,22,0,3,6,9,0,1,0,0,9,0,8,24,4,11,0,13,0,24,25,19,3,17,1,0,0,0,20,0,0,25,0,25,0,24,3,16,19,0,2,0,2,21,25,18,8,0,0,0,19,8,5,0,0,0,23,0,17,0,22,21,0,21,0,12,0,13,0,0,0,0,0,0,0,0,0,14,18,0,26,19,21,7,20,0,1,26,3,5,0,7,7,14,0,1,25,0,0,16,0,16,0,0,6,1,1,0,0,1,0,24,1,8,26,0,4,2,2,20,0,0,0,0,0,6,0,0,24,20,0,0,25,24,24,16,19,0,27,27,0,1,0,0,0,0,0,0,25,24,19,1,0,0,16,0,2,25,4,8,4,13,2,20,14,1,0,16,22,23,25,22,20,0,0,25,10,0,0,0,20,6,21,1,0,22,0,25,0,0,20,22,0,16,15,23,23,0,0,1,1,0,0,0,21,0,21,0,0,0,0,0,21,1,0,19,17,0,0,0,18,6,23,0,0,0,24,0,4,25,19,0,9,0,19,0,14,1,0,0,0,16,17,0,0,4,1,1,0,0,0,1,20,1,0,0,5,15,0,0,0,7,12,0,13,24,0,0,0,14,17,10,0,0,25,0,1,0,0,1,0,4,21,21,9,24,20,21,20,0,14,22,26,0,0,21,5,0,0,10,0,0,0,24,24,13,0,25,25,7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,18,0,0,0,25,29,22,0,0,25,25,0,6,25,24,18,0,25,18,0,0,0,0,26,19,18,19,2,18,24,2,20,10,2,0,0,25,0,0,0,0,18,14,0,0,0,17,11,23,1,0,0,0,9,17,4,0,21,0,9,22,1,20,19,19,1,0,0,0,0,0,0,0,0,0,22,0,27,0,14,0,21,0,0,3,0,0,15,0,0,15,24,24,24,0,0,1,24,20,22,0,0,1,18,25,1,27,25,0,0,0,21,25,13,0,0,0,9,8,25,8,16,25,7,0,0,19,21,0,0,0,18,17,0,0,18,0,0,0,1,0,12,24,0,19,2,25,0,0,9,26,0,0,22,0,0,16,22,0,1,0,1,0,18,0,0,22,3,12,24,1,1,0,19,26,0,1,24,25,26,0,6,14,0,10,6,21,17,0,0,26,25,25,0,1,1,1,19,0,0,0,1,27,0,0,1,0,0,20,0,0,0,0,19,0,3,4,9,13,0,0,0,0,26,0,0,0,0,27,1,3,21,1,2,4,25,0,9,0,5,0,0,0,1,0,0,16,9,16,6,20,21,9,1,0,1,0,8,0,0,20,21,0,0,6,0,23,33,0,0,22,28,5,23,25,26,24,0,16,17,0,34,0,11,24,0,8,0,22,9,18,0,1,23,20,25,26,27,1,25,23,0,0,1,0,15,19,24,0,18,0,0,0,26,0,0,15,25,17,0,20,1,0,1,0,22,18,22,18,15,0,19,20,0,0,0,1,24,7,0,0,0,0,0,25,20,6,0,0,0,15,18,22,17,0,1,25,0,0,25,0,21,4,2,0,0,20,14,19,22,17,0,0,0,23,21,21,0,20,22,0,11,25,0,26,21,1,1,27,0,0,25,0,0,0,1,0,0,1,0,0,0,19,0,24,4,0,0,0,0,0,0,0,25,0,0,0,18,1,18,18,0,24,24,0,20,24,0,0,0,0,21,25,2,16,28,0,1,0,1,0,21,15,0,0,0,12,0,17,0,0,23,21,23,0,0,0,1,2,0,0,0,0,15,0,19,30,0,21,0,19,1,0,0,0,0,22,0,0,0,0,0,0,0,0,1,14,16,0,0,0,0,8,6,0,6,0,0,23,0,0,0,0,0,4,18,0,0,0,0,1,14,0,12,0,0,20,0,1,0,20,20,1,24,0,0,0,0,21,0,0,18,26,24,0,25,23,0,12,0,24,20,0,1,0,22,21,22,0,18,1,19,0,0,1,0,0,1,1,16,19,1,19,1,1,0,1,24,1,1,5,19,25,0,25,16,20,0,25,0,21,15,0,0,0,17,19,0,0,18,7,19,0,24,0,1,2,8,16,0,0,26,23,17,23,17,0,26,1,1,0,6,0,2,21,1,1,0,0,1,1,25,25,24,16,16,19,0,0,0,0,0,0,25,20,0,24,1,1,21,3,20,1,1,22,0,0,0,19,0,26,24,16,0,0,1,1,1,0,27,1,0,24,22,6,0,0,25,23,0,0,25,0,0,0,1,1,0,4,22,27,21,5,17,2,26,0,4,3,0,27,3,2,6,4,0,0,0,24,13,0,3,17,0,23,22,17,19,0,0,0,15,0,16,19,0,0,16,18,15,0,0,0,25,24,0,0,0,0,0,0,18,8,0,20,0,1,3,15,17,1,1,0,20,1,0,1,0,0,0,7,2,22,7,17,16,24,23,23,0,21,8,0,0,0,8,0,0,0,5,24,18,23,0,26,0,25,25,1,25,25,20,19,25,1,1,0,1,0,0,0,21,0,0,0,22,11,20,0,0,0,25,26,0,0,0,15,26,1,20,0,0,25,0,0,0,0,0,0,0,0,0,0,0,7,0,21,27,21,1,0,5,1,0,21,20,16,0,19,18,25,17,20,21,18,0,21,0,0,21,19,18,0,0,20,18,17,23,24,20,0,24,0,0,2,1,25,4,26,9,1,2,18,0,0,0,0,15,25,0,0,0,0,0,0,22,0,18,0,0,0,0,19,0,0,0,10,18,8,0,8,19,1,1,0,25,22,24,11,0,0,0,12,0,27,0,0,13,24,20,0,27,0,1,0,0,25,0,25,6,0,17,24,0,20,20,24,19,0,0,17,22,20,12,21,0,0,0,0,20,0,23,25,0,0,10,22,25,25,26,0,15,0,1,7,1,1,2,25,1,0,16,0,11,1,20,1,24,0,0,0,0,22,14,21,0,21,6,6,26,0,0,28,32,15,26,26,26,25,14,15,25,7,1,0,8,0,1,20,1,1,0,24,22,24,25,0,20,22,0,19,17,4,2,18,1,1,0,23,25,24,0,6,7,1,4,27,0,14,25,23,25,0,18,20,24,0,24,0,0,23,23,0,24,6,0,25,1,25,23,21,1,1,24,24,0,6,0,20,1,0,0,0,15,0,0,0,8,0,0,19,14,24,0,0,25,24,19,0,19,5,1,19,22,25,1,3,2,25,2,0,5,4,3,2,27,0,0,22,5,22,21,0,20,20,17,22,0,2,25,0,27,20,21,21,0,24,0,6,26,22,22,6,0,11,0,8,0,4,7,0,0,7,0,25,22,21,3,22,0,0,24,0,0,19,0,0,0,1,0,25,0,0,1,19,0,0,28,16,25,1,0,13,0,15,20,1,20,22,27,0,25,0,14,0,6,20,0,0,0,0,0,2,16,16,2,24,0,2,25,25,29,21,8,0,0,23,17,25,18,7,1,18,18,14,3,21,0,0,0,29,21,23,9,21,18,9,0,20,0,0,0,0,14,26,10,17,20,25,25,3,1,1,1,22,25,26,22,5,0,8,0,25,1,26,20,9,1,1,23,1,33,0,1,20,23,1,17,26,1,24,0,16,0,0,0,0,24,21,0,22,25,0,0,0,19,28,21,20,0,21,20,0,1,8,0,1,23,25,1,0,0,1,0,1,0,18,22,25,21,21,26,16,16,21,20,25,18,0,19,9,20,0,0,0,0,0,0,25,26,12,13,21,20,0,0,22,0,0,25,6,25,18,21,14,0,0,0,1,10,5,21,20,18,0,0,1,25,19,1,1,1,1,20,25,24,24,1,0,1,1,19,2,16,19,22,3,21,21,13,1,6,2,25,21,0,25,22,8,1,23,0,1,0,0,0,0,0,0,19,19,0,19,0,21,26,0,0,0,0,16,0,19,0,0,0,26,26,0,0,1,1,0,1,0,1,1,1,1,1,20,23,1,0,11,1,23,19,1,0,7,0,22,0,17,0,12,0,0,0,0,0,25,0,25,24,0,0,22,25,0,0,25,7,0,0,24,0,0,16,0,1,14,2,11,24,0,20,18,19,0,14,1,0,0,0,0,5,6,0,25,0,24,0,24,25,26,18,22,0,3,0,27,1,7,1,2,1,25,0,15,0,22,0,6,4,0,26,22,19,0,26,14,23,14,14,18,0,24,0,24,0,17,6,0,18,0,7,0,0,0,21,21,17,25,0,19,0,19,0,0,22,20,20,20,24,21,21,0,0,26,0,0,0,9,24,0,0,10,19,0,1,1,1,0,16,11,26,26,1,9,0,0,0,1,1,3,0,1,7,18,0,1,25,25,0,0,24,25,14,1,0,0,0,0,0,23,0,0,0,25,1,1,0,0,0,21,4,21,25,0,1,0,0,3,0,0,17,26,0,0,16,0,0,20,19,19,0,0,23,26,0,8,0,0,0,22,25,19,0,25,21,0,22,1,2,1,0,3,13,26,5,9,1,1,3,24,0,0,0,6,0,0,24,0,0,24,24,0,0,0,23,0,25,0,0,0,0,0,25,0,0,1,2,1,0,0,0,1,0,0,18,22,0,1,22,20,24,20,18,0,24,20,1,25,24,15,28,26,25,0,0,0,0,24,21,26,20,0,14,5,25,5,2,23,21,1,1,0,0,0,26,25,26,29,28,0,16,8,6,19,0,1,0,25,27,19,1,18,18,11,1,23,22,0,1,0,25,20,25,32,31,0,18,20,20,0,25,24,20,19,0,0,1,0,9,1,2,1,25,0,16,1,9,25,22,0,18,0,14,0,22,23,18,8,6,19,5,0,1,1,1,1,0,0,1,24,12,8,0,18,0,20,25,25,0,0,0,23,0,21,0,6,0,0,21,0,1,1,24,2,0,0,25,13,20,0,24,2,2,25,0,26,2,1,0,24,0,1,1,1,21,0,1,1,18,18,0,25,1,0,21,17,17,8,0,0,18,16,0,21,0,0,0,1,0,18,1,0,1,1,0,13,1,24,25,24,10,0,14,0,28,28,0,25,0,0,10,25,7,2,6,21,0,0,1,0,0,0,0,15,0,0,0,18,25,1,1,7,2,21,24,23,21,1,8,8,0,20,22,0,0,21,21,8,19,21,1,25,8,24,17,8,0,0,17,0,6,0,10,0,25,11,17,8,9,18,1,5,0,19,18,7,0,1,1,27,0,26,0,1,23,22,24,18,20,22,0,19,27,1,1,0,0,0,21,0,1,0,1,0,0,18,1,3,25,22,0,3,2,0,21,13,1,2,3,1,3,2,1,2,0,3,1,0,0,21,19,19,21,0,23,24,21,0,24,1,0,3,1,5,4,26,22,24,12,9,18,2,2,1,2,2,2,2,1,20,17,22,19,2,24,2,21,1,19,3,1,1,1,2,2,1,24,26,2,20,1,2,1,2,17,2,8,3,4,2,21,2,3,13,1,15,3,10,17,0,20,20,3,20,3,20,8,12,2,2,25,21,21,3,20,4,5,17,24,9,2,1,4,25,0,0,0,7,22,0,3,1,21,2,0,21,26,23,2,3,21,1,18,24,2,2,0,2,9,25,13,2,6,9,1,1,2,14,3,2,2,3,20,2,18,23,19,3,12,5,3,23,21,3,21,16,1,19,17,24,2,0,21,22,1,25,0,18,15,15,21,20,23,20,2,0,1,9,10,0,25,14,21,24,21,1,23,24,3,2,1,22,2,1,2,0,17,2,1,0,1,26,25,25,1,19,2,16,18,3,18,25,24,1,19,2,2,2,12,7,17,4,11,3,1,25,17,25,18,16,18,2,24,16,2,19,7,20,2,22,0,0,26,25,7,1,20,0,23,24,22,21,2,6,0,0,16,0,0,0,0,5,0,24,22,25,19,23,26,17,23,26,1,25,1,1,2,26,8,19,8,11,3,20,8,3,7,0,17,18,20,2,2,7,2,2,2,3,10,12,1,2,1,18,25,18,20,20,18,33,0,19,9,2,2,21,12,2,2,13,14,18,24,1,14,13,19,11,19,17,1,1,0,18,13,1,1,1,1,22,16,6,3,24,25,25,17,1,5,27,2,24,4,10,23,3,2,20,8,5,2,2,3,2,3,24,2,24,5,5,2,2,24,2,2,9,3,5,2,1,12,2,1,1,2,1,1,2,2,2,18,11,4,14,3,2,23,3,2,2,25,9,2,25,2,17,0,1,24,12,24,1,7,1,2,21,4,2,14,16,18,21,18,9,1,17,22,3,1,4,24,24,2,9,25,17,18,1,1,14,2,1,2,2,3,8,3,2,2,2,1,1,23,2,17,26,1,1,2,20,1,16,17,0,0,7,19,17,19,0,0,0,0,1,3,0,0,1,6,22,2,2,1,13,11,20,26,1,0,0,8,1,22,21,2,2,0,1,1,2,2,3,15,10,2,11,16,17,25,15,24,25,3,1,1,0,6,22,17,2,22,1,1,5,7,13,8,2,21,21,25,2,19,2,2,5,2,12,3,2,2,14,15,21,20,21,14,16,2,22,18,1,6,2,2,1,6,2,15,17,2,23,2,2,20,5,19,25,19,11,17,10,23,2,2,24,2,2,6,2,2,21,9,16,2,3,16,1,9,1,2,19,2,2,19,19,25,18,16,24,1,10,2,20,11,3,19,2,20,6,11,3,2,2,19,2,1,19,14,24,14,1,19,0,11,19,23,18,25,22,22,24,3,19,21,21,24,2,1,15,20,6,9,19,18,21,1,1,0,9,0,0,1,9,0,3,0,0,2,9,15,2,1,24,1,1,1,16,1,2,16,1,1,4,1,2,0,2,8,2,11,19,15,19,4,3,0,5,53,1,0,15,0,5,2,15,0,1,0,1,24,1,20,21,19,12,1,0,0,0,18,9,1,0,0,1,3,2,14,2,16,1,2,2,6,2,10,1,25,10,2,18,25,17,18,24,0,0,14,17,3,1,2,2,0,9,2,1,1,2,3,16,1,1,20,2,16,19,20,1,25,0,24,19,0,17,1,0,0,0,9,12,18,18,17,17,3,20,4,19,1,18,1,2,16,1,20,1,1,6,11,16,4,0,2,4,8,5,2,1,0,0,0,20,0,22,22,20,0,0,16,0,1,0,13,17,0,2,11,16,0,1,17,1,2,1,2,1,17,1,0,18,1,1,1,1,11,1,6,1,1,0,19,1,0,15,3,16,2,4,2,1,19,20,5,17,2,0,0,0,1,0,1,1,21,21,1,3,17,2,2,3,3,3,15,0,2,18,19,22,1,18,15,20,22,24,26,1,11,1,1,24,16,11,18,24,24,26,14,15,21,21,24,8,24,18,1,15,8,0,15,20,0,0,1,10,1,1,4,2,10,18,17,9,8,0,4,1,15,22,15,8,10,18,19,0,29,18,1,0,11,17,0,1,0,0,26,0,0,0,16,0,1,0,19,14,0,1,18,0,1,0,0,1,5,17,19,18,18,26,1,0,2,2,2,1,25,1,1,1,1,22,0,25,19,0,26,6,1,19,17,0,24,5,10,2,17,19,2,22,21,1,18,1,0,1,0,18,16,1,0,15,1,5,7,1,1,1,1,8,0,19,3,2,1,18,0,2,1,0,1,20,1,0,1,24,19,1,8,4,10,6,4,0,11,0,1,10,13,15,0,11,0,25,2,1,0,0,30,1,1,10,0,24,0,1,11,0,25,24,17,8,2,0,24,15,1,1,9,0,0,4,1,0,0,23,25,0,0,0,20,15,21,0,18,0,1,1,1,17,24,1,7,1,1,27,0,7,0,8,19,0,2,2,11,13,16,19,0,17,21,24,22,1,1,12,1,9,2,12,2,20,2,2,1,14,14,14,12,1,8,3,13,33,4,0,12,1,15,1,18,17,19,18,2,0,3,1,0,1,0,2,24,28,8,3,3,7,24,24,0,18,15,16,2,11,1,1,2,5,9,6,24,25,8,13,3,1,0,16,11,1,4,19,1,5,17,20,1,18,16,1,1,16,0,1,1,3,19,13,0,0,0,18,0,14,0,4,3,2,17,14,7,3,2,19,26,2,19,12,15,24,21,1,1,1,1,1,0,1,5,2,1,1,6,10,0,14,3,10,20,0,2,11,14,1,9,11,17,1,13,1,18,3,17,6,1,0,2,13,7,25,14,8,17,18,1,25,24,1,24,1,20,1,1,1,2,0,1,24,2,25,2,1,2,11,18,12,1,1,1,18,1,15,22,14,19,0,5,0,1,1,25,19,16,0,11,0,1,11,20,16,24,25,2,16,2,14,24,8,9,0,1,1,1,1,1,14,24,0,19,2,4,19,1,20,19,1,15,1,23,1,1,2,1,2,2,7,1,0,1,2,0,1,1,18,1,24,19,18,24,10,12,17,16,1,6,1,6,1,0,13,13,5,1,14,1,24,0,19,7,18,0,11,1,24,25,22,22,2,1,24,5,15,1,3,24,1,18,1,11,0,19,3,2,1,2,1,1,6,0,24,0,1,23,0,1,23,0,12,1,24,0,21,0,2,9,19,3,0,2,19,0,0,0,4,19,18,1,1,0,0,1,1,13,1,22,25,19,21,18,24,2,23,27,2,2,12,4,3,2,2,9,17,3,2,8,8,0,0,19,4,14,1,1,19,1,16,16,15,15,11,0,26,0,1,2,8,1,15,18,25,1,0,25,0,0,1,25,0,6,1,1,0,21,21,25,1,24,15,25,0,11,18,2,1,5,1,1,7,1,24,8,0,1,0,13,0,2,0,0,0,0,0,26,1,2,17,0,24,19,0,24,24,1,0,6,1,1,4,1,0,1,14,19,2,0,1,0,12,1,0,0,1,24,1,0,4,0,5,0,1,10,4,1,5,24,8,4,24,18,2,0,1,0,13,16,0,17,1,16,1,24,8,24,7,1,5,20,22,2,24,2,4,25,23,1,7,18,17,18,1,1,11,20,1,24,1,19,0,11,1,0,17,1,22,0,20,22,1,0,0,0,1,1,6,24,25,16,15,4,17,0,17,0,13,25,17,26,20,16,0,0,14,0,0,1,1,3,23,0,15,12,0,2,1,21,17,1,0,25,10,6,2,1,1,11,1,1,0,0,24,3,16,1,15,17,7,15,24,0,0,19,19,23,12,24,19,1,17,4,0,2,9,6,2,1,18,12,0,25,20,1,0,24,1,0,18,0,0,6,2,3,1,3,0,2,1,1,1,2,23,0,24,2,6,2,11,0,22,0,25,2,20,12,12,19,0,17,1,2,2,3,1,2,1,6,2,1,1,2,2,26,19,6,2,1,0,1,0,12,18,0,1,0,1,0,25,2,6,9,0,1,0,1,18,16,12,1,0,1,24,26,1,15,16,11,26,0,0,2,14,3,2,22,2,23,2,26,18,15,8,23,3,4,4,4,19,14,6,1,0,21,2,2,4,4,0,24,22,0,5,7,9,1,1,1,0,6,1,1,0,1,1,19,20,1,5,8,0,25,23,24,1,25,1,18,19,5,3,26,25,23,19,1,8,1,1,15,5,0,24,0,0,19,0,1,1,8,0,1,17,22,0,0,0,1,24,23,25,22,20,24,23,0,1,24,0,14,0,0,3,0,26,21,23,14,1,2,1,4,24,1,2,20,21,22,17,21,0,2,24,18,1,0,7,0,0,0,16,1,3,1,16,7,3,9,17,0,0,18,1,1,1,0,7,0,1,25,20,25,1,13,25,0,0,1,1,0,18,15,24,24,1,1,1,1,1,11,6,0,0,24,0,1,19,1,0,0,25,2,1,25,0,1,3,1,1,25,1,1,0,11,14,1,4,19,1,1,0,0,0,1,26,0,0,0,1,0,0,9,12,0,0,14,12,0,0,0,21,23,18,2,25,21,16,0,1,0,0,0,1,0,0,18,0,25,0,0,0,0,20,1,1,0,0,25,1,25,25,1,18,1,12,1,19,25,1,1,1,20,19,0,13,0,0,0,1,0,15,15,16,0,2,1,3,10,2,1,0,2,20,24,25,4,2,2,1,1,1,1,1,1,1,18,25,26,24,25,22,26,13,25,1,1,24,16,27,1,3,8,20,19,12,21,1,0,1,0,21,0,1,8,20,17,5,20,0,1,1,0,0,0,0,0,1,20,0,23,0,0,1,0,16,20,2,0,1,17,9,2,22,1,1,25,25,24,0,1,3,0,1,1,4,2,1,1,1,22,0,24,23,1,23,0,0,17,0,3,6,4,2,1,2,2,2,1,2,1,24,2,4,25,0,1,1,1,0,0,2,8,18,2,1,11,20,2,1,3,7,6,7,1,25,24,23,1,16,25,12,0,2,3,1,2,0,1,24,11,16,21,20,14,10,1,1,1,0,1,5,1,0,17,15,1,18,1,13,1,0,1,5,2,2,1,1,1,0,24,24,6,1,26,17,0,1,25,0,24,1,8,8,18,15,16,0,19,1,0,1,2,18,25,18,0,0,24,1,20,25,2,0,24,1,0,9,12,1,10,1,1,23,3,0,23,1,1,1,25,0,1,2,1,24,0,1,0,0,0,5,1,0,0,0,1,0,0,0,16,1,10,0,0,20,0,20,0,6,2,1,1,24,1,1,1,1,0,1,1,0,4,8,1,14,0,0,2,0,0,17,1,21,1,0,1,0,5,17,1,20,1,25,0,0,7,0,0,0,24,1,4,0,1,2,25,1,1,24,0,2,4,5,26,26,0,18,1,3,2,19,3,0,2,1,18,1,3,3,5,23,2,2,25,23,25,2,25,25,24,25,24,6,17,0,21,0,4,1,0,8,1,25,21,2,1,2,20,1,0,0,3,5,1,4,6,1,1,17,18,11,25,0,1,0,0,3,0,25,21,0,25,0,2,20,22,22,0,2,1,1,0,0,0,13,4,7,2,1,25,26,14,1,2,2,0,2,11,5,20,0,5,18,1,1,1,0,0,0,4,14,22,0,1,1,8,11,10,0,0,0,0,12,20,1,0,0,24,0,15,2,3,0,5,25,25,2,0,1,1,20,0,1,9,16,13,0,1,9,1,2,1,25,1,2,25,0,2,1,12,25,1,1,19,20,1,2,0,3,17,13,2,18,20,25,2,1,2,0,25,1,1,0,1,2,1,17,12,19,2,6,13,0,0,0,3,21,12,2,3,1,3,2,24,25,1,3,26,1,25,5,0,2,2,0,1,2,24,6,0,0,18,25,17,0,6,0,9,22,1,1,23,0,1,1,8,17,25,27,0,0,7,19,5,1,14,22,21,0,24,25,24,24,0,0,19,1,3,18,5,1,0,12,0,8,16,1,0,24,1,2,0,17,24,1,1,1,0,1,0,1,8,2,1,0,1,1,1,1,19,19,1,23,0,2,24,25,0,17,24,23,15,24,0,1,1,26,28,1,20,0,2,8,1,1,1,0,1,2,0,1,2,1,5,2,15,0,1,2,1,2,27,13,0,2,2,23,22,24,3,0,1,0,8,0,0,2,0,12,24,3,20,0,1,0,17,0,4,1,2,0,24,0,1,19,1,1,7,25,12,12,25,23,25,0,0,25,16,0,23,0,1,0,1,10,17,2,1,20,14,3,2,2,18,25,1,1,24,2,0,0,14,1,0,0,1,0,21,8,1,1,1,7,1,2,0,0,1,15,20,25,5,26,16,0,0,0,26,1,18,0,1,0,23,0,2,25,2,1,0,0,1,1,0,3,1,3,0,0,0,0,13,27,3,24,0,4,21,5,4,3,25,1,1,1,2,0,1,1,1,0,25,0,1,24,1,25,1,1,0,1,1,9,0,15,1,1,25,1,1,0,0,13,0,26,25,21,0,20,0,15,12,26,19,23,7,20,18,14,1,1,6,0,24,9,1,16,15,4,26,0,27,0,25,1,0,1,19,21,5,11,23,2,0,0,11,0,19,2,2,2,0,24,28,9,1,9,3,1,24,24,16,23,0,24,5,4,16,1,2,0,0,9,0,8,1,0,24,0,16,24,0,4,0,8,5,17,24,0,0,24,0,0,11,7,0,24,27,0,24,0,24,18,18,0,0,0,0,0,0,0,0,26,9,16,0,0,25,25,0,1,0,0,14,2,1,26,1,24,8,0,2,25,0,3,7,0,0,0,23,0,0,0,16,0,27,1,23,18,0,1,12,10,1,0,10,1,15,1,1,3,17,5,0,6,16,1,0,0,10,1,1,13,26,0,5,2,0,12,26,23,3,6,20,25,1,4,13,18,1,17,19,0,1,15,8,15,1,6,25,6,16,10,26,0,2,2,14,19,0,20,2,2,0,2,21,16,19,24,26,17,1,25,21,0,20,0,15,1,15,15,25,0,0,0,1,0,0,0,18,22,0,0,4,16,25,0,24,26,0,0,25,1,20,0,10,0,8,23,3,0,25,25,30,2,17,24,18,1,17,0,0,27,16,0,17,0,0,0,18,6,17,0,15,26,16,0,25,0,28,25,1,0,0,1,1,24,1,1,1,5,1,1,14,25,1,24,25,4,25,1,1,1,21,25,3,0,2,26,9,20,26,19,0,1,9,0,0,21,14,12,15,1,13,0,0,0,4,0,0,7,4,0,5,5,0,0,0,0,22,24,13,19,0,0,0,0,0,26,14,25,14,0,0,0,0,0,3,17,0,9,0,0,0,0,0,12,25,0,1,6,1,0,0,14,0,26,1,21,20,0,22,26,21,25,0,21,1,0,0,0,10,0,1,24,0,15,0,3,1,2,18,2,5,0,16,16,0,7,7,18,24,19,20,27,3,0,0,1,0,0,21,1,25,2,1,20,1,1,10,25,1,21,0,0,1,18,12,4,0,28,29,26,15,6,6,0,0,15,0,26,4,0,0,0,1,0,0,26,6,0,1,0,24,0,24,21,17,24,15,25,0,4,5,0,1,0,24,1,0,11,25,19,19,25,1,18,21,24,15,1,13,12,0,20,0,9,1,0,1,17,23,2,1,14,2,0,25,24,2,0,2,0,14,24,1,20,1,25,0,25,0,2,2,26,24,2,0,1,26,26,1,18,0,5,20,25,0,9,24,25,0,1,2,4,2,0,9,0,16,12,24,23,0,25,24,0,0,1,25,0,25,2,10,10,1,10,0,0,24,20,8,23,11,0,9,0,0,1,0,1,0,0,27,0,15,1,0,26,25,22,0,16,1,22,27,24,1,1,2,0,0,0,22,11,0,20,0,0,5,6,13,7,24,0,1,1,0,25,0,1,1,0,0,0,19,16,27,0,0,0,1,0,24,15,1,7,0,1,0,0,25,24,0,20,25,20,0,2,2,1,26,2,2,2,21,6,5,0,0,1,20,1,5,7,12,13,9,18,5,0,1,1,7,17,2,1,0,15,0,5,7,0,0,0,0,0,1,1,1,0,26,0,1,1,1,0,1,1,16,25,22,24,10,17,1,1,5,0,15,19,15,15,1,18,0,25,0,0,1,1,5,1,2,1,5,1,10,0,1,0,17,16,1,1,2,2,11,1,0,26,25,24,0,0,0,0,0,29,1,27,25,0,1,23,24,0,25,0,0,12,0,0,1,11,0,0,19,19,25,2,1,25,0,13,1,19,8,15,0,12,5,2,3,3,0,0,25,5,23,13,4,2,2,1,1,2,27,26,0,0,2,19,0,6,10,0,24,1,7,0,0,1,0,0,0,0,1,0,0,28,0,1,2,0,1,3,2,16,1,2,1,0,0,0,1,1,0,1,22,0,7,1,26,4,0,0,0,0,1,2,1,0,17,1,2,1,2,2,2,5,2,0,2,9,25,24,0,1,0,0,0,0,0,1,21,2,1,2,0,18,6,0,0,0,1,0,0,0,2,0,1,1,0,2,0,7,1,8,2,25,1,1,9,0,0,1,2,0,0,3,0,24,21,4,21,24,1,0,18,21,19,0,18,18,0,0,9,0,24,21,0,26,1,0,17,25,25,1,1,18,1,0,1,26,12,21,14,1,3,29,29,26,26,0,1,1,0,1,2,1,12,25,0,0,18,22,18,18,8,0,25,21,1,25,0,19,2,24,0,27,24,1,0,1,2,16,1,1,0,1,26,13,1,1,1,25,2,2,1,1,1,1,0,0,2,1,2,0,1,1,0,2,2,26,16,0,0,1,1,2,24,1,9,1,1,5,0,0,0,0,0,1,1,1,0,17,0,1,16,0,0,0,0,24,21,1,5,20,1,0,0,18,0,0,7,15,14,16,11,19,8,0,1,0,1,1,1,11,1,21,3,3,4,2,18,12,4,5,10,1,0,1,13,8,14,15,0,0,0,0,1,1,17,0,0,0,7,0,2,1,0,0,23,7,0,24,14,13,1,0,4,0,16,23,16,21,20,0,1,1,1,22,0,0,25,1,1,25,24,4,1,18,1,1,0,0,17,15,14,1,1,0,0,24,0,16,1,10,14,25,22,0,19,10,1,0,0,22,24,0,0,0,1,1,4,17,25,4,16,28,1,0,0,1,1,15,1,0,0,24,0,24,1,0,0,0,0,0,0,1,24,0,2,1,0,1,16,1,24,0,2,0,1,0,14,1,1,1,26,1,0,2,0,0,2,24,2,2,10,15,2,7,1,25,7,0,0,23,0,0,8,0,2,2,26,1,2,0,3,25,0,25,2,12,0,1,24,18,2,24,1,2,1,0,1,1,0,0,1,2,1,1,0,0,16,15,2,0,0,1,18,0,5,0,0,0,24,0,0,0,0,1,1,0,1,0,0,0,26,0,0,1,6,1,6,15,2,0,2,1,3,0,29,1,1,25,0,20,0,0,25,0,0,1,2,2,0,0,0,0,0,1,1,25,0,1,1,1,0,20,1,1,0,0,1,0,0,0,0,6,1,0,15,22,2,0,0,0,2,0,1,0,17,0,26,19,17,10,0,0,0,2,24,25,0,24,24,16,1,0,25,25,0,0,19,0,0,12,25,5,0,0,11,0,0,0,0,0,0,0,8,3,22,1,3,25,2,0,0,27,25,0,0,3,4,5,12,0,2,1,25,21,0,1,12,4,0,26,1,21,0,15,16,0,0,0,3,20,0,2,0,24,14,2,25,0,0,0,1,25,0,28,26,15,23,19,20,1,2,0,0,6,1,0,7,24,25,14,2,18,0,19,24,0,26,0,0,0,0,0,12,27,14,18,17,21,0,0,0,4,0,7,4,25,0,0,26,0,1,15,24,29,0,2,0,1,26,1,0,9,2,0,0,11,0,0,26,22,24,0,1,3,0,0,11,1,6,1,1,2,0,15,1,21,25,16,0,1,0,10,16,5,1,0,2,22,1,1,25,2,15,0,0,26,0,1,0,0,13,12,0,0,0,12,25,14,3,13,1,25,19,0,0,1,15,12,13,28,23,24,23,0,0,0,15,0,2,1,7,3,5,19,18,1,0,0,0,0,25,1,0,0,25,1,2,4,0,1,3,15,25,25,30,27,16,32,1,1,0,2,22,26,26,0,0,26,0,0,0,1,1,1,8,0,0,1,0,1,0,0,26,25,25,23,1,0,0,24,19,33,5,0,0,0,14,1,0,24,1,23,26,11,0,24,0,12,22,1,0,0,17,0,0,0,0,1,15,0,27,7,1,2,1,0,9,2,0,0,0,15,16,20,1,4,0,2,2,0,2,0,1,4,20,2,7,0,1,1,1,0,1,26,25,2,0,2,8,25,2,27,0,28,8,13,13,0,1,26,26,5,24,0,17,1,0,0,0,2,25,1,1,0,0,0,0,0,0,0,0,0,0,0,1,10,15,14,26,0,0,30,17,2,0,1,0,0,1,1,26,1,2,1,0,1,0,3,0,0,13,26,12,13,1,0,0,0,13,15,1,1,14,19,0,0,1,2,0,1,12,2,0,7,0,0,0,2,1,24,0,2,0,0,0,0,5,0,0,25,1,1,1,24,24,0,19,0,25,1,13,0,19,24,0,0,28,1,1,0,23,1,1,27,25,28,25,16,3,16,2,4,25,1,11,13,13,6,0,1,0,14,24,1,1,2,7,2,1,11,27,2,1,1,0,1,2,1,0,0,0,24,1,24,2,26,23,24,2,0,1,0,11,15,25,1,2,2,0,0,0,1,1,1,0,1,4,15,27,1,1,0,26,0,2,23,1,1,0,24,0,1,24,24,14,1,24,0,1,2,0,22,25,0,18,24,11,0,7,10,1,0,1,0,14,25,26,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,25,6,2,26,25,5,2,0,1,2,1,25,15,25,0,25,18,2,14,1,2,1,0,1,1,1,6,26,2,0,3,0,26,0,1,24,0,1,0,27,1,1,24,0,1,0,0,2,14,0,0,7,20,19,0,0,0,0,19,1,19,1,0,0,0,0,0,0,1,0,22,0,5,1,0,1,0,2,0,1,0,0,1,0,1,0,1,25,0,20,20,9,1,0,0,3,26,16,0,0,15,0,25,0,1,24,1,0,0,0,0,0,0,0,21,0,29,26,0,0,0,11,26,0,11,0,0,0,1,0,0,1,0,3,1,1,0,7,2,12,24,2,3,2,26,0,0,6,0,1,0,0,1,0,0,1,0,1,0,1,0,25,0,0,1,0,0,21,1,0,0,9,18,2,1,1,28,1,2,1,0,2,1,1,1,0,26,27,1,0,1,1,9,1,1,0,25,20,27,22,1,2,2,1,25,26,0,22,1,0,18,19,15,2,3,4,26,3,1,4,3,8,1,14,27,14,20,2,6,0,1,0,25,2,1,0,0,0,0,27,14,18,16,2,0,0,0,3,2,2,1,0,1,1,26,22,1,0,0,0,0,12,14,17,0,15,0,13,0,29,1,6,0,1,1,1,1,1,1,1,1,1,21,12,1,1,27,1,1,24,0,13,0,1,16,4,1,20,1,1,1,1,0,2,26,2,1,0,1,24,1,2,0,2,0,0,0,26,2,26,2,1,1,1,25,0,26,26,25,1,11,1,26,19,1,13,0,14,25,1,0,21,1,0,2,0,26,24,15,2,18,26,28,25,13,15,2,27,11,1,29,1,0,1,3,26,29,2,1,0,25,33,2,32,2,0,0,1,2,0,26,27,14,2,23,19,1,22,1,0,1,1,2,0,1,1,1,7,25,2,1,2,1,0,25,26,25,1,0,2,1,0,16,2,3,1,2,2,2,22,2,2,0,23,14,1,26,1,25,1,25,1,2,1,25,28,31,0,1,4,24,0,1,24,7,1,1,0,15,13,1,0,0,1,1,0,6,0,22,2,25,5,25,1,1,1,14,15,15,1,23,14,1,1,1,7,0,28,1,0,5,18,0,25,1,19,1,1,1,0,3,0,16,29,13,16,26,0,9,2,0,26,29,1,1,26,26,0,14,29,8,27,0,1,2,1,3,0,25,1,24,1,0,14,1,25,0,1,26,13,5,1,0,23,2,1,2,1,1,2,17,24,1,1,1,19,19,9,1,20,13,14,1,8,1,0,0,1,12,6,1,1,1,3,4,21,0,1,3,2,0,25,25,0,2,25,13,0,24,2,2,1,24,1,6,2,2,1,1,1,2,2,1,0,1,1,25,28,0,0,10,0,20,0,3,1,1,1,1,16,0,0,0,0,1,0,0,1,1,0,0,11,1,1,1,1,0,0,0,1,1,1,19,19,0,9,2,26,1,26,1,3,0,16,21,1,19,1,1,22,1,0,7,25,13,0,0,0,0,19,17,1,0,2,23,14,27,1,4,21,25,1,17,15,0,0,1,1,26,3,17,19,18,2,2,3,25,24,3,0,0,7,1,1,1,25,26,0,15,25,25,1,22,0,17,1,0,4,1,10,14,1,10,1,19,1,27,27,3,2,17,6,13,1,0,1,28,0,0,24,21,16,7,25,0,2,1,16,1,0,1,5,23,1,1,1,0,1,0,1,1,0,0,1,1,1,25,8,0,12,0,0,24,1,1,3,1,0,0,1,1,25,1,24,5,15,10,0,2,11,4,0,1,9,0,0,1,25,26,1,1,0,0,16,3,1,0,25,1,0,4,12,1,0,0,22,8,28,1,26,31,0,25,0,1,4,24,1,1,14,25,12,24,24,1,4,28,12,15,1,0,18,1,27,3,28,1,2,5,27,26,2,0,2,1,2,25,1,2,1,1,2,0,0,19,27,0,0,0,0,1,0,8,1,16,25,19,12,0,1,1,1,1,0,23,1,16,21,9,25,1,0,1,19,1,13,1,0,0,0,0,1,2,1,30,0,30,25,1,8,18,1,0,0,1,0,1,1,1,0,25,1,1,3,0,2,0,2,0,1,13,2,0,25,0,3,22,2,3,1,1,2,0,2,0,0,0,30,1,4,1,1,1,1,22,1,0,1,1,23,0,25,18,0,26,29,1,0,2,0,0,0,1,1,1,1,1,0,12,13,10,0,1,0,23,27,22,10,25,0,25,2,1,1,25,0,1,1,17,2,1,19,25,1,0,1,2,0,2,2,25,26,1,1,0,4,2,1,25,25,2,1,1,9,25,0,1,23,1,27,20,1,17,1],\"hoverinfo\":\"y\",\"type\":\"box\",\"fillcolor\":\"rgba(248,118,109,1)\",\"marker\":{\"opacity\":null,\"outliercolor\":\"rgba(0,0,0,1)\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,0,0,1)\"},\"size\":5.66929133858268},\"line\":{\"color\":\"rgba(51,51,51,1)\",\"width\":1.88976377952756},\"name\":\"LeBron James\",\"legendgroup\":\"LeBron James\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"x\":[2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2],\"y\":[0,24,0,20,0,16,23,2,0,21,23,5,18,19,14,0,13,25,19,25,11,21,0,25,0,19,16,25,25,20,0,25,25,0,0,0,11,14,17,0,25,11,26,0,0,0,7,4,6,0,0,25,0,0,16,0,0,0,17,18,0,0,23,22,24,0,8,0,19,0,17,17,20,27,0,0,8,18,0,12,7,0,25,3,0,0,22,0,0,18,25,5,14,16,19,0,18,20,16,23,0,0,0,0,15,0,0,14,0,0,19,15,25,16,21,5,0,14,14,14,24,27,16,0,0,0,12,24,9,25,0,0,0,0,23,19,20,17,16,25,18,0,0,24,0,22,25,18,0,0,20,0,15,0,13,19,19,24,21,0,19,25,0,16,0,22,0,3,18,6,18,11,10,8,22,20,0,0,25,21,25,0,24,19,24,0,0,21,23,0,22,17,0,0,19,21,0,25,16,0,18,19,0,14,18,17,25,15,0,20,21,0,15,0,0,8,21,16,9,1,6,1,18,3,0,19,0,15,0,0,0,0,23,18,0,19,0,0,21,18,17,0,20,17,16,25,0,0,26,9,0,26,16,9,0,19,0,0,15,8,11,25,20,0,25,16,8,7,0,21,17,0,26,21,16,13,17,17,15,3,0,17,15,15,0,9,17,4,0,0,6,0,7,22,11,17,0,0,10,25,0,18,15,0,0,18,21,16,0,19,17,12,15,15,0,17,0,20,19,22,0,15,24,12,16,4,4,16,21,9,18,0,17,25,19,0,16,0,8,16,0,0,17,7,0,0,24,24,0,4,8,18,5,14,0,5,0,15,18,0,0,2,1,1,2,0,18,2,18,1,19,0,0,0,8,0,0,14,7,11,13,20,10,8,9,13,21,0,0,21,22,0,14,0,19,0,0,14,0,10,0,0,0,0,0,0,5,15,0,18,13,18,10,0,21,0,17,0,2,16,0,1,11,14,25,15,18,10,3,19,0,3,9,17,17,20,19,0,18,3,0,0,17,0,0,0,19,19,0,0,0,0,24,17,0,0,14,0,13,0,20,17,0,13,0,16,0,0,0,0,22,11,7,14,0,18,0,17,25,7,18,9,17,0,20,17,12,0,5,0,8,0,25,6,1,18,0,18,14,18,5,0,16,6,19,26,11,19,13,15,18,0,0,0,7,6,19,17,6,4,13,18,22,18,0,12,16,0,18,18,19,18,14,11,0,25,30,0,0,20,17,18,0,0,21,26,19,0,13,0,17,0,12,7,11,1,18,20,1,16,17,1,21,0,0,7,17,16,0,0,2,0,8,0,22,20,19,0,9,0,0,2,18,20,17,17,26,13,0,13,18,14,18,0,7,0,0,14,0,15,0,17,12,0,0,14,0,0,0,0,17,0,0,0,6,18,17,18,19,24,0,11,23,0,9,13,13,0,7,17,17,15,4,22,9,17,25,2,0,19,14,18,0,25,17,14,18,7,15,12,20,16,8,15,0,23,13,0,18,8,0,0,19,0,14,18,0,0,25,0,0,0,15,8,0,16,25,24,0,17,17,15,0,0,24,0,0,0,19,19,25,7,0,16,17,17,0,16,25,0,0,20,24,23,16,24,7,0,18,0,0,1,17,0,14,25,25,24,0,0,6,13,0,13,0,20,0,17,0,25,18,0,26,16,0,22,0,0,25,18,0,19,14,24,0,24,0,24,0,0,24,24,16,0,0,0,0,25,13,0,22,24,11,25,17,23,14,10,11,17,18,14,0,0,19,0,0,25,23,21,22,2,26,2,0,2,15,5,12,5,2,14,14,17,0,25,16,15,14,0,15,24,0,15,3,0,0,25,19,0,0,24,16,15,0,18,0,24,0,0,15,17,0,21,27,19,27,16,0,0,19,14,24,16,0,20,0,21,24,1,12,2,20,20,1,2,0,1,16,17,0,0,3,17,13,0,25,22,5,0,0,4,5,19,24,0,11,16,22,24,0,25,18,0,16,16,16,25,20,16,0,0,3,18,19,17,15,17,25,21,16,11,0,0,0,13,16,0,0,25,0,15,21,0,14,0,21,20,0,0,23,0,0,15,25,0,15,14,0,24,20,15,0,0,15,0,15,15,25,9,24,12,16,0,17,0,0,7,0,17,25,15,16,24,25,1,20,0,0,1,19,3,0,0,0,0,25,15,16,0,13,0,23,0,0,10,0,19,25,15,16,12,0,0,25,0,0,15,0,0,6,17,0,19,7,0,0,8,0,10,8,0,20,19,16,0,0,0,9,17,26,25,6,5,0,0,17,0,0,25,0,15,15,24,25,0,4,0,7,10,24,6,0,20,18,15,15,0,0,8,0,9,0,15,7,16,13,8,17,23,24,11,25,17,0,0,0,0,25,15,0,0,0,0,15,0,21,18,0,22,17,24,0,0,0,0,15,15,0,0,25,15,24,14,0,24,24,18,8,0,0,24,15,26,0,16,18,18,19,19,2,0,0,3,15,19,2,14,16,20,0,0,0,16,0,0,18,25,0,22,24,18,19,19,9,25,18,12,19,19,0,11,11,17,16,6,24,24,0,0,18,15,0,0,0,19,24,22,0,15,0,0,13,0,21,16,18,6,0,0,0,19,16,7,0,8,0,0,16,0,16,20,3,25,16,0,13,0,17,0,0,0,22,0,25,20,15,2,19,1,28,4,24,20,7,17,15,0,19,17,12,19,17,3,5,22,0,15,24,11,18,0,16,0,16,23,3,0,14,0,0,0,24,18,0,5,16,26,0,0,16,0,0,0,0,15,15,13,23,23,0,0,25,0,24,5,14,16,0,4,12,18,25,25,0,0,0,18,0,5,0,0,24,0,20,15,20,0,17,17,8,13,16,0,13,26,8,0,0,17,19,0,5,0,0,17,0,13,7,15,24,13,22,11,0,11,0,20,22,0,7,9,8,24,0,15,24,17,0,0,0,0,16,0,13,12,20,18,0,25,0,0,15,26,7,4,20,19,17,0,0,14,2,0,18,12,9,1,0,14,0,24,0,0,12,4,0,0,7,24,20,13,8,19,5,0,14,19,21,0,24,20,14,0,19,0,0,0,12,0,7,10,0,0,0,0,26,9,0,26,25,9,0,0,0,0,16,24,12,13,0,0,17,25,0,21,22,4,0,6,0,7,0,25,0,16,17,0,15,15,0,9,0,13,15,9,17,13,0,0,24,24,5,0,24,22,24,0,11,12,18,17,8,0,0,12,14,14,7,19,25,13,0,14,24,0,24,0,17,0,19,13,0,0,0,0,0,0,23,0,18,13,0,19,11,7,24,24,14,8,0,25,18,0,12,24,0,0,13,16,18,26,23,0,0,9,0,0,24,0,9,10,15,14,24,0,0,16,19,0,13,0,11,0,0,24,20,14,16,18,26,12,25,29,7,9,8,21,0,20,0,25,11,24,0,10,7,0,13,0,0,20,0,0,0,19,11,11,17,4,0,0,0,17,1,1,22,9,25,25,1,1,12,21,1,17,0,0,0,13,0,0,0,5,18,0,24,13,0,15,12,15,0,26,5,0,14,14,17,0,25,10,16,14,0,20,25,0,11,10,24,0,0,17,0,22,17,0,13,23,13,15,0,24,19,20,15,0,0,0,24,0,6,21,7,0,15,0,16,24,0,0,0,16,0,12,8,15,19,24,15,26,27,24,0,14,0,6,0,0,24,5,14,24,0,24,15,5,0,20,18,0,20,25,11,17,24,0,24,0,0,0,6,16,15,12,0,18,14,0,0,5,0,0,0,25,0,18,0,13,0,17,14,10,24,19,24,0,0,15,24,0,17,24,0,0,11,11,0,10,8,0,15,0,15,14,0,0,14,14,0,0,18,24,0,4,23,0,18,0,20,0,12,23,17,17,23,18,15,15,0,18,7,24,0,0,6,25,18,1,28,4,0,14,27,11,19,0,0,0,16,24,17,0,9,18,20,23,19,9,0,0,24,24,6,10,7,0,23,0,0,15,0,16,25,24,16,25,17,0,0,21,11,0,24,0,9,24,0,17,0,0,0,0,24,0,24,12,12,24,19,13,15,1,12,16,0,24,0,6,0,0,18,6,16,0,16,24,11,24,0,0,14,17,0,0,12,0,0,16,0,24,14,0,0,22,10,25,4,25,11,8,18,10,20,0,12,0,0,16,14,24,0,15,13,0,0,0,11,13,16,0,0,0,12,4,0,24,24,9,0,0,15,7,24,10,0,24,10,19,14,0,18,11,10,19,24,15,10,22,8,19,0,0,5,0,8,10,0,17,0,15,0,22,1,8,19,13,25,1,16,24,0,0,0,10,13,5,0,6,0,0,10,8,12,0,0,19,8,0,16,24,0,15,0,0,12,12,24,0,0,0,0,0,24,16,24,0,0,5,26,26,11,0,18,0,15,19,20,24,17,18,1,24,0,19,19,20,14,10,19,10,6,0,16,0,17,15,15,23,24,11,17,0,18,0,19,22,13,0,25,24,21,20,24,25,0,24,24,28,0,0,5,8,19,0,11,0,25,0,16,0,0,0,17,0,0,25,6,18,12,0,1,25,15,25,26,2,5,2,0,20,11,24,24,20,0,24,7,24,2,14,5,16,24,0,17,0,0,24,19,23,24,3,18,1,13,8,0,17,1,15,0,0,19,16,27,19,0,18,1,17,24,24,24,1,24,4,11,17,2,2,20,2,15,9,19,1,24,19,24,2,15,18,1,2,0,1,9,14,26,25,19,2,15,0,26,15,1,17,1,1,1,17,1,22,24,17,8,24,0,16,3,11,18,16,9,17,1,6,24,13,14,7,14,1,22,24,11,8,25,21,11,1,2,17,7,14,5,1,23,17,16,0,15,1,1,1,13,1,13,1,0,24,16,24,15,16,1,0,1,19,3,3,3,25,12,20,10,17,15,24,14,19,19,0,27,13,1,17,1,3,13,0,0,24,0,17,18,1,1,18,19,12,24,22,2,2,1,2,12,18,2,11,1,26,1,27,17,25,26,12,10,25,2,1,3,19,24,1,4,1,0,0,0,14,24,0,16,14,16,15,24,0,25,16,1,0,0,0,17,14,16,16,23,2,0,10,17,24,0,11,13,12,11,23,0,2,0,19,7,25,19,20,17,3,21,1,18,1,24,22,17,5,8,9,2,10,15,24,24,15,24,6,18,24,15,0,0,24,0,17,0,10,24,11,15,6,17,0,24,17,1,24,0,25,19,1,12,15,25,19,16,1,28,26,25,24,9,17,14,0,12,0,12,21,22,0,1,1,15,16,24,1,17,20,18,1,18,0,20,24,0,12,24,0,9,0,7,7,16,19,1,8,0,13,24,22,4,16,10,28,19,25,3,25,0,19,26,1,16,20,14,0,18,24,18,1,19,17,0,24,15,22,18,2,2,19,13,22,9,0,18,1,19,1,20,7,0,9,16,19,24,9,24,16,24,24,13,25,14,1,5,17,16,24,10,17,8,24,29,0,16,25,17,0,16,18,18,0,0,24,16,1,19,0,1,22,22,22,2,24,21,18,0,14,9,23,24,24,19,22,24,1,1,23,20,8,16,11,26,15,17,25,13,15,17,4,20,20,1,3,5,19,1,20,2,24,10,24,12,1,15,1,0,23,0,0,24,15,15,16,2,21,19,21,18,24,2,3,18,18,7,16,21,0,7,2,13,0,15,14,25,18,25,9,0,25,0,7,0,16,13,2,22,7,18,17,0,18,0,25,10,1,18,19,18,1,25,2,20,25,20,12,13,17,16,20,1,5,11,7,3,8,0,24,3,17,0,21,17,24,10,2,23,24,24,15,1,24,21,10,20,0,17,25,27,25,1,25,0,18,24,7,16,16,2,0,26,20,13,26,17,17,0,5,0,1,24,11,19,14,18,23,2,15,24,25,2,1,24,1,25,24,2,26,3,25,2,2,11,20,13,21,18,18,24,9,13,8,0,14,24,10,13,24,0,16,1,1,15,19,19,11,6,2,24,24,17,18,24,16,24,27,1,24,15,0,24,24,19,0,1,24,1,18,0,26,0,0,18,24,21,0,1,25,12,26,20,17,26,16,17,21,11,22,24,0,25,24,16,1,12,2,17,9,7,2,6,15,20,19,9,1,21,16,25,13,0,13,17,19,24,1,0,17,24,7,6,0,14,1,8,24,0,24,0,7,17,8,26,6,1,2,11,15,14,11,22,21,2,7,14,2,21,23,21,13,1,24,1,17,0,18,0,15,24,1,5,1,9,13,24,1,17,17,19,25,0,16,15,11,2,9,25,17,1,25,4,20,11,11,7,22,10,15,16,16,2,9,25,24,0,0,15,27,16,24,1,25,0,9,24,10,18,11,25,0,0,25,15,0,0,8,16,24,21,0,7,9,2,4,15,0,12,0,0,13,16,25,19,0,9,18,8,25,12,19,24,0,16,13,16,7,14,0,17,18,1,18,24,9,2,13,15,18,1,24,25,10,16,18,18,5,0,24,11,1,10,16,0,17,17,16,0,0,16,24,25,0,5,15,0,24,0,12,3,20,13,18,27,11,5,2,2,15,1,2,19,2,0,0,13,14,13,12,8,14,0,0,14,8,8,18,1,17,1,15,26,19,18,6,0,13,1,7,11,27,1,20,9,26,11,3,19,26,26,2,10,1,2,26,3,2,2,10,16,25,1,1,13,2,1,10,8,19,0,18,3,1,0,1,13,24,17,7,14,15,2,14,11,6,7,11,18,24,0,18,3,0,0,11,18,0,16,22,2,17,0,5,28,1,7,0,25,0,12,5,1,11,0,17,2,11,2,17,16,14,24,25,19,18,1,5,1,2,24,1,21,1,5,0,1,0,2,11,2,26,10,1,2,24,2,1,1,0,2,13,13,22,13,1,2,15,24,24,14,10,25,27,8,13,3,0,0,2,7,17,2,24,24,1,24,0,0,2,24,10,0,24,0,1,9,18,24,16,24,0,24,10,4,18,14,16,0,23,9,1,0,24,15,13,0,0,6,3,1,2,19,24,24,5,26,6,1,25,24,2,2,25,1,2,24,25,18,24,2,20,2,27,1,18,10,23,13,10,0,1,0,23,11,8,0,16,8,7,24,1,8,16,24,0,13,1,13,2,5,1,0,20,0,16,0,24,24,1,7,24,6,24,1,24,24,14,2,1,24,18,17,0,0,9,18,1,17,24,16,15,6,1,24,1,24,19,14,24,2,24,25,0,22,0,1,22,24,22,0,7,6,8,1,17,23,19,16,3,18,0,15,16,1,25,1,8,17,12,16,24,16,22,14,14,24,1,14,23,0,14,24,0,24,24,20,0,19,1,24,20,16,0,23,22,1,9,4,4,8,25,1,2,15,8,13,16,17,4,12,16,18,2,24,1,24,10,23,0,9,16,24,7,15,19,1,1,22,17,0,11,17,24,2,0,2,1,12,9,11,1,5,6,25,15,6,25,26,3,3,24,3,24,3,24,18,25,0,24,0,16,15,22,1,24,0,18,4,24,0,1,16,3,16,2,3,0,18,12,24,1,25,24,1,24,0,8,1,23,6,0,20,0,8,1,18,19,1,0,7,20,2,6,17,11,1,0,0,26,14,9,15,15,4,25,26,18,17,19,2,25,25,11,2,12,15,25,1,24,2,24,1,24,11,5,24,5,0,24,0,24,23,24,6,0,1,0,24,0,18,6,16,0,15,25,2,2,0,16,8,0,0,24,8,0,1,22,10,4,1,0,1,0,0,25,1,1,7,1,20,1,8,1,2,16,17,17,24,3,11,1,3,19,24,14,0,8,1,24,14,2,17,0,7,7,18,11,0,4,13,4,17,6,14,7,24,1,1,17,1,1,2,17,12,4,15,0,24,7,11,0,25,25,25,13,0,6,24,24,17,0,24,17,24,0,11,10,1,24,0,25,2,0,10,0,26,17,25,19,14,1,0,2,16,0,25,10,24,9,4,19,5,24,25,7,1,1,12,24,14,18,1,24,2,12,1,12,6,13,0,7,6,0,0,10,16,1,23,0,10,26,2,23,1,2,12,24,1,1,17,2,3,8,0,24,1,16,14,24,8,9,0,1,1,21,24,15,10,12,13,0,8,15,0,24,0,17,1,1,15,2,21,0,1,1,24,22,7,1,6,0,8,13,15,1,1,0,1,14,2,25,20,6,11,13,0,26,1,25,17,25,14,13,14,0,19,18,19,28,2,0,18,13,11,24,14,11,14,20,24,11,1,1,14,15,1,0,24,1,1,1,0,24,25,24,0,9,14,25,19,26,11,2,24,26,1,13,16,1,12,0,16,0,8,16,16,16,0,14,6,13,0,12,0,24,24,12,18,0,8,3,25,15,7,0,24,23,15,6,14,1,0,17,11,1,8,19,25,1,24,4,0,25,1,1,23,0,13,24,0,9,5,12,8,1,11,16,24,1,25,25,24,17,1,1,1,24,8,9,0,17,2,24,0,13,0,19,3,5,12,8,3,3,0,17,0,18,24,18,23,19,1,0,24,5,7,1,0,0,24,7,3,25,3,26,25,2,25,12,11,24,14,12,1,18,25,12,17,26,25,2,5,15,26,10,26,10,26,25,25,2,13,14,2,26,27,1,20,13,17,19,17,1,1,9,24,14,0,11,6,1,18,2,17,18,25,1,16,25,1,1,18,10,10,14,11,9,4,13,0,26,1,20,12,28,26,0,8,1,24,25,22,0,0,3,1,0,24,14,3,24,0,0,24,25,24,25,0,0,14,0,1,1,18,22,1,14,3,16,0,1,22,23,24,0,24,1,14,0,2,7,0,2,15,7,0,8,18,7,1,12,7,0,23,22,15,13,4,13,16,13,14,24,10,20,19,7,7,23,23,9,1,14,2,18,25,1,4,14,4,0,27,1,17,0,26,15,8,26,1,19,17,16,1,13,24,15,16,9,24,8,25,20,17,5,2,17,15,9,25,14,1,1,0,16,1,0,16,0,7,15,2,25,19,5,18,0,1,25,8,0,1,17,19,3,12,0,13,23,0,7,0,21,14,15,20,1,24,1,6,24,13,12,19,0,7,7,0,15,11,1,0,7,0,25,10,20,1,6,8,14,0,6,18,9,16,1,15,26,24,11,10,1,11,15,5,25,15,24,8,0,22,1,24,19,19,13,10,4,17,1,4,24,14,1,7,18,15,16,16,0,0,1,9,6,0,26,16,0,2,16,1,10,3,24,1,24,9,0,18,0,1,25,1,7,5,17,18,25,17,0,0,17,0,15,5,13,16,24,20,1,9,19,18,1,19,1,2,26,1,15,21,26,19,0,12,12,22,12,17,13,9,1,15,23,24,1,9,22,9,21,16,25,1,0,0,24,15,17,22,25,2,0,15,24,1,18,2,1,2,2,24,0,0,2,24,1,4,15,11,10,0,0,26,1,0,26,16,16,2,1,11,3,21,19,14,1,9,2,25,22,3,26,11,2,3,27,3,2,25,13,16,14,26,0,3,26,25,22,14,9,0,15,9,16,24,6,24,9,1,10,25,13,24,6,2,4,24,0,1,1,1,25,3,5,19,15,1,24,0,13,0,0,3,13,0,13,17,16,11,0,15,16,1,8,1,22,0,18,0,1,18,1,17,25,23,19,15,1,1,25,3,2,2,11,1,0,25,1,0,0,17,13,16,3,21,1,10,4,1,12,25,15,25,2,0,14,1,24,10,9,1,24,0,1,0,20,2,0,15,0,19,19,0,1,17,23,7,0,24,0,0,0,0,0,1,16,10,24,1,24,19,16,1,22,12,14,4,9,24,24,1,16,16,14,0,5,2,1,0,24,6,24,2,9,9,17,24,0,24,1,14,25,6,15,14,1,23,10,9,14,14,10,24,0,25,0,7,16,24,25,21,13,13,1,24,24,2,24,2,23,14,1,1,17,6,2,2,0,1,19,25,25,20,1,11,0,3,1,0,19,24,23,11,24,1,25,9,5,24,10,1,5,13,19,23,9,4,0,5,16,24,24,24,1,6,1,17,14,12,0,10,13,12,0,18,0,25,24,24,24,18,1,4,1,0,8,17,19,0,24,0,6,19,14,3,7,6,24,24,2,0,0,2,17,18,1,5,25,1,23,0,1,4,24,2,2,17,1,11,1,13,1,0,8,0,17,1,12,5,10,21,12,3,0,0,24,1,25,12,20,0,10,15,6,0,24,13,25,0,12,17,14,1,16,24,24,24,11,0,25,19,1,1,2,19,13,25,25,1,13,2,17,6,9,24,11,17,2,23,24,10,0,24,24,23,0,0,15,10,2,17,4,0,16,1,24,26,7,0,2,13,0,1,0,24,17,12,13,0,0,24,1,12,1,24,24,9,0,24,24,10,2,16,12,0,12,0,20,0,24,17,27,14,25,25,14,27,16,2,12,18,27,13,26,2,2,21,7,2,1,0,8,4,10,0,1,24,1,0,14,7,24,14,14,0,11,1,15,23,10,1,1,24,2,22,19,17,20,23,0,24,15,1,1,16,17,4,0,25,1,0,0,6,24,24,18,0,23,15,10,14,13,10,7,10,17,0,24,0,3,10,9,1,1,16,9,0,0,9,15,0,0,27,2,25,19,26,0,15,19,0,1,23,1,11,0,1,15,1,0,1,1,1,0,23,0,4,25,0,0,24,1,11,24,13,1,24,8,25,0,24,11,4,24,24,0,24,5,0,4,9,10,16,4,8,7,14,12,1,0,9,1,1,0,14,25,0,1,13,1,1,24,6,12,10,1,24,0,1,7,1,18,25,0,7,27,10,7,12,0,6,26,2,26,26,23,21,24,3,11,16,24,13,24,17,24,25,18,0,0,25,19,24,1,1,3,9,23,3,24,24,0,24,0,14,1,15,10,11,23,0,24,25,1,1,1,1,1,0,0,1,16,0,15,31,26,25,2,20,24,0,2,24,17,13,1,27,26,20,27,27,31,1,22,1,7,15,0,3,13,0,7,17,0,0,14,7,22,24,17,25,4,24,9,24,7,0,24,15,24,1,1,1,0,24,0,5,24,0,11,24,1,23,1,25,19,0,18,0,14,12,17,0,0,5,14,14,0,8,25,16,23,24,9,24,16,0,15,24,25,0,8,22,1,18,24,13,17,25,0,20,16,24,1,3,12,16,15,0,2,19,25,10,1,0,0,14,0,10,0,0,9,13,11,1,1,6,1,17,14,1,26,16,6,25,2,24,0,18,13,25,8,3,24,0,0,24,17,0,1,17,10,0,0,2,2,1,0,23,16,16,25,0,14,26,3,1,24,19,0,10,7,6,18,14,0,16,11,0,26,22,10,13,13,0,0,14,0,1,25,0,14,0,1,1,17,9,25,17,15,0,0,9,0,25,1,16,11,10,23,1,0,2,0,0,15,9,17,12,5,15,26,26,25,22,18,26,26,12,15,7,0,16,24,24,26,25,23,14,0,26,9,24,1,9,16,8,4,24,26,24,18,2,12,1,24,0,19,6,25,24,24,0,0,8,1,0,0,12,14,1,23,24,11,14,1,13,9,24,25,0,24,0,24,19,0,12,12,23,1,0,14,13,0,0,24,16,11,24,13,14,2,0,6,0,0,21,17,0,0,13,0,20,2,0,0,2,17,0,27,28,26,1,4,0,26,20,22,23,0,0,2,3,22,24,2,0,0,14,0,0,0,0,14,1,0,13,20,8,19,12,23,14,24,1,19,0,8,24,24,24,11,0,14,13,11,13,25,12,0,6,11,18,16,25,25,26,28,25,1,27,25,11,16,25,1,1,24,25,24,0,15,25,16,24,0,15,15,17,1,25,24,16,0,16,18,1,1,14,1,0,25,0,24,25,19,13,15,0,7,24,13,24,7,23,2,25,2,0,26,25,25,13,15,25,1,0,9,1,16,17,24,1,16,25,18,19,1,14,24,13,14,14,25,0,15,17,10,14,13,0,0,24,6,24,0,13,17,9,0,0,25,27,6,0,0,19,23,23,0,24,0,0,1,0,0,13,0,25,19,24,24,25,24,0,15,24,2,25,25,4,13,10,24,16,11,0,3,11,25,24,1,2,12,17,0,24,12,0,1,0,25,9,0,12,12,25,10,19,8,16,26,0,11,24,0,25,0,26,0,10,25,2,18,1,24,11,0,2,0,17,0,18,17,23,15,15,16,26,25,1,14,0,9,0,14,13,19,13,0,9,24,12,14,25,24,1,16,14,16,9,23,0,0,0,24,24,14,11,24,10,18,9,0,1,0,10,18,0,2,5,0,15,25,22,11,14,1,6,1,18,24,9,3,1,0,24,15,17,5,10,12,0,24,23,3,5,1,5,12,23,1,10,15,0,1,21,0,24,24,13,0,21,25,19,25,26,16,9,18,25,0,13,1,0,24,7,9,19,7,0,0,8,15,9,0,24,25,10,0,2,0,25,17,20,20,15,24,1,17,0,5,24,25,13,13,19,21,16,19,1,24,6,15,24,24,0,22,15,0,14,0,0,25,9,0,1,0,24,22,13,13,24,6,20,13,1,24,0,25,8,2,12,25,16,25,1,1,0,2,13,8,14,2,16,16,5,14,7,27,26,14,18,8,0,0,7,26,21,1,24,1,12,1,14,24,1,9,0,25,15,24,0,23,13,4,5,15,0,16,17,17,1,16,25,12,0,12,13,6,0,0,0,25,1,12,25,4,25,8,10,0,24,11,10,0,0,14,1,24,13,14,25,24,0,25,24,27,15,2,1,25,15,12,17,2,2,0,27,24,1,24,20,19,19,24,14,24,2,28,26,24,12,1,0,18,14,17,2,24,25,14,13,0,0,0,0,18,1,14,11,23,15,24,24,7,7,6,1,23,0,0,25,24,18,9,1,14,20,34,13,24,17,16,12,0,13,11,24,14,24,2,7,0,27,25,23,1,10,0,25,7,16,24,25,25,16,25,0,19,24,25,13,24,18,25,25,0,27,12,0,1,2,17,26,8,15,16,9,2,13,2,16,17,1,12,16,0,2,23,25,14,24,20,23,0,25,0,0,14,24,2,24,25,0,24,1,24,0,19,0,1,1,0,24,0,0,0,18,0,6,1,15,16,0,17,15,10,0,13,24,14,15,12,25,2,14,5,0,24,1,1,1,14,0,16,1,26,1,0,18,16,17,2,17,26,0,16,0,14,26,13,24,19,19,4,0,24,2,2,0,5,12,24,0,17,0,18,16,24,24,24,13,16,25,0,14,13,25,17,0,17,24,0,25,18,2,26,0,25,13,2,19,0,12,20,16,1,11,14,15,15,25,0,0,19,0,18,1,25,25,1,14,25,0,25,14,26,3,2,1,25,4,0,25,2,12,0,1,25,25,25,8,24,24,1,24,23,0,25,11,2,25,18,14,16,16,25,0,0,2,21,10,6,1,11,2,2,0,6,20,2,0,1,18,9,17,27,3,18,15,26,16,0,1,0,19,9,14,1,0,2,0,27,8,8,1,25,0,26,26,26,0,26,0,26,0,0,1,0,0,25,0,0,0,0,25,17,1,13,25,1,0,16,0,0,13,25,27,16,2,26,24,11,2,17,0,13,16,8,1,0,11,0,0,15,1,14,12,26,0,15,24,4,18,0,26,3,23,9,26,0,18,5,18,19,24,12,13,16,10,0,20,2,16,0,0,23,28,18,23,25,23,0,15,1,16,26,24,0,0,1,27,2,0,4,0,19,0,18,0,24,22,12,21,15,8,24,16,1,4,0,15,4,2,6,28,25,26,9,13,12,15,16,25,17,1,0,0,1,1,19,1,0,25,8,23,25,2,0,22,0,17,2,0,14,16,0,0,1,1,1,0,1,0,9,2,26,11,11,1,6,24,0,0,0,22,6,0,24,9,27,6,0,6,3,20,1,24,14,11,0,18,11,25,0,15,0,0,8,28,1,0,0,12,11,26,1,0,18,0,10,18,26,18,26,26,0,20,16,14,4,2,9,2,23,16,6,15,0,1,3,1,2,2,25,24,10,0,5,0,0,0,17,15,10,15,25,1,0,9,0,24,27,7,14,0,0,7,1,26,1,0,26,0,23,7,16,15,14,22,16,26,14,17,0,20,16,0,27,10,1,0,15,23,2,25,5,0,25,0,0,0,0,4,22,1,6,11,10,17,11,17,4,24,1,0,2,24,0,12,9,0,25,7,0,26,16,26,13,11,16,17,25,18,24,16,18,25,1,25,25,2,0,0,25,0,16,1,18,6,28,12,13,0,24,26,24,0,19,25,20,10,6,25,24,1,1,0,0,7,17,20,2,2,2,26,0,0,16,0,15,15,16,1,1,18,24,1,0,0,1,24,1,16,1,0,0,13,17,19,13,25,1,0,0,8,25,0,26,10,1,25,18,22,2,4,14,2,2,6,21,12,27,0,1,13,4,18,11,0,0,17,19,26,3,14,25,0,0,6,1,1,8,2,0,1,26,0,23,26,13,25,0,12,18,24,1,0,0,2,25,1,0,18,28,11,0,2,18,27,0,0,2,25,4,0,18,25,17,2,3,18,0,1,0,25,0,0,1,23,26,27,0,0,4,0,0,23,0,10,3,0,19,17,9,1,18,19,2,11,4,0,0,26,26,1,17,26,16,26,18,10,26,0,12,25,7,26,9,11,0,16,25,3,16,13,2,26,2,23,25,11,1,0,14,15,25,26,23,13,0,25,25,0,13,12,24,13,16,26,7,0,8,23,18,25,14,14,14,16,26,17,1,25,15,26,18,26,28,16,11,17,27,1,18,7,17,26,26,1,25,1,4,2,1,4,25,23,24,1,14,2,16,1,18,0,1,24,2,23,16,19,26,2,24,11,1,14,26,12,23,17,25,18,0,6,8,25,2,1,11,2,1,17,18,19,12,1,2,1,0,0,1,15,24,18,19,15,26,14,0,0,11,0,3,29,3,14,17,2,25,25,1,13,13,0,0,25,21,1,27,2,14,9,26,19,20,26,11,22,1,1,0,0,8,2,0,10,1,12,1,26,1,1,0,1,12,11,25,25,6,16,0,0,9,6,15,19,25,11,25,12,2,11,1,10,9,25,3,19,13,1,24,0,17,16,26,14,16,29,3,1,17,2,1,1,9,12,25,10,27,11,17,17,9,1,25,11,12,18,18,26,27,1,7,27,0,18,26,1,2,2,25,13,25,25,15,21,0,0,11,26,18,19,25,0,23,14,13,2,26,26,11,12,17,16,0,16,0,0,1,1,16,27,2,7,26,2,25,0,14,6,26,26,2,1,18,16,27,0,16,25,25,26,1,26,13,18,19,29,17,9,14,13,14,15,14,19,25,1,10,12,14,27,13,12,26,18,27,21,25,11,18,25,25,5,0,27,0,1,15,1,1,20,18,17,26,0,26,9,2,1,1,24,25,15,21,3,1,0,22,25,9,1,29,25,15,0,16,14,20,5,12,25,23,25,1,10,25,20,15,1,16,26,27,18,22,25,28,23,26,0,24,22,1,15,24,7,15,18,2,17,1,16,1,1,5,15,25,24,12,1,24,17,25,9,1,24,25,14,20,13,12,1,2,18,18,2,15,21,2,26,19,2,26,11,7,14,28,22,15,0,2,23,1,26,27,1,18,13,16,22,8,1,3,1,12,0,4,14,4,24,15,17,1,2,0,25,26,28,19,24,19,24,0,1,3,11,2,26,14,23,10,2,2,27,26,26,11,2,24,1,22,1,9,26,15,1,3,26,2,23,2,18,23,19,0,19,1,0,1,18,1,1,14,18,20,19,13,15,12,6,15,24,0,7,6,25,6,25,1,15,18,12,2,17,27,26,2,28,14,14,19,25,25,8,0,16,17,15,23,23,18,26,25,1,0,25,24,26,17,1,26,1,0,24,26,13,6,9,18,15,9,9,16,1,28,17,16,2,1,1,3,12,19,26,12,24,1,10,1,10,8,13,26,27,4,19,17,30,22,12,13,26,16,8,2,11,25,18,1,14,18,18,26,9,23,19,28,6,2,11,12,25,1,1,5,12,7,26,25,18,11,26,1,27,13,19,18,13,1,1,9,15],\"hoverinfo\":\"y\",\"type\":\"box\",\"fillcolor\":\"rgba(163,165,0,1)\",\"marker\":{\"opacity\":null,\"outliercolor\":\"rgba(0,0,0,1)\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,0,0,1)\"},\"size\":5.66929133858268},\"line\":{\"color\":\"rgba(51,51,51,1)\",\"width\":1.88976377952756},\"name\":\"Kevin Durant\",\"legendgroup\":\"Kevin Durant\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"x\":[3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3],\"y\":[14,0,0,26,0,0,5,0,0,0,0,0,0,25,24,24,12,20,0,0,15,0,0,24,15,0,21,0,25,0,0,0,0,0,0,0,0,0,0,0,0,0,14,19,16,0,10,0,9,0,17,24,0,10,0,0,13,11,0,1,18,2,0,0,20,0,5,0,0,0,0,0,0,0,12,0,0,25,15,0,0,0,9,0,0,16,0,0,0,15,0,0,0,0,0,0,0,0,0,8,24,0,15,5,11,0,0,0,0,1,0,0,0,0,0,16,9,16,19,10,14,7,22,0,14,0,24,18,1,5,0,0,0,13,19,0,0,0,0,17,2,4,0,13,23,19,24,25,5,0,0,5,9,0,0,0,0,23,0,3,0,0,8,0,25,0,12,0,6,11,12,0,0,17,17,0,17,24,23,0,0,17,0,0,0,14,10,0,0,9,15,19,0,0,0,7,0,17,0,0,0,13,0,0,7,0,0,17,2,25,0,17,21,19,25,23,23,2,10,26,0,19,0,24,0,17,7,12,14,0,17,13,0,0,20,0,0,0,20,0,0,0,3,0,0,3,6,0,16,5,0,2,6,12,8,10,15,12,0,0,0,0,9,25,17,0,26,0,0,0,22,23,0,0,18,0,0,16,18,0,0,24,24,25,17,15,2,2,1,1,1,1,19,7,20,8,3,0,0,14,17,11,19,0,16,0,24,0,5,0,22,0,0,19,0,5,0,11,0,0,24,15,12,21,0,0,0,9,13,19,17,0,18,0,18,13,18,0,25,16,22,0,21,20,20,21,7,5,9,18,24,0,21,0,16,19,11,0,14,18,16,16,0,7,0,0,13,22,17,13,18,11,14,0,24,19,0,13,10,0,0,6,0,0,0,0,10,0,16,0,8,11,1,13,11,22,1,2,10,17,0,4,0,1,21,2,10,0,21,12,0,0,8,0,16,12,19,19,12,0,0,16,0,0,23,16,19,0,0,24,18,0,0,0,0,0,15,16,17,0,19,8,20,18,0,0,8,0,14,0,14,0,0,0,0,11,12,0,23,0,0,0,13,22,19,16,0,0,0,25,9,0,17,24,0,24,24,24,0,0,0,2,0,24,1,14,8,3,0,22,4,16,16,14,21,0,11,14,24,22,26,4,15,0,0,24,23,0,0,11,24,0,11,0,0,0,20,4,24,0,0,0,25,0,0,0,17,17,0,0,19,0,6,0,19,6,24,0,11,14,18,7,12,24,0,25,10,0,19,0,10,0,23,0,0,0,0,0,17,15,15,7,0,12,17,0,0,6,24,24,0,0,0,0,0,0,0,15,6,18,18,17,0,13,0,18,18,0,0,6,19,17,18,0,0,0,0,15,20,0,18,0,18,0,19,4,3,5,1,25,16,11,12,0,0,12,13,0,0,8,17,0,0,0,0,0,0,0,23,20,0,5,21,0,0,0,0,0,11,0,0,0,12,19,0,0,0,17,15,0,11,16,3,0,0,0,0,5,10,0,5,15,0,17,13,19,23,19,18,15,12,17,0,18,0,0,16,0,0,18,0,0,15,1,1,1,1,12,9,0,19,0,0,0,16,0,17,0,6,0,7,0,0,7,13,0,0,0,18,0,0,0,6,0,0,0,0,0,0,17,17,2,17,0,2,0,2,13,0,20,16,0,0,0,0,0,15,7,0,0,0,0,0,0,18,0,0,13,9,0,8,11,0,6,5,0,0,17,0,0,17,5,0,16,13,0,0,19,15,14,16,4,15,17,0,6,0,0,0,19,19,0,0,0,0,0,0,0,0,0,27,13,0,11,0,19,16,0,11,0,0,0,18,20,19,0,0,6,0,13,0,16,17,0,0,7,15,9,0,0,11,0,8,19,0,19,0,15,0,19,19,0,10,18,16,19,17,0,10,10,5,1,17,13,0,15,0,0,17,16,5,0,0,15,9,12,16,13,9,16,0,0,0,15,16,0,17,3,16,1,14,5,17,0,1,0,0,0,0,0,5,16,0,0,6,11,0,0,0,0,20,23,19,0,0,9,0,14,0,0,0,0,0,0,4,17,0,1,1,1,20,0,6,0,0,0,0,4,0,0,11,2,1,1,9,2,16,0,16,0,0,0,0,0,10,1,1,1,12,1,21,3,13,24,18,1,3,1,1,16,1,14,19,19,11,14,1,1,15,18,3,7,16,2,14,10,2,1,16,18,2,0,15,1,2,1,0,1,1,20,13,24,2,4,0,10,1,14,3,18,20,18,19,2,1,19,4,8,1,0,0,4,0,15,0,19,17,24,12,25,19,0,0,19,2,9,0,0,18,1,1,17,15,12,1,2,0,15,1,0,1,15,2,3,18,9,1,3,1,16,0,9,0,0,12,10,8,0,10,7,1,1,0,0,24,10,18,1,24,2,17,11,0,1,15,16,1,8,15,18,18,2,3,18,1,1,8,1,2,6,2,7,2,4,4,1,12,0,19,2,10,6,1,3,17,0,16,5,14,9,3,11,0,2,9,3,1,24,6,25,13,0,18,0,24,2,16,16,12,1,1,9,20,3,0,0,18,0,14,2,16,5,14,0,2,1,1,0,5,1,1,7,1,11,24,2,8,2,20,14,2,2,1,0,1,0,0,0,2,1,11,16,16,17,1,1,11,18,0,0,0,1,2,1,17,11,10,13,1,13,2,19,16,5,2,1,2,18,2,3,24,10,3,0,1,17,0,1,17,3,1,17,19,2,6,18,16,1,14,2,7,8,2,14,6,12,16,7,13,16,2,0,2,11,0,2,3,12,2,3,1,18,14,13,17,11,15,15,11,18,18,1,0,1,0,0,0,0,16,2,2,8,10,16,1,2,0,3,11,1,14,3,1,0,0,0,16,1,0,0,3,17,12,2,1,0,0,10,13,0,2,0,16,24,1,16,4,3,25,19,10,2,3,2,19,24,24,1,24,18,24,12,0,2,1,15,0,2,0,2,21,3,2,18,2,15,1,16,0,27,28,2,0,0,16,16,0,0,1,15,19,6,1,16,21,16,6,2,19,17,1,7,15,2,7,14,1,17,0,1,1,12,24,0,1,0,1,1,10,20,3,1,1,3,11,20,6,0,19,0,26,1,1,1,0,17,0,11,24,15,17,9,4,8,2,0,26,1,18,1,1,18,1,0,1,0,1,4,1,26,2,1,17,0,6,0,1,24,0,0,2,0,0,1,1,2,1,1,0,16,1,2,1,0,17,2,2,3,2,26,1,16,16,1,0,0,2,2,1,1,1,17,19,14,23,12,2,1,16,3,13,1,24,1,7,0,0,0,17,1,0,2,16,24,2,11,24,24,0,24,3,1,1,1,0,1,0,6,1,6,0,0,0,18,1,2,2,14,2,18,20,12,1,13,0,13,1,5,17,14,0,11,17,3,16,0,0,0,17,26,2,25,2,12,1,26,2,2,16,0,7,1,11,14,2,3,10,18,0,2,12,1,15,2,0,0,24,13,1,0,1,4,2,1,1,6,1,14,8,1,3,2,15,4,0,16,16,8,0,1,0,3,0,18,2,1,1,0,0,9,0,2,1,1,16,15,2,24,0,1,0,1,0,4,11,1,15,1,26,3,1,18,14,11,12,18,18,6,24,0,1,5,19,18,10,0,2,7,11,19,0,0,19,1,19,15,16,2,16,1,2,1,6,17,1,0,24,24,1,14,2,13,7,12,7,23,25,25,0,3,7,14,2,1,16,6,4,18,1,5,0,1,17,22,16,6,15,1,2,2,19,0,0,13,9,11,0,1,0,2,6,21,1,1,14,0,1,6,2,0,18,1,16,1,1,24,2,8,3,2,1,1,3,2,8,6,2,16,2,17,1,24,1,2,5,2,0,15,19,25,1,15,1,21,0,24,16,0,15,0,5,0,0,24,0,18,1,0,0,0,0,0,20,19,0,1,19,19,16,1,17,15,2,26,12,19,2,2,2,2,25,18,19,16,5,2,15,1,10,18,3,25,17,1,1,17,13,19,19,19,0,11,1,0,6,0,18,2,3,2,2,25,13,0,1,21,0,16,15,1,14,0,1,1,11,13,2,0,0,20,2,20,13,24,24,1,14,15,25,16,15,18,2,14,2,17,1,18,13,24,24,0,1,0,24,1,18,15,4,3,5,17,16,24,24,20,17,11,1,15,2,1,2,1,1,12,2,1,5,10,16,14,3,2,17,12,15,25,26,13,26,16,19,17,20,18,2,22,16,14,17,25,8,1,2,0,2,15,0,24,0,24,24,24,0,1,1,11,2,2,1,23,2,8,7,11,7,15,9,18,10,1,1,19,14,20,17,4,21,19,17,0,24,26,0,1,18,1,12,24,0,18,17,16,10,1,8,10,15,13,7,25,1,12,24,15,0,10,8,2,1,15,14,19,24,11,25,24,19,24,16,8,1,12,0,1,13,1,2,11,0,1,0,7,24,6,24,5,4,1,0,1,1,17,18,0,24,0,11,19,0,0,0,19,0,4,18,14,9,12,18,17,18,15,1,2,1,17,18,12,19,27,0,2,2,17,2,17,18,15,12,0,9,19,24,0,15,1,3,12,7,11,8,2,1,18,2,6,16,26,4,15,1,9,25,0,25,15,2,2,1,4,18,0,0,25,13,25,17,22,0,13,16,18,4,24,0,1,1,17,0,2,13,1,17,2,16,10,14,19,3,24,1,1,16,25,15,1,14,25,2,1,20,10,11,13,6,0,0,15,16,2,2,15,2,1,6,17,2,17,14,0,14,16,25,17,1,14,16,0,19,16,0,19,2,1,4,2,11,2,15,1,9,1,1,11,17,2,16,1,17,19,0,14,24,25,2,1,24,23,0,18,2,1,2,24,2,1,1,1,24,0,19,1,17,1,1,7,24,31,24,24,0,0,0,0,0,0,0,12,0,17,0,2,17,1,24,2,24,0,9,3,3,24,10,16,1,1,1,17,1,24,17,19,23,7,1,0,8,24,23,9,24,1,24,15,7,0,0,1,1,1,1,18,1,24,0,2,17,2,1,1,26,2,0,1,0,13,0,10,13,7,1,24,0,1,19,24,22,9,2,3,24,16,9,0,27,2,2,2,18,6,17,24,18,18,23,24,0,9,26,1,17,17,23,24,1,17,19,24,16,12,13,13,17,0,1,16,24,1,16,20,24,1,14,24,5,1,18,0,24,13,16,12,19,10,0,1,18,0,0,26,0,0,5,1,0,0,0,11,13,23,16,14,1,3,1,16,1,24,25,1,18,1,0,2,0,21,9,2,2,14,19,24,8,1,6,8,17,1,9,1,0,25,1,16,15,2,14,24,24,2,1,2,25,1,1,24,12,1,1,2,1,16,1,24,13,18,0,26,23,19,1,1,24,4,24,17,8,19,1,26,16,1,2,2,0,8,1,1,2,24,18,18,1,14,8,13,15,9,14,2,15,0,25,1,0,5,15,19,14,9,15,3,16,18,16,1,1,1,16,1,16,1,1,2,18,0,12,21,1,0,1,2,0,1,18,1,26,16,10,13,13,2,27,2,17,26,0,17,14,2,16,14,15,7,1,25,21,26,1,3,1,8,19,24,17,17,1,9,9,9,17,0,1,12,0,15,0,18,0,0,1,10,1,17,1,1,19,13,1,1,1,9,0,2,1,4,2,0,1,1,8,1,23,1,0,0,1,4,25,0,0,30,1,0,0,24,24,13,18,2,2,13,1,1,16,17,15,0,0,13,19,3,2,26,1,2,25,26,25,17,1,17,1,1,1,24,2,0,13,0,1,23,13,16,0,1,16,0,1,14,1,0,1,1,15,17,0,1,22,23,1,1,0,19,0,35,2,0,13,2,0,1,13,2,0,13,1,11,18,0,1,16,11,2,0,1,0,9,11,1,1,1,15,2,14,12,1,19,1,10,0,24,19,1,1,10,0,14,12,26,18,24,18,1,2,1,1,2,13,24,16,0,1,16,0,1,9,9,0,1,0,1,1,25,13,1,16,1,24,24,19,14,0,0,1,0,21,0,0,11,17,19,16,1,1,16,19,1,0,1,1,2,17,21,14,13,0,19,20,2,7,1,2,18,1,0,2,14,1,1,8,12,16,0,2,0,13,17,15,1,22,2,16,0,23,0,1,18,15,9,24,0,14,12,0,12,8,7,25,25,10,2,18,9,7,0,0,0,1,19,1,18,16,13,18,8,1,1,1,3,17,0,25,17,0,13,1,27,1,0,2,0,2,24,24,0,20,1,2,27,1,13,19,1,23,11,15,1,0,14,19,17,24,1,0,24,1,24,5,19,1,2,5,2,0,14,1,1,14,1,15,0,0,1,15,0,2,0,18,0,26,0,13,23,14,24,24,1,25,3,18,26,2,2,14,2,21,1,27,21,9,21,15,12,16,18,28,16,3,0,1,0,10,15,25,17,14,1,19,1,24,2,1,15,24,24,1,4,17,0,16,1,1,1,0,18,19,13,13,1,2,25,1,22,10,24,15,24,0,25,24,15,20,25,1,2,18,1,21,25,6,19,24,0,1,2,1,2,27,26,2,13,25,13,17,3,9,6,12,24,1,1,10,13,15,1,1,12,24,0,20,25,25,2,0,0,14,24,0,17,0,1,14,1,0,12,18,1,1,0,2,19,2,0,1,1,1,24,17,1,16,17,10,0,0,0,1,18,2,25,22,1,6,16,1,15,25,10,19,18,1,1,17,1,17,24,0,21,14,2,2,24,12,0,1,1,22,1,25,16,17,0,24,25,1,25,2,24,19,14,9,13,4,18,24,25,4,1,15,14,24,1,1,23,1,29,2,1,0,0,1,0,17,1,1,24,14,1,24,25,25,27,18,0,1,15,3,33,5,0,1,2,24,2,0,24,4,17,0,1,24,24,24,25,12,25,25,2,15,3,1,14,27,24,24,1,0,0,20,18,2,9,0,16,14,0,0,0,11,1,0,0,1,18,1,1,17,25,1,14,0,0,0,1,16,4,1,15,9,0,1,1,0,27,25,1,2,17,0,18,18,0,1,1,19,16,1,17,2,1,1,20,2,1,15,0,20,15,1,1,24,26,0,27,1,2,16,2,0,1,2,2,17,1,1,0,18,24,1,11,2,0,0,23,23,14,17,2,25,1,2,16,11,1,2,25,1,19,2,17,1,18,1,14,0,1,26,0,0,19,6,0,18,0,26,0,1,14,18,0,0,0,19,0,2,17,17,1,0,19,0,5,0,1,0,14,0,17,1,0,25,24,1,1,19,9,0,1,8,16,1,24,13,1,0,0,16,8,25,1,1,13,0,16,0,1,18,26,1,12,25,1,2,26,18,18,17,17,1,1,0,16,1,15,1,16,14,4,0,0,5,1,0,0,0,0,13,1,3,20,1,1,1,24,2,23,1,13,16,0,13,12,1,1,0,1,22,17,16,7,2,0,11,1,0,14,7,12,0,24,14,1,18,4,12,13,8,13,0,6,14,0,19,14,24,19,3,3,15,0,2,9,17,0,15,2,18,2,1,25,0,0,14,0,24,0,1,15,11,0,0,13,0,24,0,0,16,0,18,12,13,24,0,1,0,0,19,10,2,12,0,10,0,0,0,1,7,0,17,7,1,5,10,6,0,5,14,11,14,0,8,12,0,19,0,1,24,1,6,26,7,27,14,13,17,22,1,24,14,1,14,1,15,0,17,1,16,15,16,1,16,26,2,2,24,1,16,0,17,15,17,0,25,1,1,0,18,24,0,14,19,16,1,1,5,1,14,25,1,0,7,11,14,24,1,15,1,2,14,24,15,0,18,25,0,1,22,1,0,0,0,6,18,9,16,6,11,0,4,24,1,1,2,11,0,25,14,15,2,24,14,13,24,0,1,15,25,2,2,1,0,7,0,1,1,15,1,12,0,1,1,6,2,1,1,1,24,24,0,15,10,9,1,15,1,1,8,24,1,1,14,14,1,25,16,16,2,1,2,15,1,17,0,1,12,1,6,17,16,2,1,1,1,2,9,3,19,1,0,25,17,0,1,17,14,16,2,5,2,16,18,1,6,10,24,1,26,1,21,1,22,3,25,18,21,1,1,13,0,1,23,2,18,23,25,29,1,25,12,0,18,17,1,32,1,1,24,14,0,25,1,2,24,24,15,1,15,5,15,2,1,2,1,24,1,2,20,1,13,17,0,0,2,24,1,0,23,1,24,24,1,2,2,24,16,0,13,0,1,25,18,1,1,0,1,24,2,0,24,2,1,1,2,2,8,14,24,2,19,2,25,16,1,0,11,25,1,0,24,18,0,0,14,18,18,2,15,4,18,0,1,24,22,1,10,1,14,1,17,0,2,17,1,11,0,0,0,0,0,24,25,24,28,17,0,25,0,0,1,14,2,16,18,18,18,11,18,1,24,13,9,26,1,15,0,12,0,13,1,1,1,16,1,15,27,25,12,0,11,26,2,28,22,1,24,1,1,8,2,17,12,15,10,0,1,25,1,1,18,0,1,25,1,18,5,12,11,25,1,14,2,1,1,24,0,1,15,23,7,1,9,1,1,1,0,2,2,1,37,16,12,2,17,1,17,2,1,0,9,0,0,25,11,1,18,24,7,26,4,1,1,24,14,23,1,13,16,24,19,25,1,5,20,2,9,1,1,18,9,0,18,9,4,1,0,0,14,25,12,27,1,0,28,0,0,2,9,0,9,24,22,24,24,2,0,1,15,1,0,1,25,27,0,15,25,0,10,25,25,10,5,1,1,10,0,1,2,19,1,14,2,0,18,1,7,12,2,22,14,5,1,13,0,24,1,23,13,1,2,14,14,24,2,0,12,18,2,1,20,9,1,14,16,13,2,0,0,0,0,18,0,0,0,1,2,25,1,1,7,1,0,14,25,18,17,2,15,14,12,0,19,0,12,1,3,1,13,1,13,1,0,25,3,7,7,1,24,24,24,18,0,9,12,14,24,7,10,20,1,24,24,12,0,8,18,1,15,15,3,5,15,2,6,2,0,16,1,16,0,1,16,17,1,12,0,25,14,1,17,0,15,18,11,10,0,2,23,0,18,13,14,25,11,12,0,1,0,13,0,0,2,0,1,2,8,1,0,1,24,16,1,13,0,7,1,24,1,1,25,8,1,0,0,9,11,1,1,0,1,1,2,1,0,2,1,0,24,1,17,18,0,0,0,15,13,0,17,0,23,24,1,2,24,15,1,0,1,1,1,1,0,1,12,12,1,0,1,1,14,8,25,1,0,4,16,24,0,23,14,1,15,0,3,0,27,0,0,24,14,0,0,0,1,0,1,2,0,0,25,25,0,0,1,18,6,2,0,0,0,0,16,0,9,1,1,3,3,2,7,1,2,19,22,0,1,1,1,1,2,1,8,2,25,1,15,0,18,1,10,1,0,2,0,1,1,2,12,2,17,1,25,0,24,25,0,0,0,1,24,18,0,12,12,9,3,4,2,1,14,13,0,14,13,11,1,0,1,0,10,2,0,25,5,2,1,25,6,15,2,6,24,0,7,25,12,16,20,1,1,24,0,2,1,24,0,0,11,24,15,12,14,12,2,14,1,0,0,1,1,25,0,11,16,10,1,15,11,16,26,24,16,2,12,20,9,2,2,2,16,25,19,25,1,2,28,0,1,11,1,14,4,0,2,0,1,17,3,24,24,14,2,12,7,26,1,18,1,25,2,1,0,18,26,11,16,2,1,2,22,3,1,12,12,0,1,14,0,14,1,24,23,24,1,0,0,0,0,24,4,1,8,0,1,12,0,13,1,0,1,8,1,2,0,25,27,25,25,0,0,1,0,2,25,0,25,12,25,30,22,5,12,0,0,14,0,15,14,0,24,3,1,0,12,16,10,12,7,16,18,0,0,0,0,28,2,0,7,0,2,4,2,1,11,15,0,5,0,0,26,27,1,25,15,1,0,25,24,18,19,25,17,1,0,1,9,1,25,5,2,25,5,0,18,19,4,0,25,2,1,14,2,1,14,2,1,15,13,18,16,0,25,16,17,12,0,0,14,15,0,15,24,0,0,24,12,1,13,1,2,0,15,24,0,22,0,24,2,24,0,25,14,18,3,13,2,1,0,2,1,7,0,10,1,0,16,13,4,26,25,9,14,25,0,24,1,24,24,0,15,0,13,12,23,15,1,11,24,24,26,0,0,1,1,19,24,13,0,25,19,25,0,1,0,1,2,0,1,12,0,12,16,18,0,16,19,0,6,0,0,0,14,1,15,0,0,13,0,24,15,1,10,1,1,22,0,2,13,1,1,12,11,12,0,0,14,16,14,1,1,1,17,1,14,14,11,24,1,1,27,14,12,2,2,1,14,1,0,15,3,27,23,13,23,24,14,1,2,25,2,1,3,17,17,25,1,25,2,27,2,12,1,1,10,1,1,1,1,1,0,0,24,27,10,1,0,13,4,1,11,1,0,11,1,1,24,1,16,18,15,0,11,0,18,2,2,0,24,2,24,1,19,13,24,24,25,17,18,16,0,1,0,1,0,1,1,2,10,15,3,14,14,0,0,1,0,24,2,11,25,1,0,0,25,2,0,1,3,24,1,0,24,25,2,1,1,16,19,13,1,3,0,24,13,4,1,14,16,1,1,19,13,16,25,17,24,18,24,18,15,11,15,2,2,1,25,1,1,17,16,15,19,26,0,0,24,26,1,9,23,12,16,5,0,26,27,17,1,0,12,0,14,25,15,11,17,1,16,1,1,1,18,1,3,18,13,16,1,1,26,1,26,0,21,14,1,2,8,0,10,27,17,0,15,0,28,0,1,16,1,17,17,1,25,0,0,15,1,24,20,14,17,24,24,24,17,14,1,2,24,26,26,24,2,25,12,1,24,26,24,15,24,25,24,28,21,9,1,8,26,21,1,15,13,13,13,12,1,16,0,2,2,25,15,17,1,0,18,0,15,1,22,26,24,24,2,0,1,24,0,1,7,17,24,1,24,20,19,0,20,0,1,0,21,18,11,14,14,24,25,4,27,20,19,26,26,2,1,27,1,27,0,16,24,13,15,2,11,1,1,1,15,16,24,21,22,25,1,2,1,25,13,0,1,18,16,26,22,25,17,2,0,1,0,24,1,24,1,0,1,1,24,26,1,24,0,25,24,0,15,1,1,13,15,0,20,1,0,10,16,12,14,19,16,24,16,13,13,25,16,15,23,25,26,3,24,24,14,0,15,1,1,16,8,16,16,17,0,20,0,1,16,0,1,0,7,26,23,0,24,0,11,17,15,17,12,0,0,18,24,0,0,1,13,1,23,24,0,1,25,0,14,1,15,2,14,12,1,18,14,25,25,13,0,0,0,25,9,24,26,1,24,28,1,0,14,0,24,3,12,26,0,25,23,25,25,26,2,1,13,0,13,18,1,1,2,24,25,1,24,1,0,0,1,0,23,2,6,0,1,16,24,4,15,14,23,22,13,24,13,0,13,16,2,12,5,14,1,5,1,0,15,1,1,18,0,18,17,0,14,25,24,24,24,3,12,0,14,20,0,26,0,4,2,18,26,1,25,16,27,17,27,27,15,24,23,23,1,0,0,15,14,24,1,0,0,26,17,1,1,3,0,9,20,9,18,1,1,0,25,1,1,0,18,24,30,10,12,1,19,1,24,0,2,18,13,0,24,7,9,15,1,0,2,19,25,17,2,25,24,0,18,25,2,2,2,24,2,30,2,2,2,13,17,24,24,16,17,0,15,25,2,1,14,1,24,1,23,26,0,24,1,26,24,24,0,1,24,25,19,8,0,1,19,25,25,25,24,25,15,26,25,17,1,24,24,2,2,19,15,18,16,6,0,13,25,3,24,3,24,26,2,1,2,35,23,13,1,1,1,1,1,0,1,20,25,0,0,25,1,25,1,0,26,1,25,1,24,24,13,14,1,1,0,25,1,0,1,0,15,1,1,1,1,25,0,1,2,12,17,2,2,1,13,18,0,1,1,11,26,1,26,15,0,24,1,24,7,15,12,24,8,16,24,0,24,2,2,1,19,18,25,2,2,2,2,19,26,1,0,1,0,1,21,0,0,1,0,1,24,24,25,24,0,0,0,24,1,17,1,11,1,1,18,1,18,20,25,25,19,17,0,0,0,1,0,25,19,1,0,0,15,16,0,0,24,5,1,18,1,1,1,2,26,1,18,7,25,1,22,24,2,26,1,26,27,27,26,2,26,0,23,0,14,11,0,0,3,15,22,1,19,1,7,13,1,25,0,14,15,25,1,1,24,14,12,19,13,0,0,12,17,19,1,18,24,24,15,1,25,0,0,0,0,20,0,18,0,0,0,2,1,2,0,24,1,17,0,27,16,13,1,27,0,25,19,21,1,19,20,1,2,20,11,16,14,0,15,17,17,12,2,1,0,17,1,0,0,19,25,10,2,18,1,1,17,11,18,20,19,0,1,1,0,1,21,16,11,1,0,16,0,0,0,17,26,14,19,13,1,17,16,16,17,25,25,0,16,12,0,15,1,0,17,1,20,1,1,21,15,20,0,0,1,0,0,15,11,2,14,14,0,0,1,16,16,0,0,2,2,1,20,1,25,0,26,0,19,14,0,0,13,0,0,9,19,1,21,10,17,19,20,2,0,1,2,2,1,1,2,2,2,0,1,4,1,0,29,18,27,16,0,19,1,8,15,0,7,1,1,1,1,10,16,25,3,17,17,13,13,0,24,20,1,1,17,1,19,1,14,16,1,1,0,25,9,1,0,13,14,1,17,0,20,1,1,16,12,1,1,16,1,1,0,1,1,1,21,22,1,18,16,0,1,1,1,17,0,1,1,25,1,19,14,0,18,16,0,0,17,13,14,17,1,0,7,2,16,1,11,1,11,0,0,20,0,0,1,19,19,0,0,25,13,1,1,7,16,11,1,25,25,22,3,2,34,5,13,1,13,0,1,1,1,2,0,0,1,24,14,20,3,2,24,2,11,3,3,24,11,22,1,15,11,2,0,0,20,25,18,14,0,0,1,0,0,0,2,16,0,13,18,1,1,16,14,18,2,3,1,26,12,25,2,17,2,18,9,1,18,2,1,19,2,19,1,0,20,0,0,2,1,13,25,28,2,18,2,1,11,16,16,4,19,6,10,6,18,11,2,20,2,0,24,0,0,5,0,1,1,0,12,0,0,0,0,25,0,13,1,18,15,1,0,1,3,2,21,25,0,14,16,9,13,15,1,0,11,16,6,0,8,1,0,1,19,20,0,12,9,12,1,0,1,8,1,10,0,1,0,0,1,0,1,25,17,1,24,13,1,0,0,1,17,1,1,0,1,15,1,1,16,0,1,1,1,20,1,22,1,0,1,8,10,1,20,9,12,29,0,1,1,1,16,2,2,0,26,1,0,0,24,16,14,0,0,18,25,21,1,25,0,11,25,0,1,0,21,17,10,2,2,0,1,2,1,18,17,1,25,7,0,22,16,25,26,1,13,19,0,0,1,1,2,0,15,20,16,1,24,1,11,24,1,0,0,10,15,13,25,0,18,0,2,0,1,0,7,21,15,0,25,0,0,19,24,1,24,0,6,15,1,0,8],\"hoverinfo\":\"y\",\"type\":\"box\",\"fillcolor\":\"rgba(0,191,125,1)\",\"marker\":{\"opacity\":null,\"outliercolor\":\"rgba(0,0,0,1)\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,0,0,1)\"},\"size\":5.66929133858268},\"line\":{\"color\":\"rgba(51,51,51,1)\",\"width\":1.88976377952756},\"name\":\"Russell Westbrook\",\"legendgroup\":\"Russell Westbrook\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"x\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"y\":[18,16,17,20,18,0,1,22,20,26,10,24,25,18,1,2,24,17,19,17,16,29,0,22,21,14,0,22,0,13,29,17,0,1,1,23,25,25,0,17,8,26,27,19,11,0,25,19,24,9,25,1,18,20,15,17,26,6,16,17,20,26,20,25,0,24,23,7,4,16,20,19,22,2,12,1,17,17,0,0,24,21,0,24,0,0,16,0,0,23,0,22,11,0,23,0,25,24,0,0,17,22,0,0,18,17,20,10,22,20,23,6,25,25,21,27,26,0,20,0,15,25,24,21,0,25,0,26,23,25,25,15,26,0,18,9,0,20,0,24,0,19,10,25,25,14,18,18,23,24,22,24,25,19,19,0,0,18,7,14,13,2,19,18,25,2,2,3,15,17,19,26,2,25,23,26,9,1,2,14,20,0,24,2,26,26,19,22,8,27,17,26,21,23,14,16,23,26,19,18,5,24,27,11,26,6,3,1,1,26,25,0,0,9,20,21,25,25,0,15,0,25,18,4,24,0,18,18,22,1,7,1,17,17,23,2,19,27,26,8,2,1,1,15,15,18,24,24,18,14,17,0,23,25,18,23,1,1,26,19,2,9,29,2,20,13,19,28,15,20,12,1,21,23,26,26,23,27,1,17,0,26,17,26,27,17,17,0,0,17,13,1,2,23,3,9,1,1,0,23,25,23,1,21,0,21,15,1,2,20,19,20,2,22,25,0,1,6,2,16,2,23,2,1,19,17,9,27,21,19,26,18,2,5,1,20,20,26,0,7,25,21,1,2,1,25,0,18,25,19,26,24,16,0,23,0,0,25,9,26,18,25,26,19,15,26,0,18,0,23,24,17,25,22,17,15,0,22,25,26,12,0,11,1,27,26,21,18,21,22,1,22,22,19,22,15,25,14,31,13,24,2,26,1,27,0,11,27,26,2,0,0,25,0,24,22,19,14,6,23,22,26,1,18,19,23,18,26,26,5,12,20,23,17,26,15,25,1,0,17,12,21,25,8,23,0,19,19,14,23,24,15,3,2,18,1,15,26,17,24,25,17,18,19,20,24,24,23,0,20,1,25,12,19,17,25,1,0,26,16,17,0,25,22,14,21,18,26,23,15,20,22,0,19,22,25,16,23,6,24,23,10,28,2,2,8,26,25,11,18,27,2,0,19,2,17,19,25,25,0,15,20,0,25,0,12,24,14,16,16,18,25,21,27,13,22,27,15,20,27,6,17,2,1,8,15,25,19,12,17,2,15,17,9,15,0,15,1,1,2,13,24,1,23,2,19,0,3,1,1,2,0,16,4,24,1,3,2,3,16,23,18,26,0,0,1,11,8,15,1,22,24,10,19,2,27,18,20,2,2,25,26,20,25,3,3,6,21,16,19,22,21,18,7,9,11,3,1,1,1,1,2,2,2,1,23,24,2,24,25,22,3,24,25,1,2,18,17,26,20,17,18,16,29,21,1,5,11,25,17,2,6,25,15,2,2,20,22,18,16,17,25,23,14,19,24,19,17,2,14,25,18,14,5,24,24,1,20,2,25,24,2,25,23,19,1,20,1,19,19,25,25,26,0,17,24,1,20,2,5,2,22,19,23,2,25,16,23,23,2,19,21,20,18,4,24,25,25,19,24,2,11,24,18,15,18,8,24,25,25,5,5,24,25,19,26,16,24,3,13,17,7,20,19,0,13,25,23,22,0,16,26,7,16,18,1,25,19,22,26,5,2,2,12,25,20,1,26,17,18,23,15,18,20,7,9,20,17,6,24,6,0,18,25,23,16,27,26,20,26,18,25,2,1,1,25,4,1,16,0,25,13,25,16,20,18,23,24,6,1,7,17,19,17,20,21,0,20,7,6,2,25,6,14,17,12,20,1,25,24,9,12,23,11,10,26,15,17,17,15,2,2,25,25,23,2,21,19,2,19,24,19,1,19,2,0,25,1,19,13,1,18,24,20,24,15,24,24,26,26,25,5,12,24,15,23,2,14,1,5,26,25,16,17,18,24,16,1,2,22,0,18,14,16,0,25,24,5,25,0,16,1,6,18,6,13,2,26,2,18,22,2,17,23,27,21,25,26,2,3,25,24,2,20,6,25,26,20,20,23,6,24,24,23,25,0,22,25,20,20,14,2,24,21,24,19,18,21,24,20,25,18,16,15,1,25,6,2,25,12,23,0,26,22,24,15,22,17,19,5,23,16,0,8,22,24,1,1,1,23,22,22,25,2,17,18,26,23,19,24,19,1,1,24,20,7,21,19,2,21,26,8,27,1,4,1,15,19,21,23,25,0,1,19,14,29,19,24,23,18,16,1,2,5,20,26,26,0,26,1,3,20,19,19,18,23,1,14,0,12,18,16,25,18,2,26,18,26,19,2,18,21,25,23,12,16,16,16,25,24,22,18,20,26,19,19,25,18,26,2,21,20,26,22,21,19,24,0,23,22,23,22,25,16,24,26,20,16,0,24,20,25,0,20,5,19,9,23,18,17,1,0,25,19,24,26,11,24,18,18,27,10,18,19,1,16,24,25,25,18,16,16,15,20,24,2,24,24,24,26,26,22,1,18,17,27,18,1,22,16,24,24,24,20,18,23,24,24,16,24,26,1,3,22,19,22,23,27,27,15,4,23,26,4,0,9,27,6,21,18,25,16,20,1,4,27,25,1,20,2,23,16,18,4,22,23,25,26,27,27,19,27,7,17,27,12,19,25,24,27,24,0,5,19,2,5,18,22,18,2,22,2,25,16,14,26,27,23,18,19,18,11,19,2,19,23,0,2,12,21,19,12,24,15,24,9,1,24,24,7,24,25,4,27,1,19,23,16,13,17,17,28,27,16,20,0,25,25,16,27,27,27,27,17,25,14,22,16,25,26,18,27,22,25,18,0,18,27,27,26,19,26,16,21,25,13,21,19,0,25,26,18,26,25,18,26,18,20,21,1,25,23,26,20,25,20,25,21,24,18,8,0,20,0,1,5,25,17,30,18,24,28,24,20,18,15,19,1,26,19,25,22,17,21,25,25,7,25,5,0,18,13,22,1,16,25,25,23,25,3,13,11,25,19,24,24,22,27,27,28,25,29,20,25,26,23,4,17,2,23,1,7,25,24,25,2,11,24,2,23,25,16,2,19,14,19,20,27,18,28,13,25,26,20,1,23,17,26,26,26,9,12,24,23,3,17,24,19,19,18,16,22,1,26,0,21,25,27,13,27,22,12,20,21,25,23,0,24,15,24,25,23,26,13,23,25,25,18,22,17,25,6,27,26,18,17,28,24,28,26,1,0,1,25,24,25,19,19,11,12,0,20,23,21,18,25,24,15,16,18,24,25,0,23,25,22,22,2,0,12,18,28,2,20,26,13,26,3,7,26,26,24,27,1,13,0,43,11,24,4,22,12,12,24,13,15,2,22,24,1,25,24,13,7,2,1,10,1,27,1,14,25,13,3,19,0,15,16,18,27,9,18,11,21,24,17,6,25,18,24,20,10,24,26,13,5,17,14,12,25,11,26,26,4,15,28,20,1,7,24,4,8,21,20,26,18,25,15,26,25,4,12,14,18,25,24,26,11,3,1,23,28,24,25,5,18,22,24,1,27,25,1,24,25,24,25,25,21,3,21,25,24,0,2,2,0,24,24,4,2,0,1,6,23,20,2,26,11,19,19,27,16,27,1,26,7,6,6,17,27,26,27,17,19,15,1,3,27,26,14,22,25,25,27,17,22,10,23,27,15,26,26,20,20,5,27,8,14,19,8,9,17,1,25,14,23,10,18,23,25,19,24,18,25,26,2,17,16,26,25,29,22,20,11,24,23,1,25,13,22,18,2,18,27,19,17,26,27,21,26,26,1,26,23,19,25,20,2,22,25,19,18,20,26,23,1,1,2,14,26,25,24,24,1,17,2,27,25,18,20,27,25,2,26,25,26,1,26,13,0,25,23,8,20,16,25,24,21,26,16,24,2,19,27,16,25,11,2,16,1,1,18,4,24,25,21,27,9,17,9,10,25,18,25,2,25,25,18,25,24,13,19,17,24,24,24,4,27,16,7,17,25,21,23,26,1,23,26,27,25,25,25,1,23,5,25,25,20,24,24,20,19,26,16,24,27,23,24,24,2,24,15,23,1,24,26,28,26,18,9,19,18,2,18,24,1,25,1,21,25,15,23,12,24,24,20,16,23,10,21,3,1,26,27,24,20,26,3,1,7,26,19,4,2,19,22,12,2,17,24,24,24,20,22,25,16,21,1,25,27,18,12,12,2,15,22,11,3,25,20,24,17,24,25,24,24,23,0,21,12,24,26,24,20,13,16,21,24,1,19,14,3,24,17,24,24,4,18,25,25,16,11,20,11,3,18,16,13,10,24,6,3,25,26,24,19,2,6,18,25,26,1,12,18,10,1,17,2,20,23,1,25,25,4,2,19,1,25,1,28,19,8,23,1,23,25,25,11,23,26,13,24,19,16,22,18,26,1,0,1,25,15,24,25,24,22,2,17,20,2,27,1,18,2,19,18,25,27,17,19,26,21,26,10,26,26,1,16,26,26,27,18,27,24,1,25,25,6,21,26,2,1,25,1,16,29,17,21,2,25,24,22,27,14,20,26,25,0,22,15,20,26,12,24,25,24,1,24,19,18,25,25,24,0,19,24,15,5,21,1,25,0,4,6,1,0,0,26,10,25,18,25,1,1,27,22,2,3,7,2,20,2,7,22,1,1,19,28,23,27,22,2,24,21,10,26,25,27,24,24,8,24,22,14,24,3,2,25,9,16,2,26,1,1,2,1,1,26,2,17,19,0,19,22,19,25,23,17,8,26,17,0,2,30,11,2,25,25,28,26,0,21,18,27,9,1,26,2,27,27,1,19,23,1,16,10,27,15,20,2,25,13,26,25,2,12,23,4,25,19,26,25,25,1,17,24,3,24,14,24,24,25,7,18,20,18,20,16,20,22,4,14,14,9,13,26,2,20,18,29,26,1,25,16,13,25,18,19,24,23,21,1,21,19,25,26,23,26,1,25,24,28,26,19,25,26,0,14,6,15,25,21,20,2,1,25,26,25,26,10,3,26,17,26,25,19,1,6,26,26,26,26,18,15,15,25,1,3,20,5,25,23,25,7,25,24,20,19,15,24,24,1,1,3,24,1,1,1,24,1,11,24,16,16,25,6,0,1,20,15,17,25,7,1,13,25,1,17,18,1,1,27,2,18,14,6,1,27,21,25,17,1,24,25,1,25,26,23,17,26,0,21,14,24,17,2,24,24,24,19,26,24,15,15,25,19,19,15,23,2,23,17,24,1,26,1,1,25,21,5,11,22,2,19,20,25,16,19,6,17,1,27,26,2,25,19,26,3,26,25,18,6,18,26,19,25,25,23,2,19,0,23,1,23,24,0,15,20,20,8,17,19,25,17,23,11,25,19,27,27,26,25,0,9,1,18,31,20,17,1,26,1,26,1,14,1,19,5,13,16,2,19,25,19,9,18,26,26,26,23,24,17,19,1,24,23,5,17,3,15,24,24,24,8,1,23,25,26,27,26,3,26,28,0,1,26,10,2,1,24,4,3,24,19,26,18,11,26,6,1,27,3,12,24,1,14,21,18,20,24,2,0,17,26,24,17,27,16,1,2,25,22,1,24,22,24,10,0,25,3,25,27,3,23,23,3,24,0,0,11,3,11,1,4,2,17,0,9,20,24,0,23,1,27,17,18,19,1,25,28,2,26,26,25,1,28,28,3,3,2,25,1,28,1,25,24,19,1,24,3,24,24,3,24,2,24,25,24,2,25,24,0,9,28,24,23,26,20,29,19,27,26,2,17,6,1,12,1,1,0,0,1,8,26,17,2,29,2,6,1,12,27,27,1,26,1,1,27,14,1,23,20,2,1,15,25,0,20,12,0,16,19,0,14,25,19,17,23,6,28,2,10,1,27,12,24,29,0,22,1,7,1,5,16,25,2,24,25,21,25,1,5,25,26,20,1,0,0,0,11,25,14,0,24,27,0,27,27,1,2,1,14,25,1,15,10,3,2,7,1,28,12,4,25,0,1,3,10,18,16,25,16,16,26,21,2,20,25,1,17,26,1,23,2,21,24,26,27,18,26,27,0,26,0,0,28,1,2,23,27,1,9,1,2,0,31,2,20,26,31,0,28,6,24,1,20,1,1,26,26,25,17,3,25,17,1,2,25,24,3,24,19,7,14,23,31,27,28,14,1,25,24,8,23,7,19,24,23,1,1,6,26,2,3,11,9,3,19,24,28,13,14,2,12,27,1,20,23,1,1,19,26,27,0,27,26,15,26,16,18,0,1,27,2,18,26,1,10,10,2,2,22,4,4,25,26,3,11,25,24,24,6,0,16,25,26,22,25,14,16,3,4,25,1,10,28,26,17,25,25,16,1,2,26,26,25,26,26,28,21,12,27,27,1,1,24,26,26,1,1,26,1,26,0,26,24,25,1,1,2,24,22,25,0,1,6,15,10,17,23,19,23,0,12,10,25,26,27,1,4,5,24,27,0,18,26,27,24,18,26,21,4,6,24,23,25,0,0,5,1,25,14,2,2,25,26,0,25,5,0,25,4,4,26,1,1,25,25,12,18,22,25,24,1,23,24,23,20,24,28,25,26,26,25,26,1,23,25,25,27,17,27,25,0,17,25,2,1,24,0,21,24,25,24,1,26,23,26,20,12,18,25,24,26,23,27,1,25,1,1,19,2,25,26,24,21,26,2,21,27,4,28,1,9,24,25,9,23,22,1,26,26,15,19,2,25,27,28,20,19,1,23,26,0,0,23,11,24,24,6,1,21,23,26,22,0,10,22,28,0,26,23,26,25,0,26,24,24,23,1,25,23,25,23,10,1,20,22,26,27,10,23,1,14,24,21,26,25,25,4,0,22,2,4,25,24,1,24,20,24,0,25,22,0,25,25,26,25,9,0,28,26,28,26,1,25,27,25,2,1,2,12,26,18,1,18,2,28,25,23,24,19,2,23,16,1,26,6,2,21,1,26,21,28,25,11,27,1,25,3,27,0,25,22,28,25,2,2,12,1,1,1,25,24,24,29,26,1,8,11,1,2,31,18,29,9,28,25,26,14,1,25,2,1,1,26,16,12,29,28,20,6,16,25,2,2,25,22,9,25,26,29,25,24,26,22,26,25,26,26,26,2,25,25,22,11,25,1,2,2,22,7,2,2,0,27,23,9,25,2,2,26,1,25,2,23,1,1,1,28,1,39,0,27,24,24,17,0,24,11,2,16,25,24,26,17,18,26,6,26,25,17,2,20,10,25,4,25,23,1,0,2,26,25,1,5,25,20,2,14,25,8,25,16,26,23,3,27,20,3,4,3,25,25,24,24,8,26,1,1,6,18,25,0,0,24,1,5,16,22,25,24,23,1,25,16,6,23,0,2,27,1,23,25,25,24,24,25,2,26,2,24,25,2,25,24,25,25,16,3,2,24,1,23,21,26,24,30,2,23,1,1,0,23,1,25,29,1,28,30,24,24,22,15,24,12,24,4,4,23,8,25,25,24,25,2,2,15,28,2,23,8,27,2,24,0,1,26,1,26,24,1,8,19,18,3,26,1,24,29,24,0,25,28,1,1,1,22,1,16,17,0,0,26,0,17,9,25,1,25,10,1,1,2,20,24,2,10,26,21,4,1,24,1,10,20,24,0,29,2,3,1,2,25,22,26,27,2,28,26,1,27,25,1,2,1,27,9,26,2,24,25,11,2,25,2,26,25,2,23,22,13,1,1,0,24,24,28,22,24,9,3,28,27,25,7,25,23,26,22,23,2,1,7,16,27,1,14,25,18,25,25,1,26,16,4,0,1,27,1,1,24,2,33,25,27,26,26,26,24,26,24,0,15,1,29,26,16,0,0,25,26,26,26,26,15,28,1,23,1,27,24,0,0,24,24,0,25,24,0,18,22,8,24,0,25,24,25,28,48,25,22,2,13,25,23,1,27,1,26,22,2,10,21,12,25,27,3,23,1,25,26,28,24,4,23,25,1,2,25,25,0,1,23,26,12,27,26,25,27,25,1,24,25,25,20,0,1,0,26,9,26,28,28,0,2,25,2,21,16,1,1,3,1,2,17,30,2,1,25,25,22,26,31,2,1,2,26,12,23,7,24,28,29,24,0,0,1,24,24,22,2,24,1,24,26,24,22,1,26,2,1,26,25,24,24,4,1,24,24,21,1,0,10,0,23,0,1,26,1,2,14,21,40,1,25,7,16,13,24,26,26,0,1,25,25,17,2,24,21,2,20,25,24,2,25,24,26,1,44,27,1,23,24,23,27,1,27,24,23,1,26,26,25,25,37,2,2,25,26,1,2,32,10,1,2,26,26,26,0,2,0,1,0,27,1,25,26,25,6,24,1,2,27,6,1,28,1,25,25,54,2,26,23,25,25,2,21,2,5,18,25,2,23,25,24,7,24,26,1,2,27,26,1,25,25,3,1,23,1,1,2,1,1,25,23,32,25,10,1,26,26,30,25,11,1,25,25,23,24,25,3,24,31,1,23,0,8,1,1,23,0,4,0,27,22,24,1,1,26,2,1,23,1,0,0,25,1,26,2,27,0,1,23,28,25,2,22,33,23,0,2,22,25,1,23,2,17,25,25,23,0,23,1,27,27,27,2,2,24,15,27,24,26,26,19,1,2,5,25,27,26,22,25,26,34,23,2,23,2,1,20,24,33,25,12,26,25,26,29,27,24,24,28,27,8,2,1,1,2,25,13,1,16,26,1,1,2,25,17,1,1,3,24,24,25,20,24,1,9,25,24,8,25,13,1,17,2,2,27,25,31,26,27,25,26,2,23,2,30,8,2,2,24,25,26,7,0,27,0,1,22,4,25,26,2,22,1,29,20,0,17,25,7,25,26,5,26,19,2,1,29,24,1,28,22,2,0,25,1,1,18,26,23,1,0,1,21,1,1,1,30,27,23,26,27,25,1,26,14,25,26,24,25,25,26,1,30,25,26,2,20,13,32,2,25,25,25,1,1,25,25,29,28,0,23,25,11,2,26,24,27,2,3,3,22,24,0,1,3,16,26,25,6,22,2,25,1,23,1,9,0,23,0,21,0,1,25,7,25,0,11,21,25,26,26,4,26,26,25,1,1,24,1,23,25,1,0,27,24,0,1,24,14,9,3,9,28,19,8,2,2,22,29,1,25,27,16,25,4,28,12,28,0,1,27,24,20,2,23,18,25,23,1,1,25,25,10,26,5,26,1,23,0,2,1,19,2,2,2,1,25,23,23,20,2,24,1,1,25,26,25,2,1,19,24,25,1,26,23,26,2,2,1,26,26,22,20,2,25,23,3,28,26,23,2,31,27,25,0,2,1,25,0,26,24,14,22,14,24,21,27,16,23,26,24,26,0,26,24,5,0,25,12,27,0,27,24,1,10,1,1,23,0,26,21,25,25,1,20,33,19,1,15,29,25,2,24,6,2,29,0,8,23,9,12,22,4,1,1,2,2,26,17,25,26,27,26,0,13,10,6,25,25,16,0,1,24,26,2,2,22,22,20,26,30,24,18,18,28,1,19,21,26,21,28,16,1,1,25,2,27,26,22,27,25,24,29,2,0,2,2,1,27,0,1,1,1,25,19,24,28,24,23,26,2,28,23,25,27,22,29,24,24,25,8,5,18,1,15,24,1,26,30,24,22,0,1,23,24,1,29,1,23,0,27,51,14,1,25,30,27,1,29,25,26,25,29,25,25,29,26,1,31,26,1,20,25,28,1,5,24,1,8,27,2,15,12,2,26,25,19,25,3,25,2,24,26,29,25,27,23,21,17,16,13,11,2,23,23,0,0,16,1,25,23,18,23,2,0,24,16,21,19,26,1,9,25,30,17,22,20,27,26,23,24,2,27,31,25,25,25,18,25,2,21,3,25,0,1,1,1,1,1,1,18,3,25,19,0,1,1,26,2,22,3,0,23,0,25,14,1,1,18,15,2,23,24,26,1,1,0,22,25,22,2,24,25,24,25,23,25,1,2,26,1,12,1,22,1,26,1,21,0,3,19,1,25,0,5,17,14,1,23,27,26,19,27,22,1,8,1,26,5,25,1,2,2,25,1,26,26,1,33,25,26,26,23,0,22,22,24,23,25,1,19,22,8,26,1,25,25,15,26,15,2,22,26,26,28,23,27,22,1,12,24,26,16,25,1,1,1,21,0,24,13,0,25,18,25,2,1,26,25,25,1,0,1,25,19,1,23,2,25,28,10,25,27,23,26,25,26,2,0,26,3,1,27,11,26,14,26,0,0,16,26,25,2,24,25,25,24,1,19,25,4,25,2,2,25,19,25,0,25,26,17,2,24,25,27,2,26,25,25,26,24,26,1,25,1,25,13,11,25,2,2,22,2,30,25,3,26,23,26,17,25,25,25,25,0,25,0,21,23,3,24,12,8,22,1,18,25,2,2,22,28,2,25,12,2,2,14,26,27,25,25,1,21,24,2,14,1,1,14,25,1,23,24,25,1,23,13,25,26,25,2,25,24,25,3,16,1,12,24,23,24,24,14,26,3,25,15,12,25,11,25,22,26,14,9,15,25,26,20,17,28,26,0,24,2,18,19,19,14,21,26,20,25,27,28,16,20,2,25,24,16,24,1,2,18,14,4,25,1,0,24,13,23,1,1,0,20,26,2,24,16,1,22,28,1,2,25,1,18,20,22,2,28,24,17,2,26,1,2,2,25,14,1,26,26,19,25,16,12,24,29,2,17,2,14,20,26,2,23,28,20,0,25,1,25,0,25,21,1,27,26,23,1,26,4,2,26,1,27,26,23,26,18,26,26,23,22,25,29,12,8,25,24,0,24,27,1,1,27,24,1,20,24,2,2,1,25,26,2,27,25,26,24,0,26,1,2,26,27,26,26,1,24,1,25,18,27,26,18,14,13,16,25,2,24,1,25,1,2,1,26,26,33,25,15,20,25,25,1,26,20,24,24,26,19,24,25,26,25,26,25,2,26,25,24,25,26,26,17,1,27,27,23,2,21,27,27,11,23,16,3,25,25,21,1,26,1,25,26,26,10,27,1,8,2,25,0,27,3,3,1,23,26,10,25,24,3,1,4,25,25,31,19,10,27,2,25,2,17,26,26,22,28,1,5,25,23,1,1,1,26,2,26,17,0,14,26,7,27,17,2,24,24,1,0,25,1,37,25,26,1,10,1,25,26,11,26,5,29,25,25,28,2,25,3,23,26,17,16,25,1,25,3,16,25,25,24,27,24,0,23,1,24,25,17,24,1,3,24,18,7,28,27,25,1,5,23,17,2,25,27,17,16,12,17,15,26,18,26,3,25],\"hoverinfo\":\"y\",\"type\":\"box\",\"fillcolor\":\"rgba(0,176,246,1)\",\"marker\":{\"opacity\":null,\"outliercolor\":\"rgba(0,0,0,1)\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,0,0,1)\"},\"size\":5.66929133858268},\"line\":{\"color\":\"rgba(51,51,51,1)\",\"width\":1.88976377952756},\"name\":\"Stephen Curry\",\"legendgroup\":\"Stephen Curry\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"x\":[5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5],\"y\":[0,0,0,0,0,0,26,0,0,0,17,0,0,0,0,5,0,0,1,1,3,0,0,24,1,19,0,25,0,0,0,24,25,17,25,0,24,11,0,6,4,0,0,22,16,0,13,0,25,0,25,0,0,0,0,12,16,3,24,24,19,0,15,0,0,0,25,17,0,0,0,0,4,0,0,20,0,17,0,23,20,0,0,0,0,18,0,0,0,25,0,25,0,0,0,0,25,0,0,24,0,0,0,0,0,0,22,3,19,25,25,0,0,6,16,25,0,22,0,25,0,24,15,0,0,0,0,0,0,0,0,24,0,0,0,26,0,0,18,0,0,0,0,0,12,14,24,20,0,0,0,17,4,0,0,22,0,19,19,2,18,0,17,0,0,19,24,18,16,0,5,0,0,11,0,0,0,0,0,0,24,24,25,0,6,18,21,24,0,26,0,24,0,0,0,0,0,17,24,26,24,0,22,0,0,0,14,0,24,17,19,0,17,0,0,0,0,0,0,14,0,26,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,17,0,22,14,4,0,0,0,0,0,0,18,21,0,18,0,0,0,0,10,0,16,6,0,0,25,18,17,21,0,0,0,0,0,0,0,0,12,20,0,0,0,0,15,15,15,19,0,0,0,21,17,19,16,0,0,0,0,24,0,0,0,6,16,0,0,0,25,17,0,0,0,0,0,0,0,24,0,21,22,0,21,24,0,0,21,21,19,0,0,20,0,0,18,0,0,0,24,20,0,0,24,0,8,0,0,12,0,16,0,0,24,0,0,19,0,22,0,20,17,25,0,15,0,0,21,18,0,0,14,0,12,0,0,22,14,20,19,0,0,0,0,0,0,0,0,0,0,25,0,0,0,0,0,0,0,0,0,0,24,0,6,0,0,0,0,0,0,0,0,0,0,24,0,0,24,17,19,20,0,0,0,0,0,0,3,1,16,25,0,13,0,1,20,1,0,22,0,25,19,0,24,0,0,25,21,0,20,0,0,18,0,21,18,25,18,0,0,0,17,0,0,0,0,0,0,0,21,18,0,18,0,21,17,14,17,0,19,18,0,15,0,0,19,0,18,19,0,18,14,18,16,0,25,0,20,0,0,17,24,0,0,0,0,0,19,0,1,0,24,23,1,1,1,0,0,20,17,0,20,17,0,18,25,20,19,0,18,6,18,0,0,0,22,0,13,19,0,0,19,22,14,19,0,0,0,5,22,20,24,20,24,0,0,21,21,21,21,17,24,0,17,17,18,25,0,7,0,0,0,20,22,15,0,0,19,0,26,24,18,0,17,9,0,18,18,0,0,0,0,0,16,25,19,0,18,14,9,0,0,20,17,20,21,0,0,17,19,17,0,0,16,0,0,0,17,0,20,0,24,2,15,0,0,21,0,17,7,20,0,16,19,0,17,25,17,19,19,0,19,0,0,0,0,18,25,18,15,5,15,0,25,0,0,0,0,0,25,0,18,0,24,22,24,6,0,0,18,0,0,0,19,0,18,0,0,18,0,17,18,16,0,17,16,0,25,16,0,18,21,24,0,0,0,0,0,19,24,0,17,0,13,0,0,0,0,0,15,15,7,2,8,21,0,7,16,0,20,0,0,14,0,16,0,18,0,16,0,8,15,17,17,0,0,17,0,0,0,0,0,0,25,19,17,17,1,1,0,0,0,17,14,0,5,21,18,12,17,0,0,18,0,0,0,15,17,0,26,25,0,0,0,18,7,24,16,17,2,24,24,18,16,20,20,16,25,0,0,14,18,5,0,16,25,24,9,13,4,0,0,4,0,16,0,0,25,0,0,24,17,17,1,0,0,25,0,0,0,0,25,19,0,0,0,18,0,16,5,17,0,0,0,0,0,0,0,1,22,24,19,2,1,0,0,5,0,16,0,0,0,25,0,21,19,0,0,0,16,16,19,13,18,26,27,16,0,0,0,10,0,15,0,0,15,0,15,1,14,16,1,2,14,0,20,16,0,14,0,24,20,22,24,0,17,19,0,24,0,0,20,16,0,24,0,0,18,0,0,0,0,18,17,0,20,17,18,16,1,0,0,0,20,19,0,0,0,0,13,0,0,0,0,0,14,0,0,0,0,0,0,0,0,9,0,25,0,0,0,0,0,0,4,0,19,0,23,18,16,14,17,0,0,14,0,0,0,0,0,0,0,0,16,0,16,0,0,0,0,0,0,0,0,0,0,15,16,0,21,0,0,3,23,1,13,0,0,0,17,0,24,0,0,0,0,0,24,0,0,0,0,24,0,0,0,0,0,0,16,0,0,16,0,24,0,0,24,0,0,0,0,24,0,0,0,24,0,0,21,20,0,19,22,21,0,20,0,0,0,0,0,0,20,0,0,0,0,0,0,18,0,16,4,17,18,0,0,14,22,17,19,23,2,21,0,0,0,0,3,0,18,0,10,0,21,0,0,20,18,0,0,0,0,0,0,0,19,19,0,18,0,0,0,0,0,0,0,0,0,0,0,0,0,19,0,0,0,0,0,0,0,0,0,19,0,0,0,7,0,21,22,22,0,17,0,17,20,0,0,19,0,19,0,16,15,0,19,0,16,0,18,18,0,0,0,1,1,1,1,1,1,13,7,0,0,0,21,16,4,0,0,0,0,24,0,17,18,0,0,15,0,17,22,0,0,0,0,0,0,18,4,0,18,0,0,0,0,17,16,0,17,0,10,0,0,21,19,0,15,18,19,0,21,7,18,0,0,0,24,0,0,18,0,0,0,0,0,0,5,0,16,0,16,0,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,17,0,0,23,18,0,0,0,0,0,17,21,19,24,0,25,15,0,19,0,17,0,19,0,18,16,7,16,16,19,0,0,17,0,0,0,0,0,0,0,0,0,0,19,20,0,23,17,8,0,0,0,0,0,0,0,0,0,0,0,16,0,6,0,0,0,0,18,24,0,21,0,18,16,1,19,0,0,25,0,10,0,0,0,0,0,0,3,0,0,0,0,0,0,0,0,0,0,0,0,24,15,19,16,25,7,0,24,0,0,20,2,21,2,17,1,2,3,2,19,0,0,0,0,0,0,0,17,0,20,0,16,25,0,0,0,18,18,0,23,0,0,21,0,0,16,0,21,0,0,18,19,0,0,0,0,0,0,0,0,0,0,0,18,15,20,0,17,21,6,0,0,16,21,0,0,0,16,0,18,22,0,25,17,25,0,15,17,25,15,0,21,14,18,0,0,0,0,0,18,23,5,18,0,0,0,5,17,18,0,13,0,8,0,24,23,0,0,0,0,0,0,0,0,0,18,15,0,0,0,0,0,0,0,0,0,20,17,0,0,0,0,19,0,19,0,16,0,0,13,0,0,5,0,0,0,17,0,18,2,17,25,0,17,0,0,17,18,0,20,0,17,0,0,0,0,0,17,0,21,0,17,0,0,17,0,16,0,0,0,19,3,17,0,0,0,18,0,19,17,14,24,0,3,5,0,12,22,0,0,20,0,21,17,22,21,17,0,0,17,0,0,20,0,0,2,2,2,2,21,2,3,2,1,2,25,1,0,20,0,18,18,16,18,0,0,20,0,17,18,0,0,18,17,0,0,0,19,0,0,0,0,0,0,0,20,18,0,19,18,5,0,0,0,0,19,0,0,16,19,0,0,0,0,0,20,0,12,0,22,25,0,0,25,22,16,22,21,0,0,0,0,23,0,0,0,0,18,5,0,0,0,0,0,22,13,0,25,17,0,18,26,25,17,16,10,19,18,0,9,16,0,0,0,0,17,16,0,17,0,14,0,19,0,0,20,0,0,20,0,18,21,0,0,0,0,0,0,8,0,19,18,0,11,18,0,20,18,6,21,0,18,1,1,21,18,1,0,15,1,0,2,17,0,20,21,0,0,11,16,0,17,13,12,18,0,20,1,10,20,2,12,12,15,1,1,18,2,18,17,0,0,15,15,18,10,19,18,19,16,0,0,0,10,22,16,0,0,0,0,17,0,14,19,24,0,21,21,20,0,0,15,20,0,17,18,18,20,19,0,0,16,0,16,0,13,0,0,0,19,15,27,8,20,19,24,19,19,20,17,0,23,0,15,0,16,0,22,0,0,17,24,7,8,24,0,0,15,0,19,16,0,18,0,24,25,11,0,0,0,0,19,0,18,24,0,19,0,0,0,18,0,19,24,0,0,17,17,15,19,19,18,0,0,0,0,0,0,10,0,20,20,0,20,0,0,21,17,0,15,0,0,20,0,0,17,18,17,7,19,20,0,0,17,0,13,0,0,17,0,8,0,20,0,0,0,0,0,0,7,0,0,19,0,0,0,20,26,16,0,0,0,0,4,6,0,0,20,0,0,0,0,0,0,6,0,16,0,0,17,20,19,0,0,0,0,0,0,18,14,18,0,0,0,0,0,14,17,0,0,18,0,14,0,0,15,0,0,17,17,0,12,0,0,0,16,22,0,0,0,0,0,0,0,24,25,25,0,0,0,0,24,16,14,18,0,1,2,20,15,2,12,0,0,18,0,3,14,0,0,0,0,0,0,0,13,0,19,15,0,15,0,0,17,0,0,0,0,17,18,0,0,15,0,0,0,0,0,18,0,0,17,0,0,0,0,0,8,0,6,0,0,0,0,16,22,0,17,24,0,0,28,5,12,0,13,24,0,24,0,0,19,0,5,16,0,18,0,0,24,0,0,0,0,0,0,0,8,0,0,0,0,0,25,0,0,18,16,0,0,19,0,10,16,19,21,18,0,0,0,0,20,0,0,18,17,0,27,0,1,18,0,1,0,0,1,13,15,0,0,1,1,16,6,0,1,17,1,2,19,4,5,20,19,27,0,17,0,10,0,0,15,15,0,0,0,4,13,0,20,20,0,0,6,16,0,0,0,0,8,19,9,0,5,7,0,0,0,0,0,0,0,0,0,0,0,0,4,0,14,7,0,0,0,0,0,0,18,0,13,24,19,0,0,18,14,18,0,0,17,20,0,0,18,0,0,0,18,15,17,17,0,0,24,18,0,0,10,0,24,0,19,0,0,5,0,21,17,0,24,0,0,18,16,4,15,0,0,0,24,0,25,0,7,14,19,14,0,0,6,0,0,7,0,16,19,0,17,4,19,16,11,10,20,16,2,18,17,0,0,14,24,16,0,0,17,12,18,16,17,0,18,20,17,17,0,0,0,0,15,0,17,0,16,0,16,18,11,20,0,20,0,0,1,0,0,7,1,16,0,14,0,17,15,0,12,0,0,0,0,24,8,16,0,0,15,16,0,15,0,17,0,0,14,0,0,0,0,19,0,0,16,18,15,0,28,0,5,0,0,0,0,8,0,0,15,22,25,12,16,0,0,7,0,0,0,0,6,24,0,18,16,0,0,0,0,0,16,15,0,0,0,18,0,6,0,7,5,18,0,19,0,0,0,0,0,0,0,0,0,0,17,4,0,0,0,0,17,17,16,0,0,0,0,0,17,19,18,0,18,0,0,18,17,0,20,18,17,0,0,16,24,7,23,0,12,0,12,0,0,0,19,20,20,11,0,23,9,0,0,0,0,0,0,3,18,0,1,0,20,0,0,0,0,19,0,20,16,0,0,0,0,0,0,0,0,0,0,0,5,0,27,0,17,0,0,15,16,0,16,0,15,0,0,20,18,22,0,19,18,15,0,17,0,0,2,0,0,0,0,0,17,0,0,0,0,24,0,18,0,0,16,4,0,5,15,0,16,0,18,5,18,9,0,24,0,0,24,17,0,0,0,4,0,0,0,0,13,24,6,20,0,19,23,0,0,18,15,0,0,16,0,0,25,24,0,0,0,24,25,25,0,25,19,0,0,0,19,24,0,0,0,0,0,15,17,24,18,26,24,0,10,0,25,0,25,14,0,15,18,0,10,0,0,0,21,19,21,0,0,7,0,15,17,17,0,0,0,20,25,22,18,16,1,20,1,22,0,15,19,24,19,17,18,24,19,2,18,18,25,1,2,2,25,15,15,8,0,17,0,0,19,0,15,0,21,17,21,19,18,12,19,0,0,0,25,0,12,8,25,0,19,17,0,17,0,0,0,9,16,14,16,16,13,24,0,26,24,19,19,18,20,18,0,17,0,12,0,19,5,0,4,25,15,16,0,0,0,0,18,14,11,14,15,25,0,20,19,18,0,18,18,15,0,14,0,0,0,20,0,18,5,0,0,0,18,0,0,21,18,0,0,24,19,0,25,0,0,15,18,16,11,0,0,0,0,0,0,17,0,4,18,0,0,19,0,0,14,0,0,23,4,0,0,0,25,12,0,0,0,13,25,1,0,1,0,18,0,0,6,25,25,0,17,8,18,17,0,16,0,15,20,22,0,0,0,0,0,0,19,21,9,16,20,0,0,0,6,5,0,10,0,14,15,3,20,0,11,0,7,0,0,0,0,18,0,21,0,0,0,6,0,12,3,3,1,15,16,1,2,0,17,8,18,0,6,0,0,15,0,21,0,9,0,15,0,17,0,0,0,0,16,0,19,0,0,0,0,0,16,18,0,17,0,17,0,0,0,0,0,0,0,0,0,0,17,0,0,0,8,0,0,24,24,0,20,17,0,0,0,0,0,0,12,0,0,10,0,0,14,0,0,0,0,0,0,0,19,0,18,18,0,0,20,16,0,0,7,0,0,0,0,8,17,0,0,0,19,0,20,20,18,0,20,5,0,15,0,15,22,14,0,14,0,0,17,0,0,18,24,0,0,0,0,0,0,0,0,6,24,26,0,16,17,18,10,0,0,16,18,26,21,0,0,19,0,0,0,13,24,15,0,25,16,2,0,19,24,15,14,15,13,0,16,16,20,0,0,0,8,9,15,17,24,25,0,10,0,0,10,0,25,22,0,0,0,17,22,21,24,12,19,0,17,0,0,23,0,0,0,0,7,16,27,16,0,0,18,24,11,0,17,0,0,10,0,2,0,0,0,20,0,0,0,13,16,0,0,0,0,0,15,0,0,19,16,18,6,13,11,13,0,0,0,0,0,0,0,17,19,0,0,16,17,0,11,19,14,17,0,0,0,0,0,5,24,0,2,16,1,17,17,2,2,3,0,0,0,0,0,0,0,0,17,0,0,0,0,0,0,0,18,0,0,0,0,0,0,0,0,18,0,16,0,25,0,24,0,15,23,0,25,5,27,0,0,0,0,5,0,11,15,19,19,0,19,0,0,8,0,0,0,16,20,17,17,24,18,15,18,0,0,0,17,27,19,18,20,9,0,18,16,0,0,0,0,0,0,0,0,0,18,18,20,0,0,0,14,18,19,10,18,0,14,0,19,0,18,0,17,16,6,0,0,0,0,0,0,0,3,16,0,25,19,0,0,0,25,0,24,0,0,19,0,24,0,0,15,21,24,0,0,12,5,22,14,20,18,25,16,0,0,13,0,15,0,15,18,19,0,14,16,2,2,18,17,0,18,19,19,16,26,0,24,0,17,0,23,17,14,19,11,0,0,0,23,0,25,19,17,21,25,23,26,9,20,2,17,2,19,21,1,1,2,15,0,4,17,0,0,18,0,0,0,19,19,0,0,0,0,0,0,0,0,16,0,16,20,25,19,13,20,24,0,1,0,1,0,0,20,22,24,12,23,0,0,7,0,0,19,0,0,23,23,12,19,25,18,0,0,8,14,14,0,0,0,24,0,19,0,19,0,0,0,0,14,0,0,0,0,20,8,25,0,18,0,0,16,0,16,7,0,18,16,18,21,0,20,17,0,26,24,0,0,18,0,20,0,0,0,5,12,8,0,7,17,5,0,0,7,0,0,24,0,25,0,15,23,11,22,25,0,0,24,0,18,4,0,0,14,0,26,25,0,18,0,24,0,0,0,23,16,24,25,0,10,0,26,0,25,0,25,23,0,23,14,10,0,20,0,0,0,12,0,0,24,17,15,12,5,19,12,26,19,7,10,0,0,18,0,18,16,0,0,18,0,0,24,0,0,19,23,17,17,0,18,0,17,18,0,16,0,20,0,10,5,0,0,0,4,19,0,0,18,0,0,21,13,3,25,18,18,19,0,18,0,0,15,12,12,24,0,0,16,15,0,0,0,19,15,1,17,15,25,0,1,1,1,6,19,0,19,24,20,0,16,0,0,19,0,0,14,16,14,16,0,0,0,10,0,0,8,20,0,20,18,17,17,25,10,19,16,0,0,0,18,0,11,0,0,0,16,9,0,0,17,0,25,25,18,17,20,22,27,17,0,16,0,0,13,15,0,18,18,26,1,1,19,20,26,18,0,20,7,0,0,0,0,0,6,20,0,0,19,3,24,3,2,0,0,16,17,18,0,16,17,16,0,7,17,0,0,24,25,17,0,12,0,0,26,17,8,0,21,0,0,0,0,14,12,0,5,6,18,20,16,0,19,0,0,9,0,9,10,6,24,21,0,0,0,27,25,0,0,0,0,0,0,0,0,16,8,0,14,24,24,25,13,0,18,16,0,25,15,16,18,15,0,18,15,0,0,6,11,8,0,19,0,17,20,23,5,0,24,0,0,16,0,0,0,17,21,26,0,6,25,26,18,20,16,19,0,23,0,0,0,19,0,0,0,0,0,18,0,0,0,0,0,0,0,0,18,0,0,17,0,17,0,8,18,10,19,25,6,17,0,0,19,14,13,22,15,0,7,16,19,18,17,0,0,21,0,15,0,0,0,0,0,21,0,0,0,0,15,20,21,12,22,0,0,23,0,0,0,0,10,24,16,3,18,0,0,0,11,14,15,20,0,25,0,0,0,17,5,17,0,13,17,0,21,14,25,26,17,9,1,1,0,0,0,0,24,0,19,24,0,18,0,0,0,19,15,4,0,14,12,0,0,21,16,20,2,3,24,0,18,24,0,16,17,19,14,0,9,14,15,0,15,0,5,0,0,0,0,0,0,0,0,20,0,17,0,11,0,11,0,0,17,0,0,16,16,0,0,19,0,0,20,18,19,0,0,0,0,25,21,0,0,0,16,15,0,0,0,11,0,18,25,0,0,0,10,0,0,0,0,0,0,0,0,15,24,17,16,0,0,5,0,25,15,0,0,15,0,0,12,19,18,21,0,0,18,25,24,0,0,20,16,8,18,0,0,25,0,0,18,0,0,0,13,0,17,0,5,0,0,24,18,25,4,14,0,13,5,13,15,13,9,15,14,1,12,16,0,15,0,19,0,9,0,0,0,0,0,0,11,0,0,0,11,0,0,20,25,0,0,0,15,0,0,0,25,11,0,0,8,0,25,19,14,19,0,18,18,0,19,0,25,7,16,14,19,15,0,17,0,8,16,17,0,0,0,0,0,0,25,11,21,0,13,18,19,0,0,0,0,0,0,0,24,0,20,0,12,17,0,22,0,20,17,6,18,18,0,18,24,0,0,0,18,0,0,22,4,14,0,0,0,0,19,0,17,0,21,22,0,16,6,0,27,3,0,9,7,18,0,0,16,25,17,18,24,25,0,19,0,0,25,0,13,0,25,8,15,15,0,6,0,0,0,0,0,0,0,0,16,1,18,15,16,1,2,19,24,0,18,18,0,24,0,0,0,15,0,18,23,0,19,25,25,0,25,5,16,0,19,25,0,8,25,0,0,8,1,12,1,0,0,24,0,15,1,17,15,20,12,18,20,1,18,2,1,18,18,14,0,7,0,24,0,1,10,4,19,2,3,5,2,2,17,20,0,0,0,0,0,0,7,4,19,13,16,0,0,17,13,22,20,0,0,0,21,23,20,18,17,20,0,0,0,21,22,18,16,25,19,0,0,0,10,16,12,19,0,16,14,0,9,0,16,17,0,0,6,19,18,21,19,0,0,19,16,0,13,0,18,16,0,16,0,0,20,21,0,17,0,0,0,0,20,14,0,0,12,0,20,12,17,18,0,24,0,0,0,21,0,0,24,22,22,24,11,21,0,13,0,18,0,0,0,0,0,0,6,0,0,0,0,19,0,20,0,0,17,0,15,11,6,18,0,0,0,16,0,0,15,7,15,7,0,9,20,0,0,0,15,0,21,20,22,14,20,0,0,21,0,2,1,1,24,16,17,2,15,17,2,2,8,1,24,13,17,18,0,12,27,0,19,0,13,0,0,15,17,17,17,24,22,16,17,0,0,0,19,21,15,6,0,0,0,0,0,6,0,18,0,0,0,0,12,22,0,0,17,4,11,18,25,24,20,17,22,0,0,24,0,16,0,22,24,0,25,0,23,0,0,0,6,0,0,8,20,0,15,0,5,6,23,13,0,0,0,2,1,3,19,15,2,20,24,16,1,16,18,2,4,18,17,19,24,15,24,0,13,0,20,14,1,17,2,16,14,25,25,20,15,15,16,1,21,1,3,26,1,21,4,0,2,1,16,16,1,0,18,17,1,2,0,17,19,22,1,1,23,14,2,1,24,25,25,15,13,19,17,2,2,2,26,22,2,2,16,18,18,0,15,24,21,18,3,17,18,2,1,18,2,2,3,10,18,5,13,0,1,14,0,2,1,3,2,13,19,1,16,1,17,21,1,8,13,2,2,19,1,1,0,2,18,5,2,6,26,2,0,2,26,2,22,2,18,16,18,15,0,2,19,2,23,18,20,20,1,25,25,1,1,1,1,17,0,19,18,17,1,0,14,18,15,1,17,18,18,0,1,20,1,13,18,16,1,14,0,1,18,2,6,2,1,1,9,1,3,1,18,12,0,18,19,0,0,25,11,2,1,1,0,1,1,2,21,1,0,16,16,2,20,18,2,1,2,20,14,1,19,1,20,7,18,1,20,13,18,21,2,17,1,3,15,4,3,2,2,2,1,3,3,0,18,15,17,5,0,0,0,19,2,21,18,25,17,17,12,25,0,18,0,18,18,20,24,0,2,0,2,0,19,2,2,0,1,0,1,1,0,17,17,3,18,2,19,0,17,4,0,0,13,1,1,0,2,1,1,1,18,17,0,22,18,1,25,26,27,15,14,26,26,23,24,1,16,15,2,21,16,9,0,0,0,5,0,0,0,0,17,1,17,1,18,1,25,26,13,2,23,16,17,19,24,17,17,18,2,19,1,22,1,1,2,16,2,0,19,15,17,0,19,2,1,25,15,19,12,1,24,0,1,2,6,15,1,13,18,13,1,16,19,4,24,1,3,1,25,0,24,18,0,25,1,15,16,1,2,0,1,7,1,2,13,2,16,24,0,26,1,25,1,1,25,12,11,23,0,1,0,18,0,0,20,19,23,8,1,0,18,2,17,16,0,16,0,10,16,1,25,1,18,16,0,16,0,4,1,2,0,0,1,0,12,24,0,7,9,1,0,25,15,0,0,25,1,19,16,2,21,9,2,2,2,25,3,2,2,1,1,1,1,4,26,0,15,1,2,1,19,2,19,23,18,0,18,11,2,4,2,2,1,2,18,0,18,20,16,1,19,24,1,11,24,24,13,19,24,6,13,23,23,20,21,21,0,1,26,17,1,3,18,20,3,19,24,15,2,8,9,24,1,16,15,2,2,18,26,0,15,23,1,26,0,1,0,1,2,23,1,25,17,0,23,19,1,0,17,16,0,11,0,26,14,23,1,25,15,20,2,15,20,1,1,23,2,25,1,0,16,17,2,17,1,15,1,1,0,18,0,18,17,0,14,16,26,8,21,25,25,1,24,2,1,2,1,24,1,1,1,1,5,24,13,10,2,2,0,18,20,20,14,20,24,25,22,23,25,29,21,11,2,1,21,1,16,26,21,25,24,25,0,19,24,25,3,11,9,2,1,1,27,14,26,0,3,20,26,25,25,16,25,25,19,17,1,25,20,14,17,21,18,6,2,25,17,18,24,24,18,25,18,15,16,17,26,1,3,25,2,0,2,1,2,14,15,6,16,17,25,24,22,1,2,2,1,2,14,18,25,2,2,1,20,11,1,12,0,3,20,24,1,1,25,22,0,0,21,0,0,17,1,19,1,17,14,20,24,16,25,23,6,5,0,17,16,25,2,18,1,2,24,12,20,1,14,0,15,10,18,25,18,1,8,2,12,1,25,2,1,11,8,0,2,13,19,1,1,2,25,2,25,22,20,14,13,17,2,0,24,14,0,11,19,2,0,1,22,25,13,18,1,15,0,6,0,23,15,1,24,1,0,11,19,2,1,11,1,16,0,15,17,10,4,13,22,24,17,16,10,17,1,14,2,26,2,15,12,1,1,1,0,0,1,0,24,2,1,18,24,18,0,16,1,1,2,1,18,0,12,0,23,1,0,0,1,2,1,1,1,1,15,0,0,24,24,13,16,19,24,19,1,2,20,17,18,19,26,0,1,18,21,0,17,24,1,17,4,18,1,2,2,2,16,25,13,1,23,25,1,24,0,1,1,0,1,26,2,2,1,1,10,18,0,24,20,0,16,1,10,0,16,1,3,1,1,1,13,0,1,1,1,13,6,14,13,1,1,1,2,18,12,1,2,1,22,24,16,21,17,0,20,1,0,1,0,0,12,19,28,19,0,0,24,2,0,10,17,16,21,17,3,2,16,4,9,1,2,4,18,11,3,26,18,1,22,25,17,26,19,19,14,24,11,24,11,0,0,2,13,10,7,1,2,26,0,25,21,25,19,0,18,3,16,1,1,1,15,16,0,16,17,0,0,25,14,0,26,13,0,13,1,16,2,3,1,13,2,2,0,13,0,21,1,19,21,9,1,24,3,1,0,24,12,1,1,17,18,25,14,2,1,24,1,18,1,15,25,25,2,17,24,25,7,1,25,8,8,17,18,25,1,0,20,16,1,1,20,18,2,25,23,25,1,1,22,1,1,14,12,11,5,14,12,2,1,2,25,1,26,18,18,2,25,18,24,23,15,25,25,0,15,1,15,1,24,18,2,2,2,15,17,15,16,1,25,5,1,1,17,16,16,1,1,1,0,15,23,17,22,1,1,20,19,0,1,0,15,0,2,14,25,15,1,22,0,25,14,19,1,0,0,20,16,24,17,7,1,1,1,1,16,2,1,24,24,3,10,7,2,17,16,13,2,25,1,14,16,2,24,23,24,25,24,1,25,0,8,0,24,25,1,9,24,24,18,1,25,20,24,17,0,22,4,28,26,14,26,0,0,16,0,14,16,18,23,2,24,1,15,0,24,1,10,14,23,24,10,25,1,1,25,25,11,24,15,16,2,19,16,25,25,13,14,25,1,16,3,19,1,16,23,20,24,26,2,25,1,1,25,20,1,13,25,24,25,20,16,0,2,17,0,5,24,10,1,23,24,10,11,25,1,0,24,13,24,17,2,1,9,1,9,4,2,1,9,0,25,24,0,2,10,0,24,1,17,15,14,15,21,19,27,27,15,27,1,2,19,11,24,24,25,16,1,15,26,1,13,9,14,24,1,24,25,0,17,17,0,0,16,8,19,1,25,23,17,26,2,1,14,25,12,1,13,15,26,19,24,6,24,24,1,6,1,1,8,0,26,24,1,1,3,1,25,25,14,20,19,4,15,20,25,23,19,25,27,15,26,0,15,0,20,18,18,1,16,1,0,16,16,13,14,25,0,20,0,0,13,1,2,11,25,1,20,17,2,2,16,2,20,1,1,14,24,17,24,24,25,24,17,24,24,25,24,22,21,14,1,1,24,1,14,16,19,1,24,24,19,0,1,24,24,2,9,18,24,19,16,23,3,24,24,1,1,15,14,13,25,27,16,0,2,15,26,0,26,2,17,17,20,0,17,24,1,23,0,8,0,7,19,24,2,1,6,18,11,21,24,12,24,25,5,25,13,1,13,1,1,24,3,13,3,1,23,14,1,2,3,4,26,14,21,15,16,2,1,3,15,23,1,1,1,19,1,1,0,25,18,10,17,19,24,9,16,12,15,0,16,25,12,1,0,25,24,16,18,1,12,18,1,1,1,0,3,1,2,23,1,1,18,24,1,1,10,24,2,24,1,1,26,2,14,25,11,13,16,17,10,17,7,12,9,15,1,0,11,13,22,10,1,0,0,25,24,19,1,0,14,1,3,20,18,1,1,10,0,15,11,23,0,11,9,10,1,18,16,0,2,1,24,16,13,23,0,1,20,18,18,15,24,24,16,14,25,20,17,25,16,15,24,23,20,14,16,22,19,12,0,3,25,16,17,0,20,13,7,16,18,19,19,17,1,18,0,23,1,2,2,18,26,9,2,13,22,24,0,14,20,19,24,14,16,0,10,1,18,24,21,1,24,1,23,1,1,1,24,24,0,13,13,2,1,1,19,18,25,21,2,18,17,19,10,3,2,18,0,15,1,1,2,2,16,15,25,1,12,0,1,25,16,0,0,16,11,18,21,24,17,1,0,1,24,19,13,11,2,1,1,4,25,1,2,17,10,25,14,12,1,1,1,16,24,15,17,23,10,18,6,28,10,21,16,2,2,24,15,24,0,12,1,16,14,0,1,15,1,18,3,7,7,2,14,0,25,16,25,20,10,18,1,25,15,11,16,0,16,0,1,2,1,24,24,1,0,2,20,11,9,1,1,2,2,25,2,1,2,17,26,0,17,1,15,0,17,2,1,4,2,0,1,10,1,6,9,25,16,1,2,13,19,27,19,2,17,16,14,10,0,19,2,1,25,16,0,14,9,21,17,1,17,17,0,1,0,1,20,2,17,1,2,1,1,16,0,9,8,10,1,2,16,19,25,25,22,14,1,1,0,16,19,13,16,0,1,12,2,0,13,17,25,15,15,16,0,0,25,15,13,24,24,19,0,0,2,0,15,25,10,15,17,1,13,17,12,25,25,2,3,2,25,1,24,0,1,3,1,1,19,18,19,24,2,17,22,18,0,1,1,1,1,14,1,18,20,9,24,21,17,0,27,1,26,25,18,25,19,23,1,10,17,20,8,0,25,25,13,25,25,19,17,25,19,19,11,17,24,10,14,2,21,24,20,1,9,20,23,24,9,3,19,2,21,10,16,16,0,21,24,17,5,22,0,1,24,5,23,22,24,0,17,9,23,2,0,17,0,1,10,18,24,24,1,24,18,1,15,15,17,21,0,15,15,24,27,17,16,1,24,7,0,12,1,6,15,1,15,17,15,11,15,19,24,1,16,18,26,0,26,24,0,26,0,15,0,1,25,1,24,18,16,17,24,10,16,25,18,24,25,24,25,15,26,24,13,17,18,2,15,17,19,18,15,20,15,41,15,1,10,25,26,2,25,9,18,13,14,19,17,24,15,25,22,16,15,1,13,1,23,24,16,13,2,12,15,24,25,24,13,0,0,0,2,24,18,1,19,18,2,25,13,14,1,25,24,14,16,16,0,16,15,25,0,25,25,2,8,1,13,25,18,0,10,14,2,15,18,13,14,0,25,16,14,24,17,22,24,1,17,1,14,8,24,9,23,24,12,17,20,1,24,17,24,24,17,19,2,2,19,11,18,23,14,1,13,25,1,2,14,18,25,17,15,19,1,21,1,11,14,25,25,22,18,22,1,12,24,21,26,17,20,19,1,25,25,16,16,26,15,4,25,16,16,1,26,24,16,19,20,24,4,24,1,15,18,24,24,11,25,1,24,25,19,14,18,1,24,24,2,2,3,16,23,17,1,1,1,25,20,12,14,25,25,12,16,1,16,12,23,25,17,13,7,1,0,19,26,25,1,16,0,24,15,21,1,16,25,23,13,16,1,12,13,17,22,0,18,25,24,14,12,11,17,18,19,24,19,23,25,21,28,25,14,14,12,9,17,2,2,18,6,19,24,24,24,17,18,19,14,15,26,15,19,19,11,18,20,0,17,15,26,15,13,4,1,13,17,15,13,15,8,2,14,16,18,25,15,23,24,15,12,24,18,9,25,24,19,13,19,24,11,0,16,11,1,16,1,17,25,22,21,25,25,24,27,18,11,14,17,14,15,7,18,24,17,25,16,13,21,16,16,9,15,7,1,20,1,24,1,24,20,24,25,16,2,16,1,1,18,0,19,16,19,20,11,16,17,24,1,18,17,8,19,10,25,0,13,11,16,14,17,14,10,11,19,24,19,8,1,16,25,15,17,0,18,22,19,0,25,16,15,25,13,1,1,21,13,26,8,9,1,15,22,18,18,2,25,21,12,2,12,16,10,0,9,17,24,1,2,17,25,4,0,21,18,11,2,26,14,19,15,2,1,2,19,0,1,15,12,23,19,8,8,25,17,9,16,19,24,9,1,2,21,13,1,12,0,1,18,16,1,1,0,1,25,14,6,19,3,1,22,19,14,16,24,25,0,17,15,25,19,14,16,0,0,21,24,25,0,24,24,17,0,16,24,23,24,10,22,24,9,5,17,1,20,1,2,20,19,1,1,0,16,20,25,2,11,14,21,19,0,13,1,1,23,19,22,0,0,21,10,0,21,16,17,13,2,6,12,26,23,1,2,21,6,11,14,25,18,1,25,21,1,14,24,14,16,15,15,17,0,16,10,13,15,1,24,16,25,24,17,22,21,0,24,1,23,1,25,1,1,1,20,10,13,16,16,14,19,3,14,26,3,10,15,11,24,6,20,13,1,15,20,1,2,14,13,19,2,1,11,17,22,1,10,0,20,19,1,18,1,17,4,3,14,6,18,2,17,16,2,17,23,25,17,2,19,1,1,0,1,2,16,16,1,26,0,17,21,17,16,0,26,14,24,25,1,25,18,1,5,0,17,1,25,26,1,18,0,13,25,4,0,24,1,24,7,2,16,0,2,2,2,24,21,0,14,15,1,18,25,2,13,26,16,8,11,22,1,8,7,0,1,16,14,25,11,15,25,19,19,6,24,12,1,16,12,2,1,17,2,0,11,19,2,23,24,24,17,25,9,18,18,8,1,1,27,3,1,1,2,1,15,4,12,2,1,1,26,14,23,17,17,20,24,30,17,20,20,23,14,14,11,25,17,3,24,16,3,3,20,24,11,2,1,24,18,0,8,18,0,25,14,14,17,2,25,14,20,26,16,20,20,2,1,11,13,25,18,15,24,16,11,16,24,24,6,24,13,21,24,16,25,0,25,18,2,0,7,15,13,15,2,16,2,12,2,1,15,15,24,1,23,4,9,16,14,0,9,13,17,1,17,25,1,12,16,9,3,18,12,17,20,15,24,8,26,1,25,21,15,2,0,25,1,22,6,10,1,23,25,24,2,24,24,1,16,14,26,1,8,15,0,25,1,2,9,8,24,18,8,24,17,17,3,1,13,15,2,26,2,16,18,14,11,2,26,1,12,17,26,13,2,2,15,24,18,15,0,12,7,16,15,1,10,1,1,2,1,25,0,16,0,10,20,2,17,24,2,22,17,25,18,14,14,12,12,0,18,18,0,15,1,0,0,0,1,1,13,25,20,0,26,0,25,1,10,0,19,14,25,15,16,13,16,18,15,0,19,24,2,2,2,6,2,0,19,0,2,2,1,7,1,19,19,12,7,21,16,1,18,17,16,15,25,25,1,16,14,14,2,1,1,1,9,10,11,25,14,25,0,19,25,2,16,16,11,11,26,7,2,26,5,1,0,12,1,1,18,19,12,13,0,2,0,11,24,14,16,0,19,17,1,18,14,24,15,2,25,9,22,15,0,19,15,8,19,20,8,1,23,16,27,20,0,16,16,17,16,21,1,1,24,16,0,14,15,16,25,7,14,2,24,2,24,23,15,19,5,18,24,2,21,1,20,17,1,18,21,0,14,19,17,18,15,0,13,17,25,9,1,19,25,17,2,15,1,1,0,0,0,16,10,10,19,12,2,12,6,22,25,2,2,19,1,2,16,2,1,16,2,17,2,26,19,12,13,25,1,3,18,19,0,17,18,8,12,16,21,19,25,7,17,5,0,11,18,12,1,2,0,17,24,1,2,2,25,18,17,1,3,11,26,12,24,24,19,0,6,10,20,18,19,1,1,10,16,14,1,12,24,25,20,2,15,24,25,12,12,26,15,19,17,0,16,26,27,9,1,2,9,20,24,24,23,19,24,23,25,11,18,1,17,20,24,10,8,1,25,23,24,25,18,21,24,26,20,25,24,24,22,19,16,18,10,18,17,15,11,17,25,23,19,11,14,19,3,10,10,24,1,12,2,20,17,18,26,10,12,13,15,14,17,13,24,19,1,6,15,0,5,15,23,22,0,16,15,18,14,24,15,16,10,0,17,20,17,12,26,26,9,2,1,0,1,2,2,20,1,24,1,12,2,11,18,20,4,25,23,9,22,8,21,15,2,15,14,2,20,1,1,0,25,25,1,19,21,13,10,16,17,2,14,2,22,17,6,15,23,24,13,18,17,25,20,7,22,1,13,0,12,25,24,17,0,24,0,25,19,24,25,18,20,15,14,19,1,17,26,14,13,17,18,21,18,18,24,0,26,26,25,17,4,24,25,24,16,5,12,24,25,14,19,14,15,0,24,13,9,9,19,12,0,12,24,1,24,11,13,19,0,18,10,16,2,25,20,18,25,19,25,24,19,26,16,23,1,18,0,10,25,9,24,24,6,1,0,12,24,16,12,2,7,24,24,19,0,16,16,10,1,19,2,14,6,24,16,10,23,17,22,18,1,14,13,20,22,14,25,19,18,0,0,25,1,16,17,17,24,20,9,10,10,20,25,25,25,25,18,25,25,14,1,12,18,2,24,1,1,14,20,4,24,1,1,24,17,12,1,17,12,25,15,19,4,12,2,1,1,17,18,12,1,21,14,12,4,25,20,1,0,17,24,24,24,0,26,26,17,9,9,14,27,28,2,18,16,13,13,25,20,27,10,25,12,9,20,26,17,13,1,23,21,16,17,14,19,24,18,22,17,18,25,17,2,14,17,25,8,25,21,25,22,2,21,19,21,20,15,26,24,20,20,7,17,12,25,25,13,23,2,18,24,2,24,15,18,16,23,17,22,2,22,1,24,15,24,1,23,25,14,24,21,1,20,25,19,19,13,3,14,12,17,25,28,9,18,3,7,24,19,19,13,22,20,24,1,0,17,20,14,9,19,14,16,16,16,24,20,27,9,18,15,20,1,15,17,20,24,18,13,10,18,12,28,15,17,18,15,1,18,0,3,27,16,2,1,5,0,19,24,0,8,9,14,9,1,1,19,0,17,9,1,18,13,17,14,24,1,24,16,24,23,19,17,20,24,12,24,24,24,7,24,24,1,16,0,24,25,13,0,24,11,17,23,24,11,18,9,27,24,16,22,25,10,24,24,13,1,17,15,19,13,19,9,6,15,11,12,1,13,24,16,15,24,17,0,17,11,16,19,24,9,1,14,13,25,16,21,25,23,11,10,12,16,24,26,11,14,24,3,21,22,0,11,13,0,25,27,14,27,22,22,16,1,14,1,3,19,16,24,18,18,11,18,24,24,1,17,25,18,11,19,7,15,16,10,26,2,15,12,10,2,10,13,23,18,24,12,22,12,22,9,24,23,24,24,14,11,16,15,14,0,19,18,19,27,24,18,19,24,1,26,1,16,15,11,10,0,18,0,12,11,25,16,24,16,1,17,13,25,22,18,18,14,15,25,14,2,24,3,22,19,13,13,0,11,25,14,0,1,23,1,24,17,13,24,25,18,18,11,1,20,19,21,27,11,20,13,21,25,1,24,24,25,24,19,1,15,16,25,14,16,16,0,22,16,20,15,24,0,24,26,25,9,25,24,24,18,26,24,14,25,0,0,27,15,10,1,15,25,13,17,15,19,23,19,26,20,10,22,6,25,1,14,1,15,26,24,12,22,2,15,23,25,18,1,15,13,25,1,13,15,9,25,21,25,13,24,25,25,23,23,10,26,16,24,17,14,16,26,11,18,1,11,1,0,13,14,0,20,15,19,17,18,11,16,26,25,19,18,0,25,15,20,17,0,27,18,28,12,9,12,15,7,11,15,26,22,22,19,17,25,12,0,25,1,23,1,21,1,2,24,25,23,25,24,17,24,24,24,24,23,20,14,25,25,25,24,20,23,11,14,20,11,21,13,18,0,17,8,14,10,11,1,1,18,0,25,13,24,10,24,24,23,1,26,27,2,1,14,25,14,17,1,23,21,14,19,18,9,24,24,12,25,11,16,20,21,11,18,23,24,16,14,15,24,12,17,22,0,23,19,15,19,0,24,25,26,24,1,6,0,23,25,24,11,13,0,7,25,17,18,10,0,1,0,22,14,28,26,15,18,7,14,0,18,1,25,0,0,17,24,18,8,17,15,12,25,16,16,1,25,26,25,26,1,0,26,12,1,3,10,5,12,1,12,3,0,26,0,0,16,11,1,24,20,0,10,25,13,14,25,14,20,26,15,26,25,25,17,20,26,22,25,25,25,25,23,25,17,15,1,18,20,27,11,25,0,0,5,24,3,4,25,1,9,7,19,6,16,0,25,24,9,14,0,24,0,25,25,25,25,15,0,24,24,24,13,25,25,15,25,1,25,24,24,25,25,25,1,22,25,16,19,27,16,25,13,11,19,25,24,16,14,0,2,0,25,24,24,9,25,24,0,10,24,7,25,25,26,13,22,24,25,13,1,23,1,20,16,25,16,25,1,0,24,0,21,11,25,25,12,17,0],\"hoverinfo\":\"y\",\"type\":\"box\",\"fillcolor\":\"rgba(231,107,243,1)\",\"marker\":{\"opacity\":null,\"outliercolor\":\"rgba(0,0,0,1)\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,0,0,1)\"},\"size\":5.66929133858268},\"line\":{\"color\":\"rgba(51,51,51,1)\",\"width\":1.88976377952756},\"name\":\"Carmelo Anthony\",\"legendgroup\":\"Carmelo Anthony\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":28.1800844517574,\"r\":7.30593607305936,\"b\":42.1344223513008,\"l\":37.2602739726027},\"plot_bgcolor\":\"rgba(235,235,235,1)\",\"paper_bgcolor\":\"rgba(255,255,255,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[0.4,5.6],\"tickmode\":\"array\",\"ticktext\":[\"LeBron James\",\"Kevin Durant\",\"Russell Westbrook\",\"Stephen Curry\",\"Carmelo Anthony\"],\"tickvals\":[1,2,3,4,5],\"categoryorder\":\"array\",\"categoryarray\":[\"LeBron James\",\"Kevin Durant\",\"Russell Westbrook\",\"Stephen Curry\",\"Carmelo Anthony\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"y\",\"title\":\"player_name\",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-4.1,86.1],\"tickmode\":\"array\",\"ticktext\":[\"0\",\"20\",\"40\",\"60\",\"80\"],\"tickvals\":[0,20,40,60,80],\"categoryorder\":\"array\",\"categoryarray\":[\"0\",\"20\",\"40\",\"60\",\"80\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(255,255,255,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"x\",\"title\":\"shot_distance\",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":\"rgba(255,255,255,1)\",\"bordercolor\":\"transparent\",\"borderwidth\":1.88976377952756,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.689497716895}},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"modeBarButtonsToAdd\":[{\"name\":\"Collaborate\",\"icon\":{\"width\":1000,\"ascent\":500,\"descent\":-50,\"path\":\"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z\"},\"click\":\"function(gd) { \\n        // is this being viewed in RStudio?\\n        if (location.search == '?viewer_pane=1') {\\n          alert('To learn about plotly for collaboration, visit:\\\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\\n        } else {\\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\\n        }\\n      }\"}],\"cloud\":false},\"source\":\"A\",\"attrs\":{\"276428ab6b84\":{\"x\":{},\"y\":{},\"fill\":{},\"type\":\"box\"}},\"cur_data\":\"276428ab6b84\",\"visdat\":{\"276428ab6b84\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"base_url\":\"https://plot.ly\"},\"evals\":[\"config.modeBarButtonsToAdd.0.click\"],\"jsHooks\":[]}\nDit keer heb ik ook de gemiste schoten eruit gefilterd omdat ik benieuwd was hoeveel van de schoten van ver weg daadwerkelijk zijn gemaakt. Steph Curry plaatst gemiddeld de meeste afstandsschoten (blijkt een van zijn handelsmerken) - maar de uitschieters zijn van LeBron!\n\n\nIn 2007 maakte LeBron een schot van meer dan 25 meter in een wedstrijd tegen de Celtics, toen de zoemer het einde van het 3de kwartaal aangaf.\nNBA Schoten App\nIk verwijs naar wat er in de ‘shiny_nba’ map staat. De ui.R en server.R bestanden definiëren de eigenlijke Shiny app.\neenui.R file\neenserver.R file\neen R project dat shiny_nba.Rproj heet\nde nba_shots.RData data\neen helper.R file\nStart de app door de shiny_nba.Rproj te openen en vervolgens runApp() in je console te typen of het ui.R of server.R bestand te openen en te klikken op de Run App knop. Je zou een eenvoudige app moeten zien met alleen de titel “NBA Schotpogingen”, omdat de code voor widgets en plots zijn uitgezet.\nLayout van de zijkant\nDe app heeft een grijs vakje aan de linkerkant, genaamd de zijbalk, waar we widgets zullen plaatsen. De witte ruimte aan de rechterkant wordt het hoofdpaneel genoemd, en hier plaatsen we de figuren. Dit ontwerp heet sidebarLayout(). [Er zijn ook veel meer flexibele indelingen mogelijk] (https://shiny.rstudio.com/articles/layout-guide.html), maar die zullen we hier niet behandelen. Haal het commentaar uit de volgende regel in de ui.R om onze eerste widget toe te voegen, een uitklapmenu waarmee de gebruiker een basketbalspeler kan selecteren.\n\n\n\nWe willen ook een widget waarmee de gebruiker een bepaald speelseizoen voor een bepaalde speler kan selecteren. Maar dat vereist de mogelijke keuze van de seizoenen afhankelijk van de player_choice input Door een uiOutput() statement in ui.R en een renderUI() statement in server.R toe te voegen, kunnen we de UI opties aanpassen aan de input van de gebruiker. Verwijder het commentaar van de volgende code in je app.\n\n\n\nKrijg je ook een fout die er zo uitziet?\nERROR: Error sourcing /Users/juliawrobel/Downloads/shiny_nba/ui.R\nZorg ervoor dat u het , tussen de invoeroproepen in het ui.R bestand uitschakelt.\n Tot slot voegen we een radioknop-widget toe waarmee de gebruiker kan filteren op gemaakte of gemiste schoten:\n\n\n\nJe moet niet de volgende widget in jouw zijbalk zien:\n\nOefening: voeg een andere widget toe aan jouw UI.\nPlot van het speelveld\nLaten we de plot toevoegen die de ruimtelijke verdeling van de schoten op het veld laat zien. We hebben een plotOutput verklaring nodig in het ui.R bestand om Shiny te vertellen waar de plot moet verschijnen in de lay-out van de app, en een renderPlot verklaring in de server.R bestand dat de plot construeert.\n\n\n\nTip om fouten op te sporen : als de niet-Shiny-versie van je plot al niet werkt, zal je Shiny-versie ook niet werken! Zorg ervoor dat je je code test voordat je hem in het Shiny-raamwerk plaatst.\nAlles van de servercode dat verandert op basis van gebruikersinput, komt binnen de renderPlot verklaring te staan. We laten de plot veranderen op basis van de keuzes van de speler of het seizoen, die zijn opgeslagen in input$player_choice en input$season_choice.\nOefening: probeer de app zo te bewerken dat de plot van de schoten ook verandert op basis van de radio button input.\nPlotly en Shiny\nOm Plotly-plots toe te voegen aan Shiny apps moet je de functies plotlyOutput() en renderPlotly() gebruiken in plaats van plotOutput() en renderPlot(). Voeg de Plotly boxplot van de schietafstanden toe aan de shiny_nba app door de onderstaande code uit te schakelen. We staan de gebruiker toe om te filteren op het feit of er schoten zijn gemaakt of gemist door de input$shots_made UI input van de radioButtons widget te openen.\n\n\n\nZorg ervoor dat de , tussen de output calls in het ui.R bestand uit te schakelen. Er zijn geen komma’s nodig tussen de codeblokken in het serverbestand.\nDe Plotly Shiny gallery bevat nog veel meer voorbeelden van wat er mogelijk is als Plotly en Shiny samen gebruikt worden.\nHipper worden\nNu heb je een coole Shiny app! Ik heb een uitgebreide versie van de app shiny_nba app toegevoegd om meer dingen te laten zien die Shiny kan doen. Download het hier, open dan de shiny_nba_complete.Rproj en start de app. De app heeft een paar updates:\nReactieve uitdrukkingen in de server.R voor efficiëntere code\nTabbladindeling, met één plot op elk tabblad\nNieuwe plots op de derde en vierde tabbladen\nMuisgestuurde gekoppelde gebeurtenissen op het vierde tabblad\nReactiviteit\nShiny gebruikt reactief programmeren, wat het mogelijk maakt om de uitvoer bij te werken op basis van de input van de gebruiker. Er zijn drie soorten reactieve objecten in het reactieve programmeerparadigma van Shiny: reactieve bronnen, reactieve geleiders en reactieve eindpunten.\n\nIn wat we tot nu toe hebben gedaan, zijn input$ statements de reactieve bronnen en output$ statements zijn de reactieve eindpunten. We hebben geen reactieve geleiders gebruikt. Uit onze eenvoudige shiny_nba app:\n\nEchter, soms vereisen Shiny-apps een langzame berekening, en als één bron meerdere eindpunten heeft, dan zullen deze berekeningen meerdere keren moeten worden gedaan. Reactieve geleiders kunnen dit versnellen. Reactieve expressies zijn een implementatie van reactieve geleiders die een input$ waarde krijgen, een operatie doen en cache de resultaten. De code our_expression = reactive({}) creëert een reactieve uitdrukking genaamd our_expression. Aangezien reactieve expressies eigenlijk functies zijn, roepen we de reactieve expressie op door het tussen haakjes te plaatsen: our_expression().\nDe hippe shiny_nba_complete app gebruikt een reactieve uitdrukking om een dataset op te slaan die gefilterd is op de huidige waarde van input$player_name. In de onderstaande code, van server.R van deze app, wordt een dataframe genaamd player_data gedefinieerd met behulp van een reactieve uitdrukking en vervolgens benaderd door het reactieve eindpunt output$court_shots door player_data() op te roepen.\n\n\n\nOmdat zowel output$court_shots als output$court_position deze gegevens gebruiken, besparen we ons het doen van de berekening twee keer. Het reactieve diagram hiervoor is:\n\nGekoppelde gebeurtenissen\nWanneer u Plotly en Shiny samen gebruikt, kunt u gekoppelde muisgebeurtenissen gebruiken om nieuwe, door de gebruiker gestuurde plots te creëren. Met Gekoppelde gebeurtenissen (https://plot.ly/r/shiny-coupled-events/) kunt u bijvoorbeeld punten op een plot aanklikken of selecteren en informatie op basis van die klikken of selecties in een ander plot laten verschijnen. Het tabblad “Resterende tijd” van de volledige app maakt gebruik van gekoppelde gebeurtenissen - in het bovenste diagram kan de gebruiker datapunten selecteren en de subset van foto’s die overeenkomt met de selectie van de gebruiker zal op het onderstaande diagram verschijnen.\n\nAlle code die nodig is om gekoppelde gebeurtenissen toe te voegen aan een set van plots, gaat in het server.R bestand. Voor onze gekoppelde gebeurtenis wordt de eerste plot gemaakt met behulp van Plotly. De hieronder aangegeven delen van de code zijn nodig om de koppeling van de muisgebeurtenis voor dit plot aan te zetten. In de app is de code voor dit plot veel langer, maar de rest van de code is alleen voor de esthetiek en heeft niets te maken met koppeling.\n\nDe source = \"time_plot\" geeft uw gekoppelde gebeurtenis een id (wat handig is als u meerdere plots wilt koppelen), en key = ~shot_id identificeert een variabele in de dataset die u kunt gebruiken om toegang te krijgen tot de gegevens.\nDe tweede plot wordt gemaakt met behulp van ggplot2, maar alleen gegevens die de gebruiker selecteert op de plot hierboven zullen in deze plot verschijnen.\n\nWe hebben toegang tot de geselecteerde gegevens met behulp van \"plotly_selected\". Vervolgens stellen we de nba_shots gegevens in op basis van de selectie van de gebruiker.\nOm toegang te krijgen tot de hover of klik op de gegevens gebruikt u respectievelijk \"plotly_hover\" of \"plotly_click\".\nInzet van uw app\nWe lieten onze app al lokaal draaien, maar het hosten van de app in het openbaar kan lastiger zijn. U kunt het niet alleen op GitHub hosten zoals een [R Markdown of blogdown website] (http://www.emilyzabor.com/tutorials/rmarkdown_websites_tutorial.html) omdat R op de achtergrond moet lopen. Echter, u kunt Shiny apps openbaar hosten op Shinyapps.io.\nHet rsconnect pakket zorgt ervoor dat Shiny apps draaien op shinyapps.io. Laad dit pakket en maak je eigen account aan bij [shinyapps.io] (https://www.shinyapps.io/). Zodra u een shinyapps.io account heeft aangemaakt en het rsconnect pakket heeft geconfigureerd met uw account (volg deze instructies, u hoeft het maar één keer te doen en u bent u klaar om uw app te hosten. Het enige wat u hoeft te doen is naar de map te navigeren waar uw app zich bevindt (zo eenvoudig als u een R-project gebruikt!) en de volgende code te draaien:\n\n\n\n\nGefeliciteerd, u heeft uw Shiny app gepubliceerd! De app van Julia Wrober wordt hier gehost.\nEen paar andere dingen over het gebruik:\nJe kunt wijzigingen in je app aanbrengen en vervolgens deployApp() opnieuw uitvoeren. Het zou na de eerste keer sneller moeten zijn.\nTenzij je een speciale, niet-gratis account hebt, kun je maar één publieke Shiny app tegelijk hosten.\nJe kunt problemen met het inzetten van Shiny apps krijgen wanneer datasets niet in dezelfde map staan als de ui.R en server.R bestanden. Dit is de reden waarom de nba_shots.RData in dezelfde map als de andere bestanden staan.\nAanvullende bronnen\nEmbedding Shiny in Rmarkdown documents\n“Case Studies in Shiny”: Dean Attali’s short course at DataCamp\nGallery of user-submitted Shiny apps\nShiny cheatsheet\nEliminate warnings produced by Plotly with Shiny\nGreat blog post on Shiny’s reactivity\n",
    "preview": "posts/2019-04-23-shiny_files/distill-preview.png",
    "last_modified": "2020-03-13T23:28:58+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-03-18-interactieve-grafiek/",
    "title": "Interactieve grafiek met plotly",
    "description": "In deze twee maanden wilde ik toch eens kijken naar interactieve mogelijkheden die het programma R/RStudio ons biedt. `Plotly` is zo'n mogelijkheid en daarover gaat dit blog. `Shiny` is de andere mogelijkheid en daar zal ik een volgende keer aandacht aan besteden. `Plotly` heeft een eigen website waar veel informatie over het programma is te vinden [hier adres website](https://plot.ly/). Er is ook een uitgebreide handleiding over `Plotly` geschreven [hier handleiding](https://plotly-r.com/the-plotly-cookbook.html). Onlangs stond er op de blog van RBloggers een goede introductie van Laura Ellis, die mij veel vertelde over het gebruik van `Plotly`. Haar bijdrage [zie hier](https://www.r-bloggers.com/create-interactive-ggplot2-graphs-with-plotly/) heb ik hier naar het Nederlands overgezet en hier en daar iets bewerkt.",
    "author": [
      {
        "name": "Laura Ellis op R-bloggers (13 maart 2019), bewerking Harrie Jonkman.",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2019-04-03",
    "categories": [],
    "contents": "\nInteractieve ggplot2 grafieken met plotly\nEen van de belangrijkste redenen dat ik verliefd werd op R, schrijft Laura Ellis in haar blog, kwam door ggplot2. Zoals Jennifer Thompson het zo welsprekend verwoordde:\n\n“Ik gebruikte R voor ggplot, maar tot dan toe hield ik er nooit van”.\n—Jennifer Thompson\n\nVoor iemand die zeer geïnteresseerd is in het vertellen van verhalen, wordt ggplot2 al snel het favoriete datavisualisatietool. Het is eigenlijk het Zwitserse zakmes voor datavisualisatie. Het kan een grafiek met allerlei afmetingen aan. Deze mogelijkheid is ongelofelijk handig tijdens data verkenningsfasen. Soms wil je echter wel eens naar trends kijken zonder dat het je al te veel moeite kost. Dan bekijk je dichte scatterplots met uitschieters. ggplot2 is hier geweldig in. Maar soms kun je niet alle mogelijke dimensies in de statische grafieken onderzoeken. Dan kun je plotly gebruiken.\nVoer plotly in. Het plotly-pakket en de ggploty-functie gaan goed samen, ze bewaren de hoge kwaliteit van ggplot2 grafieken en kunnen het ook nog interactief maken.\nIn deze tutorial onderzoeken we de mediaan van de gerapporteerde lonen van creatieve beroepen. We onderzoeken de gegevens van deze beroepsgroep van de stad Austin (Texas, USA) voor de jaren 2016 en 2017. Deze dataset is afkomstig van het open dataportaal Austin hier. Het is verbazendwekkend om te zien hoe het AustinGO2.0-team steeds nieuwe en spannende lokale datasets beschikbaar stelt om te verkennen!\nInstalleer en laden van pakketten\nVoordat we verder gaan, moeten we eerst de pakketten installeren. Als je dat een keer hebt gedaan moet je ze wel steeds voor gebruik laden.\n\n\n\nDataset laden\nOm het reproduceer te maken moet je de dataset van de Ausatin open data portal halen en uploaden en Laura Ellis heeft dat hier in haar github repo geplaatst githubrepo\n\n\n[1] 54  4\n\n   SOC.Code          Occupation        X2016.Median.hourly.earnings\n Length:54          Length:54          Min.   : 6.79               \n Class :character   Class :character   1st Qu.:14.29               \n Mode  :character   Mode  :character   Median :20.24               \n                                       Mean   :19.68               \n                                       3rd Qu.:24.25               \n                                       Max.   :34.37               \n X2017.Median.hourly.earnings\n Min.   : 6.88               \n 1st Qu.:15.29               \n Median :20.10               \n Mean   :20.12               \n 3rd Qu.:25.69               \n Max.   :34.37               \n\n'data.frame':   54 obs. of  4 variables:\n $ SOC.Code                    : chr  \"11-2011\" \"13-1011\" \"25-1099\" \"25-4011\" ...\n $ Occupation                  : chr  \"Advertising and promotions managers\" \"Agents and business managers of artists, performers, and athletes\" \"Postsecondary teachers\" \"Archivists\" ...\n $ X2016.Median.hourly.earnings: num  30.1 21.8 26.9 26.7 32.6 ...\n $ X2017.Median.hourly.earnings: num  29.7 22.3 26.3 27.3 30.5 ...\n\n  SOC.Code\n1  11-2011\n2  13-1011\n3  25-1099\n4  25-4011\n5  25-4012\n6  25-4013\n                                                         Occupation\n1                               Advertising and promotions managers\n2 Agents and business managers of artists, performers, and athletes\n3                                            Postsecondary teachers\n4                                                        Archivists\n5                                                          Curators\n6                               Museum technicians and conservators\n  X2016.Median.hourly.earnings X2017.Median.hourly.earnings\n1                        30.11                        29.73\n2                        21.83                        22.31\n3                        26.92                        26.31\n4                        26.66                        27.31\n5                        32.56                        30.46\n6                        24.33                        25.74\n\nTransformeer de data\nWe voeren wat eenvoudige transformaties uit om de data voor te bereiden voor een makkelijke scatterplot. Eerst geven we de kolommen een andere naam en daarna hebben we een nieuwe kolom gemaakt om de veranderingen in het jaar 2016 tot 2017 te berekenen dat iets zegt over veranderingen in het mediane loon per beroep.\n\n\n\nCreëer de scatterplot\nWe creëerden een eenvoudige ggplot2-scatterplot van de beroepen met de 2017 mediane loon vs percentage jaarverbetering. We voegden een eenvoudige horizontale lijn toe om de nul in de plot te markeren. Dit stelt ons in staat om de jaarveranderingen in het mediane loon gemakkelijker te verwerken. Vervolgens zetten we nog een laatste stap en voeren we onze ggplot2-scatterplot in de ggplotly-functie in. Zo ontstaat een interactieve grafiek!\n\n\n{\"x\":{\"data\":[{\"x\":[29.73,22.31,26.31,27.31,30.46,25.74,30.4,17.47,28.42,18.09,6.88,9.53,22.13,11.14,25.75,29.46,20.66,12.67,22.34,24.12,15.76,24.34,16,27.91,15.28,17.68,21.1,19.43,23.34,17.52,22.15,20.1,17.6,25.53,15.68,21.39,27.72,34.37,9.77,27.52,10.83,28.01,13.98,12.47,9.36,11.78,15.22,25.53,19.02,15.32,13.8,14.59,15.57,20.1],\"y\":[1.26,-2.2,2.27,-2.44,6.45,-5.8,-1.91,-19.17,-0.96,2.32,-1.33,3.64,-56.06,1.33,1.23,-3.08,-2.08,-9.04,-5.78,-1.43,4.19,-8.18,-8.55,-5.88,1.55,-1.09,-0.14,1.82,-4.43,-0.4,0.67,5.68,21.85,-5.28,0.06,4.38,-0.69,0,-1.45,-22.91,1.72,3.75,-0.72,-5.5,2.5,0.76,-6.88,-5.37,-9.37,-2.61,-6.56,-0.97,-12.1,0.69],\"text\":[\"Median_2017: 29.73<br />Percent_Improvement:   1.26\",\"Median_2017: 22.31<br />Percent_Improvement:  -2.20\",\"Median_2017: 26.31<br />Percent_Improvement:   2.27\",\"Median_2017: 27.31<br />Percent_Improvement:  -2.44\",\"Median_2017: 30.46<br />Percent_Improvement:   6.45\",\"Median_2017: 25.74<br />Percent_Improvement:  -5.80\",\"Median_2017: 30.40<br />Percent_Improvement:  -1.91\",\"Median_2017: 17.47<br />Percent_Improvement: -19.17\",\"Median_2017: 28.42<br />Percent_Improvement:  -0.96\",\"Median_2017: 18.09<br />Percent_Improvement:   2.32\",\"Median_2017:  6.88<br />Percent_Improvement:  -1.33\",\"Median_2017:  9.53<br />Percent_Improvement:   3.64\",\"Median_2017: 22.13<br />Percent_Improvement: -56.06\",\"Median_2017: 11.14<br />Percent_Improvement:   1.33\",\"Median_2017: 25.75<br />Percent_Improvement:   1.23\",\"Median_2017: 29.46<br />Percent_Improvement:  -3.08\",\"Median_2017: 20.66<br />Percent_Improvement:  -2.08\",\"Median_2017: 12.67<br />Percent_Improvement:  -9.04\",\"Median_2017: 22.34<br />Percent_Improvement:  -5.78\",\"Median_2017: 24.12<br />Percent_Improvement:  -1.43\",\"Median_2017: 15.76<br />Percent_Improvement:   4.19\",\"Median_2017: 24.34<br />Percent_Improvement:  -8.18\",\"Median_2017: 16.00<br />Percent_Improvement:  -8.55\",\"Median_2017: 27.91<br />Percent_Improvement:  -5.88\",\"Median_2017: 15.28<br />Percent_Improvement:   1.55\",\"Median_2017: 17.68<br />Percent_Improvement:  -1.09\",\"Median_2017: 21.10<br />Percent_Improvement:  -0.14\",\"Median_2017: 19.43<br />Percent_Improvement:   1.82\",\"Median_2017: 23.34<br />Percent_Improvement:  -4.43\",\"Median_2017: 17.52<br />Percent_Improvement:  -0.40\",\"Median_2017: 22.15<br />Percent_Improvement:   0.67\",\"Median_2017: 20.10<br />Percent_Improvement:   5.68\",\"Median_2017: 17.60<br />Percent_Improvement:  21.85\",\"Median_2017: 25.53<br />Percent_Improvement:  -5.28\",\"Median_2017: 15.68<br />Percent_Improvement:   0.06\",\"Median_2017: 21.39<br />Percent_Improvement:   4.38\",\"Median_2017: 27.72<br />Percent_Improvement:  -0.69\",\"Median_2017: 34.37<br />Percent_Improvement:   0.00\",\"Median_2017:  9.77<br />Percent_Improvement:  -1.45\",\"Median_2017: 27.52<br />Percent_Improvement: -22.91\",\"Median_2017: 10.83<br />Percent_Improvement:   1.72\",\"Median_2017: 28.01<br />Percent_Improvement:   3.75\",\"Median_2017: 13.98<br />Percent_Improvement:  -0.72\",\"Median_2017: 12.47<br />Percent_Improvement:  -5.50\",\"Median_2017:  9.36<br />Percent_Improvement:   2.50\",\"Median_2017: 11.78<br />Percent_Improvement:   0.76\",\"Median_2017: 15.22<br />Percent_Improvement:  -6.88\",\"Median_2017: 25.53<br />Percent_Improvement:  -5.37\",\"Median_2017: 19.02<br />Percent_Improvement:  -9.37\",\"Median_2017: 15.32<br />Percent_Improvement:  -2.61\",\"Median_2017: 13.80<br />Percent_Improvement:  -6.56\",\"Median_2017: 14.59<br />Percent_Improvement:  -0.97\",\"Median_2017: 15.57<br />Percent_Improvement: -12.10\",\"Median_2017: 20.10<br />Percent_Improvement:   0.69\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(81,160,213,1)\",\"opacity\":0.7,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(81,160,213,1)\"}},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[5.5055,35.7445],\"y\":[0,0],\"text\":\"yintercept: 0\",\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(44,82,140,1)\",\"dash\":\"dash\"},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":45.4063926940639,\"r\":7.30593607305936,\"b\":41.8264840182648,\"l\":43.1050228310502},\"plot_bgcolor\":\"rgba(255,255,255,1)\",\"paper_bgcolor\":\"rgba(255,255,255,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"title\":\"Austins Mediaan Creatief Uurloon\",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":17.5342465753425},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[5.5055,35.7445],\"tickmode\":\"array\",\"ticktext\":[\"10\",\"20\",\"30\"],\"tickvals\":[10,20,30],\"categoryorder\":\"array\",\"categoryarray\":[\"10\",\"20\",\"30\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":true,\"linecolor\":\"rgba(0,0,0,1)\",\"linewidth\":0.66417600664176,\"showgrid\":false,\"gridcolor\":null,\"gridwidth\":0,\"zeroline\":false,\"anchor\":\"y\",\"title\":\"Mediaan Beroepsuurloon in 2017\",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-59.9555,25.7455],\"tickmode\":\"array\",\"ticktext\":[\"-40\",\"-20\",\"0\",\"20\"],\"tickvals\":[-40,-20,0,20],\"categoryorder\":\"array\",\"categoryarray\":[\"-40\",\"-20\",\"0\",\"20\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":true,\"linecolor\":\"rgba(0,0,0,1)\",\"linewidth\":0.66417600664176,\"showgrid\":false,\"gridcolor\":null,\"gridwidth\":0,\"zeroline\":false,\"anchor\":\"x\",\"title\":\"% Toename Jaar na Jaar (2016 tot 2017)\",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":\"rgba(255,255,255,1)\",\"bordercolor\":\"transparent\",\"borderwidth\":1.88976377952756,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.689497716895}},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"modeBarButtonsToAdd\":[{\"name\":\"Collaborate\",\"icon\":{\"width\":1000,\"ascent\":500,\"descent\":-50,\"path\":\"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z\"},\"click\":\"function(gd) { \\n        // is this being viewed in RStudio?\\n        if (location.search == '?viewer_pane=1') {\\n          alert('To learn about plotly for collaboration, visit:\\\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\\n        } else {\\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\\n        }\\n      }\"}],\"cloud\":false},\"source\":\"A\",\"attrs\":{\"c34f5c7e3d\":{\"x\":{},\"y\":{},\"type\":\"scatter\"},\"c34218a17bd\":{\"yintercept\":{}}},\"cur_data\":\"c34f5c7e3d\",\"visdat\":{\"c34f5c7e3d\":[\"function (y) \",\"x\"],\"c34218a17bd\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"base_url\":\"https://plot.ly\"},\"evals\":[\"config.modeBarButtonsToAdd.0.click\"],\"jsHooks\":[]}\nVoeg de labels toe\nDe bovenstaande grafiek is geweldig omdat we met succes gebruik hebben gemaakt van plotly om de ggplot2-scatterplot interactief te maken. De data hebben echter niet alles wat we willen en het is niet erg mooi geformatteerd. Laten we dit oplossen!\nCorinne Leopold, van het team van Laura Ellis, vond een veel efficiëntere manier om labels toe te wijzen in de plots dan de oplossing die Laura eerder vond eerdere oplossing. We voegen eenvoudigweg de details toe via de esthetische tekstuele eigenschap van de ggplot. Vervolgens wijzen we het toe aan de tooltip in de ggplotly-functie.\n\n\n{\"x\":{\"data\":[{\"x\":[29.73,22.31,26.31,27.31,30.46,25.74,30.4,17.47,28.42,18.09,6.88,9.53,22.13,11.14,25.75,29.46,20.66,12.67,22.34,24.12,15.76,24.34,16,27.91,15.28,17.68,21.1,19.43,23.34,17.52,22.15,20.1,17.6,25.53,15.68,21.39,27.72,34.37,9.77,27.52,10.83,28.01,13.98,12.47,9.36,11.78,15.22,25.53,19.02,15.32,13.8,14.59,15.57,20.1],\"y\":[1.26,-2.2,2.27,-2.44,6.45,-5.8,-1.91,-19.17,-0.96,2.32,-1.33,3.64,-56.06,1.33,1.23,-3.08,-2.08,-9.04,-5.78,-1.43,4.19,-8.18,-8.55,-5.88,1.55,-1.09,-0.14,1.82,-4.43,-0.4,0.67,5.68,21.85,-5.28,0.06,4.38,-0.69,0,-1.45,-22.91,1.72,3.75,-0.72,-5.5,2.5,0.76,-6.88,-5.37,-9.37,-2.61,-6.56,-0.97,-12.1,0.69],\"text\":[\"Beroep: Advertising and promotions managers<br />2017: 29.73<br />2016: 30.11<br />% Toename Jaar na Jaar: 1.26<br />\",\"Beroep: Agents and business managers of artists, performers, and athletes<br />2017: 22.31<br />2016: 21.83<br />% Toename Jaar na Jaar: -2.2<br />\",\"Beroep: Postsecondary teachers<br />2017: 26.31<br />2016: 26.92<br />% Toename Jaar na Jaar: 2.27<br />\",\"Beroep: Archivists<br />2017: 27.31<br />2016: 26.66<br />% Toename Jaar na Jaar: -2.44<br />\",\"Beroep: Curators<br />2017: 30.46<br />2016: 32.56<br />% Toename Jaar na Jaar: 6.45<br />\",\"Beroep: Museum technicians and conservators<br />2017: 25.74<br />2016: 24.33<br />% Toename Jaar na Jaar: -5.8<br />\",\"Beroep: Librarians<br />2017: 30.4<br />2016: 29.83<br />% Toename Jaar na Jaar: -1.91<br />\",\"Beroep: Library technicians<br />2017: 17.47<br />2016: 14.66<br />% Toename Jaar na Jaar: -19.17<br />\",\"Beroep: Audio-visual and multimedia collections specialists<br />2017: 28.42<br />2016: 28.15<br />% Toename Jaar na Jaar: -0.96<br />\",\"Beroep: Art directors<br />2017: 18.09<br />2016: 18.52<br />% Toename Jaar na Jaar: 2.32<br />\",\"Beroep: Craft artists<br />2017: 6.88<br />2016: 6.79<br />% Toename Jaar na Jaar: -1.33<br />\",\"Beroep: Fine artists, including painters, sculptors, and illustrators<br />2017: 9.53<br />2016: 9.89<br />% Toename Jaar na Jaar: 3.64<br />\",\"Beroep: Multimedia artists and animators<br />2017: 22.13<br />2016: 14.18<br />% Toename Jaar na Jaar: -56.06<br />\",\"Beroep: Artists and related workers, all other<br />2017: 11.14<br />2016: 11.29<br />% Toename Jaar na Jaar: 1.33<br />\",\"Beroep: Commercial and industrial designers<br />2017: 25.75<br />2016: 26.07<br />% Toename Jaar na Jaar: 1.23<br />\",\"Beroep: Fashion designers<br />2017: 29.46<br />2016: 28.58<br />% Toename Jaar na Jaar: -3.08<br />\",\"Beroep: Graphic designers<br />2017: 20.66<br />2016: 20.24<br />% Toename Jaar na Jaar: -2.08<br />\",\"Beroep: Merchandise displayers and window trimmers<br />2017: 12.67<br />2016: 11.62<br />% Toename Jaar na Jaar: -9.04<br />\",\"Beroep: Set and exhibit designers<br />2017: 22.34<br />2016: 21.12<br />% Toename Jaar na Jaar: -5.78<br />\",\"Beroep: Designers, all other<br />2017: 24.12<br />2016: 23.78<br />% Toename Jaar na Jaar: -1.43<br />\",\"Beroep: Actors<br />2017: 15.76<br />2016: 16.45<br />% Toename Jaar na Jaar: 4.19<br />\",\"Beroep: Producers and directors<br />2017: 24.34<br />2016: 22.5<br />% Toename Jaar na Jaar: -8.18<br />\",\"Beroep: Dancers<br />2017: 16<br />2016: 14.74<br />% Toename Jaar na Jaar: -8.55<br />\",\"Beroep: Choreographers<br />2017: 27.91<br />2016: 26.36<br />% Toename Jaar na Jaar: -5.88<br />\",\"Beroep: Music directors and composers<br />2017: 15.28<br />2016: 15.52<br />% Toename Jaar na Jaar: 1.55<br />\",\"Beroep: Musicians and singers<br />2017: 17.68<br />2016: 17.49<br />% Toename Jaar na Jaar: -1.09<br />\",\"Beroep: Entertainers and performers, sports, and related workers *<br />2017: 21.1<br />2016: 21.07<br />% Toename Jaar na Jaar: -0.14<br />\",\"Beroep: Radio and television announcers<br />2017: 19.43<br />2016: 19.79<br />% Toename Jaar na Jaar: 1.82<br />\",\"Beroep: Editors<br />2017: 23.34<br />2016: 22.35<br />% Toename Jaar na Jaar: -4.43<br />\",\"Beroep: Writers and authors<br />2017: 17.52<br />2016: 17.45<br />% Toename Jaar na Jaar: -0.4<br />\",\"Beroep: Media and communication workers, all other<br />2017: 22.15<br />2016: 22.3<br />% Toename Jaar na Jaar: 0.67<br />\",\"Beroep: Audio and video equipment technicians<br />2017: 20.1<br />2016: 21.31<br />% Toename Jaar na Jaar: 5.68<br />\",\"Beroep: Radio operators *<br />2017: 17.6<br />2016: 22.52<br />% Toename Jaar na Jaar: 21.85<br />\",\"Beroep: Sound engineering technicians<br />2017: 25.53<br />2016: 24.25<br />% Toename Jaar na Jaar: -5.28<br />\",\"Beroep: Photographers<br />2017: 15.68<br />2016: 15.69<br />% Toename Jaar na Jaar: 0.06<br />\",\"Beroep: Camera operators, television, video, and motion picture<br />2017: 21.39<br />2016: 22.37<br />% Toename Jaar na Jaar: 4.38<br />\",\"Beroep: Film and video editors<br />2017: 27.72<br />2016: 27.53<br />% Toename Jaar na Jaar: -0.69<br />\",\"Beroep: Media and communication equipment workers, all other<br />2017: 34.37<br />2016: 34.37<br />% Toename Jaar na Jaar: 0<br />\",\"Beroep: Ushers, lobby attendants, and ticket takers<br />2017: 9.77<br />2016: 9.63<br />% Toename Jaar na Jaar: -1.45<br />\",\"Beroep: Costume attendants<br />2017: 27.52<br />2016: 22.39<br />% Toename Jaar na Jaar: -22.91<br />\",\"Beroep: Entertainment attendants and related workers, all other<br />2017: 10.83<br />2016: 11.02<br />% Toename Jaar na Jaar: 1.72<br />\",\"Beroep: Makeup artists, theatrical and performance<br />2017: 28.01<br />2016: 29.1<br />% Toename Jaar na Jaar: 3.75<br />\",\"Beroep: Library assistants, clerical<br />2017: 13.98<br />2016: 13.88<br />% Toename Jaar na Jaar: -0.72<br />\",\"Beroep: Musical instrument repairers and tuners<br />2017: 12.47<br />2016: 11.82<br />% Toename Jaar na Jaar: -5.5<br />\",\"Beroep: Sewers, hand *<br />2017: 9.36<br />2016: 9.6<br />% Toename Jaar na Jaar: 2.5<br />\",\"Beroep: Tailors, dressmakers, and custom sewers *<br />2017: 11.78<br />2016: 11.87<br />% Toename Jaar na Jaar: 0.76<br />\",\"Beroep: Cabinetmakers and bench carpenters *<br />2017: 15.22<br />2016: 14.24<br />% Toename Jaar na Jaar: -6.88<br />\",\"Beroep: Model makers, wood *<br />2017: 25.53<br />2016: 24.23<br />% Toename Jaar na Jaar: -5.37<br />\",\"Beroep: Furnace, kiln, oven, drier, and kettle operators and tenders *<br />2017: 19.02<br />2016: 17.39<br />% Toename Jaar na Jaar: -9.37<br />\",\"Beroep: Jewelers and precious stone and metal workers<br />2017: 15.32<br />2016: 14.93<br />% Toename Jaar na Jaar: -2.61<br />\",\"Beroep: Photographic process workers and processing machine operators *<br />2017: 13.8<br />2016: 12.95<br />% Toename Jaar na Jaar: -6.56<br />\",\"Beroep: Etchers and engravers *<br />2017: 14.59<br />2016: 14.45<br />% Toename Jaar na Jaar: -0.97<br />\",\"Beroep: Molders, shapers, and casters (except metal and plastic) *<br />2017: 15.57<br />2016: 13.89<br />% Toename Jaar na Jaar: -12.1<br />\",\"Beroep: <br />2017: 20.1<br />2016: 20.24<br />% Toename Jaar na Jaar: 0.69<br />\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(81,160,213,1)\",\"opacity\":0.7,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(81,160,213,1)\"}},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[5.5055,35.7445],\"y\":[0,0],\"text\":\"\",\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(44,82,140,1)\",\"dash\":\"dash\"},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":45.4063926940639,\"r\":7.30593607305936,\"b\":41.8264840182648,\"l\":43.1050228310502},\"plot_bgcolor\":\"rgba(255,255,255,1)\",\"paper_bgcolor\":\"rgba(255,255,255,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"title\":\"Austins Mediaan Creatief Uurloon\",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":17.5342465753425},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[5.5055,35.7445],\"tickmode\":\"array\",\"ticktext\":[\"10\",\"20\",\"30\"],\"tickvals\":[10,20,30],\"categoryorder\":\"array\",\"categoryarray\":[\"10\",\"20\",\"30\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":true,\"linecolor\":\"rgba(0,0,0,1)\",\"linewidth\":0.66417600664176,\"showgrid\":false,\"gridcolor\":null,\"gridwidth\":0,\"zeroline\":false,\"anchor\":\"y\",\"title\":\"Mediaan Beroepsuurloon in 2017\",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[-59.9555,25.7455],\"tickmode\":\"array\",\"ticktext\":[\"-40\",\"-20\",\"0\",\"20\"],\"tickvals\":[-40,-20,0,20],\"categoryorder\":\"array\",\"categoryarray\":[\"-40\",\"-20\",\"0\",\"20\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":true,\"linecolor\":\"rgba(0,0,0,1)\",\"linewidth\":0.66417600664176,\"showgrid\":false,\"gridcolor\":null,\"gridwidth\":0,\"zeroline\":false,\"anchor\":\"x\",\"title\":\"% Toename Jaar na Jaar (2016 tot 2017)\",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":null,\"line\":{\"color\":null,\"width\":0,\"linetype\":[]},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":\"rgba(255,255,255,1)\",\"bordercolor\":\"transparent\",\"borderwidth\":1.88976377952756,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.689497716895}},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"modeBarButtonsToAdd\":[{\"name\":\"Collaborate\",\"icon\":{\"width\":1000,\"ascent\":500,\"descent\":-50,\"path\":\"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z\"},\"click\":\"function(gd) { \\n        // is this being viewed in RStudio?\\n        if (location.search == '?viewer_pane=1') {\\n          alert('To learn about plotly for collaboration, visit:\\\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\\n        } else {\\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\\n        }\\n      }\"}],\"cloud\":false},\"source\":\"A\",\"attrs\":{\"c344d5348c5\":{\"x\":{},\"y\":{},\"text\":{},\"type\":\"scatter\"},\"c3459c369c7\":{\"yintercept\":{}}},\"cur_data\":\"c344d5348c5\",\"visdat\":{\"c344d5348c5\":[\"function (y) \",\"x\"],\"c3459c369c7\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"base_url\":\"https://plot.ly\"},\"evals\":[\"config.modeBarButtonsToAdd.0.click\"],\"jsHooks\":[]}\nOver het opslaan en delen van de grafiek vermeldt Laure Ellis ook nog het een en ander. Maar daar kijken we hier verder niet naar. Mij ging het erom plotly zelf beter in de vingers te krijgen. De volgende keer maar eens kijken hoe het met Shiny gaat.\n\n\n",
    "preview": {},
    "last_modified": "2020-03-13T23:28:58+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-02-20-bbc-en-data-journalisme/",
    "title": "BBC en data-journalisme",
    "description": "Een blog over hoe de BBC omgaat met visualisatie en data-journalisme",
    "author": [
      {
        "name": "R-bloggers, bewerking Harrie Jonkman.",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2019-02-20",
    "categories": [],
    "contents": "\nHoe kunnen we grafieken in BBC-stijl produceren\nBij het BBC datateam hebben ze een R-pakket en een R-kookboek ontwikkeld. Met dat pakket en kookboek wordt het proces van het maken van publicatieklare grafieken in hun huisstijl geautomatiseerd. Met behulp van R’s ggplot2-pakket wordt er een meer reproduceerbaar proces van gemaakt. Voor nieuwe R-gebruikers wordt het zo ook gemakkelijker gemaakt om deze grafieken te maken.\n\nHet kookboek, dat in februari 2019 hier verscheen, heb ik in het Nederlands vertaald om zelf goed door te krijgen hoe het allemaal werkt. Het kookboek en deze Nederlandse versie wil iedereen helpen om dit soort grafieken te maken:\nLaten we eens zien hoe we de verschillende elementen van deze grafieken in elkaar kunnen zetten. Maar eerst eens enkele administratieve zaken…\nLaad eerst alle pakketten die je nodig hebt in R\nEen paar van de stappen in dit kookboek - om grafieken in R in het algemeen te kunnen maken - vereisen dat bepaalde pakketten worden geïnstalleerd en geladen. Zo hoeft u ze niet één voor één te installeren en te laden. Door de p_load-functie in het pacman-pakket kunt u ze allemaal tegelijk laden met de volgende code.\n\n\n\nInstalleer in ieder geval het bbplot pakket\nbbplot staat niet op CRAN (het algemene platform voor R-pakketten). U moet het direct vanaf Github installeren met behulp van devtools.\nAls u het devtools-pakket niet heeft geïnstalleerd, moet u ook de eerste regel in de code hieronder uitvoeren.\n\n\n\nVoor meer informatie over bbplot-pakket, bekijk het volgende Github repo. Dat wat u moet weten over het gebruik van het pakket en de functies, is hieronder gedetailleerd te vinden.\nWanneer u het pakket hebt gedownload en met succes hebt geïnstalleerd, bent u in staat om grafieken te gaan maken.\nHoe werkt dat bbplot-pakket eigenlijk?\nHet pakket heeft twee functies, bbc_style() en finalise_plot().\nbbc_style(): heeft geen argumenten en wordt toegevoegd aan de ggplot ‘keten’ nadat je een plot hebt gemaakt. Wat het doet is over het algemeen tekstgrootte, lettertype en kleur, aslijnen, aslijnen, as-tekst, marges en vele andere standaardgrafiekonderdelen in BBC-stijl. Die zijn geformuleerd op basis van aanbevelingen en feedback van het ontwerpteam.\nMerk op dat kleuren voor lijnen in het geval van een lijndiagram bijvoorbeeld of balken voor een staafdiagram, niet uit het kader van de bbc_style() functie komen. Die moeten expliciet worden ingesteld in uw standaard ggplot-grafiekfuncties.\nDe code hieronder laat zien hoe de bbc_style() gebruikt moet worden binnen de standaard workflow om grafieken te maken. Dit is een voorbeeld van een zeer eenvoudige lijndiagram. De data waar gebruik van wordt gemaakt komen uit het gapminder-pakket.\n\n\n\n\n\n\nDit is wat de bbc_style()-functie eigenlijk onder de motorkap doet. Het wijzigt in wezen bepaalde argumenten in de thema functie van ggplot2.\nHet eerste argument is bijvoorbeeld het instellen van het lettertype, de grootte, het lettertype en de kleur van het titelelement van de grafiek.\n\n\nfunction () \n{\n    font <- \"Helvetica\"\n    ggplot2::theme(plot.title = ggplot2::element_text(family = font, \n        size = 28, face = \"bold\", color = \"#222222\"), plot.subtitle = ggplot2::element_text(family = font, \n        size = 22, margin = ggplot2::margin(9, 0, 9, 0)), plot.caption = ggplot2::element_blank(), \n        legend.position = \"top\", legend.text.align = 0, legend.background = ggplot2::element_blank(), \n        legend.title = ggplot2::element_blank(), legend.key = ggplot2::element_blank(), \n        legend.text = ggplot2::element_text(family = font, size = 18, \n            color = \"#222222\"), axis.title = ggplot2::element_blank(), \n        axis.text = ggplot2::element_text(family = font, size = 18, \n            color = \"#222222\"), axis.text.x = ggplot2::element_text(margin = ggplot2::margin(5, \n            b = 10)), axis.ticks = ggplot2::element_blank(), \n        axis.line = ggplot2::element_blank(), panel.grid.minor = ggplot2::element_blank(), \n        panel.grid.major.y = ggplot2::element_line(color = \"#cbcbcb\"), \n        panel.grid.major.x = ggplot2::element_blank(), panel.background = ggplot2::element_blank(), \n        strip.background = ggplot2::element_rect(fill = \"white\"), \n        strip.text = ggplot2::element_text(size = 22, hjust = 0))\n}\n<bytecode: 0x000000001ed90ee8>\n<environment: namespace:bbplot>\n\nU kunt deze instellingen voor uw grafiek wijzigen of extra thema-argumenten toevoegen. Dit kan door thema-functie aan te roepen met de argumenten die u wilt. Maar, let op: om het te laten werken, moet u de bbc_style functie hebben aangeroepen. Anders zal bbc_style() deze overrulen.\nDit voegt enkele rasterlijnen toe, door extra thema-argumenten toe te voegen aan wat er in de bbc_style() functie zit. Er zitten verschillende gelijkaardige voorbeelden in het kookboek.\n\n\n\nSla jouw afgeronde grafiek op\nNa het toevoegen van de bbc_style() aan uw grafiek is er nog een stap om uw grafiek klaar te maken voor publicatie. finalise_plot(), de tweede functie van het bbplot-pakket, zal de titel links uitlijnen, ondertitelen en de footer toevoegen met een bron en een afbeelding in de rechter benedenhoek van uw grafiek. Het zal het ook opslaan op de door u opgegeven locatie. De functie heeft vijf argumenten:\nHier zijn de argumenten van de functie: finalise_plot(plot_name, source, save_filepath, width_pixels = 640, height_pixels = 450).\nplot_name: de variabelenaam die u uw plot heeft genoemd, bijvoorbeeld voor het voorbeeld hierboven zou “lijn” de plot_name zijn\nsource: de brontekst die u linksonder in uw plot wilt weergeven. U moet het woord \"Source: \"Bron:\"voor het woord typen, dus bijvoorbeeld source = \"Bron\": Gapminder\" zou de juiste manier zijn om dat te doen.\nSave_filepath: type hier het precieze bestandspad waarin u uw afbeelding wilt opslaan, inclusief de .png extensie aan het einde. Dit is afhankelijk van uw werkmap en of u zich in een specifiek R-project bevindt. Een voorbeeld van een bestandspad zou zijn: Desktop/R_projecten/figuren/lijngrafiek.png.\nwidth_pixels: dit is standaard ingesteld op 640px, dus noem dit argument alleen als u wilt dat de grafiek een andere breedte heeft, en geef aan wat u wilt dat het is.\nhoogte_pixels: dit is standaard ingesteld op 450px, dus noem dit argument alleen als u wilt dat de grafiek een andere hoogte heeft en geef aan wat u wilt dat het is.\nlogo_image_path: dit argument specificeert het pad voor het beeld/logo in de rechter benedenhoek van het diagram. De standaardinstelling is voor een PNG-bestand met een achtergrond die overeenkomt met de achtergrondkleur van het diagram. Dus specificeer het argument niet als u wilt dat het zonder logo verschijnt. Als u uw eigen logo wilt toevoegen, hoeft u alleen het pad naar uw PNG-bestand te specificeren. Het pakket is opgezet met een brede en dunne afbeelding in gedachten.\nVoorbeeld van hoe de finalise_plot() wordt gebruikt in een standaard workflow. Deze functie wordt aangeroepen zodra u uw grafiekgegevens, titels en de bbc_style() eraan hebt toegevoegd:\n\n\n\nAls u eenmaal uw plot hebt gemaakt en er relatief tevreden mee bent, kunt u de finalise_plot() functie gebruiken om de laatste aanpassingen te maken en uw grafiek op te slaan zodat u er buiten RStudio naar kunt kijken.\nHet is belangrijk om te vermelden dat het een goed idee is om dit in een vroeg stadium te doen omdat de positie van de tekst en andere elementen niet nauwkeurig worden weergegeven in het RStudio Plots paneel. Hier is het afhankelijk van de grootte en de aspect ratio die u wilt dat uw plot verschijnt. Het opslaan en openen van de bestanden geeft u een nauwkeurige weergave van hoe de grafiek eruit ziet.\nDe finalise_plot()-functie doet meer dan alleen het opslaan van je grafiek. Het lijnt ook de titel en ondertiteling links uit zoals standaard is voor BBC-grafieken, voegt een footer toe met het logo aan de rechterkant en laat je de brontekst aan de linkerkant invoeren.\nDus hoe kunt u de bovenstaande voorbeeldplot opslaan?\n\n\n\nMaak een lijn figuur\n\n\n\n\n\n\nMake een grafiek met verschillende lijnen\n\n\n\n\n\n\nMaak een staafdiagram\n\n\n\n\n\n\nMaak een gestapelde staaf diagram\n\n\n\n\n\n\nDit voorbeeld toont proporties, maar u wilt misschien een gestapelde staafdiagram maken met nummerwaarden - dit is eenvoudig te veranderen!\nDe waarde die wordt doorgegeven aan het position-argument zal bepalen of uw gestapelde grafiek verhoudingen of werkelijke waarden toont.\npositie = \"fill\" zal uw stapels vertonen als proporties, en position = \"identity\" zal getalwaarden laten zien.\nMaak een gegroepeerde staafdiagram\nHet maken van een gegroepeerde staafdiagram lijkt erg op het maken van een staafdiagram.\nJe hoeft alleen maar position = \"identity\" te veranderen in position=\"dodge\" en de vulling wordt esthetisch.\n\n\n\n\n\n\nMaak een halter grafiek\nEen andere manier om het verschil te laten zien is een ‘dumbbell’-grafiek:\n\n\n\nMaak een histogram\n\n\n\nBreng veranderingen aan in de legenda\nVerwijder de legenda\nVerwijder de legende - het is beter om de gegevens direct te labelen met tekstannotaties.\nGebruik guides(colour=FALSE) om de legende te verwijderen voor een specifieke esthetiek (vervang kleur door de relevante esthetiek).\n\n\n\nU kunt ook alle legenden in één keer verwijderen met behulp van theme(legend.position = \"none\"):\n\n\n\nVerander de positie van de legenda\nDe standaardpositie van de legenda staat bovenaan uw grafiek. Verplaats de legenda naar links, rechts of onderaan buiten de plot met:\n\n\n\nOm echt precies te zijn over waar we onze legenda naartoe willen, in plaats van “rechts” of “boven” te specificeren om de algemene positie van waar de legende in onze grafiek verschijnt te veranderen, kunnen we het specifieke coördinaten geven.\nBijvoorbeeld legend.position=c(0.98,0.1) zal de legenda naar rechtsonder verplaatsen. Ter referentie, c(0,0) is linksonder, c(1,0) is rechtsonder, c(0,1) is linksboven en zo verder). Het vinden van de exacte positie kan wat vallen en opstaan met zich meebrengen.\nOm de exacte positie te controleren waar de legenda in uw definitieve plot verschijnt, moet u het bestand controleren dat is opgeslagen nadat u uw finalise_plot() functie hebt uitgevoerd, aangezien de positie relevant is voor de afmetingen van de grafiek.\n\n\n\nOm de legenda tegen de linkerkant van uw grafiek te laten aankomenen, kan het makkelijker zijn om een negatieve linkermarge voor de legenda in te stellen met behulp van legend.margin. De syntaxis is margin(top, right, bottom, left).\nU zult moeten experimenteren om het juiste getal te vinden om de marge voor uw grafiek in te stellen. Sla het op met finalise_plot() en bekijk hoe het eruit ziet.\n\n\n\nVerwijder de titel van de legenda\nVerwijder de titel van de legenda door uw thema()aan te passen. Vergeet niet dat eventuele wijzigingen aan het thema moeten worden toegevoegd nadat u bbc_style() hebt opgeroepen!\n\n\n\nGooi de volgorde van de legenda om\nSoms moet u de volgorde van uw legenda wijzigen, zodat deze overeenkomt met de volgorde van uw balken. Hiervoor heb je guides nodig:\n\n\n\nVerander de layout van jouw legenda\nAls u veel waarden in uw legenda hebt, moet u de lay-out misschien om esthetische redenen aanpassen.\nU kunt het aantal rijen opgeven dat u wilt dat u in uw legenda door als argument guides te gebruiken gebruiken. Het onderstaande codefragment, bijvoorbeeld, zal een legende met 4 rijen maken:\n\n\n\nHet kan nodig zijn om fill in de bovenstaande code te veranderen in een esthetiek die uw legenda beschrijft, zoals size, colour, enz.\nLaat de legendasymbolen er anders uit zien\nU kunt het standaardinstelling van de legendesymbolen overschrijven, zonder de manier waarop ze in de plot verschijnen, door het argument override.aes toe te voegen aan guides.\nHet onderstaande maakt bijvoorbeeld de grootte van de legendesymbolen groter:\n\n\n\nBreng wat ruimte aan tussen de legenda labels\nDe standaard ggplot-legenda heeft bijna geen ruimte tussen individuele legenda-items. Niet ideaal.\nU kunt ruimte toevoegen door de schaallabels handmatig te wijzigen.\nAls u bijvoorbeeld de kleur van uw geoms zo hebt ingesteld dat deze afhankelijk is van uw gegevens, krijgt u een legenda voor de kleur en kunt u de exacte labels aanpassen om wat extra ruimte te krijgen met behulp van het onderstaande fragment:\n\n\n\nAls uw legende iets anders laat zien, moet u de code dienovereenkomstig wijzigen. Bijvoorbeeld, voor het vullen heeft u scale_fill_manual() in plaats daarvan nodig.\nPas de assen aan\nGooi de coordinaten van een plot om\nVoeg coord_flip() toe om verticale staven horizontaal te maken:\n\n\n\n\n\n\nToevoegen/weghalen van gridlijnen\nHet standaard thema heeft alleen rasterlijnen voor de y-as. Voeg x rasterlijnen toe met panel.grid.major.x = element_line.\n(Verwijder op dezelfde manier de rasterlijnen op de y-as met panel.grid.major.y=element_blank())\n\n\n\n\n\n\nVerander de astekst met de hand\nU kunt de tekstlabels van de assen vrij wijzigen met scale_y_continuous of scale_x_continuous:\n\n\n\nDit specificeert ook de grenzen van uw grafiek en waar u as-tekens wilt hebben.\nVoeg duizend scheidingstekens toe aan u as-labels\nU kunt aangeven dat u wilt dat uw as-tekst duizend scheidingstekens heeft met een argument schale_y_continuous.\nEr zijn twee manieren om dit te doen, een in basis R die een beetje lastig is:\n\n\n\nDe tweede manier is gebaseerd op het scales- pakket, maar is veel beknopter:\n\n\n\nVoeg een percentagesymbool toe aan jouw aslabels\nDit is ook gemakkelijk met een argument toe te voegen aan scale_y_continuous:\n\n\n\nVerander de limieten van de plot\nDe lange manier om de grenzen van uw grafiek expliciet in te stellen is met scale_y_continuous zoals hierboven. Maar als u de pauzes of labels niet hoeft op te geven, dan is dat met xlim of ylim:\n\n\n\nVoeg astitels toe\nOns standaardthema heeft geen as-titels, maar u kunt ze misschien handmatig toevoegen. Dit doet u door theme() - merk op dat je dit moet doen na de aanroep naar bbc_style() anders worden je wijzigingen overruled:\n\n\n\nPas astitels aan\nAls u in de assen titels toevoegt, zijn dit standaard de kolomnamen van uw dataset. U kunt dit veranderen door labs()op te roepen in wat u doet.\nAls u bijvoorbeeld wilt dat de x-as de titel “Ik ben een as” krijgt en de y-as label leeg is, dan is dit het formaat:\n\n\n\nAdd axis ticks\nU kunt as-streepjes toevoegen door axis.ticks.x of axis.ticks.y toe te voegen aan uw theme:\n\n\n\nToevoegen van annotaties\nVoeg een annotatie toe\nDe eenvoudigste manier om een tekstannotatie toe te voegen aan uw grafiek is met behulp van geom_label:\n\n\n\nDe exacte positionering van de annotatie zal afhangen van de x en y argumenten (wat een beetje lastig is!) en de tekstuitlijning, met behulp van hjust en vjust - maar meer van dat hieronder.\nVoeg regelafbrekingen waar nodig in uw label toe met n\\ en stel de regelhoogte in met lineheight.\n\n\n\nLaten we onze directe labels erin krijgen!\n\n\n\n\n\n\nTekst links en rechts uitlijnen\nDe argumenten hjust en vjust dicteren horizontale en verticale tekstuitlijning. Ze kunnen een waarde tussen 0 en 1 hebben, waarbij 0 links-uitlijnend en 1 rechts-uitlijnend is (of onder- en boven-uitlijnend voor verticale tekstuitlijning).\nVoeg labels toe op basis van jouw data\nMet de bovenstaande methode voor het toevoegen van annotaties aan uw grafiek kunt u de x- en y-coördinaten precies aangeven. Dit is erg handig als we een tekstannotatie op een bepaalde plaats willen toevoegen, maar het zou erg vervelend zijn om te herhalen.\nGelukkig, als u labels wilt toevoegen aan al uw datapunten, kunt u in plaats daarvan eenvoudigweg de positie instellen op basis van uw gegevens.\nLaten we zeggen dat we gegevenslabels willen toevoegen aan onze staafdiagram:\n\n\n\nBovenstaande code voegt automatisch één tekstlabel toe voor elk continent zonder dat we vijf keer geom_label moeten toevoegen.\n(Als je in de war raakt over waarom we de x instellen als de continenten en y als levensverwachting, als de grafiek ze andersom lijkt te tekenen, dan is dat omdat we de coördinaten van de plot hebben omgedraaid met coord_flip(), wat je kunt doen lees meer over hier.)\nVoeg links uitgelijnde labels toe aan jouw staafdiagrammen\nAls u liever links uitgelijnde labels voor uw balken toevoegt, stelt u gewoon het x argument in op basis van uw gegevens, maar specificeer dan het y argument direct, met een numerieke waarde.\nDe exacte waarde van y hangt af van het bereik van uw gegevens.\n\n\n\nVoeg een lijn toe\nVoeg een lijn toe met geom_segment:\n\n\n\nHet size-argument specificeert de dikte van de lijn.\nVoeg een gecurfte lijn toe\nVoor een gekromde lijn gebruikt u geom_curve in plaats van geom_segment:\n\n\n\nHet curvature argument bepaalt de kromming van de curve: 0 is een rechte lijn, negatieve waarden geven een linkse curve en positieve waarden geven een rechtse curve.\nVoeg een pijl toe\nEen lijn in een pijl omzetten is vrij eenvoudig: voeg gewoon het arrow argument toe aan je geom_segment of geom_curve:\n\n\n\nHet eerste argument, unit, stelt de grootte van de pijlpunt in.\nVoeg een lijn over de hele figuur toe\nDe eenvoudigste manier om een lijn over het hele perceel toe te voegen is met geom_vline(), voor een verticale lijn, of geom_hline(), voor een horizontale lijn.\nOptionele extra argumenten stellen u in staat om de grootte, kleur en het type lijn te specificeren (de standaard optie is een effen lijn).\n\n\n\nDe lijn voegt natuurlijk niet veel toe in dit voorbeeld, maar dit is handig als je iets wilt benadrukken, bijvoorbeeld een drempelwaarde of een gemiddelde waarde.\nHet is ook vooral handig omdat onze ontwerpstijl - zoals u misschien al hebt gemerkt in de grafieken op deze pagina - bestaat uit het toevoegen van een verticale of horizontale basislijn aan onze grafieken. Dit is de code om te gebruiken:\n\n\n\nWerken met kleinere figuren\nKleine, meervoudige kaarten zijn eenvoudig te maken met ggplot: het heet facetteren.\nFacetteren\nAls u gegevens hebt die u wilt visualiseren opgesplitst naar een variabele, moet u facet_wrap of facet_grid gebruiken.\nVoeg de variabele die je wilt delen aan deze regel code toe: facet_wrap( ~ variabele) `.\nEen extra argument bij facet_wrap, ncol, stelt u in staat om het aantal kolommen te specificeren:\n\n\n\n\n\n\nVrije schalen\nHet is u wellicht opgevallen dat Oceanië, met zijn relatief kleine bevolking, volledig is verdwenen in de bovenstaande grafiek.\nStandaard wordt bij facetteren gebruik gemaakt van vaste asschalen over de kleine veelvouden. Het is altijd het beste om dezelfde y-as schaal over kleine veelvouden te gebruiken, om misleiding te voorkomen, maar soms moet je deze onafhankelijk van elkaar instellen voor elk veelvoud, wat we kunnen doen door het argument schales= \"free\".\nAls je alleen de schalen voor één as wilt vrijgeven, zet je het argument op free_x of free_y.\n\n\n\n\n\n\nDoe iets anders uiteindelijk\nToename en afname van marges\nU kunt de marge rond bijna elk element van uw plot - de titel, ondertitels, legenda - of de grafiek zelf wijzigen.\nNormaal gesproken hoeft u de standaardmarges van het thema niet te wijzigen, maar als u dat wel doet, is de syntaxis theme(ELEMENT=element_text(margin=margin(0, 5, 10, 0))).\nDe getallen specificeren respectievelijk de boven-, rechter-, onder-, en linkermarge - maar u kunt ook direct aangeven welke marge u wilt wijzigen. Laten we bijvoorbeeld proberen de ondertitel een extra grote ondermarge te geven: You can change the margin around almost any element of your plot - the title, subtitles, legend - or the plot itself.\n\n\n\nHm… misschien niet.\nExporteer jouw figuur en x-as marges\nU moet wel nadenken over uw x-as margematen wanneer u een figuur produceert dat buiten de standaardhoogte in bbplot ligt, dat is 450px. Dit kan bijvoorbeeld het geval zijn als u een staafdiagram maakt met veel balken en ervoor wilt zorgen dat er wat ademruimte is tussen elke staaf en labels. Als u de marges laat zoals ze zijn voor figuren met een grotere hoogte, dan kunt u een grotere afstand tussen de as en uw labels krijgen.\nHier is een handleiding waar we aan werken als het gaat om de marges en de hoogte van uw staafdiagram (met coord_flip erop toegepast):\n\nsize\nt\nb\n550px\n5\n10\n650px\n7\n10\n750px\n10\n10\n850px\n14\n10\n\nDus wat u zou moeten doen is deze code toevoegen aan uw grafiek als u bijvoorbeeld de hoogte van je plot 650px wilde hebben in plaats van 450px.\n\n\n\nHoewel het veel minder waarschijnlijk is, maar als u het equivalent wilt doen voor een lijndiagram en het wilt exporteren op een grotere hoogte dan de standaard hoogte, moet u hetzelfde doen. Maar ook uw waarden voor t veranderen in negatieve waarden op basis van de bovenstaande tabel.\nHeroderdenen van de staven op basis van de grootte\nStandaard zal R uw gegevens in alfabetische volgorde weergeven, maar in plaats daarvan herordenen op basis van grootte is eenvoudig: gewoon reorder() om de x of y variabele leggen die u wilt herschikken, en geef aan voor welke variabele u de gegevens wilt herschikken.\nBijvoorbeeld x = reorder (country, pop). Oplopende volgorde is de standaard, maar u kunt deze veranderen in aflopende volgorde door desc() rond de variabele te plaatsen waar u de volgorde van wilt wijzigen:\n\n\n\n\n\n\nStaven met de hand herordenen\nSoms moet u uw gegevens ordenen op een manier die niet alfabetisch of gerangschikt op grootte is.\nOm deze correct te ordenen moet u de factorniveaus van uw gegevens instellen voordat u het figuur maakt.\nSpecificeer de volgorde waarin u de categorieën in het levels argument wilt afdrukken:\n\n\n\nU kunt dit ook gebruiken om de delen van een gestapelde staafdiagram opnieuw te ordenen.\nKleur staven conditioneel\nU kunt esthetische waarden zoals vulling, alpha en grootte voorwaardelijk instellen met ifelse().\nDe syntaxis is fill=ifelse(logical_condition, fill_if_true, fill_if_false).\n\n\n\n\n\n",
    "preview": "posts/2019-02-20-bbc-en-data-journalisme/bbc-en-data-journalisme_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2020-03-13T23:28:58+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-01-23-data-visualization-a-practical-introduction/",
    "title": "Data visualisatie. Een practische introductie",
    "description": "Naar aanleiding van het nieuwe boek van Kieran Healey. Data visualization/A Practical Introduction.",
    "author": [
      {
        "name": "Harrie Jonkman.",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2019-01-23",
    "categories": [],
    "contents": "\n\n\n\nIn 2014 schreeft Kieran Healey samen met James Moody in de ‘Annual Review of Sociology’ (2014. 40: 105-28) een artikel over datavisualisatie in de sociologie. Visualiseren van data verdient meer aandacht in de sociale wetenschappen. Ondanks dat er in de beginjaren van de sociologie hier de nodige aandacht voor was, zijn de gereedschappen om dit goed te maken achtergebleven. Mensen als Du Bois, geïnspireerd door expressieve schilderkunst, deden dat op een geweldige manier. Vele jaren later waren er ook mensen als Cleveland en, vooral, Tufte die hier baanbrekend werk hebben verricht. Zij lieten vooral zien hoe de visualisatie eruit zou moeten zien en hadden weer minder aandacht voor hoe het zou doen en wat daarvoor nodig is. Methodes en gereedschappen ontbraken lange tijd. In deze tijd van delen van codes en data delen is er veel meer mogelijk. Onderzoekers en publicisten kunnen nu een stap voorwaarts zetten.\nSinds dat artikel heeft Kieran Healey niet stil gezeten. Vijf jaar lang hield hij zich bezig met datavisualisatie en publiceerde hier regelmatig over en hield de ene na de andere workshop. Ikzelf hield hem de afgelopen twee jaar nauwlettend in het oog omdat ik grote waardering voor zijn werk heb. Recent verscheen zijn nieuwe boek ‘Data visualization. A practical introduction’ (Princeton and Oxford: Princeton University Press, 2019). De tekst heb ik letterlijk op internet zien ontstaan in bookdown-vorm. Het boek heb ik deze maand ontvangen. Het is een ‘must’ voor mensen die willen leren hoe je data visualiseert maar ook voor mensen die willen leren hoe je op een moderne manier met data omgaat. Een prachtig boek, ik kan dat niet voldoende benadrukken.\nIn het eerste hoofdstuk (Look at Data) kijkt Kieran Healy met ons naar data. Visualisaties zijn, volgens hem, bedoeld om naar te kijken en daarom moet je ook weten wie er naar kijkt en waarom. Visualiseren is een goede manier om data nader te onderzoeken, te begrijpen en samenhang in de data te kunnen verklaren. Figuren kunnen slecht gemaakt worden, bijvoorbeeld vanwege een slechte smaak of omdat iets niet goed te lezen is. Aan een slecht figuur kunnen ook slechte data ten grondslag liggen. Tot slot kan de grafiek je ook misleiden en kan er een gat zitten tussen de data en de esthetica. Perceptie.\nVervolgens legt hij ons in het tweede hoofdstuk (Get Started) uit hoe we grafiek gaan maken. Healey werkt met R en het pakket ggplot2 waar hij in het hele boek mee werkt. Hij laat zien hoe je met R en zijn bedieningspaneel RStudio moet werken, hoe je met RMarkdown kunt werken en hoe je een project start. Vervolgens legt hij een aantal basiszaken van R uit die je ook kunt overslaan als je deze kennis al hebt. Maar Healey is heel scherp en duidelijk en een hele goede leermeester, volgens mij, voor mensen die er weinig van weten. Maar dat is hij ook voor mensen die al meer weten en vervolgens laat hij aan het einde van het hoofdstuk de eerste, zeer eenvoudige figuur zien van de samenhang tussen wat mensen verdienen en de levensduur in de wereld.\n\n\n# A tibble: 1,704 x 6\n   country     continent  year lifeExp      pop gdpPercap\n   <fct>       <fct>     <int>   <dbl>    <int>     <dbl>\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# ... with 1,694 more rows\n\n\n\n\n(#fig:01-first_plot)Life expectancy plotted against GDP per capita for a large number of country-years.\n\n\n\nIn hoofdstuk 3 (Make a Plot) gaat hij veel uitgebreider in op hoe we een figuur maken. Belangrijk is steeds dat data heel netjes zijn opgebouwd. Als dat het geval is werkt het grafiekenprogramma ggplot2 goed. Niets voor niets is het onderdeel van het tidyverse-pakket. ggplot2 is laagsgewijs opgebouwd waarbij je steeds een aantal stappen achter elkaar moet zetten:\n1. Je vertelt ggplot()functie eerst over welke data we het hebben.\n2. Dan vertel je ggplot() welke relaties je wilt zien.\n3. Vervolgens vertel je ggplot() hoe je de relaties wilt zien.\n4. Voeg er dan nog een laag aan toe (geom) als dat nodig is en voeg die toe aan p, waar Healey de hele tijd mee werkt.\n5. Tot slot gebruik je, eventueel, nog enkele schalen, labels, titels en dergelijk. En die voeg je er aan het einde aan toe.\nJe krijgt dan zo’n commando met zo’n resultaat:\n\n\n\nFigure 1: A more polished plot of Life Expectancy vs GDP.\n\n\n\nIn het vierde hoofdstuk (Show the Right Numbers) gaat hij verder met het uitleggen van ggplot en allerlei andere zaken die er mee kunt doen. In dit hoofdstuk laat hij bijvoorbeeld zien hoe je een aantal figuren naast elkaar kunt zetten.\n\n\n\n\n\n\nEn Healey laat hele andere figuren zien, zoals hieronder, waar hij positieve en negeatieve verschillen laat zien tussen de USA en de OECD-landen.\n\n\n\nIn het vijfde hoofdstuk (Graph Tables, Add Labels, Make Notes) gaat hij verder met het maken van een figuur, maar hij laat dan veel meer mogelijkheden zien. Het ggplot2-pakket is, zoals gezegd, onderdeel van Wickhams tidyverse-pakket dat je in staat stelt de data op een eenvoudige en logische manier aan te passen, hier dus voordat je de grafiek maakt.\nEerst pas je de dataset aan:\n\n\n\n\n\n# A tibble: 4 x 2\n  bigregion total\n  <fct>     <dbl>\n1 Northeast   100\n2 Midwest     101\n3 South       100\n4 West        101\n\nVervolgens maak je de grafiek en zet je preferenties van religie naast elkaar.\n\n\n\nEn dan verdeel je het duidelijk per regio en trek je die regio’s uit elkaar.\n\n\n\nIn dit hoofdstuk laat hij ook nog enkele aanvullingen zien waarmee je kunt werken. Je kunt er een tekst inzetten.\n\n\n\nOf een tekst en een blok om iets extra’s aan te geven.\n\n\n\nHij geeft ook aan hoe je de labels eraan zet.\n\n\n\nWanneer je het maken van een grafiek en wat daarbij komt kijken onder de knie hebt, gaat hij in de twee volgende hoofdstukken in op specieke onderwerpen. Modelleren is een belangrijk onderdeel van omgaan met data. Ook modellen kun je visualiseren en dat werken met modellen is voor hem het onderwerp van het zesde hoofdstuk (Work with Models). Hieronder zie je bijvoorbeeld drie statistische modellen netjes op een rij gezet:\n\n\n[1] \"#E41A1C\" \"#377EB8\" \"#4DAF4A\"\n\n\nbroom is een R-pakket waar je op een goede en eenvoudige manier mee kunt modelleren. Hiermee krijg je schattingen en intervallen, maar die kun je ook weer visualiseren. Hoe je dat doet, zie je hieronder.\n\n\nClasses 'tbl_df', 'tbl' and 'data.frame':   1704 obs. of  6 variables:\n$ country : Factor w/ 142 levels \"Afghanistan\",..: 1 1 ...\n$ continent: Factor w/ 5 levels \"Africa\",\"Americas\",..: 3 3 ...\n$ year : int 1952 1957 ...\n$ lifeExp : num 28.8 ...\n$ pop : int 8425333 9240934 ...\n$ gdpPercap: num 779 ...\n\n\n\n\n\n\nCall:\nlm(formula = lifeExp ~ gdpPercap + pop + continent, data = gapminder)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-49.161  -4.486   0.297   5.110  25.175 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       4.781e+01  3.395e-01 140.819  < 2e-16 ***\ngdpPercap         4.495e-04  2.346e-05  19.158  < 2e-16 ***\npop               6.570e-09  1.975e-09   3.326 0.000901 ***\ncontinentAmericas 1.348e+01  6.000e-01  22.458  < 2e-16 ***\ncontinentAsia     8.193e+00  5.712e-01  14.342  < 2e-16 ***\ncontinentEurope   1.747e+01  6.246e-01  27.973  < 2e-16 ***\ncontinentOceania  1.808e+01  1.782e+00  10.146  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.365 on 1697 degrees of freedom\nMultiple R-squared:  0.5821,    Adjusted R-squared:  0.5806 \nF-statistic: 393.9 on 6 and 1697 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\n# A tibble: 7 x 5\n  term              estimate std.error statistic p.value\n  <chr>                <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)          47.8      0.34     141.         0\n2 gdpPercap             0        0         19.2        0\n3 pop                   0        0          3.33       0\n4 continentAmericas    13.5      0.6       22.5        0\n5 continentAsia         8.19     0.570     14.3        0\n6 continentEurope      17.5      0.62      28.0        0\n7 continentOceania     18.1      1.78      10.2        0\n\n\n\n\n\n\n# A tibble: 7 x 7\n  term         estimate std.error statistic p.value conf.low conf.high\n  <chr>           <dbl>     <dbl>     <dbl>   <dbl>    <dbl>     <dbl>\n1 (Intercept)     47.8      0.34     141.         0    47.2      48.5 \n2 gdpPercap        0        0         19.2        0     0         0   \n3 pop              0        0          3.33       0     0         0   \n4 continentAm~    13.5      0.6       22.5        0    12.3      14.6 \n5 continentAs~     8.19     0.570     14.3        0     7.07      9.31\n6 continentEu~    17.5      0.62      28.0        0    16.2      18.7 \n7 continentOc~    18.1      1.78      10.2        0    14.6      21.6 \n\n\n\n\n\n\n\nLanden van Europa, provincies van Nederland of steden van een provincie kun je goed visualiseren. Soms kun je gegevens van die landen, provincies of steden ook goed zichtbaar maken. Dat kan ook met ggplot2 en daarover schrijft Healey in hoofdstuk 7 (Draw maps). Hieronder zie je bijvoorbeeld het percentage ‘black Americans’ per countie afgebeeld.\n\n\n       long      lat group order  region subregion\n1 -87.46201 30.38968     1     1 alabama      <NA>\n2 -87.48493 30.37249     1     2 alabama      <NA>\n3 -87.52503 30.37249     1     3 alabama      <NA>\n4 -87.53076 30.33239     1     4 alabama      <NA>\n5 -87.57087 30.32665     1     5 alabama      <NA>\n6 -87.58806 30.32665     1     6 alabama      <NA>\n\n[1] 15537     6\n\n\n\n\n\n\n\nFigure 2: Mapping the results\n\n\n\n\n\n\n\n\n\nFigure 3: Election 2016 by State\n\n\n\nOp het verfijnen van de figuren gaat Healey in het laatste en achtste hoofdstuk (Refine Your Plots) in. Dat komt natuurlijk vooral in de laatste fase van het maken van figuren aan de orde wanneer de figuren klaar moeten worden gemaakt om gepubliceerd te worden of als je een speciale aanpassing in je hoofd hebt. Hier bespreekt hij kleurengebruik en gaat hij in op het gebruik van een bepaald thema (zeg je wilt de figuur in de stijl van de Economist hebben) of wanneer je een van een slecht figuur een goed figuur wilt maken, zoals hieronder:\n\n\n\nFigure 4: Redrawing as a connected scatterplot.\n\n\n\nEn dan hier de betere figuur:\n\n\n\nFigure 5: Plotting the ratio of revenue to employees against time.\n\n\n\nHealey’s boek is een prachtig boek. Niet alleen omdat hij ons goed naar figuren laat kijken en ons leert hoe je die moet maken. Met Data Visualization leert hij ons hoe je op een moderne manier met data om kunt gaan: elegant, logisch en coherent. Naast zijn boek heeft hij een groot aantal codes geschreven en beschikbaar gesteld voor vrij gebruik (zie zijn website: https://kieranhealy.org/ of zijn codes op github: https://github.com/kjhealy). Hij laat je heel goed zien hoe hij alles heeft gemaakt en het klopt allemaal. Zijn boek zal ik blijven lezen en zijn werk zal ik blijven volgen. Mijn petje af Kieran, en heel hartelijke dank voor al die dingen die je gedaan hebt en met anderen deelt. Mij heb in in ieder geval geïnspireerd om er een cursus rondom op te bouwen. Binnenkort volgt hier de link van de cursus. Binnenkort staat hier ook de link naar de code van dit document (.rmd).\n\n\n",
    "preview": "posts/2019-01-23-data-visualization-a-practical-introduction/data-visualization-a-practical-introduction_files/figure-html5/01-first_plot-1.png",
    "last_modified": "2020-03-13T23:28:58+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-23-statistisch-omdenken/",
    "title": "Statistisch omdenken",
    "description": "Over 'Statistical rethinking' van Richard McElreath (2016).",
    "author": [
      {
        "name": "Harrie Jonkman met dan aan Solomon Kurz.",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2018-12-23",
    "categories": [],
    "contents": "\nOver Bayesiaanse statistiek zijn ondertussen verschillende boeken geschreven die jou leren om hiermee te werken. Het boek van Nzoufras (Bayesian modeling using WinBUGS), Lunn et al. (The BUGS Book. A Practical Introduction to Bayesian Analysis), Kéry (Introduction to WinBUGS for ecologists), Broemeling (Bayesian Methods in Epidemiology) en Cowles (Applied Bayesian Statistics) introduceren niet alleen het concept maar laten ook zien hoe je hier als onderzoeker mee kunt werken. Deze wetenschappers gebruiken het programma WinBUGS of OpenBUGS. Jim Alberts (Bayesian Computation with R) en John Kruschke’s (Doing Bayesian Data Analysis) zijn ook wetenschappelijke boeken die deze vorm van onderzoek introduceren, maar zij beiden werken met R. In de serie cursusboeken Bayesiaanse statistiek is het boek van Richard McElreath (Statistical Rethinking. A Bayesian Course with Examples in R and Stan) het meest recent. Het is een echt cursusboek waar je onder begeleiding of zelf mee aan de slag kunt. Het boek zit vol met codes want McElreath is ervan overtuigd dat als je met deze analysetechniek wilt kunnen werken, je ook moet weten hoe je er mee kunt werken. Interessant zoals hij vaststelt dat tegenwoordig studenten computerwijs maar dat zij van computercodes zelf weer betrekkelijk weinig weten. Om daarmee verder te komen, is het nodig dat ze dat wel leren. McElreaths boek is een oefenboek en de lezer wordt expliciet uitgenodigd problemen met de computer uit te werken. Om ermee te werken moet je R en RStudio op de computer installeren, het pakket binnen halen en je moet rstan (een C++ compiler) installeren. McElreath levert er de codes bij en via internet kun je ook nog zijn interesssante colleges volgen. Allemaal geen eenvoudige kost maar degene die er de moeite voor neemt krijgt veel terug voor zijn of haar inspanningen.\nEen algemene introductie\nDe eerste drie hoofdstukken van Statistical rethinking zijn een algemene introductie op Bayesiaanse statistiek. Wetenschappers maken wetenschappelijke modellen die hij ziet als golems, Joodse kleibeeldjes die na de middeleeuwen voor de waarheid stonden en deden wat hen werd opgedragen, althans dat dachten mensen. Wetenschappelijke modellen zijn ook constructen die voor de waarheid staan en een duidelijk doel hebben. Ze berekenen zaken voor ons, voeren indrukwekkende calculaties uit en vinden patronen die anders voor ons onder de oppervlakte zouden blijven. Volgens McElreath is het huidige statistisch instrumentarium te beperkt om antwoorden te geven op complexere zaken waar we tegenwoordig mee worden geconfronteerd. Wetenschap kan deductieve falsificatie, waar wetenschappers na Popper steeds op gewezen hebben, nauwelijks waar maken omdat hypotheses toch iets anders zijn dan modellen die wel te onderzoeken zijn. Modellen kun je toetsen en hypothesen eigenlijk niet. Hoe je de modellen onderzoekt, doet er dan wel toe. In die nieuwe gereedschapskist die ons ter beschikking staat, zitten voor McElreath drie hele duidelijke gereedschappen:\n- Baysiaanse data analyse waarbij het gaat om waarschijnlijkheid en waarbij je het aantal mogelijkheden die volgens onze aannames kunnen gebeuren steeds moet tellen. Het is de meest logische en consistente manier om met informatie om te gaan.\n- Multilevel modellen waarbij clusters of groepen worden onderscheiden en waarbij steeds andere waarden kunnen gelden juist omdat ze zo bij elkaar horen.\n- Model vergelijkingen en informatiecriteria waarbij het gaat om criteria waarmee vergelijkingen worden gemaakt en accuratesse wordt ingeschat.\nMcElreath begint zijn boek heel eenvoudig. Stel, zo legt hij voor, dat we plastic aardbol hebben (een opblaasbol, zeg maar) en deze een aantal keren in de lucht gooien. We vangen het met een vinger op en kijken wat er onder die vinger zit (water of land). Stel dat we dat negen keer achter elkaar doen en dat deze activiteit dan de volgende gegevens oplevert.\n\n\n# A tibble: 9 x 1\n  toss \n  <chr>\n1 w    \n2 l    \n3 w    \n4 w    \n5 w    \n6 l    \n7 w    \n8 l    \n9 w    \n\nStel dat water het succes is en we negen keer gooien en laten we uitdrukken wat we gevonden hebben:\n\n\n# A tibble: 9 x 3\n  toss  n_trials n_success\n  <chr>    <int>     <int>\n1 w            1         1\n2 l            2         1\n3 w            3         2\n4 w            4         3\n5 w            5         4\n6 l            6         4\n7 w            7         5\n8 l            8         5\n9 w            9         6\n\nAls we dat dan vervolgens ook nog in tekeningen uitdrukken, zien die er achtereenvolgens zo een beetje uit:\n\n\n\nDe Bayesiaanse techniek bestaat steeds uit een vast aantal componenten. Allereerst is er de likelihood en dat zijn de data waar je mee te maken hebt. Hier boven is dat bijvoorbeeld het aantal keren dat je gegooid hebt en de keren dat de vinger water raakt. Dan heb je de parameters waarmee je werkt (bv. aantal keren water en de kans op water). Dan heb je de prior, de inschatting die je van te voren maakt. Als je niks weet kun je zeggen dat de kans op water=.5 (net zo groot als de kans op land). Maar als je weet dat er meer water is dan land kun je zeggen dat de kans op water tussen .5 en .9 ligt. Tot slot is er de posterior, de combinatie van alle drie (likelihood, parameters en prior) en veelal uitgedrukt wordt als:\n\\[\\text{Posterior} = \\frac{\\text{Likelihood} \\times \\text{Prior}}{\\text{Average Likelihood}}\\]\nNu zijn er verschillende technieken om deze posterior uit te rekenen. Om met complexe modellen te kunnen werken,worden MCMC-technieken gebruikt en daar maakt Richard Elreath in zijn boek ruim gebruik van. Als het model dat is gekozen een posterior distributie maakt, is eigenlijk het werk gedaan en kunnen hiermee verschillende vragen worden beantwoord als:\n- wat is de kans dat een bepaalde waarde voorkomt?\n- wat is de kans dat de waarde tussen 50% en 80% in ligt?\n- welke waarde heeft de hoogste posterior waarschijnlijkheid?\netc. Met Bayesiaanse technieken maak je samples die je weer gebruikt om intervallen te definiëren, om puntschattingen te maken, om voorspellingen te doen, om de gegevens op een andere manier te simuleren of wat je ook maar wilt.\nEenvoudige Regressies\nNadat McElreath in het eerste deel van het boek de basis heeft uitgelegd, begint hij in het tweede deel verschillende modellen uit te leggen. Hij begint met betrekkelijk simpele lineaire modellen. Stel dat we een dataset nemen die in het pakket zit. De meest simpele vorm druk je zo uit:\n\\[\n\\begin{eqnarray}\n\\text{outcome}_i & \\sim & \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i & = & \\beta \\times \\text{predictor}_i \\\\\n\\beta & \\sim & \\text{Normal}(0, 10) \\\\\n\\sigma & \\sim & \\text{HalfCauchy}(0, 1)\n\\end{eqnarray}\n\\] Stel dat we een dataset nemen die in het pakket zit, bv.;\n\n\n\nWe openen een ander pakket, waar ik in de conclusie op terug kom.\n\n\n\nWe kijken eens even hoe het data bestand eruit ziet:\n\n\n'data.frame':   544 obs. of  4 variables:\n $ height: num  152 140 137 157 145 ...\n $ weight: num  47.8 36.5 31.9 53 41.3 ...\n $ age   : num  63 63 65 41 51 35 32 27 19 54 ...\n $ male  : int  1 0 0 1 0 1 0 1 0 1 ...\n\nDan halen we alleen de variabele lengte eruit:\n\n\n   height\n1 151.765\n2 139.700\n3 136.525\n4 156.845\n5 145.415\n6 163.830\n\nVervolgens gebruiken we alleen de data van de volwassenen:\n\n\n\nHieronder draaien we dan een analyse. En dit wordt steeds op dezelfde manier gedefinieerd. Je definieert het model, dan gebruik je brm en zegt welke data je gebruikt en welke statistische familie je gebruikt, vervolgens definieer je het statistische model, je definieert de parameters die gebruikt, je definieert het aantal iteraties en hoeveel je daarbij als warming up gebruikt, je definieert het aantallen kettingen en het aantal computerdelen.\n\n\n\nLaten we zien wat het grafische oplevert:\n\n\n\nLaten we de gegevens ook in een tabel samenvatten:\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 1 \n   Data: d2 (Number of observations: 352) \nSamples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup samples = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat\nIntercept   154.61      0.42   153.82   155.47       3481 1.00\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat\nsigma     7.75      0.30     7.19     8.34       3570 1.00\n\nSamples were drawn using sampling(NUTS). For each parameter, Eff.Sample \nis a crude measure of effective sample size, and Rhat is the potential \nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nAls we aan deze simpele regressie nog een andere, onafhankelijke variabele toevoegen (gewicht), krijgen we de volgende analyse:\n\n\n\nOok hier eerst het grafische:\n\n\n\nEn vervolgens de gegevens voor de tabel:\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: height ~ 1 + weight \n   Data: d2 (Number of observations: 352) \nSamples: 4 chains, each with iter = 41000; warmup = 40000; thin = 1;\n         total post-warmup samples = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat\nIntercept   113.87      1.95   109.96   117.68        770 1.00\nweight        0.91      0.04     0.82     0.99        757 1.00\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat\nsigma     5.11      0.20     4.73     5.51       2485 1.00\n\nSamples were drawn using sampling(NUTS). For each parameter, Eff.Sample \nis a crude measure of effective sample size, and Rhat is the potential \nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nNadat McElreath heeft laten zien hoe het werkt met eenvoudige regressies (inclusief interacties dat in het volgende hoofdstuk aan de orde komt), gaat hij in op het gebruik van informatie criteria. Dat zijn criteria waarmee je modellen met elkaar kunt vergelijken. De DIC en WAIC zijn informatiecriteria die in deze techniek het beste te gebruiken zijn.\nLaten we eens zien. Haal een dataset binnen, bv. de dataset milk die in het pakket rethinking zit. Daar gebruiken we alleen de complete gegevens van en we passen de variabele neocortex aan.\n\n\n\nHoeveel dimensies d zitten er in:\n\n\n[1] 17  9\n\nLaad dan weer het pakket brms.\n\n\n\nWe onderzoeken vier verschillende kcal.per.g modellen .\n\n\n\nVervolgens kun je deze vier modellen met elkaar vergelijken en de modellen met de laagste waic-score laat de beste balans zien tussen eenvoud en complexiteit.\n\n\n                WAIC   SE\nb6.11          -8.73 3.70\nb6.12          -7.45 3.19\nb6.13          -8.91 4.24\nb6.14         -16.94 5.20\nb6.11 - b6.12  -1.28 1.16\nb6.11 - b6.13   0.17 2.33\nb6.11 - b6.14   8.21 4.94\nb6.12 - b6.13   1.45 2.99\nb6.12 - b6.14   9.49 5.05\nb6.13 - b6.14   8.04 3.55\n\nAndere regressies\nVoor Bayesiaanse technieken worden tegenwoordig MCMC-technieken gebruikt die met ingewikkelde random berekeningen als het ware het complexe geheel kunnen opsplitsen in kleinere eenheden. Door dit heel vaak te draaien kunnen de posterior samples worden gemaakt. Met Gibbs en Metropolitan wordt al sinds de negentiger jaren gewerkt. Gelman en anderen hebben de laatste jaren de Hamiltonian Monte Carlo ontwikkelt die nog weer beter werkt in complexe modellen (Stan). Stan heeft weer een eigen taal. Maar met McElreaths MAP en met Brueckners brms pakket kan hier makkelijker mee worden gewerkt. Het betrekkelijke ingewikkelde HMC werkt in deze pakketten achter de coulissen. Laten we een voorbeeld nemen:\nWe laden eerst de rugged data in.\n\n\n\nDan gaan we over naar brms.\n\n\n\nWe doen wat data manipulatie.\n\n\n\nVervolgens werken we met HMC en we zien bij het brm-pakket ook weer dezelfde volgorde. Eerst het model definiëren, dan brm en vaststellen welke data je gebruikt en welke statistische familie. Dan schrijf je het model uit en vervolgens definieer je de priors.\n\n\nSAMPLING FOR MODEL 'ae400a3fc447dbdc9dc2cf4b2f0adb9b' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.141 seconds (Warm-up)\nChain 1:                0.105 seconds (Sampling)\nChain 1:                0.246 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'ae400a3fc447dbdc9dc2cf4b2f0adb9b' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.134 seconds (Warm-up)\nChain 2:                0.107 seconds (Sampling)\nChain 2:                0.241 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'ae400a3fc447dbdc9dc2cf4b2f0adb9b' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0.001 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 10 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.112 seconds (Warm-up)\nChain 3:                0.12 seconds (Sampling)\nChain 3:                0.232 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'ae400a3fc447dbdc9dc2cf4b2f0adb9b' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.109 seconds (Warm-up)\nChain 4:                0.113 seconds (Sampling)\nChain 4:                0.222 seconds (Total)\nChain 4: \n\nEn dan, in dit geval alleen, de resultaten voor de tabel.\n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: log_gdp ~ 1 + rugged + cont_africa + rugged:cont_africa \n   Data: dd (Number of observations: 170) \nSamples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup samples = 4000\n\nPopulation-Level Effects: \n                   Estimate Est.Error l-95% CI u-95% CI Eff.Sample\nIntercept              9.22      0.14     8.95     9.49       2923\nrugged                -0.20      0.08    -0.35    -0.05       2603\ncont_africa           -1.94      0.23    -2.39    -1.50       2523\nrugged:cont_africa     0.39      0.13     0.13     0.65       2377\n                   Rhat\nIntercept          1.00\nrugged             1.00\ncont_africa        1.00\nrugged:cont_africa 1.00\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat\nsigma     0.95      0.05     0.85     1.06       3900 1.00\n\nSamples were drawn using sampling(NUTS). For each parameter, Eff.Sample \nis a crude measure of effective sample size, and Rhat is the potential \nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nAls we met deze MCMC-techniek kunnen werken, kunnen we vervolgens ook andere statistische modellen (GLM-modellen) draaien. Denk aan exponentiële, gamma en poisson distributies en daarmee zijn andere regressies (andere families) mogelijk.\nLaten we een logistische regressie als voorbeeld nemen en de chimpanzees data laden.\n\n\n\nDan weer naar brms.\n\n\n\nEerst een logistische regressie met alleen een intercept.\n\\[\n\\begin{eqnarray}\n\\text{pulled_left}_i & \\sim & \\text{Binomial} (1, p_i) \\\\\n\\text{logit} (p_i) & = & \\alpha \\\\\n\\alpha & \\sim & \\text{Normal} (0, 10)\n\\end{eqnarray}\n\\] En dan weer het model draaien.\n\n\n\nLaten we alleen eens naar het intercept kijken.\n\n\n          Estimate Est.Error Q2.5 Q97.5\nIntercept     0.32      0.09 0.14   0.5\n\nDeze resultaten kunnen ook omgevormd worden tot de logistische functie.\n\n\n[1] 0.5448789 0.6130142\n\n           Estimate Est.Error      Q2.5     Q97.5\nIntercept 0.5786748 0.5228834 0.5348199 0.6226705\n\nVervolgens voegen we aan het logistisch model enkele predictoren aan toe en draaien nog eens twee modellen.\n\n\n\nEn dan vergelijken we de drie modellen die we tot dan toe hebben gemaakt met elkaar:\n\n\n\n\n\n                WAIC   SE\nb10.1         688.00 7.06\nb10.2         680.38 9.36\nb10.3         682.60 9.46\nb10.1 - b10.2   7.62 6.20\nb10.1 - b10.3   5.40 6.28\nb10.2 - b10.3  -2.22 0.81\n\nOp dezelfde manier werkt McElreath ook andere count-regressies en dergelijke uit, waar we hier verder niet op ingaan.\nMultilevel en andere zaken\nWanneer hij heeft uitgelegd hoe Bayesiaanse analyse werkt en je ook kunt werken met informatiecriteria, komt hij bij de de multilevel modellen uit (zijn derde uitgangspunt). Multilevel analyse is toch wel zo’n beetje de ‘state of art’ in regressie analyses omdat het enkele voordelen heeft, waaronder:\n- het maakt betere schattingen over herhaalde metingen;\n- het verbetert de schattingen als er verschillen zijn tussen subsamples en; - het kan variaties en gemiddelden over deze subsamples beter inschatten en het voorkomt zo versimpelen.\nOm dit duidelijk te maken onderzoeken we de overleving van kikkers in verschillende omgevingen en definiëren enkele modellen. We werken met de reedfrogs data van het rethinking pakket.\n\n\n\nWe laten rethinking los en laden brms.\n\n\n\nWat zit er in het reedfrogs-data bestand? Laten we dat eens onderzoeken we met tidyverse pakket.\n\n\nObservations: 48\nVariables: 5\n$ density  <int> 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, ...\n$ pred     <fct> no, no, no, no, no, no, no, no, pred, pred, pred...\n$ size     <fct> big, big, big, big, small, small, small, small, ...\n$ surv     <int> 9, 10, 7, 10, 9, 9, 10, 9, 4, 9, 7, 6, 7, 5, 9, ...\n$ propsurv <dbl> 0.90, 1.00, 0.70, 1.00, 0.90, 0.90, 1.00, 0.90, ...\n\nWe maken de tank variabele (hogere orde variabele, de omgevingen van de kikker).\n\n\n\nHier is de formule voor het model zonder multilevel karakter nog.\n\\[\n\\begin{eqnarray}\n\\text{surv}_i & \\sim & \\text{Binomial} (n_i, p_i) \\\\\n\\text{logit} (p_i) & = & \\alpha_{\\text{tank}_i} \\\\\n\\alpha_{\\text{tank}} & \\sim & \\text{Normal} (0, 5)\n\\end{eqnarray}\n\\]\nHier is de code daarvan:\n\n\n\nVervolgens is hier het multilevel model\n\\[\n\\begin{eqnarray}\n\\text{surv}_i & \\sim & \\text{Binomial} (n_i, p_i) \\\\\n\\text{logit} (p_i) & = & \\alpha_{\\text{tank}_i} \\\\\n\\alpha_{\\text{tank}} & \\sim & \\text{Normal} (\\alpha, \\sigma) \\\\\n\\alpha & \\sim & \\text{Normal} (0, 1) \\\\\n\\sigma & \\sim & \\text{HalfCauchy} (0, 1)\n\\end{eqnarray}\n\\]\nEn dat multilevel model specificeer je weer zo (inclusief hyperparameter tank:\n\n\n\nLaten we de twee modellen (geen en wel multilvel) eens naast elkaar zetten:\n\n\n                WAIC   SE\nb12.1         201.06 9.23\nb12.2         200.53 7.21\nb12.1 - b12.2   0.53 4.45\n\nNaast dat intercept model laat je ook zien dat er verschillen in de slope kunnen zitten. Want qua drukte zijn café’s niet alleen verschillend van elkaar, ook is de drukte over de dag heen (drukte*tijdstip) verschillend. Ook dat kun je in jouw modellen op nemen.\nTot slot\nMet het OpenBugs programma weet ikzelf goed te werken en ook werken met het Bayesiaanse MLwiN-deel gaat mij goed af. De laatste jaren wordt er steeds weer met steeds beterem, nieuwe programma’s gewerkt (zoals nu met Stan). Voor mensen die dagelijks met deze programma’s werken is het geen probleem om zich dat eigen te maken. Ikzelf werk er soms mee maar niet dagelijks en dan is het wel een grote inspanning om bij te blijven. Ik heb het idee dat de ontwikkelaars daar niet altijd bij stil staan. In dit blog heb ik gebruik gemaakt van het nieuwe brms-programma van Bürkner. Salomon Kurz heeft dit heel goed voor het boek van McElreath verwerkt (https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/). Toch heb ik van het boek van Richard McElreath heel veel geleerd en ik raad het mensen aan het te lezen.\n\n\n",
    "preview": "posts/2018-12-23-statistisch-omdenken/statistisch-omdenken_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2020-03-13T23:28:57+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-11-26-communiceren-met-rmarkdown/",
    "title": "Communiceren met RMarkdown",
    "description": "RMarkdown is de nieuwe manier om diverse wetenschappelijke producten te delen met anderen. Het kan op verschillende manieren gereproduceerd worden en het kan de opbrengsten aantrekkelijk communiceren naar de buitenwereld. Hier een introductie op de werkwijze en enkele mogelijke producten.",
    "author": [
      {
        "name": "M. Schmidt en bewerkt door Harrie Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2018-11-26",
    "categories": [],
    "contents": "\n\n\n\n\nVan \\(\\Latex\\) naar RMarkdown\nEerder schreef ik een snelle introductie op enkele algemene kenmerken van LaTeX, een opensource software syteem om verschillende soorten documenten te zetten. Dit programmma wordt vooral gebruikt voor maken van wetenschappelijke documenten. Die introductie is hier  te vinden. Daarin laat ik zien hoe \\(\\Latex\\) werkt en welke verschillende soorten documenten je ermee kunt maken (waaronder artikel, boek, rapport, een poster, een proefschrift, een presentatie). Verder kun je hier \\(\\Latex\\)-tutorial ook informatie vinden en hier Snel overzicht.\n\\(\\Latex\\) werkt met veel verschillende commando’s en je moet de tijd nemen dit te leren. Op internet is overigens wel goede informatie te vinden en de meeste problemen kun je zelf oplossen. De laatste jaren vinden er binnen het programma R veel ontwikkelingen plaats die het maken van wetenschappelijke documenten vergemakkelijken. De ontwikkelingen vallen onder de term RMarkdown dat in 2015 met de introductie van het knitr-pakket werd geïntroduceerd. Hier wordt binne R gebruik gemaakt van de Markdown taal waarmee technische documenten betrekkelijk eenvoudig te maken zijn. Het pakket maakt het mogelijk om verschillende soorten documenten te maken (waaronder pdf, html en word). Tot slot maakt RMarkdown het mogelijk om tekst (inclusief bv. grafieken, tabellen en referentie) en analyses in een keer te draaien. Met RMarkdown, knitr en alles wat hier aan vastzit kun je onder anderen:\n- Een goed uitziend wetenschappelijk document maken in verschillende formats tegelijkertijd (pdf, html en word);\n- Kun je met notebooks werken waarin naast tekst analyses zijn opgenomen;\n- Kun je nieuwe vormen van presentaties voorbereiden;\n- Kun je dashboards maken waarin informatie overzichtelijk, aantrekkelijk, flexibel en interactief wordt gepresenteerd;\n- Kun je interactieve toepassingen maken bijvoorbeeld door de inzet van Shiny; - Kun je wetenschappelijke artikelen maken; - Kun je boeken zelf maken; - KUn je een blog en website maken.\nEen uitgebreide gids hierover vind je hier RMarkdown: The Definitive Guide. En een handleiding vind je hier Cheat sheet RMarkdown.\nInstallatie\nOm met onderstaande te kunnen werken moet je in ieder geval R installeren R alsmede RStudio RStudio. Om pdf te draaien moet je ook Latex op je computer installeren. Heb je dat niet kun je ook binnen R/RStudio het pakket tinytex. Als je R en RStudio hebt binnengehaald moet je de pakketten knitr en rmarkdown binnenhalen. Voor achtergrondinformatie over R, RStudio en RMarkdown vind je hier RRStudioRMarkdowninformatie\nEerder schreef ik al uitgebreid over Reproducable Research en de workshop die Schmidt op 11 Mei 2016 en er waren wat aanvullende materialen beschikbaar zie post Reproducable Research.Delen daarvan breng ik hier nog eens onder de aandacht en ik update de informatie.\nInstructies bij het installeren\nVoordat je aan het werk gaat, zorg ervoor dat je het volgende hebt gedaan:\nOpen RStudio.\nInstalleer en download het devtools R pakket door het volgende commando te runnen.\n\n\ninstall.packages(\"devtools\") # Nodig voor deze sessie\nlibrary(\"devtools\")          # Nodig voor deze sessie   \n\nCheck of je de goede versie hebt van R en RStudio door devtools::session_info() in de R console te draaien.\nHier geeft devtools:: aan om de session_info() functie in R te gebruiken ipv het devtools pakket en de sessionInfo() functie binnen het utils pakket. Het runnen van devtools::session_info() stelt ons in staat de versie van R en RStudio vast te stellen.\nHeb je de volgende versie van R en RStudio?\nR: Versie 3.3.0 (2016-05-03)\nRStudio: 0.99.1172\nZo ja dan kun je van start gaan!\nZo nee dan heb je nieuwe versies van R en RStudio nodig, volg dan Setup in dit document.\n\nInstalleer vervolgens enkele R pakketten die je nodig hebt.\n\n\n## Installeer de goede pakketten\ninstall.packages(\"rmarkdown\")  # Dit zorgt voor koele dynamische documenten\ninstall.packages(\"knitr\")      # Hier kun je R code Chunks mee runnen\ninstall.packages(\"ggplot2\")    # Voor het plotten van mooie figuren\ninstall.packages(\"DT\")         # Om interactieve HTML tabellen te maken\n\n\n\n## Deze pakketten ook laden om er zeker van te zijn dat je de goede pakketten hebt\nlibrary(\"rmarkdown\")           # Dit zorgt voor koele dynamische documenten\nlibrary(\"knitr\")               # Hier kun je R code Chunks mee runnen\nlibrary(\"ggplot2\")             # Voor het plotten van mooie figuren\nlibrary(\"DT\")                  # Om interactieve HTML tabellen te maken\n\nAls je de pakketten zonder fouten hebt geladen, kun je beginnen!\nGoede bronnen\nDeze tutorial kon niet samengesteld worden zonder onderstaande goede bronnen:\nDe RMarkdown website van RStudio.\nDr. Yuhui Xie’s boek: Dynamic Documents with R and Knitr 2nd Edition [@Xie2015] en zijn Knitr website.\nHEEL VEEL DANK aan Dr. Xie voor het schrijven van het knitr pakket!!\n\nDr. Karl Broman’s “Knitr in a Knutshell”.\nCheatsheets released by RStudio.\nSinds 2016 zijn er meer interessante documenten uitgekomen: 1. Het standaardboek over RMarkdown RMarkdown: The Definitive Guide 2. Christopher Gandrud, Reproducable Research with R and RStudio.\nDynamische documenten\nLiterate programming, zoals dat in het Engels wordt genoemd, is het basisidee achter dynamische documenten en is geintroduceerd door Donald Knuth in 1984. Oorspronkelijk om de broncode en de bijbehorende software documentatie samen te brengen. Tegenwoordig creeren we dynamische documenten waarin het programma of de analyse code samen draaien om tot ‘outputs’ te komen (bv. tabellen, plots, modellen, etc) die worden uitgelegd via narratief schrijven.\nTraditioneel gebruikten mensen commentaren om het verhaal in de code file kwijt te kunnen raken (voor R zou dat een .R file zijn). Deze file zou het volgende in kunnen houden:\n\n\n# Titel:  Relatie tussen Autogewicht en Gasefficientie/of-verbruik\n# Door :  Harrie Jonkman  \n# Datum:  11 Mei 2016\n\n# Ik verspel dat er een relatie is tussen het gewicht van de auto en de afstand die met de brandstof afgelegd kan worden.  \n# Dat test ik met een lineaire analyse van de ''mtcarsdataset' als onderdeel van de R datasets\n\n# Hoe zien de data eruit?\n#datatable(mtcars) # Interactieve tabel \n\n# Is er een relatie tussen het gewicht en de afstand per, in dit geval, gallon?\nlm_mpg <- lm(mpg ~ wt, data = mtcars) # Run het lineaire model dat mpg voorspelt op basis van wt\ncoef_lm_mpg <- coef(summary(lm_mpg))  # Haal de coefficienten eruit voor de tabel die komt \nkable(coef_lm_mpg)                    # Maak een niet-interactieve tabel - een functie in knitr\n\n# Plot de relatie tussen gewicht en afstand in mijl per gallon   \nplot <- ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() + # \n  geom_smooth(method = \"lm\") + theme_bw() +           # Maak een lineair model en maak het zwart en wit\n  xlab(\"Weight (1000lbs)\") + ylab(\"Miles per Gallon\") # Voeg tekst aan de assen toe\n\n\n# Het lijkt erop dat met een toename van 1000 pounds er een afname is van brandstof gebruik met 5.34 mijl per gallon\n# Het eind\n\nDe gebruiker zal de commentaren lezen en de codes zelf runnen.\nEchter, ‘literate programming’ stelt ons in staat de code te runnen en de resultaten te beschrijven, allemaal in een document dat we kunnen delen. We zouden bijvoorbeeld het volgende kunnen doen:\n``` Relatie tussen gewicht van de auto en het brandstofverbruikDoor: Harrie JonkmanDatum: 27 november 2018\nIk voorspel dat er een relatie is tussen het gewicht en de afstand die met de brandstof kan worden afgelegd. Ik test dat met een lineair model op een dataset in R en zet dit als volgt in het programma.\n\n\n# Hoe zien de data eruit?\ndatatable(mtcars) # Interactieve tabel \n\n{\"x\":{\"filter\":\"none\",\"data\":[[\"Mazda RX4\",\"Mazda RX4 Wag\",\"Datsun 710\",\"Hornet 4 Drive\",\"Hornet Sportabout\",\"Valiant\",\"Duster 360\",\"Merc 240D\",\"Merc 230\",\"Merc 280\",\"Merc 280C\",\"Merc 450SE\",\"Merc 450SL\",\"Merc 450SLC\",\"Cadillac Fleetwood\",\"Lincoln Continental\",\"Chrysler Imperial\",\"Fiat 128\",\"Honda Civic\",\"Toyota Corolla\",\"Toyota Corona\",\"Dodge Challenger\",\"AMC Javelin\",\"Camaro Z28\",\"Pontiac Firebird\",\"Fiat X1-9\",\"Porsche 914-2\",\"Lotus Europa\",\"Ford Pantera L\",\"Ferrari Dino\",\"Maserati Bora\",\"Volvo 142E\"],[21,21,22.8,21.4,18.7,18.1,14.3,24.4,22.8,19.2,17.8,16.4,17.3,15.2,10.4,10.4,14.7,32.4,30.4,33.9,21.5,15.5,15.2,13.3,19.2,27.3,26,30.4,15.8,19.7,15,21.4],[6,6,4,6,8,6,8,4,4,6,6,8,8,8,8,8,8,4,4,4,4,8,8,8,8,4,4,4,8,6,8,4],[160,160,108,258,360,225,360,146.7,140.8,167.6,167.6,275.8,275.8,275.8,472,460,440,78.7,75.7,71.1,120.1,318,304,350,400,79,120.3,95.1,351,145,301,121],[110,110,93,110,175,105,245,62,95,123,123,180,180,180,205,215,230,66,52,65,97,150,150,245,175,66,91,113,264,175,335,109],[3.9,3.9,3.85,3.08,3.15,2.76,3.21,3.69,3.92,3.92,3.92,3.07,3.07,3.07,2.93,3,3.23,4.08,4.93,4.22,3.7,2.76,3.15,3.73,3.08,4.08,4.43,3.77,4.22,3.62,3.54,4.11],[2.62,2.875,2.32,3.215,3.44,3.46,3.57,3.19,3.15,3.44,3.44,4.07,3.73,3.78,5.25,5.424,5.345,2.2,1.615,1.835,2.465,3.52,3.435,3.84,3.845,1.935,2.14,1.513,3.17,2.77,3.57,2.78],[16.46,17.02,18.61,19.44,17.02,20.22,15.84,20,22.9,18.3,18.9,17.4,17.6,18,17.98,17.82,17.42,19.47,18.52,19.9,20.01,16.87,17.3,15.41,17.05,18.9,16.7,16.9,14.5,15.5,14.6,18.6],[0,0,1,1,0,1,0,1,1,1,1,0,0,0,0,0,0,1,1,1,1,0,0,0,0,1,0,1,0,0,0,1],[1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,1,1,1,1,1,1,1],[4,4,4,3,3,3,3,4,4,4,4,3,3,3,3,3,3,4,4,4,3,3,3,3,3,4,5,5,5,5,5,4],[4,4,1,1,2,1,4,2,2,4,4,3,3,3,4,4,4,1,2,1,1,2,2,4,2,1,2,2,4,6,8,2]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>mpg<\\/th>\\n      <th>cyl<\\/th>\\n      <th>disp<\\/th>\\n      <th>hp<\\/th>\\n      <th>drat<\\/th>\\n      <th>wt<\\/th>\\n      <th>qsec<\\/th>\\n      <th>vs<\\/th>\\n      <th>am<\\/th>\\n      <th>gear<\\/th>\\n      <th>carb<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3,4,5,6,7,8,9,10,11]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\n# Is er een relatie tussen het gewicht van de auto en de afstand die het kan afleggen met de brandstof?\nlm_mpg <- lm(mpg ~ wt, data = mtcars) # Run het lineair model dat mpg voorspelt op basis van wt\ncoef_lm_mpg <- coef(summary(lm_mpg))  # Haal de co?ffici?nten voor de tabel eruit\nkable(coef_lm_mpg)                    # Maak een niet-interactieve tabel - functie in knitr\n\nEstimate\nStd. Error\nt value\nPr(>|t|)\n(Intercept)\n37.285126\n1.877627\n19.857575\n0\nwt\n-5.344472\n0.559101\n-9.559044\n0\n\n# Plot de relatie tussen gewicht en mijl per gallon  \nplot <- ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() + # \n  geom_smooth(method = \"lm\") + theme_bw() +           # voeg lineair model toe en maak het zwart en wit \n  xlab(\"Weight (1000lbs)\") + ylab(\"Miles per Gallon\") # Voeg tekst aan x en y-as toe\n\nHet lijkt erop dat met elke 1000 pond er een afname is in brandstof gebruik met 5.3444716 mijl per gallon\nHet einde\n```\nReproduceerbaar onderzoek\nGoede uitvoering van reproduceerbaar onderzoek houdt in ieder geval in:\nHet hele project in een directory plaatsen die wordt ondersteund door de ‘version control’.\nCode en data vrijlaten.\nAlles documenteren en de code als documentatie gebruiken!\nFiguren, tabellen en de statistiek zijn het resultaat van scripts en codes die in de tekst staan.\nSchrijf in de codes de paden die worden gebruikt.\nStel ‘seed’ in zodat een volgende persoon dezelfde resultaten krijgt.\nLaat ook informatie zien waarmee de codefile wordt uitgevoerd. Je kunt bijvoorbeeld de devtools::session_info() gebruiken.\nWat het belang van reproduceerbaar onderzoek betreft, zie ook het document van de Koninklijke Nederlandse Academie voor Wetenschappen\nMarkdown\nOm RMarkdown helemaal te begrijpen moeten we het eerst iets over Markdown weten. Dat is een systeem om een simpele, leesbare tekst te maken die eenvoudig kan worden omgezet naar HTML. Markdown is essentieel voor twee dingen:\nEen kale tekst die de syntax vormt.\nEen software gereedschap dat in Perl is geschreven.\nHet zet de kale tekst om in HTML.\n\nBelangrijkste doel van Markdown:\nMaakt de syntax van het orginele (pre-HTML) document zo leesbaar mogelijk.\n\nZou je deze code liever in HTML lezen?\n\n<body>\n  <section>\n    <h1>Paklijst voor bergklimmen<\/h1>\n    <ul>\n      <li>Bergschoenen<\/li>\n      <li>Klimgordel<\/li>\n      <li>Rugzak<\/li>\n      <li>Touw<\/li>\n      <li>Zelfzekering<\/li>\n    <\/ul>\n  <\/section>\n<\/body>\nOf deze code in Markdown?\n\n# Paklijst voor bergklimmen\n\n* Bergschoenen\n* Klimgordel\n* Rugzak  \n* Touw\n* Zelfzekering\nMarkdown is makkelijker om te lezen!\nWe zullen meer over de syntax van Markdown praten nadat we RMarkdown hebben geintroduceerd maar laten we ons allereerst beseffen hoeveel makkelijker ons leven is/zal zijn omdat Markdown bestaat! Dank je John Gruber en Aaron Swartz (RIP) voor het ontwikkelen van Markdown in 2004!\nRMarkdown\nRMarkdown is een variant van Markdown dat het makkelijker maakt om met RStudio dynamische documenten, presentaties en rapporten te maken. Het omvat ‘R code chunks’ (ik laat hier even het Engels staan) om met knitr te gebruiken waarmee makkelijker reproduceerbare (web-based) rapporten gemaakt kunnen worden die automatisch aangepast worden wanneer de onderliggende code is veranderd.\nRMarkdown laat jou Markdown combineren met plaatjes, linken, tabellen, \\(\\LaTeX\\) en de code zelf.\nRStudio zorgt ervoor dat het maken van documenten met RMarkdown makkelijk wordt.\nRStudio is (net als R) vrij te gebruiken en draait op elk systeem.\nRMarkdown geeft verschillende typen files waaronder onder anderen:\nHTML\nPDF\nMarkdown\nMicrosoft Word\nPresentaties:\nOpvallende HTML5 presentaties:\nioslides\nSlidy\nSlidify\n\nPDF presentaties:\nBeamer\n\nHandouts:\nTufte Handouts\n\n\nHTML R Package Vignettes\nEven Entire Websites!\nTerwijl er heel veel verschillende documenten kunnen worden geleverd met RMarkdown, ligt hier de nadruk in de eerste plaats op HTML output files omdat die voor mijn onderzoek misschien het meest bruikbaar en flexibel zijn.\nWaarom R Markdown?\nEen aantrekkelijk gereedschap voor reproduceerbare en dynamische rapporten!\nTerwijl het was gemaakt voor R, accepteert het veel programmeertalen. Om het eenvoudig te houden, werken we vandaag alleen met R.\nEen code kan op een aantal manieren worden uitgevoerd:\nInline Code: Een korte code die in de geschreven tekst van het document wordt uitgevoerd.\nCode Chunks: Delen van het document omvatten verschillende zinnen analyse code. Dat kan een plot of een tabel zijn, maar ook berekeningen van de samenvattende statistiek, pakketten laden, etc.\n\nHet is makkelijk om:\nPlaatjes op te nemen.\nDe Markdown syntax te leren.\n\\(\\LaTeX\\) elementen op te nemen.\nInteractieve tabellen op te nemen.\nGebruik de versie via Git.\nDan is het makkelijker om te delen en samen te werken in analyses, projecten en publicaties!\n\nExterne linken toe te voegen - Rmarkdown begrijpt zelfs enige html codes!\nOm mooie documenten te maken.\n\nJe hoeft je geen zorgen te maken over pagina breuken of het plaatsen van de figuren.\nConsolideer jouw code en plaats het in een file:\nPowerpoint, PDFs, html documenten en word files\n\nEenvoudige werkwijze\nIn het kort, om een rapport te maken:\nOpen een .Rmd file.\nMaak een YAML kop (meer hierover zo dadelijk!)\n\nSchrijf de inhoud met RMarkdown syntax.\nNeem mee de R code in code chunks of met een inline code.\nDraai de document output.\nOverzicht van de stappen die RMarkdown maakt om een ‘gerenderd’ document te krijgen:\nMaak een .Rmd rapport met ‘R code chunks’ en markdown verhalen (zoals hierboven in stappen beschreven).\nGeef de .Rmd-file aan knitr om de ‘R code chunks’ uit te voeren en een nieuwe .md file te maken.\nKnitr is een pakket binnen R die jou in staat stelt de code binnen RMarkdown documenten uit te voeren zoals HTML, latex, pdf, word en andere document types.\n\nGeef de .md file aan pandoc, die er een definitief document van maakt (b.v. html, Microsoft word, pdf, etc.).\nPandoc is een universeel gereedschap om documenten te converteren en zet het ene document type (in dit geval: .Rmd) om in een ander (in dit geval: HTML)\n\nHoewel dit mogelijk wat ingewikkeld lijkt, kunnen we op de Knit knop drukken boven aan de pagina\nof we kunnen de volgende code runnen:\n\nrmarkdown::render(\"RMarkdown_LesNederlandsBeperkt.Rmd\", \"html_document\")\n\nMaak een .Rmd file\nLaten we eens met een RMarkdown gaan werken!\nIn de menu bar, klik je op File -> New File -> RMarkdown\nDan krijg je het volgende te zien\nHierbinnen kies je het type output dat je wilt hebben. Opgelet: deze output kan later heel makkelijk worden aangepast!\nKlik OK\nYAML koppen\nYAML staat voor “YAML Ain’t Markup Language” en is eigenlijk de structuur voor de metadata van het document. Het staat tussen twee regels van drie streepjes --- en wordt automatisch omgezet door RStudio. Een eenvoudig voorbeeld:\n\n---\ntitle:  \"Analyse Rapport\"  \nAuthor:  \"Harrie Jonkman\"  \ndate: \"1 Maart 2017\"  \noutput:  html_document\n---\nHet voorbeeld boven zal een HTML document maken. Echter, de volgende opties zijn ook beschikbaar.\nhtml_document\npdf_document\nword_document\nbeamer_presentation (pdf powerpoint)\nioslides_presentation (HTML powerpoint)\nen nog meer …\nHier ligt de nadruk op HTML files. Echter voel je vrij als je hier wat mee wilt spelen door bv. word en pdf documenten te maken. Presentatie-documenten kennen een wat andere syntax (bv. om aan te geven wanneer de ene dia eindigt en de andere begint) en dan is er nog wat markdown syntax specifiek voor presentaties maar die gaat voorbij het doel van deze workshop.\nMarkdown Basis\nKijk hiernaar RMarkdown Reference Guide\nHaal hier ook informatie vandaan RMarkdown Cheatsheet:\nMarkdown Basis van RStudio’s RMarkdown CheatsheetHandige tips:\nEindig elke regel met drie spaties om een nieuwe regel te beginnen.\nWoorden binnen een code moeten aan beide kanten zo’n kommateken kennen: `\nOm iets tot superscript te maken moet je een ^ aan beide zijden plaatsen. Superscript werd gevormd door Super^script^ te typen.\nVergelijkingen kunnen in een inline code worden geplaatst met $ en als blok gecentreerd binnen het document door $$. Bijvoorbeeld \\(E = mc^2\\) staat tussen de regels terwijl het volgende geblokt wordt opgenomen: \\[E = mc^2\\]\nAnder wiskundig materiaal:\n- Vierkantswortel: $\\sqrt{b}$ zal \\(\\sqrt{b}\\) maken - Breuken: $\\frac{1}{2}$ = \\(\\frac{1}{2}\\)\n- - Vergelijkingen met breuken: $f(x)=\\frac{P(x)}{Q(x)}$ = \\(f(x)=\\frac{P(x)}{Q(x)}\\)\n- Binomiale Coefficienten: $\\binom{k}{n}$ = \\(\\binom{k}{n}\\)\n- Integralen: $$\\int_{a}^{b} x^2 dx$$ = \\[\\int_{a}^{b} x^2 dx\\]\nShareLaTeX is een prachtige bron voor LaTeX-codes.\n\nNog wat wiskundig materiaal:\nBeschrijving\nCode\nVoorbeelden\nGriekse letters\n$\\alpha$ $\\beta$ $\\gamma$ $\\rho$ $\\sigma$ $\\delta$ $\\epsilon$ $mu$\n\\(\\alpha\\) \\(\\beta\\) \\(\\gamma\\) \\(\\rho\\) \\(\\sigma\\) \\(\\delta\\) \\(\\epsilon\\) \\(\\mu\\)\nBinaire handelingen\n$\\times$ $\\otimes$ $\\oplus$ $\\cup$ $\\cap$\n\\(\\times\\) \\(\\otimes\\) \\(\\oplus\\) \\(\\cup\\) \\(\\cap\\) \\(\\times\\)\nRelationele handelingen\n$< >$ $\\subset$ $\\supset$ $\\subseteq$ $\\supseteq$\n\\(< >\\) \\(\\subset\\) \\(\\supset\\) \\(\\subseteq\\) \\(\\supseteq\\)\nVerder\n$\\int$ $\\oint$ $\\sum$ $\\prod$\n\\(\\int\\) \\(\\oint\\) \\(\\sum\\) \\(\\prod\\)\n\nUitdaging: Probeer eens de volgende output te maken:\n\nVandaag voel ik mij vet omdat ik RMarkdown leer.\nhoning is heel zoet.\nYAS!!!!!!\nR2 waarden zijn informatief!\n\\(R^{2}\\) beschrijft de variantie verklaard door het model.\nIk kende geen RMarkdown Vandaag heb ik RMarkdown geleerd\nRStudio link\nOutput van het volgende:\n\n# RMarkdown   \n## R   \n### Knitr   \n#### Pandoc  \n##### HTML  \n\\(\\sqrt{b^2 - 4ac}\\)\n\\[\\sqrt{b^2 - 4ac}\\]\n\\(X_{i,j}\\)\n\nVandaag maak ik een dynamisch document!\n\nHet volgende lijstje:\nChocolade Chips Kook Recept\nboter\nsuiker\nEen mengsel van bruine en witte suiker maakt het lekkerder\nmix dat met boter voordat je de eieren eraan toevoegt\n\n\neieren\nvanille\nMix wat droge ingredienten:\nmeel, zout, bak soda\n\nchocolade chips\nEen Code in het document\nEr zijn twee manieren om een code in een RMarkdown document op te nemen.\nCode in het document: Korte code als een onderdeel van het geschreven document.\nCode Chunks: Delen van het document die verschillende programmeer of analyse codes omvatten. Daarmee kan een figuur of tabel worden gemaakt, statistieken worden berekend, pakketten worden geladen, etc.\nR Code in het document\nEen R code kan in het document wordt gemaakt door een komma hoog achterwaarts (`) en de letter r gevolgd door nog zo’n komma.\nBijvoorbeeld: 211 is 2048.\nStel dat je een p-waarde rapporteert en je wilt niet terug om de statistische test steeds weer uit te voeren. De p-waarde was eerder 0.0045.\nDit is echt handig als de resultaten op papier moeten worden gezet. Bijvoorbeeld, je hebt een aantal statistieken uitgevoerd voor jouw wetenschappelijke vragen is dit een manier waarop R die waarde in a variabele naam bewaart. Bijvoorbeeld: Wijkt het brandstofverbruik van de automaat significant af de auto met handtransmissie significant af binnen de mtcars data set?\n\n\nmpg_auto <- mtcars[mtcars$am == 0,]$mpg # automatic transmission mileage\nmpg_manual <- mtcars[mtcars$am == 1,]$mpg # manual transmission mileage\ntransmission_ttest <- t.test(mpg_auto, mpg_manual)\n\nOm de p-waarde vast te stellen kunnen we transmission_ttest$p.value als R code in het document gebruiken.\nDe p-waarde is dan 0.0013736.\nR Code Chunks\nR code chunks (nogmaals ik gebruik maar de Engelse benaming hier, sorry)kunnen worden gebruikt om de R output in het document te krijgen of om de code als illustratie zichtbaar te maken.\nDe anatomie van een code chunk:\nOm een R code chunk te plaatsen, kun je met de hand typen door ```{r} gevolgd door ``` op een volgende regel. Je kunt ook de Insert a new code chunk knop gebruiken of de ‘shortcut key’. Dat geeft dan de volgende code chunk:\nEen code chunk invoeren\n```{r}\nn <- 10\nseq(n)\n```\nGeef de code chunk een betekenisvolle naam die samenhangt met wat het doet. Hieronder heb ik code chunk 10-random-numbers genoemd:\n\n```{r 10-random-numbers}\nn <- 10\nseq(n)\n```\nDe code chunk input en output zien er dan als volgt uit:\n\n\nn = 10\nseq(n)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nKnitr\nKnitr is een R-pakket dat werkt met\nIdentificeren van de code zowel van de chunks als in de tekst zelf\nEvalueren van de hele code en geeft de resultaten terug\nTeruggeven van de geformuleerde resultaten en combineert met de orginele file.\nKnitr draait de code zoals die in de R console zou draaien.\nKnitr werkt vooral met code chunks.\nEen code chunk ziet er als volgt uit:\n\n<div class=\"layout-chunk\" data-layout=\"l-body\">\n\n```r\nx <- rnorm(100)  \ny <- 2*x + rnorm(100)\n```\n\n<\/div>\nGoede praktijken met betrekking tot code chunks:\nBenoem/label jouw code chunks!\nIn plaats van de chunk opties te specificeren in iedere chunk, kun je de algemene chunk opties aan het begin van het document vastzetten.\nChunk Labels\nChunk labels krijgen unieke IDs in een document en zijn goed voor:\nOm externe files te genereren zoals plaatjes en ‘cached’ documenten.\nChunk labels zijn vaak output als fouten omhoog komen(vaker voor codes in het document).\nAls je de code chunk een naam geef, gebruik dan - of _ tussen woorden voor code chunks labels in plaats van ruimtes. Dat helpt jou en andere gebruikers bij het navigeren in het document.\nChunk labels moeten uniek zijn in het document - anders zal er een fout optreden!\nChunk Opties\nDruk tab als tussen de haakjes code chunk opties omhoog komen.\nresults = \"asis\" staat voor “as is” en geeft de output van een niet geformateerde versie.\ncollapse is een andere chunk optie die handig kan zijn, zeker als een code chunk veel korte R uitdrukking heeft met wat output.\nEr zijn teveel chunk opties om hier te behandelen. Kijk na deze workshop nog eens wat rond voor deze opties.\nEen mooie website om dat op te doen is Knitr Chunk Options.\n\nUitdaging\nDraai de code chunk hieronder en speel wat met de volgende knitr code chunk opties:\n\n\neval = TRUE/FALSE\necho = TRUE/FALSE\ncollapse = TRUE/FALSE\nresults = \"asis\",\"markup en \"hide\n\n\nSla je resultaten op in markdown.Opgelet: Wees er zeker van dat je jouw chunks een naam geeft!\n\n\n\n1+1\n2*5\nseq(1, 21, by = 3)\nhead(mtcars)\n\nEnkele voorbeelden voortbouwend op de chunk hierboven\nResultaten van results=\"markup\", collapse = TRUE}:\n\n\n1+1\n[1] 2\n2*5\n[1] 10\nseq(1, 21, by = 3)\n[1]  1  4  7 10 13 16 19\nhead(mtcars)\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nResultaten van results=\"asis\", collapse = TRUE}:\n\n\n1+1\n[1] 2\n\n2*5\n[1] 10\n\nseq(1, 21, by = 3)\n[1] 1 4 7 10 13 16 19\n\nhead(mtcars)\n\n               mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1\n\nGlobale opties\nHet kan zijn dat je dezelfde chunk settings wilt handhaven voor het gehele document. Het kan daarom handig zijn om de opties in een keer te typen in plaats van het iedere keer weer voor een chunk te moeten doen. Om dat te doen kun je de globale chunk opties bovenaan het document vaststellen.\n\nknitr::opts_chunk$set(echo = FALSE, \n                      eval = TRUE, \n                      message = FALSE,\n                      warning = FALSE, \n                      fig.path = \"Figures/\",\n                      fig.width = 12, \n                      fig.height = 8)\nAls je bijvoorbeeld met iemand samenwerkt die de code niet wil zien, kun je schrijven eval = TRUE en echo = FALSE gebruiken zodat de code wel gedraaid wordt maar niet getoond. In aanvulling wil je misschien message = FALSE en warning = FALSE gebruiken zodat jouw samenwerkingspartner geen enkele boodschap of waarschuwing van R ziet.\nAls je figuren wilt opslaan en bewaren in een subdirectory binnen het project, gebruik dan fig.path = \"Figures/\". Hier verwijst de \"Figures/\" naar een folder Figures binnen de huidige directory waar de figuur die gemaakt wordt in het document wordt opgeslagen.Opgelet: de figuren worden niet standaard opgeslagen.\nGlobale chunk opties zullen voor de rest van het documenten worden vastgezet. Als je wilt dat een bepaalde chunk afwijkt van de globale opties, maak dat aan het begin van die bepaalde chunk duidelijk.\nFiguren\nKnitr maakt vrij eenvoudig figuren. Als een analyse code binnen een chunk een bepaald figuur moet produceren, dan zal hij dat in het document afdrukken.\nEnkele knitr chunk opties gerelateerd aan figuren:\nfig.width en fig.height\nStandaard: fig.width = 7, fig.height = 7\n\nfig.align: Hoe het figuur uit te lijnen\nOpties omvatten: \"left\", \"right\" en \"center\"\n\nfig.path: Een file pad naar de directory waar knitr de grafische output moet opslaan die er met de chunk wordt gemaakt.\nStandaard: 'figure/'\n\nEr is zelfs een fig.retina(alleen voor HTML output) voor hogere figuur resoluties met retina afdrukken.\n\n\n\nEen enkelvoudig figuur maken:\nMet fig.align = \"center\"\n\n\nggplot(mtcars, aes(x = mpg)) + xlab(\"Miles per Gallon\") +\n    geom_histogram(bins = 30, fill = \"cornflowerblue\", color = \"black\") +\n    geom_vline(xintercept=mean(mtcars$mpg), col=\"red\") \n\n\nMet fig.align = \"right\"\n\n\nggplot(mtcars, aes(x = mpg)) + xlab(\"Miles per Gallon\") +\n    geom_histogram(bins = 30, fill = \"cornflowerblue\", color = \"black\") +\n    geom_vline(xintercept=mean(mtcars$mpg), col=\"red\") \n\n\nMet fig.align = \"left\"\n\n\nggplot(mtcars, aes(x = mpg)) + xlab(\"Miles per Gallon\") +\n    geom_histogram(bins = 30, fill = \"cornflowerblue\", color = \"black\") +\n    geom_vline(xintercept=mean(mtcars$mpg), col=\"red\") \n\n\nMet fig.width = 2, fig.height = 2\n\n\nggplot(mtcars, aes(x = mpg)) + xlab(\"Miles per Gallon\") +\n    geom_histogram(bins = 30, fill = \"cornflowerblue\", color = \"black\") +\n    geom_vline(xintercept=mean(mtcars$mpg), col=\"red\") \n\n\nMet fig.width = 10, fig.height = 10\n\n\nggplot(mtcars, aes(x = mpg)) + xlab(\"Miles per Gallon\") +\n    geom_histogram(bins = 30, fill = \"cornflowerblue\", color = \"black\") +\n    geom_vline(xintercept=mean(mtcars$mpg), col=\"red\") \n\n\n\n\nmyplots <- list()  # new empty list\nfor(i in 1:ncol(mtcars)){\n  col <- names(mtcars)[i]\n  ggp <- ggplot(mtcars, aes_string(x = col)) +\n    geom_histogram(bins = 30, fill = \"cornflowerblue\", color = \"black\") +\n    geom_vline(xintercept = mean(mtcars[[col]]), col = \"red\") \n  myplots[[i]] <- ggp  # add each plot into plot list\n}\nmultiplot(plotlist = myplots, cols = 4) # must load in multiplot function from the Rcookbook see http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/\n\n\nTabellen\nTabellen kunnen in Markdown voor nogal wat hoofdpijn kosten. We gaan er hier verder niet op in. Als je meer wilt leren over Markdown-tabellen kijk naar documentation on tables op de RMarkdown website.\nEr zijn enkele tabeltypen die handig kunnen zijn. Hier zullen we ons vorig voorbeeld gebruiken van de mtcars data\nIn zijn Knitr in a Knutshell introduceert Dr. Karl Broman: kable, panderen xtable en vooral die eerste twee deden mij plezier:\nkable: Binnen het knitr pakket - niet veel opties maar het ziet er goed uit.\npander: Binnen het pander pakket - heeft veel opties en handigheden. Makkelijk voor het vetmaken van waarden (bv. waarden onder een bepaalde waarde).\nkable en pander tabellen zijn mooi en handig bij het maken van niet-interactieve tabellen:\n\n\nkable(head(mtcars, n = 4)) # kable table with 4 rows\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n# Pander table\n# install.packages(\"pander\") # install pander first\nlibrary(pander)\npander(head(mtcars, n = 4))\nTable continues below\n \nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\nMazda RX4\n21\n6\n160\n110\n3.9\n2.62\n16.46\n0\n1\nMazda RX4 Wag\n21\n6\n160\n110\n3.9\n2.875\n17.02\n0\n1\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.32\n18.61\n1\n1\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n \ngear\ncarb\nMazda RX4\n4\n4\nMazda RX4 Wag\n4\n4\nDatsun 710\n4\n1\nHornet 4 Drive\n3\n1\n\nZie ook hoe je mooie tabellen kunt maken hierKable\nHTML Widgets\nMet de uitgave van de nieuwe RMarkdown v2 is het makkelijker dan ooit tevoren om HTML Widgets te gebruiken. Volg de link om uit te zoeken in welke widgets jij ge?nteresseerd bent!\nOnlangs ontdekte ik bijvoorbeeld het DT pakket waarmee tabellen interactief kunnen worden gemaakt in de HTML output. Daarbij levert Plotly for R echt mooie interactieve grafieken op, welke gebaseerd zijn op Plotly.\nCool, of niet?\n\n\n# DT table = interactive\n# install.packages(\"DT\") # install DT first\nlibrary(DT)\ndatatable(head(mtcars, n = nrow(mtcars)), options = list(pageLength = 5)) \n\n{\"x\":{\"filter\":\"none\",\"data\":[[\"Mazda RX4\",\"Mazda RX4 Wag\",\"Datsun 710\",\"Hornet 4 Drive\",\"Hornet Sportabout\",\"Valiant\",\"Duster 360\",\"Merc 240D\",\"Merc 230\",\"Merc 280\",\"Merc 280C\",\"Merc 450SE\",\"Merc 450SL\",\"Merc 450SLC\",\"Cadillac Fleetwood\",\"Lincoln Continental\",\"Chrysler Imperial\",\"Fiat 128\",\"Honda Civic\",\"Toyota Corolla\",\"Toyota Corona\",\"Dodge Challenger\",\"AMC Javelin\",\"Camaro Z28\",\"Pontiac Firebird\",\"Fiat X1-9\",\"Porsche 914-2\",\"Lotus Europa\",\"Ford Pantera L\",\"Ferrari Dino\",\"Maserati Bora\",\"Volvo 142E\"],[21,21,22.8,21.4,18.7,18.1,14.3,24.4,22.8,19.2,17.8,16.4,17.3,15.2,10.4,10.4,14.7,32.4,30.4,33.9,21.5,15.5,15.2,13.3,19.2,27.3,26,30.4,15.8,19.7,15,21.4],[6,6,4,6,8,6,8,4,4,6,6,8,8,8,8,8,8,4,4,4,4,8,8,8,8,4,4,4,8,6,8,4],[160,160,108,258,360,225,360,146.7,140.8,167.6,167.6,275.8,275.8,275.8,472,460,440,78.7,75.7,71.1,120.1,318,304,350,400,79,120.3,95.1,351,145,301,121],[110,110,93,110,175,105,245,62,95,123,123,180,180,180,205,215,230,66,52,65,97,150,150,245,175,66,91,113,264,175,335,109],[3.9,3.9,3.85,3.08,3.15,2.76,3.21,3.69,3.92,3.92,3.92,3.07,3.07,3.07,2.93,3,3.23,4.08,4.93,4.22,3.7,2.76,3.15,3.73,3.08,4.08,4.43,3.77,4.22,3.62,3.54,4.11],[2.62,2.875,2.32,3.215,3.44,3.46,3.57,3.19,3.15,3.44,3.44,4.07,3.73,3.78,5.25,5.424,5.345,2.2,1.615,1.835,2.465,3.52,3.435,3.84,3.845,1.935,2.14,1.513,3.17,2.77,3.57,2.78],[16.46,17.02,18.61,19.44,17.02,20.22,15.84,20,22.9,18.3,18.9,17.4,17.6,18,17.98,17.82,17.42,19.47,18.52,19.9,20.01,16.87,17.3,15.41,17.05,18.9,16.7,16.9,14.5,15.5,14.6,18.6],[0,0,1,1,0,1,0,1,1,1,1,0,0,0,0,0,0,1,1,1,1,0,0,0,0,1,0,1,0,0,0,1],[1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,1,1,1,1,1,1,1],[4,4,4,3,3,3,3,4,4,4,4,3,3,3,3,3,3,4,4,4,3,3,3,3,3,4,5,5,5,5,5,4],[4,4,1,1,2,1,4,2,2,4,4,3,3,3,4,4,4,1,2,1,1,2,2,4,2,1,2,2,4,6,8,2]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>mpg<\\/th>\\n      <th>cyl<\\/th>\\n      <th>disp<\\/th>\\n      <th>hp<\\/th>\\n      <th>drat<\\/th>\\n      <th>wt<\\/th>\\n      <th>qsec<\\/th>\\n      <th>vs<\\/th>\\n      <th>am<\\/th>\\n      <th>gear<\\/th>\\n      <th>carb<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"pageLength\":5,\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3,4,5,6,7,8,9,10,11]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"lengthMenu\":[5,10,25,50,100]}},\"evals\":[],\"jsHooks\":[]}\n# plotly\n# install.packages(\"plotly\")\nlibrary(plotly)\nmtcars$car <- row.names(mtcars)\nplot_ly(mtcars, x = wt, y = mpg, \n        text = paste(\"Car: \", car),\n        mode = \"markers\", color = wt, size = wt)\n\nError in plot_ly(mtcars, x = wt, y = mpg, text = paste(\"Car: \", car), : object 'wt' not found\n\nSpelling controleren\nIn de spelling kunnen natuurlijk altijd fouten zitten en daarom kan het nodig zijn dat we onze spelling in het document willen controleren. Er zijn twee manieren om de spelling te controleren:\nDruk op de “ABC check mark”  links van de vergrootglasknop in RStudio.\nGebruik de aspell() functie van het utils pakket. Je kunt dan echter beter de code chunks overslaan. De aspell() functie kan een filter functie overnemen om bepaalde regels in de files over te slaan en kan worden gebruikt met de knit_filter() die ontworpen is om de code chunks in een file over te slaan.\nKnitr Thema’s\nHet knitr-syntax-thema kan worden aangepast of helemaal naar de hand worden gezet. Als je de standaardthema’s niet wilt, gebruik dan het knit_theme om het te veranderen. Er zijn 80 thema’s opgenomen binnen knitr en we kunnen de namen ervan zien door knit_theme$get().\nWat zijn de eerste 30 knitr thema’s?\n\n\nhead(knit_theme$get(), 30)\n\n [1] \"acid\"          \"aiseered\"      \"andes\"         \"anotherdark\"  \n [5] \"autumn\"        \"baycomb\"       \"bclear\"        \"biogoo\"       \n [9] \"bipolar\"       \"blacknblue\"    \"bluegreen\"     \"breeze\"       \n[13] \"bright\"        \"camo\"          \"candy\"         \"clarity\"      \n[17] \"dante\"         \"darkblue\"      \"darkbone\"      \"darkness\"     \n[21] \"darkslategray\" \"darkspectrum\"  \"default\"       \"denim\"        \n[25] \"dusk\"          \"earendel\"      \"easter\"        \"edit-anjuta\"  \n[29] \"edit-eclipse\"  \"edit-emacs\"   \n\nWij kunnen knit_theme$set() gebruiken om het thema vast te zetten. Om het thema op fruit vast te zetten, kunnen we de bijvoorbeeld de volgende code gebruiken:\n\n\nknit_theme$set(\"fruit\")\n\nHier is de link naar jouw favoriete thema 80 knitr highlight themes.\nInhoudsopgave\nEen inhoudsopgave kan aan het gerenderd document worden toegevoegd door de toc optie in de YAML kop te gebruiken.\nOpties hierbij:\ntoc: of de inhoudsopgave moeten worden meegenomen:\ntoc: true: hier wordt de inhoudsopgave meegenomen\nDefault:toc: false: Hier wordt de inhoudsopgave niet meegenomen\n\ntoc_depth:: Hoeveel niveau’s moeten in de inhoudsopgave worden worden meegenomen?\nDefault: doc_depth: 3 zal koppen tot en met ### meenemen.\n\nnumber_sections: Voegt sectienummers toe aan de koppen. Bijvoorbeeld, dit document heeft number_sections: true\nDefault: number_sections: false\nOpgelet: Met elk # zal er een decimaal punt worden toegevoegd aan alle koppen.\n\ntoc_float:\n2 andere mogelijke parameters binnen toc_float:\ncollapsed: Controleert of de inhoudsopgave alleen aan het begin verschijnt. Het zal met de cursor erover verschijnen.\nDefault: collapsed: TRUE\n\nsmooth_scroll: Controleert of de pagina scrolls werken wanneer op de onderdelen van de inhoudsopgave wordt geklikt.\nDefault: smooth_scroll: true\n\n\n\nBijvoorbeeld:\n\noutput:\n  html_document:\n    toc: true\n    toc_depth: 2\n---\n\nUitdaging: Maak de YAML kop voor een HTML document die het volgende inhoudt:\n\n\nInhoudsopgave\nLaat de inhoudsopgave vloeien\nSectie koppen met twee hashtags (##)\nGenummerde secties\nGeen makkelijke scrolling\n\nThema’s\nRMarkdown heeft verschillende opties voor de HTML documenten. Enkele mogelijkheden waaruit kan worden gekozen, hier met de Engelse termen:\ntheme\nhighlight\nsmart\nDe HTML output thema’s komen van Bootswatch library. Valide HTML themes omvatten de volgende:\ncerulean, cosmo,flatly, journal, readable,spacelab en united.\nBijvoorbeeld, het thema van de pagina is readable.\n\nZet het op nul voor geen thema (in dit geval kun je de css parameter gebruiken om jouw eigen stijl te gebruiken).\nHighlight specificeert de wijze waarop de syntax stijl oplicht. Stijlen die mogelijk zijn omvatten de volgende:\ndefault, espresso, haddock, kate, monochrome, pygments, tango, textmate en zenburn.\nOok hier, plaats nul om syntax oplichting te voorkomen.\nSmart indiceert of de typografisch correcte output wordt weergegeven, zet rechte aanhalingstekens om in gekru, — rechte aanhalingstekens, – om in gekrulde aanhalingstekens en … in ellipsen. Smart is standaard ingesteld.\nBijvoorbeeld:\n\n---\noutput:\n  html_document:\n    theme: slate\n    highlight: tango\n---\nAls je wilt kun je ook jouw eigen stijl-thema produceren en gebruiken. Als je dat zou doen, zou de output sectie van jouw YAML kop er z’on beetje zo uitzien:\n\noutput:\n  html_document:\n    css: styles.css\nAls je nog wat verder wilt gaan en jouw eigen thema wilt schrijven in aanvulling op het oplichten, zou de YAML kop er beetje zo uitzien:\n\n---\noutput:\n  html_document:\n    theme: null\n    highlight: null\n    css: styles.css\n---\nHier is een link naar Pr?sance en Stijl in de HTML output.\nBibliografie\nHet is ook mogelijk om een bibliografie file in de YAML kop mee te nemen. Bibliografie formats die door Pandoc gelezen kunnen worden zijn:\nFormat\nFile extension\nMODS\n.mods\nBibLaTeX\n.bib\nBibTeX\n.bibtex\nRIS\n.ris\nEndNote\n.enl\nEndNote XML\n.xml\nISI\n.wos\nMEDLINE\n.medline\nCopac\n.copac\nJSON citeproc\n.json\nOm een bibliografie in RMarkdown te maken, zijn er twee files nodig:\nEen bibliografie file met informatie over elke referentie.\nEen citaat stijl taal (CSL) om het format de referentie te bepalen.\nEen voorbeeld YAML kop met een bibliografie en een citaat stijl taal (CSL) file is:\n\noutput: html_document\nbibliography: bibliography.bib\ncsl: nature.csl\nBekijk de erg behulpzame webpagina van het R Core team op bibliographies and citations.\nAls je R pakketten wilt citeren, heeft knitr zelfs een functie die write_bib() heet en die .bib overzicht van R pakketten kan leveren. Het wordt zelfs in een file geschreven!\n\n\nwrite_bib(file = \"r-packages.bib\") # will write all packages  \nwrite_bib(c(\"knitr\", \"ggplot2\"), file = \"r-packages2.bib\") # Only writes knitr and ggplot2 packages\n\nPlaatsen\nDe bibliografie wordt automatisch aan het einde van het document geplaatst. Daarom moet je jouw .Rmd document met # Referenties eindigen zodat de bibliografie naar de kop voor bibliografie komt.\nStylen\nCitation Sylte Language (CSL) is een op XML-gebaseerde taal die het format van citaten en bibliografie?n vaststelt. Referentie management programma’s zoals Zotero, Mendeley en Papers gebruiken allemaal CSL.\nZoek jouw favoriete tijdschrift en CSL in de Zotero Style Repository, waar nu meer dan 8,152 CSLs inzitten. Is er een stijl waar je naar zoekt en die er niet in zit?\n\noutput: html_document\nbibliography: bibliography.bib\ncsl: nature.csl\nIn de github repo voor deze workshop heb ik de nature.csl en the-isme-journal.csl toegevoegd om mee te spelen. Download anders een stijl van de Zotero Style Repository!\nCitaten\nCitaten gaan tussen vierkante haakjes [ ] en worden afgescheiden door punt-komma’s’ ;. Elk citaat moet een sleutel hebben, samen de @ + de citaat identificatie van de database vormen en die optioneel a prefix, a locator en a suffix hebben. Om te controleren wat de citaatsleutel is van een referentie, werp dan een blik op de .bib file. Hier in die file, kun je de sleutel voor elke referentie veranderen. Echter, wees er wel van bewust dat elke ID uniek is!\nHier zijn wat voorbeelden met bijpassende code in het Engels:\nMicrobes control Earth’s biogeochemical cycles [@Falkowski2008].\nCode: Microbes contorl Earth's biogeochemical cycles  [@Falkowski2008].\n\nI love making beautiful plots with ggplot2 [@R-ggplot2]\nCode: I love making beautiful plots with ggplot2 [@R-ggplot2]\n\nDr. Yuhui Xie’s book about Dynamic Documents [@Xie2015] inspired me to host this workshop.\nCode: Dr. Yuhui Xie's book about Dynamic Documents [@Xie2015] inspired me to host this workshop.\n\nA great article in Science regarding biogeography of microbes asks readers to imagine their Alice in Wonderland to shrink down to understand the microbial world [@Green2008].\nCode: A great article in *Science* regarding biogeography of microbes asks readers to imagine they are Alice in Wonderland to and shrink down to understand the microbial world [@Green2008].\n\nHet is cool dat de enige refenties die aan het document worden toegevoegd degene zijn die jijzelf citeert!\nPubliceren via RPubs\nAls je een keer een mooi dynamisch document hebt gemaakt wil je dat mogelijk delen met anderen. Een mogelijkheid om het te delen met de wereld is om het te hosten op RPubs. Met RStudio kan dit heel makkelijk! Doe het volgende:\nMaak een aansprekend .Rmd document.\nKlik op de knop om jouw gerenderd HTML document te puliceren.\nIn de rechter bovenhoek van het previewscherm klik je op de publiceerknop en volg je de aanwijzingen.\nOpgelet: Je moet een RPubs profiel hebben aangemaakt.\n\nAls je het profiel hebt, let dan op het volgende:\nDe titel van het document.\nEen beschrijving van het document.\nDe URL waar de website wordt gehost.\nOpgelet: Het begin van de URL zal zijn: www.rpubs.com/your_username/name_of_your_choice\n\n\nRPubs vernieuwen\nAls je veranderingen in het document wilt aanbrengen is het makkelijk om de webpagina te vernieuwen. Als je een keer jouw aangepaste documentg hebt gerenderd klik je op de  knop rechtsboven in de hoek van de preview scherm. Het aangepaste document zal dezelfde URL hebben als het orginele document.\nEnkele RMarkdown-documenten\nNet als \\(\\Latex\\) leent RMarkdown zich voor het opmaken van hele verschillende wetenschappelijke producten. Van een aantal belangrijke artikele heb ik wat voorbeelden gemaakt. Als basis heb ik een klein rapport van twee collega’s van mij genomen. Voor mensen die hiermee willen werken, is er een syntax. Met behulp daarvan kun je het product makkelijk en eenvoudig maken.\nArtikel\nIedereen die wel eens een wetenschappelijk artikel maakt, weet dat hij of zij soms tegen ingewikkelde zaken aanloopt. De consistente opbouw bijvoorbeeld, het toevoegen van een tabel of een figuur, de referenties goed plaatsen. Als je in Word werkt is het soms lastig als je van volgorde verandert. Bij RMarkdown kun je het pakket rticlesbinnenhalen. Dan krijg je enkele templates waaruit je een keuze kunt maken. Hier JSS-voorbeeld het artikel voor Journal of Statistical Software.\nBoek\nEen boek bestaat uit verschillende onderdelen, bijvoorbeeld uit een een inhoudsopgave, de hoofdstukken en de literatuur. Hier wordt een stramien voor de opbouw van een boek aangeboden dat eenvoudig is aan te passen. Van het rapprt heb ik een klein boekje gemaakt, dat vind je hier DenHaagboekhtml\nHet is dan ook makkelijk om van hetzelfde een eboek te maken. Dat eboek vind je hier EboekDenHaag\nRapport\nVan hetzelfde onderzoek heb ik ook een fraai vormgegeven rapport gemaakt in de Tufte-stijl. Dat staat hier DenHaagRapport\nPresentatie\nDe presentaties kunnen er ook net wat strakker uitzien. Hier geef ik twee voorbeelden Beamer en ook in een andere stijl kun je het zelf draaien ioslides\nEen blog en website\nDit blog is met het pakket Radix gemaakt. Van Den Haag heb ik ook een blog gemaakt en dat vind je hier.Blog\nTutorial\nOver RMarkdown heb ik eerder deze tutorial van Schmidt bewerkt, zie Dynamische documenten\nEen dashboard\nEen dashboard is een fraaie manier om kort maar krachtig enkele resultaten te presenteren. Zie hier Dashboard\nTot slot\nHartelijke dank Marian Schmidt voor het opzetten van de workshop RMarkdown waar ik heel veel van heb geleerd en in dit document gebruik van heb gemaakt. Hartelijk dank natuurlijk ook naar Yihui Xie die zoveel hieromtrent heeft ontwikkeld en dit in alle openbaarheid met anderen deelt.\n\n\n",
    "preview": "posts/2018-11-26-communiceren-met-rmarkdown/Figures/single-fig-center-1.png",
    "last_modified": "2020-03-13T23:28:57+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-11-14-simple-features/",
    "title": "Bewerking geografische data in R: Nieuwe ontwikkelingen",
    "description": "Het nieuwe R sf-pakket, dat sp vervangt om met geografische objecten om te gaan, is  ontworpen om makkelijk met Tidyverse om te gaan. Hier laat ik zien hoe sf-objecten als data-frames worden opgeslagen en jou in staat stelt om met  ggplot2, dplyr en tidyr te werken. Ook het R-pakket tmap biedt veel nieuwe mogelijkheden.",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2018-11-14",
    "categories": [],
    "contents": "\nOorspronkelijk was sp het standaardpakket om in R met geografische data om te gaan. Dit pakket(samen met andere pakketten zoals raster) maakt van R een krachtig GIS-gereedschap. Echter, spis vandaag de dag wat gedateerd. Ik heb wat gelezen en gewerkt met R package sf dat bedoeld is om sp op te volgen. Dit pakket is onderdeel van R Simple Features, kan files makkelijk inlezen, topologische handelingen uitvoeren en files schrijven.\nIk ben erg onder de indruk van watsfkan; het lijkt alles te kunnen wat sp, rgdalen rgeos kunnen, maar op een meer moderne intuïtieve manier. Maar wat vooral aantrekkelijk is vansfis dat de ontwikkelaars van dit pakket aansluiten bij de ontwerp principes van Hadley Wickhams Tidyverse. Het zijn de volgende zaken die opvallen:\nGeografische objecten worden opgeslagen als data frames, waarbij de geometrische kenmerken in één list worden opgeslagen;\nAlle functies beginnen met st_ om het eenvoudig te maken;\nFuncties zijn ‘pipe-vriendelijk’;\ndplyr en tidyr werken met de sf objecten;\nggplot2 is binnenkort in staat om sf objecten direct te plotten.\nMet deze kenmerken pastsf veel beter bij moderne data analyse-opzet dan sp. Je kunt nu direct metdplyr functies als mutate() of select() werken.\nPakketten die je voor onderstaande nodig hebt\nNatuurlijk moeten we sf en tidyverse (waarin ggplot2, dplyr en tidyr zitten) openen. Daarnaast openen we ook viridis (voor palette-kleuren) en rvestpakket (om html-data van het web te halen).\n\n\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(viridis)\nlibrary(rvest)\n\nSimple Features as data frames\nSimple Features is een open source standaard voor de weergave van objecten (d.w.z. functies). Het eerste vignet voor het sf pakket beschrijft in detail de verschillende soorten functies die kunnen worden weergegeven (bijvoorbeeld POINT, LINESTRING, POLYGON, etc.) en hoe ermee te werken met de functies in sf.sf maakt gebruik van het bekende data frame om functies op te slaan. Het allereerste materiaal hieronder komt van van het eerste vignet.\nIn dit pakket worden functies opgeslagen als data frames van de sf klasse. Elke rij bestaat uit een functie/eenheid en elke kolom uit een attribuut/kenmerk. Het verschil met een normaal dataframe is dat er een extra ‘list’-kolom van de klasse sfc is toegevoegd waarin de geometrische kenmerken zijn opgeslagen.\nLaten we beginnen met het laden van wat voorbeeldgegevens die in het pakket zitten. Dit is een vormbestand van provincies in North Carolina. Ik zal dit ook omzetten naar een sp object ter vergelijking.\n\n\n\nHet resulterende sf object is in wezen slechts een data.frame met een extra kolom voor de geografische informatie.\n\n\n[1] \"sf\"         \"data.frame\"\n\nObservations: 2\nVariables: 15\n$ AREA      <dbl> 0.114, 0.061\n$ PERIMETER <dbl> 1.442, 1.231\n$ CNTY_     <dbl> 1825, 1827\n$ CNTY_ID   <dbl> 1825, 1827\n$ NAME      <fct> Ashe, Alleghany\n$ FIPS      <fct> 37009, 37005\n$ FIPSNO    <dbl> 37009, 37005\n$ CRESS_ID  <int> 5, 3\n$ BIR74     <dbl> 1091, 487\n$ SID74     <dbl> 1, 0\n$ NWBIR74   <dbl> 10, 10\n$ BIR79     <dbl> 1364, 542\n$ SID79     <dbl> 0, 3\n$ NWBIR79   <dbl> 19, 12\n$ geometry  <MULTIPOLYGON [Â°]> MULTIPOLYGON (((-81.47276 3..., M...\n\n# A tibble: 2 x 15\n   AREA PERIMETER CNTY_ CNTY_ID NAME  FIPS  FIPSNO CRESS_ID BIR74\n  <dbl>     <dbl> <dbl>   <dbl> <fct> <fct>  <dbl>    <int> <dbl>\n1 0.114      1.44  1825    1825 Ashe  37009  37009        5  1091\n2 0.061      1.23  1827    1827 Alle~ 37005  37005        3   487\n# ... with 6 more variables: SID74 <dbl>, NWBIR74 <dbl>, BIR79 <dbl>,\n#   SID79 <dbl>, NWBIR79 <dbl>, geometry <MULTIPOLYGON [Â°]>\n\nHet mooie hiervan is dat iedereen weet hoe te werken met data-frames in R. Dus deze sf objecten zijn eenvoudig te inspecteren en mee te spelen. Bovendien houdt dit de geometrie en attribuutgegevens bij elkaar op één plaats, d.w.z. ze staan in dezelfde rij van het gegevensframe. Vergelijk dat maar met sp, dat deze gegevens deze gegevens heel anders opslaat:\n\n\n[1] \"SpatialPolygonsDataFrame\"\nattr(,\"package\")\n[1] \"sp\"\n\nFormal class 'SpatialPolygonsDataFrame' [package \"sp\"] with 5 slots\n  ..@ data       :'data.frame': 2 obs. of  14 variables:\n  .. ..$ AREA     : num [1:2] 0.114 0.061\n  .. ..$ PERIMETER: num [1:2] 1.44 1.23\n  .. ..$ CNTY_    : num [1:2] 1825 1827\n  .. ..$ CNTY_ID  : num [1:2] 1825 1827\n  .. ..$ NAME     : Factor w/ 100 levels \"Alamance\",\"Alexander\",..: 5 3\n  .. ..$ FIPS     : Factor w/ 100 levels \"37001\",\"37003\",..: 5 3\n  .. ..$ FIPSNO   : num [1:2] 37009 37005\n  .. ..$ CRESS_ID : int [1:2] 5 3\n  .. ..$ BIR74    : num [1:2] 1091 487\n  .. ..$ SID74    : num [1:2] 1 0\n  .. ..$ NWBIR74  : num [1:2] 10 10\n  .. ..$ BIR79    : num [1:2] 1364 542\n  .. ..$ SID79    : num [1:2] 0 3\n  .. ..$ NWBIR79  : num [1:2] 19 12\n  ..@ polygons   :List of 2\n  .. ..$ :Formal class 'Polygons' [package \"sp\"] with 5 slots\n  .. .. .. ..@ Polygons :List of 1\n  .. .. .. .. ..$ :Formal class 'Polygon' [package \"sp\"] with 5 slots\n  .. .. .. .. .. .. ..@ labpt  : num [1:2] -81.5 36.4\n  .. .. .. .. .. .. ..@ area   : num 0.114\n  .. .. .. .. .. .. ..@ hole   : logi FALSE\n  .. .. .. .. .. .. ..@ ringDir: int 1\n  .. .. .. .. .. .. ..@ coords : num [1:27, 1:2] -81.5 -81.5 -81.6 -81.6 -81.7 ...\n  .. .. .. ..@ plotOrder: int 1\n  .. .. .. ..@ labpt    : num [1:2] -81.5 36.4\n  .. .. .. ..@ ID       : chr \"1\"\n  .. .. .. ..@ area     : num 0.114\n  .. ..$ :Formal class 'Polygons' [package \"sp\"] with 5 slots\n  .. .. .. ..@ Polygons :List of 1\n  .. .. .. .. ..$ :Formal class 'Polygon' [package \"sp\"] with 5 slots\n  .. .. .. .. .. .. ..@ labpt  : num [1:2] -81.1 36.5\n  .. .. .. .. .. .. ..@ area   : num 0.0614\n  .. .. .. .. .. .. ..@ hole   : logi FALSE\n  .. .. .. .. .. .. ..@ ringDir: int 1\n  .. .. .. .. .. .. ..@ coords : num [1:26, 1:2] -81.2 -81.2 -81.3 -81.3 -81.3 ...\n  .. .. .. ..@ plotOrder: int 1\n  .. .. .. ..@ labpt    : num [1:2] -81.1 36.5\n  .. .. .. ..@ ID       : chr \"2\"\n  .. .. .. ..@ area     : num 0.0614\n  ..@ plotOrder  : int [1:2] 1 2\n  ..@ bbox       : num [1:2, 1:2] -81.7 36.2 -80.9 36.6\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:2] \"x\" \"y\"\n  .. .. ..$ : chr [1:2] \"min\" \"max\"\n  ..@ proj4string:Formal class 'CRS' [package \"sp\"] with 1 slot\n  .. .. ..@ projargs: chr \"+proj=longlat +datum=NAD27 +no_defs +ellps=clrk66 +nadgrids=@conus,@alaska,@ntv2_0.gsb,@ntv1_can.dat\"\n\nMerk hier op dat de attribuutgegevens worden opgeslagen als een data.frame in het data slot en dat de functies afzonderlijk worden opgeslagen. Dit kan heel verwarrend zijn om direct mee te werken.\nDe geometrie lijst-kolom van een sf object is een object van klasse sfc en een extra klasse die overeenkomt met het geometrietype, in dit geval sfc_MULTIPOLYGON. Het is toegankelijk met st_geometrie(). Aanvullende informatie over de kenmerken, zoals het coördinatensysteem, wordt als attributen opgeslagen:\n\n\nGeometry set for 2 features \ngeometry type:  MULTIPOLYGON\ndimension:      XY\nbbox:           xmin: -81.74107 ymin: 36.23436 xmax: -80.90344 ymax: 36.58965\nepsg (SRID):    4267\nproj4string:    +proj=longlat +datum=NAD27 +no_defs\n\n[1] \"sfc_MULTIPOLYGON\" \"sfc\"             \n\n$class\n[1] \"sfc_MULTIPOLYGON\" \"sfc\"             \n\n$precision\n[1] 0\n\n$bbox\n     xmin      ymin      xmax      ymax \n-81.74107  36.23436 -80.90344  36.58965 \n\n$crs\nCoordinate Reference System:\n  EPSG: 4267 \n  proj4string: \"+proj=longlat +datum=NAD27 +no_defs\"\n\n$n_empty\n[1] 0\n\nTot slot zijn er individuele eenvoudige kenmerken sfg objecten met extra klassen die overeenkomen met het specifieke type eigenschap. De klassen XY en MULTIPOLYGON geven aan dat dit een tweedimensionale MULTIPOLYGON geometrie is.\n\n\n[1] \"XY\"           \"MULTIPOLYGON\" \"sfg\"         \n\nIntern zijn deze sfg objecten vectoren voor punten, matrices voor LINESTRING objecten, en lijsten voor al het andere. Meer details zijn beschikbaar in de vignetten van het pakket.\nOndersteuning van Tidyverse\nWat we vooral van de vorige sectie hebben geleerd is dat sf-objecten data frames zijn! Aangezien data frames de kern vormen van de Tidyverse-pakketten, mag je veronderstellen dat de functies van Tidyverse pakketten van toepassing zouden moeten zijn op de geografische objecten van sf. Zeker, de makers van sf hebben methoden geleverd voor alle standaard dplyr en tidyr handelingen die we kennen en liefhebben. Verder ondersteunt de ontwikkelingsversie van ggpplot2 het plotten van sf objecten.\nggplot\nMet sp moesten geografische objecten eerst worden geconverteerd naar dataframes (bijv. met fortify())) voordat ze met ggplot2 werden geplot (dat was betrekkelijk ingewikkeld allemaal). Maar omdatsf-objecten al dataframes zijn, kunnen ze met behulp van de nieuwe geom_sf() direct worden geplot.\n\n\n\nDaarnaast kan de nieuwe coord_sf() gebruikt worden om deze kenmerken in een andere projectie te plotten, bijvoorbeeld een Albers equal area projectie (in de geografie worden verschillende systemen gebruikt).\n\n\n\ndplyr\ndplyr is de gouden standaard voor datamanipulatie en biedt een verscheidenheid aan voordelen ten opzichte van basis R-functies. Het is speciaal ontworpen voor het werken met data.frame-achtige objecten zoals die uit het sf pakket. De volgende werkwoorden werken alleen op de attribuutgegevens en laten de geometrieën onaangeroerd:\nselect() behoudt de gespecificeerde variabelen, eventueel onder een andere naam\nrename() een variabele een andere naam geven en alle andere ongewijzigd laten\nfilter() returns the rows that match the given conditions\nmutate() voegt nieuwe variabelen toe op basis van bestaande variabelen\ntransmute() creëert nieuwe variabelen en laat bestaande variabelen vallen\narrange() sorteert op basis van de gegeven variabelen\nslice() selecteert rijen op basis van rijnummer\nsample_n() trekt steekproeven met n kenmerken willekeurig\nHieronder zien we enkele voorbeelden:\n\n\nSimple feature collection with 3 features and 2 fields\ngeometry type:  MULTIPOLYGON\ndimension:      XY\nbbox:           xmin: -79.45597 ymin: 33.94867 xmax: -78.11374 ymax: 35.31512\nepsg (SRID):    4267\nproj4string:    +proj=longlat +datum=NAD27 +no_defs\n      name area_km2                       geometry\n1  Sampson     2410 MULTIPOLYGON (((-78.11377 3...\n2  Robeson     2400 MULTIPOLYGON (((-78.86451 3...\n3 Columbus     2400 MULTIPOLYGON (((-78.65572 3...\n\nMerk op dat de geometrie steeds ongewijzigd blijft.\n\n\n[1] \"area_km2\" \"geom\"    \n\nWe kunnen een willekeurige steekproef van functies uit de set nemen met behulp van sample_n():\n\n\n# A tibble: 4 x 2\n   AREA                                                       geometry\n  <dbl>                                            <MULTIPOLYGON [Â°]>\n1 0.111 (((-79.24619 35.86815, -79.23799 35.83725, -79.54099 35.83699~\n2 0.219 (((-78.92107 35.57886, -78.99881 35.60132, -78.93889 35.76144~\n3 0.203 (((-77.10377 35.55019, -77.11939 35.5855, -77.14835 35.598, -~\n4 0.121 (((-79.68596 34.80526, -79.91995 34.80792, -79.90142 34.85241~\n\nHet is ook mogelijk om functies uit sf te gebruiken die inwerken op de geometrie-kolom binnen een mutatie-instructie. Als er bijvoorbeeld nog geen gebiedskolom bestaat, kan men een gebiedskolom maken met behulp van st_area():\n\n\n# A tibble: 6 x 4\n  name     area_m2     area                                   geometry\n  <fct>    <S3: unit> <dbl>                        <MULTIPOLYGON [Â°]>\n1 Ashe     113738860~ 0.114 (((-81.47276 36.23436, -81.54084 36.27251~\n2 Allegha~  61107726~ 0.061 (((-81.23989 36.36536, -81.24069 36.37942~\n3 Surry    142348991~ 0.143 (((-80.45634 36.24256, -80.47639 36.25473~\n4 Curritu~  69454629~ 0.07  (((-76.00897 36.3196, -76.01735 36.33773,~\n5 Northam~ 152074053~ 0.153 (((-77.21767 36.24098, -77.23461 36.2146,~\n6 Hertford  96772795~ 0.097 (((-76.74506 36.23392, -76.98069 36.23024~\n\nGegroepeerde handelingen\ndplyr staat ook toe om in groepen te werken op sf objecten. group_by()groepeert een gegevensframe op basis van variabelen in de tabel. Vervolgens wordt summarise() gebruikt om groepssamenvattingen van de gegevens uit te voeren. Laten we beginnen met het toevoegen van een willekeurige groeperingsvariabele en vervolgens het gemiddelde van de gebieden over deze variabele berekenen.\n\n\n\nMerk op dat naast de attribuutgegevens die worden geaggregeerd, ook de geometrieën zijn geaggregeerd. Alle geometrieën in elke groep zijn samengevoegd en de grenzen tussen aangrenzende geometrieën zijn opgelost. Intern wordt de functie st_union() gebruikt om dit te bereiken.\nNet als bij een normaal gegevensframe kunnen gegroepeerde filtering en mutatie worden uitgevoerd op sf objecten. Bijvoorbeeld, om de proportionele verdeling van geboorten tussen provincies binnen elke groep te berekenen, gebruikt u een gegroepeerde mutate():\n\n\n# A tibble: 100 x 4\n   group  AREA area_prop                                      geometry\n   <chr> <dbl>     <dbl>                           <MULTIPOLYGON [Â°]>\n 1 A     0.114      12.6 (((-81.47276 36.23436, -81.54084 36.27251, -~\n 2 C     0.061      12.6 (((-81.23989 36.36536, -81.24069 36.37942, -~\n 3 C     0.143      12.6 (((-80.45634 36.24256, -80.47639 36.25473, -~\n 4 B     0.07       12.6 (((-76.00897 36.3196, -76.01735 36.33773, -7~\n 5 B     0.153      12.6 (((-77.21767 36.24098, -77.23461 36.2146, -7~\n 6 A     0.097      12.6 (((-76.74506 36.23392, -76.98069 36.23024, -~\n 7 A     0.062      12.6 (((-76.00897 36.3196, -75.95718 36.19377, -7~\n 8 A     0.091      12.6 (((-76.56251 36.34057, -76.60424 36.31498, -~\n 9 C     0.118      12.6 (((-78.30876 36.26004, -78.28293 36.29188, -~\n10 B     0.124      12.6 (((-80.02567 36.25023, -80.45301 36.25709, -~\n# ... with 90 more rows\n\nOm alleen landen te behouden binnen groepen die een groter gebied hebben dan een bepaalde drempel, kan een gegroepeerde filter() worden gebruikt:\n\n\n# A tibble: 38 x 3\n   group  AREA                                                geometry\n   <chr> <dbl>                                     <MULTIPOLYGON [Â°]>\n 1 B     0.07  (((-76.00897 36.3196, -76.01735 36.33773, -76.03288 36~\n 2 B     0.153 (((-77.21767 36.24098, -77.23461 36.2146, -77.29861 36~\n 3 B     0.124 (((-80.02567 36.25023, -80.45301 36.25709, -80.43531 3~\n 4 B     0.153 (((-79.53051 36.24614, -79.53058 36.23616, -80.02567 3~\n 5 B     0.072 (((-78.49252 36.17359, -78.51472 36.17522, -78.51709 3~\n 6 B     0.081 (((-81.80622 36.10456, -81.81715 36.10939, -81.82231 3~\n 7 B     0.064 (((-81.94135 35.95498, -81.9614 35.93922, -81.94495 35~\n 8 B     0.128 (((-78.25455 35.81553, -78.26685 35.84838, -78.30841 3~\n 9 B     0.17  (((-79.53782 35.89097, -80.0426 35.91681, -80.0381 36.~\n10 B     0.111 (((-79.24619 35.86815, -79.23799 35.83725, -79.54099 3~\n# ... with 28 more rows\n\nSamenvoegen\ndplyr heeft een reeks functies voor het samenvoegen van gegevensframes op basis van gedeelde kolommen. Deze functies zijn allemaal geïmplementeerd in sf en zijn een geweldige manier om extra attribuutgegevens uit andere bronnen aan uw ruimtelijke gegevens toe te voegen. Het is echter alleen mogelijk om een sf object te verbinden met een gewoon data.frame. Je kunt niet twee sf objecten met elkaar verbinden.\nLaten we beginnen met enkele county-level populatiegegevens van Wikipedia af te halen.\n\n\n\nNu voegen we deze populatiegegevens samen met onze ruimtelijke gegevens en plotten ze.\n\n\n\nAlle andere verbindingsfuncties (bijv. left_join(), anti_join(), etc.) werken op dezelfde manier. Als het tweede argument van een van deze functies een sf object is, en geen normaal gegevensframe, zal er een fout optreden. Vermoedelijk komt dit omdat het onduidelijk is hoe de twee verschillende geometrieën gecombineerd moeten worden, hoewel er wel wat discussie lijkt te zijn over hoe je verbindingen met twee sets van geometrieën kunt implementeren:\n\n\nError: y should be a data.frame; for spatial joins, use st_join\n\nDeze dplyr functies zijn allemaal voor het verbinden op basis van attribuutgegevens. Als je op zoek bent naar een ruimtelijke verbinding (bijv. twee sf objecten op basis van een snijpunt van geometrieën) dan moet je de functie st_join()gebruiken.\ntidyr handelingen\nDe tidyr werkwoorden gather() en spread() worden gebruikt om de data frames te transformeren van breed naar lang formaat of vice versa. Bijvoorbeeld, zeg dat u gegevens wilt opslaan over het BBP voor alle landen en een set van jaren. Dit kan worden opgeslagen in een lang formaat (met kolommen land, jaar en gdp), wat als een “tidy” formaat wordt beschouwd, of in een breed formaat (met kolommen land, gdp2000, gdp2001, ….), wat beter is voor weergavedoeleinden. tidyr kan overstappen naar een ander format en nu kan dit ook worden gedaan met sf objecten.\nAls we de North Carolina dataset als voorbeeld nemen, zijn BIR74 en BIR79 het aantal geboorten in de provincie in respectievelijk 1974 en 1979. Met gather() kunnen we dit gemakkelijk omzetten in een lang formaat:\n\n\nSimple feature collection with 6 features and 3 fields\ngeometry type:  MULTIPOLYGON\ndimension:      XY\nbbox:           xmin: -81.74107 ymin: 36.23388 xmax: -80.43531 ymax: 36.58965\nepsg (SRID):    4267\nproj4string:    +proj=longlat +datum=NAD27 +no_defs\n     county  year births                       geometry\n1      Ashe BIR74   1091 MULTIPOLYGON (((-81.47276 3...\n2 Alleghany BIR74    487 MULTIPOLYGON (((-81.23989 3...\n3     Surry BIR74   3188 MULTIPOLYGON (((-80.45634 3...\n4      Ashe BIR79   1364 MULTIPOLYGON (((-81.47276 3...\n5 Alleghany BIR79    542 MULTIPOLYGON (((-81.23989 3...\n6     Surry BIR79   3616 MULTIPOLYGON (((-80.45634 3...\n\nMerk op dat de attribuutgegevens mooi getransponeerd zijn. Het resultaat hiervan is dat elke functie twee rijen heeft en dat de functiegeometrieën gedupliceerd zijn. Voor mij lijkt dit vreemd om dezelfde geometrie op meerdere plaatsen op te slaan, dus ik ben niet zeker van wat deze gather() functie op sf objecten oplevert.\nWe kunnen dit terugzetten naar het originele brede formaat met spread():\n\n\nSimple feature collection with 3 features and 3 fields\ngeometry type:  MULTIPOLYGON\ndimension:      XY\nbbox:           xmin: -81.74107 ymin: 36.23388 xmax: -80.43531 ymax: 36.58965\nepsg (SRID):    4267\nproj4string:    +proj=longlat +datum=NAD27 +no_defs\n     county BIR74 BIR79                       geometry\n1 Alleghany   487   542 MULTIPOLYGON (((-81.23989 3...\n2      Ashe  1091  1364 MULTIPOLYGON (((-81.47276 3...\n3     Surry  3188  3616 MULTIPOLYGON (((-80.45634 3...\n\nDe blog (hier verwijzingHJ) biedt een aanvullende introductie op het sf-pakket. Het bevat een tutorial die jou door een aantal basisprincipes van het pakket leidt, waaronder lezen en schrijven van/naar shapefiles, herprojecten, afdrukken met ggplot, filteren en andere datavormen vinden met dplyr. Het maakt gebruik van een dataset van FiveThirtyEight die bijhoudt hoe vaak elk lid van het Amerikaanse Congres heeft gestemd in lijn met President Trumps. We zullen sf gebruiken om deze gegevens samen te voegen met een ander bestand om ze ruimtelijk te verkennen. Voordat u begint, moet u de benodigde datasets downloaden en in uw werkmap plaatsen. Laad vervolgens de benodigde pakketten voor deze tutorial.\n\n\n\nHet lezen van gegevens in R met sf is een relatief eenvoudige taak. Het ondersteunt het direct importeren van eenvoudige functies uit een PostGIS database met verschillende R database tools. In dit geval lezen we gewoon uit een shapefile in onze werkmap.\n\n\nReading layer `congressional_districts' from data source `H:\\MapsinR\\congressional-trump-scores-master\\congressional_districts.shp' using driver `ESRI Shapefile'\nSimple feature collection with 433 features and 2 fields\ngeometry type:  MULTIPOLYGON\ndimension:      XY\nbbox:           xmin: -124.7631 ymin: 24.5231 xmax: -66.9499 ymax: 49.38436\nepsg (SRID):    4269\nproj4string:    +proj=longlat +datum=NAD83 +no_defs\n\nSimple feature collection with 6 features and 2 fields\ngeometry type:  MULTIPOLYGON\ndimension:      XY\nbbox:           xmin: -124.4096 ymin: 33.87981 xmax: -114.0428 ymax: 42.00105\nepsg (SRID):    4269\nproj4string:    +proj=longlat +datum=NAD83 +no_defs\n  district state                       geometry\n1       40    CA MULTIPOLYGON (((-118.281 34...\n2        2    CA MULTIPOLYGON (((-122.4463 3...\n3       31    CA MULTIPOLYGON (((-117.7044 3...\n4       10    CA MULTIPOLYGON (((-121.557 37...\n5        9    CA MULTIPOLYGON (((-121.8513 3...\n6        4    NV MULTIPOLYGON (((-119.4398 3...\n\nOm de attributen geassocieerd met deze polygonen te bekijken of te manipuleren, kunnen we het object eenvoudigweg behandelen als een conventioneel R data frame. De basisplotfunctie geeft u een snel en handig overzicht van elk attribuut dat in kaart is gebracht over alle polygonen. Vervolgens moeten we de stemgegevens van elk lid van het Huis van Afgevaardigden laden. Deze gegevens zitten in een ander databestand.\n\n\n  X.1 X             name chamber party state district trump_score\n1   1 1 David G. Valadao   House     R    CA       21       1.000\n2   2 2   Carlos Curbelo   House     R    FL       26       0.935\n3   3 3     Erik Paulsen   House     R    MN        3       1.000\n4   4 4 Barbara Comstock   House     R    VA       10       0.970\n5   5 5  Darrell E. Issa   House     R    CA       49       1.000\n6   6 6  Edward R. Royce   House     R    CA       39       0.970\n  trump_margin predicted_score trump_plus_minus\n1       -0.155           0.296            0.704\n2       -0.161           0.284            0.652\n3       -0.094           0.428            0.572\n4       -0.100           0.413            0.556\n5       -0.075           0.478            0.522\n6       -0.086           0.449            0.521\n\nDe mensen van ‘FiveThirtyEight’ hebben een reeks samenvattende statistieken berekend die bij elke Vertegenwoordiger hoort en gekoppeld aan de stemrealiteit van Trump’s standpunt over die maatregel. Zij hebben zo een Trump Score (Trump_score) berekend die ons vertelt welk deel van de tijd elke wetgever in lijn met de president heeft gestemd. We zijn vooral geinteresseerd in het visualiseren van deze variabele. Om deze variabele in kaart te brengen zullen we de tabelvormige stemgegevens moeten samenvoegen met onze districtspolygonen. Omdat we sf objecten kunnen behandelen als data frames, is het eenvoudig om dit te doen met behulp van de join-functies van dplyr.\n\n\n\nNu we beide datasets hebben gecombineerd tot een sf-object, kunnen we ze in kaart brengen met ggplot. Wanneer je ggplot een sf-object geeft, weet het commando geom_sf de punten, lijnen of veelhoeken te tekenen op basis van het bekende tekstgeometrieveld in het object. Het andere sf-specifieke ggplot commando in dit voorbeeld is coord_sf, waarmee je een alternatieve projectie voor je kaart kunt specificeren. Je kunt een co?rdinatensysteem selecteren aan de hand van zijn epsg-code, die te vinden is op spatialreference.org.\n\n\n\nDe kaart versterkt ons begrip van het sterk partijdige karakter van het Congres. De meeste troefscores liggen aan de uitersten, omdat maar weinig wetgevers zich bereid hebben getoond om met hun partijlijnen te breken.\nOnze volgende stap zal zijn om in te zoomen op een bepaald deel van het land. We zullen dplyr gebruiken om slechts drie staten in het hoger gelegen Midwesten (Minnesota, Wisconsin en Iowa) in kaart te brengen. Onderweg zullen we ook een sf object opnieuw projecteren, polygon centroiden extraheren, en die centroiden gebruiken om elk district van een label te voorzien.\n\n\n\nDit voorbeeld laat zien hoe je sf-functies in magrittr pijpleidingen kunt nestelen met behulp van de %>% operator waarmee een gebruiker van een ander pakket waarschijnlijk bekend is. Vervolgens zul je zien hoe je dplyr kunt gebruiken om polygonen op te lossen. Laten we eens kijken naar de originele kaart van de hele Verenigde Staten, maar deze keer voegen we de Trump Scores samen tot het niveau van de Staat.\n\n\nSimple feature collection with 6 features and 3 fields\ngeometry type:  GEOMETRY\ndimension:      XY\nbbox:           xmin: -124.4096 ymin: 30.22333 xmax: -71.78699 ymax: 42.05059\nepsg (SRID):    4269\nproj4string:    +proj=longlat +datum=NAD83 +no_defs\n# A tibble: 6 x 4\n  state avg_trump_score districts                             geometry\n  <chr>           <dbl>     <int>                      <GEOMETRY [Â°]>\n1 AL              0.861         7 MULTIPOLYGON (((-88.05338 30.50699,~\n2 AR              0.992         4 POLYGON ((-94.48558 33.65331, -94.4~\n3 AZ              0.650         9 POLYGON ((-109.0476 32.42638, -109.~\n4 CA              0.377        53 MULTIPOLYGON (((-119.5773 33.27858,~\n5 CO              0.615         7 POLYGON ((-104.9434 40.99808, -104.~\n6 CT              0.147         5 MULTIPOLYGON (((-73.63057 40.98071,~\n\nWe kunnen de bekende dplyr-syntax gebruiken om de districten te groeperen op staat en samenvattende statistieken te berekenen. Omdat sf-objecten zo goed integreren met dplyr, groepeert de functie automatisch de ruimtelijke gegevens samen met de tabel. Het eindresultaat is vergelijkbaar met dat van de geoprocessing dissolve tool in een traditioneel desktop GIS.\n\n\n\ntmap\nNaast sfis tmapeen ander R-pakket dat jou kan ondersteunen bij het maken van geografische kaarten. Met het tmap-pakket kunnen thematische kaarten met grote flexibiliteit worden gegenereerd. De syntaxis voor het maken van plots is ook hier vergelijkbaar met die van ggplot2, maar dan op maat gemaakt voor kaarten. Het pakket tmapbiedt een instructief vignet dat is bedoeld voor degenen die binnen een paar minuten aan de slag willen met tmap. Een meer gedetailleerde beschrijving van tmap is te vinden in een artikel gepubliceerd in het Journal of Statistical Software (JSS), dat tmap versie 1.11-2 beschrijft. De wijzigingen in versie 2.0 worden beschreven in vignette(\"tmap-changes-v2\").\nEen goede plek om te beginnen is om een kaart van de wereld te maken. Na installeren tmap, moeten we met de volgende coderegels de onderstaande kaart maken:\n\n\n\nHet object Wereld is een ruimtelijk object van klasse sf uit het [sf-pakket] (https://CRAN.R-project.org/package=sf); het is een data.frame met een speciale kolom die een geometrie voor elke rij bevat, in dit geval polygonen. Om het in tmap te tekenen, moet je het eerst specificeren met tm_shape. Plot-lagen kunnen worden toegevoegd met de + operator, in dit geval tm_polygonen. Er zijn veel laagfuncties in tmap, die gemakkelijk te vinden zijn in de documentatie door hun tm_ prefix. Zie ook `?‘tmap-element’``.\nMeerdere vormen en lagen\nEen vorm is een ruimtelijk object (met een klasse van sf, sp of raster). Meerdere vormen en ook meerdere lagen per vorm kunnen worden uitgezet:\n\n\n\nFacetten\nFacetten kunnen op drie manieren worden gemaakt:\nDoor meerdere variabele namen toe te kennen aan één esthetiek:\n\n\n\nDoor de ruimtelijke gegevens te splitsen met het by argument van tm_facets:\n\n\n\nDoor gebruik te maken van de tmap_arrange functie:\n\n\n\nSnelle thematische kaart\nKaarten kunnen snell worden gemaakt met slechts één functie op te roepen. Deze functie is qtm:\n\n\n\n\n\n",
    "preview": "posts/2018-11-14-simple-features/simple-features_files/figure-html5/ggplot-1.png",
    "last_modified": "2020-03-13T23:28:57+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-11-14-visualisatiegapminder/",
    "title": "Visualisatie met inzet van Gapminder",
    "description": "Een voorbeeld van datavisualisatie: Trends op het gebied van de wereldgezondheid\nen de economie",
    "author": [
      {
        "name": "Rafael A. Irizarry, bewerkt H. Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2018-08-14",
    "categories": [],
    "contents": "\nEen praktijkvoorbeeld: Trends op het gebied van de wereldgezondheid en de economie\nIn dit deel laten we zien hoe relatief eenvoudig de ggplot -code inzichtelijke en esthetisch aangename plots kan maken die ons helpen trends in de wereldgezondheid en economie beter te begrijpen. Later breiden we de code iets uit om de plots te perfectioneren en beschrijven we enkele algemene principes als leidraad voor datavisualisatie.\nVoorbeeld 1: Levensverwachting en vruchtbaarheidcijfers\nHans Rosling was medeoprichter van de Gapminder Foundation, een organisatie die het publiek wil stimuleren om gegevens te gebruiken om veelvoorkomende mythes over de zogenaamde ontwikkelingswereld te verdrijven. De organisatie gebruikt gegevens om aan te tonen hoe de daadwerkelijke tendensen in gezondheid en economie de verhalen tegenspreken die van de media komen en sensationeel berichten over catastrofes, tragedies en andere ongelukkige gebeurtenissen, zoals die op de website van de Gapminder Foundation staan.\n\n\n\nJournalisten en lobbyisten vertellen dramatische verhalen. Dat moeten ze want dat is hun taak. Ze vertellen verhalen over bijzondere gebeurtenissen en ongewone mensen. De stapels dramatische verhalen die een al te dramatisch wereldbeeld vormen en zorgen voor sterke negatieve stressgevoelens: “De wereld wordt erger”, “Wij vs. zij! , ,,Andere mensen zijn vreemd”, “De bevolking blijft maar groeien” en “Het maakt niemand wat uit”!\n\n\n\nHans Rosling bracht actuele data-gebaseerde trends op een eigen dramatische manier over aan de hand van effectieve datavisualisatie. Dit hoofdstuk is gebaseerd op twee gesprekken die deze benadering van onderwijs illustreren: Nieuwe inzichten in armoede en De beste Statistiek die je gezien hebt.\nIn deze paragraaf willen we aan de hand van gegevens een antwoord geven op de volgende twee vragen:\nIs het een eerlijke typering van de wereld van vandaag om te zeggen dat ze verdeeld is in westerse rijke naties en de ontwikkelingslanden in Afrika, Azi? en Latijns-Amerika?\nIs de inkomensongelijkheid tussen landen de afgelopen 40 jaar toegenomen?\nOm deze vraag te beantwoorden zullen we gebruik maken van de gapminder dataset in dslabs. Deze dataset is gemaakt met behulp van een aantal spreadsheets die beschikbaar zijn bij de Stichting [Gapminder] (http://www.gapminder.org/). U kunt op deze manier toegang krijgen tot de tafel:\n\n\nlibrary(dslabs)\ndata(gapminder)\nhead(gapminder)\n\n              country year infant_mortality life_expectancy fertility\n1             Albania 1960           115.40           62.87      6.19\n2             Algeria 1960           148.20           47.50      7.65\n3              Angola 1960           208.00           35.98      7.32\n4 Antigua and Barbuda 1960               NA           62.97      4.43\n5           Argentina 1960            59.87           65.39      3.11\n6             Armenia 1960               NA           66.86      4.55\n  population          gdp continent          region\n1    1636054           NA    Europe Southern Europe\n2   11124892  13828152297    Africa Northern Africa\n3    5270844           NA    Africa   Middle Africa\n4      54681           NA  Americas       Caribbean\n5   20619075 108322326649  Americas   South America\n6    1867396           NA      Asia    Western Asia\n\nDe quiz van Hans Rosling\nZoals gedaan in de New Insights on Poverty video, beginnen we met het testen van onze kennis over verschillen in kindersterfte tussen verschillende landen.\nVoor elk van de zes onderstaande paren landenparen, willen we weten welk land volgens u de hoogste kindersterfte in 2015 had? Welke paren lijken volgens jou het meest op elkaar?\nSri Lanka of Turkije\nPolen of Zuid-Korea\nMaleisi? of Rusland\nPakistan of Vietnam\nThailand of Zuid-Afrika\nWanneer deze vragen zonder gegevens worden beantwoord, worden de niet-Europese landen doorgaans gekozen als landen met hogere sterftecijfers: Sri Lanka boven Turkije, Zuid-Korea boven Polen en Maleisi? boven Rusland. Ook in landen die als ontwikkelingslanden worden beschouwd, Pakistan, Vietnam, Thailand en Zuid-Afrika, is het sterftecijfer even hoog.\nOm deze vragen __ met data__ te beantwoorden kunnen we het R-pakket dplyr gebruiken. Voor de eerste vergelijking zien we bijvoorbeeld dat\n\n\n    country infant_mortality\n1 Sri Lanka              8.4\n2    Turkey             11.6\n\nTurkije heeft een hogere score.\nWe kunnen deze code op alle vergelijkingen plakken en dan zien we het volgende:\n\ncountry\ninfant_mortality\ncountry1\ninfant_mortality1\nSri Lanka\n8.4\nTurkey\n11.6\nPoland\n4.5\nSouth Korea\n2.9\nMalaysia\n6.0\nRussia\n8.2\nPakistan\n65.8\nVietnam\n17.3\nThailand\n10.5\nSouth Africa\n33.6\n\nWe zien dat de Europese landen hogere cijfers hebben: Polen heeft een hoger percentage dan Zuid-Korea en Rusland een hoger percentage dan Maleisi?. We zien ook dat Pakistan een veel hoger percentage heeft dan Vietnam en Zuid-Afrika een veel hoger percentage dan Thailand. Het blijkt dat de meeste mensen het slechter doen dan wanneer ze zouden raden. We lijken wel onwetend zijn en we zijn verkeerd ge?nformeerd.\nLevensverwachting en vruchtbaarheidscijfers\nDe reden hiervoor is het idee dat de wereld in twee groepen te verdelen is: de Westerse wereld (West-Europa en Noord-Amerika), met z’n lange levensduur en kleine gezinnen, tegenover de ontwikkelingslanden (Afrika, Azi? en Latijns-Amerika), gekenmerkt door een korte levensduur en grote gezinnen. Maar rechtvaardigen de gegevens deze dichotomie van twee groepen wel?\nDe nodige gegevens om deze vraag te beantwoorden zitten ook in onze gapminder tabel. Met behulp van onze nieuw aangeleerde vaardigheden om data te visualiseren, zullen we in staat zijn om deze vraag te beantwoorden.\nDe eerste plot die we maken om te zien wat de gegevens zeggen over dit wereldbeeld is een spreidingplot van levensverwachting versus vruchtbaarheidscijfers (gemiddeld aantal kinderen per vrouw). We kijken eerst naar gegevens van zo’n vijftig jaar geleden, toen dit standpunt misschien nog te rechtvaardigen was.\n\n\n\nDe meeste punten vallen in twee verschillende categorie?n uiteen:\nLevensverwachting rond 70 jaar en 3 of minder kinderen per gezin\nLevensverwachting lager dan 65 jaar en met meer dan 5 kinderen per gezin.\nOm te bevestigen dat de landen inderdaad afkomstig zijn uit de regio’s die wij verwachten, kunnen wij kleur gebruiken om het continent te vertegenwoordigen.\n\n\n\nDus de visie in 1962, “het westen versus de ontwikkelingslanden”, was gebaseerd op een of andere realiteit. Maar is dat 50 jaar later nog steeds het geval?\nFacetteren\nWe konden de gegevens voor 2012 gemakkelijk in kaart brengen op dezelfde manier als voor 1962. Maar om de gegevens te vergelijken, kunnen we de inzichten het beste naast elkaar zetten. In ggplot kunnen we dit doen met’ faceting variabelen’: we stratificeren de gegevens met een of andere variabele en maken dezelfde plot voor elke groep.\nOm te facetteren (ik weet niet of het een Nederlands woord is, maar goed) voegen we een laag toe met de functie facet_grid, die automatisch de groepen scheidt. Met deze functie kunt u maximaal twee variabelen facetteren met behulp van kolommen om de ene variabele weer te geven en rijen om de andere weer te geven. De functie verwacht de rij- en kolomvariabelen die door een ~ van elkaar zijn gescheiden. Hier is een voorbeeld van een verstrooiingsplot met een facet_grid als laatste laag toegevoegd:\n\n\n\nWe zien een plot voor elk continent/jaarpaar. Maar dit is slechts een voorbeeld en is al meer dan wat we willen, want dat is gewoon om 1962 en 2012 te vergelijken. In dit geval is er maar ??n variabele en gebruiken we . om de facet te laten weten dat we geen van de variabelen gebruiken:\n\n\n\nDeze plot laat duidelijk zien dat de meerderheid van de landen zich heeft ontwikkeld van ontwikkelingslandcluster naar het ontwikkeldlandcluster. In 2012 heeft het oude perspectief geen zin meer. Dit wordt vooral duidelijk bij een vergelijking van Europa en Azi?, want vooral binnen dat laatste continent zijn er verschillende landen waarbinnen grote verbeteringen hebben doorgevoerd.\nfacet_wrap\nOm te onderzoeken hoe deze transformatie door de jaren heen is gegaan, kunnen we het perceel voor meerdere jaren maken. We kunnen bijvoorbeeld 1970, 1980, 1990, 2000 toevoegen. Als we dit doen, willen we niet dat alle percelen op dezelfde rij staan, het standaard gedrag van facet_grid, omdat ze te dun worden om de gegevens weer te geven. In plaats daarvan zullen we meerdere rijen en kolommen gebruiken. Dat kan met de functie facet_wrap , waarmee automatisch een reeks percelen onstaat met ded juiste afmetingen:\n\n\n\nDit plot toont duidelijk aan hoe de meeste Aziatische landen zich veel sneller hebben verbeterd dan de Europese.\nVaste schalen voor betere vergelijkingen\nMerk op dat de standaard keuze van het bereik van de assen een belangrijke is. Wanneer geen facet' wordt gebruikt, wordt het bereik bepaald door de gegevens die in de grafiek worden getoond. Bij gebruik vanfacet’ wordt dit bereik bepaald door de gegevens die op alle percelen worden weergegeven en wordt het dus voor alle percelen vastgehouden. Dit maakt vergelijkingen tussen percelen veel gemakkelijker. In bovenstaand perceel zien we bijvoorbeeld dat de levensverwachting in de meeste landen is gestegen en de vruchtbaarheid is afgenomen. We zien dit omdat de puntenwolk beweegt. Dit is niet het geval als we de schalen aanpassen:\n\n\n\nIn de plot hierboven moeten we speciale aandacht besteden aan het bereik om op te merken dat de rechter plot aan de rechter kant een grotere levensverwachting vertoont.\nAnimatie\nMet het gganimate pakket kunt u eenvoudig facetten omzetten in een animatie:\n\n\n\nTijdreeksfiguren\nBovenstaande visualisaties laten duidelijk zien dat data het oude beeld van het westen tegenover ontwikkelingslanden niet meer ondersteunen. Als we deze figuren eenmaal hebben gezien, rijzen er nieuwe vragen. Welke landen verbeteren bijvoorbeeld meer en welke minder? Was de verbetering de afgelopen 50 jaar constant of was er in bepaalde perioden sprake van een versnelling? Om deze vraag beter te kunnen beantwoorden, gaan we dieper in op de tijdreeksfiguren.\nIn tijdreeksfiguren staat tijd op de x-as en een uitkomst of meting die van belang is op de y-as. Hier is bijvoorbeeld een trendfiguur voor de vruchtbaarheidscijfers van Nederland:\n\n\n\nWe zien dat de trend helemaal niet lineair is. In plaats daarvan zien we een scherpe daling tijdens de jaren ’60 en ’70 naar onder de 2. Dan komt de trend terug op 2 en stabiliseert zich tijdens de jaren ’90.\nWanneer de punten regelmatig en dicht op elkaar liggen, zoals hier, maken we krommen door de punten als lijnen met elkaar te verbinden om aan te geven dat deze gegevens uit ??n land afkomstig zijn. Hiervoor gebruiken we de functie geom_line in plaats van geom_point.\n\n\n\nDit is met name nuttig wanneer we naar twee landen kijken. Als we de gegevens zo onderverdelen in twee landen, ??n uit Europa en ??n uit Azi?, dan kopieer je de gegevens naar de bovenstaande code:\n\n\n\nMerk op dat dit niet de figuur is die we willen. In plaats van een lijn voor elk land, worden de punten voor beide landen samengevoegd. Dit wordt eigenlijk verwacht omdat we ggplot niets hebben verteld over het willen hebben van twee aparte lijnen. Om ggplot te laten weten dat we twee afzonderlijke curven willen hebben, wijzen we elk punt toe aan een ‘groep’, ??n voor elk land:\n\n\n\nMaar welke lijn gaat over welk land? We kunnen kleuren toewijzen om dat onderscheid te maken. Een nuttig neveneffect van het gebruik van het ‘kleur’-argument om verschillende kleuren toe te wijzen aan de verschillende landen, is dat de gegevens automatisch worden gegroepeerd:\n\n\n\nUit het perceel blijkt duidelijk dat het vruchtbaarheidscijfer van Zuid-Korea in de jaren ’60 en ’70 drastisch is gedaald en in 1990 even hoog was als in Duitsland.\nDe voorkeur van labels boven legenda’s\nVoor trendplots raden we aan om de lijnen te labelen in plaats van legenda’s te gebruiken omdat de kijker snel kan zien welke lijn welk land is. Deze suggestie is eigenlijk van toepassing op de meeste figuren: labeling heeft meestal de voorkeur boven legenda’s.\nAan de hand van de gegevens over de levensverwachting laten we zien hoe we dit kunnen doen. We defini?ren een datatabel met de labellocaties en gebruiken dan een tweede mapping alleen voor deze labels:\n\n\n\nDe figuur toont duidelijk aan hoe een verbetering van levensverwachting de dalingen in vruchtbaarheidscijfers volgde. Terwijl de Duitsers in 1960 meer dan 15 jaar meer Zuid-Koreanen woonden, is de kloof in 2010 volledig gedicht. Het is een voorbeeld van de verbetering die veel niet-westerse landen in de afgelopen veertig jaar hebben bereikt.\nVoorbeeld 2: Inkomensverdeling\nEen andere veelgehoorde opmerking is dat de verdeling van de welvaart over de wereld de laatste decennia is verslechterd. Wanneer het algemene publiek wordt gevraagd of arme landen armer zijn geworden en rijke landen rijker, antwoordt de meerderheid ja. Door gebruik te maken van stratificatie, histogrammen, vloeiende verdeling en boxplots zullen we in staat zijn om te begrijpen of dit inderdaad het geval is. We leren dan ook hoe transformaties soms kunnen helpen om meer informatieve samenvattingen en figuren aan te bieden.\nTransformaties\nDe `gapminder’-gegevenstabel bevat een kolom met het bruto binnenlands product (BBP) van de landen. Het BBP meet de marktwaarde van de goederen en diensten die een land in een jaar produceert. Het BBP per persoon wordt vaak gebruikt als een ruwe samenvatting van hoe rijk een land is. Hier delen we deze hoeveelheid door 365 om de meer interpreteerbare maat dollars per dag te krijgen. Wanneer we de huidige US-dollar als eenheid gebruiken, wordt een persoon die met een inkomen van minder dan $ 2 per dag overleeft, gedefinieerd als een persoon die in absloute armoede leeft. Deze variabele voegen we toe aan de datatabel:\n\n\n\nMerk op dat de BBP-waarden zijn gecorrigeerd voor inflatie en staan voor de huidige US-dollar. Dus deze waarden zijn bedoeld om over de jaren heen vergelijkbaar te zijn. Merk ook op dat het hier om landsgemiddelden gaat en dat er binnen elk land veel variatie is. Alle hieronder beschreven grafieken en inzichten hebben betrekking op landsgemiddelden en staan dus niet individuele personen.\nVerdeling van het landinkomen\nHier is een histogram van de inkomens per dag uit 1970:\n\n\n\nWe gebruiken het color = \"black\" om een grens te trekken en de bins (‘bakjes’) duidelijk van elkaar te onderscheiden.\nIn dit diagram zien we dat voor de meeste landen gemiddelden zijn onder $10 per dag. Het grootste deel van de x-as is echter gewijd aan de 35 landen met gemiddelden boven $10. De grafiek is dus niet erg informatief over landen met waarden onder $10 per dag.\nHet is misschien informatiever om snel te kunnen zien hoeveel landen een gemiddeld daginkomen hebben van ongeveer $1 (extreem arm), $2 (zeer arm), $4 (arm), $8 (midden), $16 (welgesteld), $32 (rijk), $64 (zeer rijk) per dag. Deze veranderingen zijn vermenigvuldigend en logtransformaties.\nHier is de verdeling als we een log2 transformatie toepassen:\n\n\n\nIn zekere zin geeft dit een close up van de landen met een gemiddeld tot lager inkomen.\nWelke basis?\nIn het bovenstaande geval hebben we basis 2 gebruikt in de log-transformaties. Andere veelvoorkomende keuzes zijn basis \\(e\\) (de natuurlijke log) en basis 10.\nOver het algemeen raden wij het gebruik van het natuurlijke logboek voor het verkennen en visualiseren van gegevens aan.Dit is omdat \\(2^2, 2^3, 2^4, \\dots\\) or \\(10^1, 10^2, \\dots\\) makkelijk zijn te berekenen in ons hoofd. Hetzelfde geldt niet voor \\(\\mathrm{e}^2, \\mathrm{e}^3, \\dots\\).\nIn het voorbeeld dollars per dag gebruikten we basis 2 in plaats van basis 10 omdat het resulterende bereik gemakkelijker te interpreteren is. Het bereik van de waarden die worden uitgezet is 0.3269426, 48.8852142.\nIn basis 10 verandert dit in een bereik dat zeer weinig gehele getallen omvat: slechts 0 en 1. Met basis twee omvat ons assortiment -2, -1, 0, 1, 2, 3, 4 en 5. Het is gemakkelijker om \\(2^x\\) en \\(10^x\\) te berekenen wanneer \\(x\\) een geheel getal is en tussen -10 en 10 ligt. Dus geven we de voorkeur aan meer kleine gehele getallen in de schaal. Een ander gevolg van een beperkt bereik is dat het kiezen van de ‘bin’-breedte een grotere uitdaging is. Met log base 2 weten we dat een ‘bin’-breedte van 1 zal vertalen naar een bin met bereik van \\(x\\) tot \\(2x\\).\nAls voorbeeld waarbij basis 10 zinvoller is, overweeg dan de populatiegrootte. Een logbasis 10 is zinvoller omdat het bereik hiervoor ongeveer 1000 tot 10 miljard is. Hier is het histogram van de getransformeerde waarden:\n\n\n\nHier zien we al snel dat de bevolking van een land varieert tussen de tienduizend en tien miljard.\nTransform the values or the scale?\nEr zijn twee manieren waarop we log-transformaties in grafieken kunnen gebruiken. We kunnen de waarden loggen voordat we ze plotten of gebruik maken van logschalen in de assen. Beide benaderingen zijn nuttig en hebben verschillende sterke punten. Als we de gegevens loggen, kunnen we gemakkelijker tussenliggende waarden interpreteren in de schaal. Bijvoorbeeld als we zien\n\n\n—-1—-x—-2——–3—-\n\n\nvoor log getransformeerde gegevens weten we dat de waarde van \\(x\\) is ongeveer 1,5. Als de weegschalen gelogd zijn\n\n\n—-1—-x—-10——100—\n\n\nom x te bepalen, moeten we \\(10^{1.5}\\) berekenen. Dat is niet gemakkelijk te doen in onze hoofden. Het voordeel van het tonen van gelogde schalen is echter dat de originele waarden worden weergegeven in de plot, die gemakkelijker te interpreteren zijn. Bijvoorbeeld, we zouden “32 dollar per dag” zien in plaats van “5 log basis 2 dollar per dag”.\nZoals we eerder leerden, als we de as willen schalen met logs kunnen we de functie ‘schaal_x_ccontinue’ gebruiken. Dus in plaats van eerst de waarden te loggen, passen we deze laag toe:\n\n\n\nMerk op dat de log base 10 transformatie zijn eigen functie heeft: scale_x_log10(), maar momenteel base 2 niet. Hoewel we dit zelf gemakkelijk konden defini?ren.\nMerk op dat er andere transformatie beschikbaar zijn via het trans argument. Zoals we later leren, is bijvoorbeeld de vierkantsworteltransformatie (sqrt) nuttig bij het tellen. De logistieke transformatie (logit) is nuttig bij het plotten van proporties tussen 0 en 1. De omgekeerde transformatie is nuttig als we willen dat kleinere waarden rechts of bovenop staan.\nModus\nIn de statistieken wordt deze hobbel ook wel modus genoemd. De modus van een verdeling is de waarde met de hoogste frequentie. De modus van de normale verdeling is het gemiddelde. Wanneer een distributie, zoals de bovenstaande, niet eentonig afneemt van de modus, noemen we de locaties waar het op en neer gaat weer lokale modi en zeggen dat de distributie meerdere modi heeft.\nHet histogram hierboven suggereert dat de inkomensverdeling van het land in 1970 twee modi kent: ??n met ongeveer 2 dollar per dag (1 in de log 2 schaal) en ??n met ongeveer 32 dollar per dag (5 in de log 2 schaal). Deze bimodaliteit is consistent met een dichotome wereld die bestaat uit landen met een gemiddeld inkomen van minder dan $8 (3 in de log 2 schaal) per dag en landen daarboven.\nStratificeren en boxplot\nHet histogram liet zien dat de inkomensverdelingswaarden een tweedeling vertonen. Het histogram laat echter niet zien of de twee groepen landen west tegenover de ontwikkelings wereld zijn.\nOm de verdeling naar geografische regio te zien, stratificeren we eerst de gegevens naar regio’s en onderzoeken we vervolgens de verdeling voor elke regio.\n\n\n[1] 22\n\nVanwege het aantal regio’s zijn histogrammen of gladde dichtheden voor elk niet nuttig. In plaats daarvan kunnen we boxplots naast elkaar stapelen:\n\n\n\nMerk op dat we de regionamen niet kunnen lezen omdat het standaard ggplot-gedrag is om de labels horizontaal te schrijven en hier lopen we dan de ruimte uit. We kunnen dit eenvoudig repareren door de etiketten te draaien. In de sheetuitleg vinden we dat we de namen kunnen roteren door het thema te veranderen via element_text. Het just=1 zorgt ervoor dat deze zich naast de as bevindt.\n\n\n\nWe zien nu dat er inderdaad een tweedeling is tussen het westen en de rest.\nOrden niet alfabetisch\nEr zijn nog een paar aanpassingen die we kunnen maken om in de grafiek deze realiteit beter bloot te leggen. Ten eerste helpt het om de regio’s in de boxplots te ordenen van arm naar rijk in plaats van alfabetisch. Dit kan worden gedaan met behulp van de reorder functie. Deze functie laat ons de orde van de niveaus van een factorvariabele op basis van een samenvatting veranderen die op een numerieke vector wordt berekend. Een karaktervector wordt in een factor gedwongen:\nHieronder staat een voorbeeld. Merk op hoe de volgorde van de niveaus verandert:\n\n\n[1] \"Asia\" \"West\"\n\n[1] \"West\" \"Asia\"\n\nTen tweede kunnen we kleur gebruiken om de verschillende continenten te onderscheiden, een visuele kleurschakering die helpt om specifieke regio’s te vinden. Hier is de code:\n\n\n\nDit figuur toont twee duidelijke groepen, met de rijke groep bestaande uit Noord-Amerika, Noord- en West-Europa, Nieuw-Zeeland en Australi?. Net als met het histogram, als we de plot herschikken met behulp van een logschaal zijn we in staat om verschillen binnen de deconcentratiewereld beter te zien.\n\n\n\nDe data tonen\nIn veel gevallen tonen we de gegevens niet omdat het rommel aan het figuur toevoegt en het bericht vertroebelt. In bovenstaand voorbeeld hebben we niet zoveel punten. Dan kunnen deze laag toevoegen met behulp van geom_point() en door punten toe te voegen kunnen we eigenlijk alle gegevens zien\n\n\n\nVerdelingen vergelijken\nDe bovenstaande verkennende gegevensanalyse heeft twee kenmerken van de gemiddelde inkomensverdeling in 1970 aan het licht gebracht. Aan de hand van een histogram vonden we een bimodale verdeling met de modi voor arme en rijke landen. Door in het onderzoek stratificatie per regio toe te passen zagen we met boxplots dat de rijke landen meestal in Europa, Noord-Amerika, Australi? en Nieuw-Zeelandm lagen. Met deze regio’s defini?ren we een vector:\n\n\n\nNu willen we ons richten op het vergelijken van de verschillen in verdelingen in de tijd.\nWe bevestigen eerst de bimodaliteit die we in 1970 waarnamen en dat deze wordt verklaard door de westelijk tegenover de ontwikkelingswereld- dichotomie. Dit doen we door histogrammen te maken voor de groepen die we hebben ge?dentificeerd. Merk op dat we de twee groepen maken met ifelse binnen een mutaat en dat we facet_grid gebruiken om een histogram te maken voor elke groep:\n\n\n\nNu kunnen we kijken of de scheiding vandaag de dag slechter is dan veertig jaar geleden. Dit doen we door zowel per regio als per jaar te facetteren:\n\n\n\nVoordat we de bevindingen van deze plot interpreteren, stellen we vast dat er meer landen vertegenwoordigd zijn in de histogrammen van 2010 dan in 1970: het aantal tellingen is groter. Een van de redenen hiervoor is dat verschillende landen na 1970 zijn opgericht. De Sovjet-Unie is in de jaren negentig is veranderd in verschillende landen, waaronder Rusland en Oekra?ne. Een andere reden is dat in 2010 voor meer landen gegevens beschikbaar zijn.\nWe hebben de figuren opnieuw gemaakt met behulp van alleen landen met gegevens die beschikbaar zijn voor beide jaren. In het hoofdstuk over datawisselingen leren we tidyverse tools waarmee we hiervoor effici?nte codes kunnen schrijven, maar hier een eenvoudige code met behulp van de kruispunt functie:\n\n\n\nThese 108 account for 86 % of the world population, so this subset should be representative.\nLaten we de plot opnieuw maken, maar alleen voor deze subset door simpelweg land % in% country_list aan de filterfunctie toe te voegen:\n\n\n\nWe zien nu dat terwijl de rijke landen procentueel wat rijker zijn geworden, de arme landen meer lijken te zijn verbeterd. We zien vooral dat het aandeel van ontwikkelingslanden waar mensen meer dan $16 per dag verdienen aanzienlijk toeneemt.\nOm te zien welke specifieke regio’s het meest verbeterden, kunnen we de boxplots die we hierboven hebben gemaakt met nu 2010 toegevoegd, opnieuw maken\n\n\n\nen dan met behulp van facet om de twee jaar te vergelijken:\n\n\n\nHier pauzeren we om nog een krachtige ggplot2-functie te introduceren. Omdat we elke regio voor en na willen vergelijken, zou het handig zijn om de 1970 boxplot naast de 2010 boxplot voor elke regio te hebben. Over het algemeen zijn vergelijkingen gemakkelijker wanneer gegevens naast elkaar worden uitgezet.\nDus in plaats van facetteren houden we de gegevens van elk jaar bij elkaar, maar vragen we ggplot om ze afhankelijk van het jaar te kleuren (of te vullen). Ggplot scheidt ze automatisch en plaatst de twee boxplots naast elkaar. Omdat jaar een getal is, maken we er een factor van omdat ggplot automatisch een kleur toewijst aan elke categorie van een factor:\n\n\n\nTot slot wijzen we erop dat als wat we het meest ge?nteresseerd zijn in het vergelijken van voor en na waarden, kan het zinvoller zijn om de verhoudingen, of verschil in de log schaal plot te zetten. We zijn nog steeds niet klaar om te leren om dit te leren coderen, maar hier hoe het figuur eruit zou zien:\n\n\n\nSubtiele verdelingplots\nMet behulp van dataverkenning hebben we ontdekt dat de inkomenskloof tussen rijke en arme landen de afgelopen veertig jaar aanzienlijk is gedicht. We gebruikten een reeks histogrammen en boxplots om dit te laten zien. Hier suggereren we een beknopte manier om deze boodschap over te brengen met slechts een plot. We zullen hiervoor subtiele verdelingplots gebruiken.\nLaten we beginnen met op te merken dat verdelingplots voor inkomensverdeling in 1970 en 2010 de boodschap afgeven dat de kloof aan het dichten is:\n\n\n\nIn de 1970 plot zien we twee heldere modi, arme en rijke landen. In 2010 lijkt het erop dat sommige arme landen naar rechts zijn verschoven, dat aangeeft dat de kloof is gedicht.\nDe volgende boodschap die we moeten uitdragen is dat de reden voor deze verandering in de verdeling is dat arme landen rijker werden in plaats van dat sommige rijke landen armer werden. Om dit te doen hoeven we alleen een kleur toe te wijzen aan de groepen die we hebben ge?dentificeerd tijdens de dataverkenning.\nVoordat we dit echter kunnen doen, moeten we leren hoe we deze vlotte dichtheden zo kunnen maken dat de informatie over het aantal landen in elke groep behouden blijft. Om te begrijpen waarom we dit nodig hebben, let dan op de discrepantie in de grootte van elke groep:\n\ngroup\nn\nDeveloping\n87\nWest\n21\n\nmaar als twee verdelingen overlappen, is de standaardinstelling dat het gebied dat door elke distributie wordt weergegeven tot 1 wordt opgeteld, ongeacht de grootte van elke groep:\n\n\n\nwaardoor het lijkt alsof er in elke groep evenveel landen zitten. Om dit te veranderen, zullen we moeten leren om berekende variabelen met de geom_density functie te benaderen.\nToegang tot computervariabelen\nOm de oppervlakten van deze verdelingen evenredig te laten zijn met de grootte van de groepen, kunnen we de y-aswaarden eenvoudig vermenigvuldigen met de grootte van de groep. Vanuit de geom_density help-file zien we dat de functie, die een variabele genaamd count berekenen, precies dit doet. We willen dat deze variabele op de y-as komt te staan.\nIn ggplot krijgen we toegang tot deze variabelen door ze te omringen met de naam ... Daarom zullen we de volgende mapping gebruiken:\n\n\n\nNu kunnen we de gewenste plot maken door simpelweg de mapping in het vorige codebrok te veranderen:\n\n\n\nAls we willen dat de verdelingen wordt subtieler worden, gebruiken we het ‘bw’-argument. We probeerden er een paar en kozen voor 0,75:\n\n\n\nDee plot laat nu heel duidelijk zien wat er aan de hand is. De verdeling in de ontwikkelingslanden is aan het veranderen. Een derde modus lijkt te bestaan en bestaat uit de landen die de kloof het meest hebben gedicht.\ncase_when\nWe kunnen dit cijfer zelfs iets informatiever maken. Uit de verkennende gegevensanalyse merkten we dat veel van de landen die het meest verbeterden, afkomstig waren uit Azi?. We kunnen de plot gemakkelijk wijzigen door belangrijke regio’s afzonderlijk weer te geven.\nWe introduceren de case_when functie die handig is voor het defini?ren van groepen. Het heeft momenteel geen data-argument (dit kan veranderen), dus we moeten de onderdelen van onze gegevenstabel benaderen met behulp van de stip plaatsbewerker:\n\n\n\nWe maken van deze ‘groep’-variabele een factor om de volgorde van de niveaus te bepalen:\n\n\n\nWe kiezen deze specifieke volgorde vanwege een reden die later duidelijk wordt.\nNu kunnen we eenvoudig de verdeling voor elk in kaart brengen. We gebruiken kleur' engrootte’ om de toppen duidelijk te laten zien:\n\n\n\nDe plot is rommelig en wat moeilijk te lezen. Soms krijg je een duidelijker beeld door de dichtheden op elkaar te stapelen:\n\n\n\nHier zien we duidelijk dat de verdelingen voor Oost-Azi?, Latijns-Amerika e.a. duidelijk naar rechts verschuiven. Terwijl Afrika bezuiden de Sahara blijft stagneren.\nMerk op dat we de niveaus van de groep zo ordenen dat de West verdeling eerst wordt uitgezet, dan Sub-Sahara Afrika. Als we eerst de twee uitersten in kaart brengen, zien we de resterende bimodaliteit beter.\nGewogen verdelingen\nTot slot merken we op dat deze uitkeringen in alle landen hetzelfde wegen. Dus als het grootste deel van de bevolking zich verbetert, maar in een heel groot land woont, zoals China, zullen we dit misschien niet op prijs stellen. We kunnen de vloeiende verdelingen eigenlijk wegen met behulp van het gewicht in kaart brengen argument. Het plot ziet er dan als volgt uit:\n\n\n\nDeze specifieke figuur laat heel duidelijk zien hoe de inkomenskloof wordt gedicht, waarbij de meeste armen in Afrika bezuiden de Sahara blijven.\nEcologische denkfout\nIn deze sectie hebben we regio’s van de wereld met elkaar vergeleken. We hebben gezien dat sommige regio’s het gemiddeld beter doen dan andere. Hier richten we ons op het beschrijven van het belang van verschillen binnen de groepen.\nHier richten we ons op de relatie tussen de overlevingskans van kinderen in een land en het gemiddelde inkomen. We beginnen met het vergelijken van deze hoeveelheden tussen regio’s. We defini?ren nog een paar regio’s:\n\n\n\nVervolgens berekenen we deze hoeveelheden per regio.\n\n\n# A tibble: 7 x 3\n  group              income infant_survival_rate\n  <chr>               <dbl>                <dbl>\n1 Sub-Saharan Africa   1.76                0.936\n2 Southern Asia        2.07                0.952\n3 Pacific Islands      2.70                0.956\n4 Northern Africa      4.94                0.970\n5 Latin America       13.2                 0.983\n6 East Asia           13.4                 0.985\n7 The West            77.1                 0.995\n\nDit laat een dramatisch verschil zien. Terwijl in het westen minder dan 0,5 procent van de kinderen sterft, is dat in Afrika bezuiden de Sahara meer dan 6 procent! De relatie tussen deze twee variabelen is bijna perfect lineair\n\n\n\nIn deze plot introduceren we het gebruik van het limit argument dat laat het bereik van de assen veranderen. We maken het bereik groter dan de gegevensbehoeften omdat we dit plot later zullen vergelijken met een plot met meer variabiliteit; we willen dat de bereiken hetzelfde zijn. We introduceren ook het `breaks’ argument, waarmee we de locatie van de aslabels kunnen instellen. Eindelijk introduceren we een nieuwe transformatie, de logistieke transformatie.\nLogistische transformatie\nDe logistische of logistieke transformatie voor een deel of een koers van \\(p\\) wordt gedefinieerd als\n\\[f(p) = \\log \\left( \\frac{p}{1-p} \\right)\\]\nWanneer \\(p\\) een proportie of waarschijnlijkheid is, wordt de hoeveelheid die wordt gelogd, \\(p/(1-p)\\) de odds genoemd. In het geval \\(p\\) is het aandeel van een kind dat overleefde. De kansen vertellen ons hoeveel meer kinderen worden uitgedreven om te overleven dan om te sterven. De logtransformatie maakt dit symmetrisch. Als de snelheden gelijk zijn, dan is de log odds 0. Bij toe- of afname verandert het in positieve en negatieve stappen.\nDeze schaal is handig als we verschillen in de buurt van 0 of 1 willen markeren. Voor overlevingskansen is dit van belang omdat een overlevingsgraad van 90% onaanvaardbaar is, terwijl een overlevingsgraad van 99% relatief goed is. We zouden veel liever een overlevingskans hebben die dichter bij 99,9 procent ligt. We willen dat onze schaal dit verschil benadrukt en de logit doet dit. Merk op dat 99.9/0.1 ongeveer 10 keer groter is dan 99/1, wat ongeveer 10 keer groter is dan 90/10. En door gebruik te maken van de log worden deze wissels steeds groter.\nToon de gegevens\nNu, terug naar onze plot. Kan nu geconcludeerd worden dat op basis van het bovenstaande een land met een laag inkomen een lage overlevingskans zal hebben? Kunnen we concluderen dat alle overlevingskansen in Afrika bezuiden de Sahara lager zijn dan in Zuid-Azi?, dat op zijn beurt lager is dan op de eilanden in de Stille Oceaan, enzovoort?\nDe conclusie gebaseerd op een plot dat weer wordt gebaseerd op gemiddelden wordt ecologische denkfout genoemd. De bijna perfecte relatie tussen overlevingskansen en inkomen zien we alleen bij de gemiddelden op regionaal niveau. Als we alle gegevens eenmaal hebben getoond, krijgen we een wat ingewikkelder verhaal:\n\n\n\nWe zien we een grote mate van variabiliteit. We zien dat landen uit dezelfde regio’s heel verschillend kunnen zijn en dat landen met hetzelfde inkomen verschillende overlevingskansen kunnen hebben. Terwijl Afrika bezuiden de Sahara bijvoorbeeld gemiddeld de slechtste gezondheids- en economische resultaten had, is er binnen die groep sprake van grote variabiliteit. Merk bijvoorbeeld op dat Mauritius en Botswana het beter doen dan Angola en Sierra Leone en Mauritius zelfs vergelijkbaar is met Westerse landen.\n\n\n",
    "preview": "posts/2018-11-14-visualisatiegapminder/visualisatiegapminder_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2020-03-13T23:28:57+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-11-14-sem/",
    "title": "SEM",
    "description": "In de sociale wetenschappen kunnen sommige constructen, zoals intelligentie, vertrouwen, motivatie, vervreemding of conservatisme, niet direct worden geobserveerd. Het zijn in essentie constructen of concepten waarvoor geen methode bestaat om ze direct te meten. Onderzoekers gebruiken hiervoor geobserveerde maten die indicatoren zijn voor een latente variabele. Structural equation modeling is een onderzoeks-raamwerk dat rekening kan houden met de meetfouten in de geobserveerde variabelen die in het model zitten. SEM is een flexibel en krachtige methode om tegelijkertijd op een goede manier de kwaliteit van het meten in de gaten te houden als om causale relaties tussen de constructen vast te stellen. In de map vind je een korte presentatie over SEM",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2018-07-07",
    "categories": [],
    "contents": "\nDe presentatie vind je hier\n\n\n",
    "preview": {},
    "last_modified": "2020-03-13T23:28:57+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-11-14-rrstudiormarkdown/",
    "title": "RRStudioRmarkdown",
    "description": "Hier een klein boekje om jou te laten wennen aan reproduceerbaar onderzoek. Het introduceert het programma R, de RStudio-schil en de programmeertaal RMarkdown.",
    "author": [
      {
        "name": "Chester Ismay, bewerkt Harrie Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2018-06-05",
    "categories": [],
    "contents": "\nDat boekje vind je hier. http://www.harriejonkman.nl/wp-content/uploads/2018/01/rbasics.pdf\n\n\n",
    "preview": {},
    "last_modified": "2020-03-13T23:28:57+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-11-14-bayes/",
    "title": "Bayes",
    "description": "Over de geschiedenis van de Bayesiaanse statistiek",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2018-05-14",
    "categories": [],
    "contents": "\n“Als de feiten veranderen, verander ik mijn mening. Wat doe jij dan, meneer?” (Keynes)\nThomas Bayes was een Engels wiskundige en presbyteriaans predikant die zichzelf in de tweede helft van de 18e eeuw enkele belangrijke vragen stelde, zoals hoe moeten we onze overtuigingen aanpassen als we nieuwe informatie krijgen en hoe houden we vast aan oude veronderstellingen, ook als ze onhoudbaar zijn geworden. Of laten we die overtuigingen heel makkelijk los, ook als we een klein beetje gaan twijfelen?\nBayes maakt ons ervan bewust dat we onze opvattingen steeds geleidelijk aan passen aan de werkelijkheid. Zijn stelling, of theorema, is een belangrijk instrument geworden voor allerlei wetenschappers en de Bayesiaanse regel staat niet zelden heel stoer op een t-shirt afgebeeld:\n\\[P(A|B) = \\frac {P(A) \\times P(B|A)}{P(B)}\\]\nBayes draaide dingen om en wilde iets zeggen over een hypothese gegeven het bewijsmateriaal dat we hebben. Hoe moet je een veronderstelling begrijpen in het licht van empirisch materiaal? Bayes’ theorie kent vervolgens een lange, ingewikkelde en verrassende geschiedenis die tot op de dag van vandaag doorgaat. Die geschiedenis heeft Sharon Bertsch McGrayne, een Amerikaanse wetenschapsjournalist, heel mooi in beeld gebracht. Maar het was niet Bayes zelf, maar zijn vriend Richard Price, een amateur wiskundige, die deze ideeën openbaar maakte na Bayes’ dood. Zonder Price had die theorie helemaal niet bestaan. Price stuurde Bayes wetenschappelijke werk in en zorgde er zo voor dat er een publicatie van kwam. Naar die stelling werd vervolgens helemaal niet meer omgekeken tot de beroemde Franse wiskundige Pierre Simon Laplace in het begin van de 19e eeuw deze ideeën uitbreidde en op slimme manieren toepaste. Daarna was het maatschappelijk aan en uit met die theorie, werd het gebruikt en raakte het ook weer uit de mode. De ideeën werden op het ene na het andere gebied toegepast, om vervolgens weer te worden veroordeeld omdat het zou werken met vage, subjectieve of onwetenschappelijke uitgangspunten. Rivaliserende kampen (Frequentisten en Baysianen) voerden hier lange tijd grote strijd over.\nDe theorie zelf kan eenvoudig worden uitgelegd. Je hebt een bepaalde hypothese, bijvoorbeeld over een munt. Je denkt dat de kans op kop of munt hetzelde is. Daar ga je van te voren van uit. Als je vijf keer gooit en je gooit steeds kop dan denk je nog dat het toeval is en je past jouw veronderstelling nog niet aan. Anders wordt het als dit 100 keer gebeurt, dan ga je toch echt twijfelen over de munt en ga je denken dat het misschien wel alleen maar kop kent. Bij waarschijnlijkheid gaat het om uitsluitende mogelijkheden die je toekent. Je hebt van tevoren een idee over iets (prior), vervolgens heb je de data (likelihood) en dat zorgt vervolgens voor jouw geupdated kennis (posterior). Bayes’ theorie is een manier om de waarschijnlijkheid steeds op een consistent en logische manier te herberekenen. Dat herberekenen van de hypothese (of kennis die we hebben) vindt dus steeds plaats in het licht van nieuwe bewijzen. Deze geupdated of aangepaste waarschijnlijkheid wordt de posterior probability of gewoon de posterior genoemd.\nOm het iets ingewikkelder te zeggen, de stelling van Bayes’s houdt nu in dat de posterior waarschijnlijkheid van een hypothese gelijk is aan het product van de voorafgaande waarschijnlijkheid van de hypothese (dus wat je weet al, de prior) en de waarschijnlijkheid van het bewijs gegeven de hypothese (de data, de likelihood). Dit deel je vervolgens door de waarschijnlijkheid van alle bewijzen. Dat laatste gebeurt omdat je zo steeds een waarde tussen 0 en 1 te krijgen, en hoe dichter bij 1 hoe groter de kans. Dat is precies wat op dat stoere t-shirt staat.\nHeel veel kennis is er opgebouwd met munten, kaarten en dergelijke. Moeilijker wordt het wanneer je de stelling van Bayes toepast op het echte leven. Maar ook hier gebeurt het op een zelfde manier. Stel dat je naar buitenloopt en ziet dat jouw tuinpad nat is. Dan denk je misschien dat het geregend heeft. Als je verder loopt en ziet dat de straten droog zijn, ga je toch denken dat jouw vrouw misschien de tuin heeft gesproeid. Wanneer je het weerbericht hebt gehoord (waarin regen wordt voorspeld) voordat je naar buitenloopt, denk je nog eerder dat het geregend heeft als je over het natte tuinpad loopt. Met dat weersbericht en dat natte tuinpad word je minder snel van jouw regengedachte afgehaald als je de droge straat ziet. In het dagelijks leven worden gedachten en veronderstellingen voortdurend geüpdated. Kennis wordt gebruikt en nieuw bewijsmateriaal wordt voortdurend daarmee gefilterd en verwerkt in jouw Bayesiaanse hoofd. Steeds meer doen we kennis op over hoe kansen zijn toe te wijzen en bewijs kan worden geëvalueerd in situaties die veel ingewikkelder zijn dan het gooien van munten of het inschatten van regen.\nMcGrayne besteedt veel aandacht aan allerlei bijdragen van individuele wetenschappers aan die boeiende geschiedenis van de Bayesiaanse theorie. Zo bespreekt ze uitgebreid hoe het in oorlogsvoering is gebruikt, in het proces van kolonel Dreyfus, hoe Alan Turing met deze theorie Duitse codes kraakte en er met dat kleine groepje slimme mensen voor zorgde dat de Tweede Wereldoorlog minder lang duurde. Maar veel meer voorbeelden komen aan de orde.\nBayesiaanse theorie wordt tegenwoordig op allerlei gebieden van wetenschap toegepast. Ook in onze dagelijks leven hebben we ermee te maken via vertaalmachines, spamfilters en stuurloze auto’s bijvoorbeeld. We zijn er onszelf nauwelijks van bewust. McGrayne heeft niet alleen oog voor statistici die zich succesvol wijdden aan Bayesiaanse statistiek. Ze laat ook zien hoe deze door andere statici (zoals Fischer, als vertegenwoordiger van de zogenaamde Frequentisten) heftig worden tegengewerkt. De kern van de verschillen tussen deze twee groepen is dat volgens Bayesianen de prior een subjectieve uitdrukking kan zijn van de mate van geloof in een hypothese. Je moet de kennis gebruiken die je al hebt, het is vreemd om steeds maar opnieuw en van voren af aan te beginnen. Frequentisten erkennen dit subjectieve element niet. Voor hen moet wetenschap een objectieve basis hebben, liefste in relatieve frequentie van gebeurtenissen in herhaalbare, welomschreven experimenten.Dat subjectieve element staat daar ver van.\nMcGrayne’s boek is een prachtig wetenschapshistorisch boek Zie ook haar presentatie hier. Ze laat zien dat de Bayesiaanse theorie weer helemaal terug is en dat komt natuurlijk vooral ook door de ontwikkeling van de computer en moderne algoritmen waarmee de theorie niet enkel theorie blijft maar ook praktisch kan worden toegepast. David Spiegelhalter en zijn biostatistische onderzoeksgroep hebben daar op een ongekende manier aan bijgedragen. De Bayesiaanse methode wordt in allerlei onderzoek gebruikt en in verschillende omstandigheden heeft het grote voordelen zien ten opzichte van de frequentistische statistiek. Er wordt vandaag de dag veel pragmatischer mee hiermee omgegaan. De tegenwoordige wetenschapper loopt steeds vaker met twee gereedschapskisten rond en op basis van het probleem besluit hij of zij tot een onderzoekswijze.\n\n\n",
    "preview": {},
    "last_modified": "2020-03-13T23:28:57+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-11-14-reproducable-research/",
    "title": "Dynamische documenten maken met RMarkdown en Knitr",
    "description": "RMarkdown en Knitr zijn pakketten die je in staat stellen om reproduceerbare en dynamische documenten te maken. In deze blog wordt uitgelegd hoe je hiermee kunt werken.",
    "author": [
      {
        "name": "Marian L. Schmidt, bewerkt door Harrie Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2018-03-15",
    "categories": [],
    "contents": "\n\nWelkom!\nDeze workshop is eerder gegeven door Schmidt op 11 Mei 2016 en er waren wat aanvullende materialen beschikbaar:\nKlas notities die voor de workshop konden worden gebruikt etherpad.\nDe workshop is ook live opgenomen en beschikbaar op YouTube. Kijk naar dit college op YouTube! Zeer leerzaam.\nDeze tutorial is gemaakt als onderdeel van een programma van Dr. C Titus Brown’s Data Intensive Biology (DIB) trainingsprogramma op de Universiteit van Californie, Davis. Dit DIB trainingsprogramma omvat lokale + remote workshops die onderwerpen en gereedschappen behandelen in de bio-informatica en data analyse.\nHet college werd gegeven vanuit Ann Arbor Michigan en drie klassen van elders deden tegelijkertijd mee:\nUniversiteit van Californi?, Davis\nSimon Fraser Universiteit\nOntario Instituut for Kanker Onderzoek\nDe Github repository, die ik binnen heb gehaald en hier gebruik voor deze les, kan worden gevonden op hier.\nOpgelet: Dit soort werk introduceert wel een groot aantal Engelse woorden in onze Nederlandse taal. Enkele daarvan heb ik laten staan, niet omdat ik ze mooi vind maar omdat ze al overal gebruikt worden. Gewone mensen praten al over bijvoorbeeld renderen, cachen, files, widgets en output. Met excuses hiervoor, bij voorbaat.\nInstructies bij het installeren\nVoordat je gaat werken met de workshopmaterialen, zorg ervoor dat je het volgende hebt gedaan:\nOpen RStudio.\nInstalleer en download het devtools R pakket door het volgende commando te runnen.\n\n\n\nCheck of je de goede versie hebt van R en RStudio door devtools::session_info() in de R console te draaien.\nHier geeft devtools:: aan om de session_info() functie in R te gebruiken ipv het devtools pakket en de sessionInfo() functie binnen het utils pakket. Het runnen van devtools::session_info() stelt ons in staat de versie van R en RStudio vast te stellen.\nHeb je de volgende versie van R en RStudio?\nR: Versie 3.3.0 (2016-05-03)\nRStudio: 0.99.1172\nZo ja dan kun je van start gaan!\nZo nee dan heb je nieuwe versies van R en RStudio nodig, volg dan Setup in dit document.\n\nInstalleer andere R pakketten die nodig zijn voor deze workshop.\n\n\n\n\n\n\nAls je de pakketten zonder fouten hebt geladen, ben je klaar voor deze workshop!\nZijn er nog problemen, meld het ajb!\nGoede bronnen\nDeze tutorial kon niet samengesteld worden zonder onderstaande goede bronnen:\nDe RMarkdown website van RStudio.\nDr. Yuhui Xie’s boek: Dynamic Documents with R and Knitr 2nd Edition [@Xie2015] en zijn Knitr website.\nHEEL VEEL DANK aan Dr. Xie voor het schrijven van het knitr pakket!!\n\nDr. Karl Broman’s “Knitr in a Knutshell”.\nCheatsheets released by RStudio.\nDynamische documenten\nLiterate programming, zoals dat in het Engels wordt genoemd, is het basisidee achter dynamische documenten en is ge?ntroduceerd door Donald Knuth in 1984. Oorspronkelijk om de broncode en de bijbehorende software documentatie samen te brengen. Tegenwoordig cre?ren we dynamische documenten waarin het programma of de analyse code samen draaien om tot ‘outputs’ te komen (bv. tabellen, plots, modellen, etc) die worden uitgelegd via narratief schrijven.\nDe drie stappen van Literate Programming:\nParse het bron document en haal de code en het verhaal uit elkaar.\nExecute bron code en laat de resultaten zien.\nMix resultaten van de broncode met het orginele verhaal.\nEr blijven 2 stappen voor ons voor wat betreft het schrijven:\nAnalyse code\nEen verhaal om de resultaten van de analyse code toe te lichten.\nTraditioneel gebruikten mensen commentaren om het verhaal in de code file kwijt te kunnen raken (voor R zou dat een .R file zijn). Deze file zou het volgende in kunnen houden:\n\n\n# Titel:  Relatie tussen Autogewicht en Gasefficientie/of-verbruik\n# Door :  Marian Schmidt  \n# Datum:  11 Mei 2016\n\n# Ik verspel dat er een relatie is tussen het gewicht van de auto en de afstand die met de brandstof afgelegd kan worden.  \n# Dat test ik met een lineaire analyse van de ''mtcarsdataset' als onderdeel van de R datasets\n\n# Hoe zien de data eruit?\n#datatable(mtcars) # Interactieve tabel \n\n# Is er een relatie tussen het gewicht en de afstand per, in dit geval, gallon?\nlm_mpg <- lm(mpg ~ wt, data = mtcars) # Run het lineaire model dat mpg voorspelt op basis van wt\ncoef_lm_mpg <- coef(summary(lm_mpg))  # Haal de coefficienten eruit voor de tabel die komt \nkable(coef_lm_mpg)                    # Maak een niet-interactieve tabel - een functie in knitr\n\n# Plot de relatie tussen gewicht en afstand in mijl per gallon   \nplot <- ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() + # \n  geom_smooth(method = \"lm\") + theme_bw() +           # Maak een lineair model en maak het zwart en wit\n  xlab(\"Weight (1000lbs)\") + ylab(\"Miles per Gallon\") # Voeg tekst aan de assen toe\nggplotly(plot)                                        # Maak de plot interactief  \n\n# Het lijkt erop dat met een toename van 1000 pounds er een afname is van brandstof gebruik met 5.34 mijl per gallon\n# Het eind\n\nDe gebruiker zal de commentaren lezen en de codes zelf runnen.\nEchter, ‘literate programming’ stelt ons in staat de code te runnen en de resultaten te beschrijven, allemaal in een document dat we kunnen delen. We zouden bijvoorbeeld het volgende kunnen doen:\nRelatie tussen gewicht van de auto en het brandstofverbruikDoor: Marian SchmidtDatum: 11 Mei 2016\nIk voorspel dat er een relatie is tussen het gewicht en de afstand die met de brandstof kan worden afgelegd.\nIk test dat met een lineair model op een dataset in R.\n\n\n{\"x\":{\"filter\":\"none\",\"data\":[[\"Mazda RX4\",\"Mazda RX4 Wag\",\"Datsun 710\",\"Hornet 4 Drive\",\"Hornet Sportabout\",\"Valiant\",\"Duster 360\",\"Merc 240D\",\"Merc 230\",\"Merc 280\",\"Merc 280C\",\"Merc 450SE\",\"Merc 450SL\",\"Merc 450SLC\",\"Cadillac Fleetwood\",\"Lincoln Continental\",\"Chrysler Imperial\",\"Fiat 128\",\"Honda Civic\",\"Toyota Corolla\",\"Toyota Corona\",\"Dodge Challenger\",\"AMC Javelin\",\"Camaro Z28\",\"Pontiac Firebird\",\"Fiat X1-9\",\"Porsche 914-2\",\"Lotus Europa\",\"Ford Pantera L\",\"Ferrari Dino\",\"Maserati Bora\",\"Volvo 142E\"],[21,21,22.8,21.4,18.7,18.1,14.3,24.4,22.8,19.2,17.8,16.4,17.3,15.2,10.4,10.4,14.7,32.4,30.4,33.9,21.5,15.5,15.2,13.3,19.2,27.3,26,30.4,15.8,19.7,15,21.4],[6,6,4,6,8,6,8,4,4,6,6,8,8,8,8,8,8,4,4,4,4,8,8,8,8,4,4,4,8,6,8,4],[160,160,108,258,360,225,360,146.7,140.8,167.6,167.6,275.8,275.8,275.8,472,460,440,78.7,75.7,71.1,120.1,318,304,350,400,79,120.3,95.1,351,145,301,121],[110,110,93,110,175,105,245,62,95,123,123,180,180,180,205,215,230,66,52,65,97,150,150,245,175,66,91,113,264,175,335,109],[3.9,3.9,3.85,3.08,3.15,2.76,3.21,3.69,3.92,3.92,3.92,3.07,3.07,3.07,2.93,3,3.23,4.08,4.93,4.22,3.7,2.76,3.15,3.73,3.08,4.08,4.43,3.77,4.22,3.62,3.54,4.11],[2.62,2.875,2.32,3.215,3.44,3.46,3.57,3.19,3.15,3.44,3.44,4.07,3.73,3.78,5.25,5.424,5.345,2.2,1.615,1.835,2.465,3.52,3.435,3.84,3.845,1.935,2.14,1.513,3.17,2.77,3.57,2.78],[16.46,17.02,18.61,19.44,17.02,20.22,15.84,20,22.9,18.3,18.9,17.4,17.6,18,17.98,17.82,17.42,19.47,18.52,19.9,20.01,16.87,17.3,15.41,17.05,18.9,16.7,16.9,14.5,15.5,14.6,18.6],[0,0,1,1,0,1,0,1,1,1,1,0,0,0,0,0,0,1,1,1,1,0,0,0,0,1,0,1,0,0,0,1],[1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,1,1,1,1,1,1,1],[4,4,4,3,3,3,3,4,4,4,4,3,3,3,3,3,3,4,4,4,3,3,3,3,3,4,5,5,5,5,5,4],[4,4,1,1,2,1,4,2,2,4,4,3,3,3,4,4,4,1,2,1,1,2,2,4,2,1,2,2,4,6,8,2]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>mpg<\\/th>\\n      <th>cyl<\\/th>\\n      <th>disp<\\/th>\\n      <th>hp<\\/th>\\n      <th>drat<\\/th>\\n      <th>wt<\\/th>\\n      <th>qsec<\\/th>\\n      <th>vs<\\/th>\\n      <th>am<\\/th>\\n      <th>gear<\\/th>\\n      <th>carb<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3,4,5,6,7,8,9,10,11]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false}},\"evals\":[],\"jsHooks\":[]}\nEstimate\nStd. Error\nt value\nPr(>|t|)\n(Intercept)\n37.285126\n1.877627\n19.857575\n0\nwt\n-5.344472\n0.559101\n-9.559044\n0\n\n{\"x\":{\"data\":[{\"x\":[2.62,2.875,2.32,3.215,3.44,3.46,3.57,3.19,3.15,3.44,3.44,4.07,3.73,3.78,5.25,5.424,5.345,2.2,1.615,1.835,2.465,3.52,3.435,3.84,3.845,1.935,2.14,1.513,3.17,2.77,3.57,2.78],\"y\":[21,21,22.8,21.4,18.7,18.1,14.3,24.4,22.8,19.2,17.8,16.4,17.3,15.2,10.4,10.4,14.7,32.4,30.4,33.9,21.5,15.5,15.2,13.3,19.2,27.3,26,30.4,15.8,19.7,15,21.4],\"text\":[\"wt: 2.620<br />mpg: 21.0\",\"wt: 2.875<br />mpg: 21.0\",\"wt: 2.320<br />mpg: 22.8\",\"wt: 3.215<br />mpg: 21.4\",\"wt: 3.440<br />mpg: 18.7\",\"wt: 3.460<br />mpg: 18.1\",\"wt: 3.570<br />mpg: 14.3\",\"wt: 3.190<br />mpg: 24.4\",\"wt: 3.150<br />mpg: 22.8\",\"wt: 3.440<br />mpg: 19.2\",\"wt: 3.440<br />mpg: 17.8\",\"wt: 4.070<br />mpg: 16.4\",\"wt: 3.730<br />mpg: 17.3\",\"wt: 3.780<br />mpg: 15.2\",\"wt: 5.250<br />mpg: 10.4\",\"wt: 5.424<br />mpg: 10.4\",\"wt: 5.345<br />mpg: 14.7\",\"wt: 2.200<br />mpg: 32.4\",\"wt: 1.615<br />mpg: 30.4\",\"wt: 1.835<br />mpg: 33.9\",\"wt: 2.465<br />mpg: 21.5\",\"wt: 3.520<br />mpg: 15.5\",\"wt: 3.435<br />mpg: 15.2\",\"wt: 3.840<br />mpg: 13.3\",\"wt: 3.845<br />mpg: 19.2\",\"wt: 1.935<br />mpg: 27.3\",\"wt: 2.140<br />mpg: 26.0\",\"wt: 1.513<br />mpg: 30.4\",\"wt: 3.170<br />mpg: 15.8\",\"wt: 2.770<br />mpg: 19.7\",\"wt: 3.570<br />mpg: 15.0\",\"wt: 2.780<br />mpg: 21.4\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(0,0,0,1)\",\"opacity\":1,\"size\":5.66929133858268,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,0,0,1)\"}},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1.513,1.56250632911392,1.61201265822785,1.66151898734177,1.7110253164557,1.76053164556962,1.81003797468354,1.85954430379747,1.90905063291139,1.95855696202532,2.00806329113924,2.05756962025316,2.10707594936709,2.15658227848101,2.20608860759494,2.25559493670886,2.30510126582278,2.35460759493671,2.40411392405063,2.45362025316456,2.50312658227848,2.55263291139241,2.60213924050633,2.65164556962025,2.70115189873418,2.7506582278481,2.80016455696203,2.84967088607595,2.89917721518987,2.9486835443038,2.99818987341772,3.04769620253165,3.09720253164557,3.14670886075949,3.19621518987342,3.24572151898734,3.29522784810127,3.34473417721519,3.39424050632911,3.44374683544304,3.49325316455696,3.54275949367089,3.59226582278481,3.64177215189873,3.69127848101266,3.74078481012658,3.79029113924051,3.83979746835443,3.88930379746835,3.93881012658228,3.9883164556962,4.03782278481013,4.08732911392405,4.13683544303798,4.1863417721519,4.23584810126582,4.28535443037975,4.33486075949367,4.3843670886076,4.43387341772152,4.48337974683544,4.53288607594937,4.58239240506329,4.63189873417722,4.68140506329114,4.73091139240506,4.78041772151899,4.82992405063291,4.87943037974684,4.92893670886076,4.97844303797468,5.02794936708861,5.07745569620253,5.12696202531646,5.17646835443038,5.2259746835443,5.27548101265823,5.32498734177215,5.37449367088608,5.424],\"y\":[29.1989406778126,28.9343555091934,28.6697703405742,28.4051851719549,28.1406000033357,27.8760148347165,27.6114296660973,27.3468444974781,27.0822593288588,26.8176741602396,26.5530889916204,26.2885038230012,26.023918654382,25.7593334857627,25.4947483171435,25.2301631485243,24.9655779799051,24.7009928112859,24.4364076426666,24.1718224740474,23.9072373054282,23.642652136809,23.3780669681898,23.1134817995705,22.8488966309513,22.5843114623321,22.3197262937129,22.0551411250937,21.7905559564744,21.5259707878552,21.261385619236,20.9968004506168,20.7322152819976,20.4676301133783,20.2030449447591,19.9384597761399,19.6738746075207,19.4092894389015,19.1447042702822,18.880119101663,18.6155339330438,18.3509487644246,18.0863635958054,17.8217784271861,17.5571932585669,17.2926080899477,17.0280229213285,16.7634377527093,16.49885258409,16.2342674154708,15.9696822468516,15.7050970782324,15.4405119096132,15.1759267409939,14.9113415723747,14.6467564037555,14.3821712351363,14.1175860665171,13.8530008978978,13.5884157292786,13.3238305606594,13.0592453920402,12.794660223421,12.5300750548017,12.2654898861825,12.0009047175633,11.7363195489441,11.4717343803249,11.2071492117056,10.9425640430864,10.6779788744672,10.413393705848,10.1488085372288,9.88422336860955,9.61963819999033,9.35505303137111,9.09046786275189,8.82588269413267,8.56129752551345,8.29671235689423],\"text\":[\"wt: 1.513000<br />mpg: 29.198941\",\"wt: 1.562506<br />mpg: 28.934356\",\"wt: 1.612013<br />mpg: 28.669770\",\"wt: 1.661519<br />mpg: 28.405185\",\"wt: 1.711025<br />mpg: 28.140600\",\"wt: 1.760532<br />mpg: 27.876015\",\"wt: 1.810038<br />mpg: 27.611430\",\"wt: 1.859544<br />mpg: 27.346844\",\"wt: 1.909051<br />mpg: 27.082259\",\"wt: 1.958557<br />mpg: 26.817674\",\"wt: 2.008063<br />mpg: 26.553089\",\"wt: 2.057570<br />mpg: 26.288504\",\"wt: 2.107076<br />mpg: 26.023919\",\"wt: 2.156582<br />mpg: 25.759333\",\"wt: 2.206089<br />mpg: 25.494748\",\"wt: 2.255595<br />mpg: 25.230163\",\"wt: 2.305101<br />mpg: 24.965578\",\"wt: 2.354608<br />mpg: 24.700993\",\"wt: 2.404114<br />mpg: 24.436408\",\"wt: 2.453620<br />mpg: 24.171822\",\"wt: 2.503127<br />mpg: 23.907237\",\"wt: 2.552633<br />mpg: 23.642652\",\"wt: 2.602139<br />mpg: 23.378067\",\"wt: 2.651646<br />mpg: 23.113482\",\"wt: 2.701152<br />mpg: 22.848897\",\"wt: 2.750658<br />mpg: 22.584311\",\"wt: 2.800165<br />mpg: 22.319726\",\"wt: 2.849671<br />mpg: 22.055141\",\"wt: 2.899177<br />mpg: 21.790556\",\"wt: 2.948684<br />mpg: 21.525971\",\"wt: 2.998190<br />mpg: 21.261386\",\"wt: 3.047696<br />mpg: 20.996800\",\"wt: 3.097203<br />mpg: 20.732215\",\"wt: 3.146709<br />mpg: 20.467630\",\"wt: 3.196215<br />mpg: 20.203045\",\"wt: 3.245722<br />mpg: 19.938460\",\"wt: 3.295228<br />mpg: 19.673875\",\"wt: 3.344734<br />mpg: 19.409289\",\"wt: 3.394241<br />mpg: 19.144704\",\"wt: 3.443747<br />mpg: 18.880119\",\"wt: 3.493253<br />mpg: 18.615534\",\"wt: 3.542759<br />mpg: 18.350949\",\"wt: 3.592266<br />mpg: 18.086364\",\"wt: 3.641772<br />mpg: 17.821778\",\"wt: 3.691278<br />mpg: 17.557193\",\"wt: 3.740785<br />mpg: 17.292608\",\"wt: 3.790291<br />mpg: 17.028023\",\"wt: 3.839797<br />mpg: 16.763438\",\"wt: 3.889304<br />mpg: 16.498853\",\"wt: 3.938810<br />mpg: 16.234267\",\"wt: 3.988316<br />mpg: 15.969682\",\"wt: 4.037823<br />mpg: 15.705097\",\"wt: 4.087329<br />mpg: 15.440512\",\"wt: 4.136835<br />mpg: 15.175927\",\"wt: 4.186342<br />mpg: 14.911342\",\"wt: 4.235848<br />mpg: 14.646756\",\"wt: 4.285354<br />mpg: 14.382171\",\"wt: 4.334861<br />mpg: 14.117586\",\"wt: 4.384367<br />mpg: 13.853001\",\"wt: 4.433873<br />mpg: 13.588416\",\"wt: 4.483380<br />mpg: 13.323831\",\"wt: 4.532886<br />mpg: 13.059245\",\"wt: 4.582392<br />mpg: 12.794660\",\"wt: 4.631899<br />mpg: 12.530075\",\"wt: 4.681405<br />mpg: 12.265490\",\"wt: 4.730911<br />mpg: 12.000905\",\"wt: 4.780418<br />mpg: 11.736320\",\"wt: 4.829924<br />mpg: 11.471734\",\"wt: 4.879430<br />mpg: 11.207149\",\"wt: 4.928937<br />mpg: 10.942564\",\"wt: 4.978443<br />mpg: 10.677979\",\"wt: 5.027949<br />mpg: 10.413394\",\"wt: 5.077456<br />mpg: 10.148809\",\"wt: 5.126962<br />mpg:  9.884223\",\"wt: 5.176468<br />mpg:  9.619638\",\"wt: 5.225975<br />mpg:  9.355053\",\"wt: 5.275481<br />mpg:  9.090468\",\"wt: 5.324987<br />mpg:  8.825883\",\"wt: 5.374494<br />mpg:  8.561298\",\"wt: 5.424000<br />mpg:  8.296712\"],\"type\":\"scatter\",\"mode\":\"lines\",\"name\":\"fitted values\",\"line\":{\"width\":3.77952755905512,\"color\":\"rgba(51,102,255,1)\",\"dash\":\"solid\"},\"hoveron\":\"points\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[1.513,1.56250632911392,1.61201265822785,1.66151898734177,1.7110253164557,1.76053164556962,1.81003797468354,1.85954430379747,1.90905063291139,1.95855696202532,2.00806329113924,2.05756962025316,2.10707594936709,2.15658227848101,2.20608860759494,2.25559493670886,2.30510126582278,2.35460759493671,2.40411392405063,2.45362025316456,2.50312658227848,2.55263291139241,2.60213924050633,2.65164556962025,2.70115189873418,2.7506582278481,2.80016455696203,2.84967088607595,2.89917721518987,2.9486835443038,2.99818987341772,3.04769620253165,3.09720253164557,3.14670886075949,3.19621518987342,3.24572151898734,3.29522784810127,3.34473417721519,3.39424050632911,3.44374683544304,3.49325316455696,3.54275949367089,3.59226582278481,3.64177215189873,3.69127848101266,3.74078481012658,3.79029113924051,3.83979746835443,3.88930379746835,3.93881012658228,3.9883164556962,4.03782278481013,4.08732911392405,4.13683544303798,4.1863417721519,4.23584810126582,4.28535443037975,4.33486075949367,4.3843670886076,4.43387341772152,4.48337974683544,4.53288607594937,4.58239240506329,4.63189873417722,4.68140506329114,4.73091139240506,4.78041772151899,4.82992405063291,4.87943037974684,4.92893670886076,4.97844303797468,5.02794936708861,5.07745569620253,5.12696202531646,5.17646835443038,5.2259746835443,5.27548101265823,5.32498734177215,5.37449367088608,5.424,5.424,5.424,5.37449367088608,5.32498734177215,5.27548101265823,5.2259746835443,5.17646835443038,5.12696202531646,5.07745569620253,5.02794936708861,4.97844303797468,4.92893670886076,4.87943037974684,4.82992405063291,4.78041772151899,4.73091139240506,4.68140506329114,4.63189873417722,4.58239240506329,4.53288607594937,4.48337974683544,4.43387341772152,4.3843670886076,4.33486075949367,4.28535443037975,4.23584810126582,4.1863417721519,4.13683544303798,4.08732911392405,4.03782278481013,3.9883164556962,3.93881012658228,3.88930379746835,3.83979746835443,3.79029113924051,3.74078481012658,3.69127848101266,3.64177215189873,3.59226582278481,3.54275949367089,3.49325316455696,3.44374683544304,3.39424050632911,3.34473417721519,3.29522784810127,3.24572151898734,3.19621518987342,3.14670886075949,3.09720253164557,3.04769620253165,2.99818987341772,2.9486835443038,2.89917721518987,2.84967088607595,2.80016455696203,2.7506582278481,2.70115189873418,2.65164556962025,2.60213924050633,2.55263291139241,2.50312658227848,2.45362025316456,2.40411392405063,2.35460759493671,2.30510126582278,2.25559493670886,2.20608860759494,2.15658227848101,2.10707594936709,2.05756962025316,2.00806329113924,1.95855696202532,1.90905063291139,1.85954430379747,1.81003797468354,1.76053164556962,1.7110253164557,1.66151898734177,1.61201265822785,1.56250632911392,1.513,1.513],\"y\":[26.9637596243046,26.748211630978,26.532293643118,26.3159798039527,26.0992420315869,25.8820498075489,25.6643699464367,25.4461663458981,25.2273997165231,25.0080272917409,24.7880025185342,24.5672747307859,24.3457888084175,24.1234848272503,23.9002977068049,23.6761568661453,23.4509859014399,23.2247023032092,22.9972172362447,22.7684354108148,22.5382550797684,22.3065682020549,22.073260818275,21.8382136871272,21.6013032316477,21.3624028393291,21.1213845488153,20.8781211364057,20.6324885872813,20.3843688997083,20.1336531278785,19.8802445251398,19.624061610802,19.3650409583153,19.1031394977534,18.8383361460623,18.5706326249216,18.3000533934652,18.0266447019082,17.7504728499618,17.4716217986802,17.190190327058,16.9062889412588,16.6200367360143,16.3315583798572,16.0409813560006,15.748433546582,15.4540412060478,15.157927333949,14.8602104304317,14.5610035995316,14.2604139550846,13.9585422800956,13.6554828910183,13.3513236620105,13.0461461695019,12.740025923372,12.4330326570007,12.1252306540028,11.8166790943701,11.5074324069286,11.1975406184823,10.8870496928064,10.5760018548558,10.2644358972571,9.95238746743981,9.6398893347169,9.32697163731397,9.01366210982825,8.69998629192387,8.38596771927446,8.07162809788066,7.75698746294225,7.44206432347029,7.1268757937988,6.81143771310854,6.49576475401476,6.17987052120298,5.86376764102542,5.5474678428992,5.5474678428992,11.0459568708893,11.2588274100015,11.4718948670624,11.685170971489,11.8986683496337,12.1124006061819,12.3263824137488,12.5406296115153,12.7551593138153,12.96999002966,13.185141794249,13.400636313583,13.6164971233358,13.8327497631713,14.0494219676868,14.266543875108,14.4841482547477,14.7022707540356,14.9209501655981,15.1402287143902,15.3601523641871,15.5807711417929,15.8021394760335,16.0243165469005,16.2473666380091,16.4713594827389,16.6963705909696,16.9224815391307,17.1497802013801,17.3783608941716,17.6083244005099,17.8397778342311,18.0728342993708,18.307612296075,18.5442348238948,18.7828281372767,19.023520118358,19.266438250352,19.5117072017912,19.7594460674074,20.0097653533643,20.2627638386563,20.5185254843377,20.7771165901197,21.0385834062175,21.3029503917649,21.5702192684414,21.8403689531932,22.1133563760938,22.3891181105935,22.6675726760022,22.9486233256676,23.2321611137817,23.5180680386105,23.8062200853351,24.096490030255,24.3887499120139,24.6828731181045,24.9787360715631,25.276219531088,25.5752095372801,25.8755980490886,26.1772833193625,26.4801700583702,26.7841694309033,27.0891989274822,27.3951821442752,27.7020485003464,28.0097329152165,28.3181754647066,28.6273210287383,28.9371189411946,29.2475226490581,29.5584893857579,29.8699798618842,30.1819579750846,30.4943905399572,30.8072470380303,31.1204993874088,31.4341217313206,26.9637596243046],\"text\":[\"wt: 1.513000<br />mpg: 29.198941\",\"wt: 1.562506<br />mpg: 28.934356\",\"wt: 1.612013<br />mpg: 28.669770\",\"wt: 1.661519<br />mpg: 28.405185\",\"wt: 1.711025<br />mpg: 28.140600\",\"wt: 1.760532<br />mpg: 27.876015\",\"wt: 1.810038<br />mpg: 27.611430\",\"wt: 1.859544<br />mpg: 27.346844\",\"wt: 1.909051<br />mpg: 27.082259\",\"wt: 1.958557<br />mpg: 26.817674\",\"wt: 2.008063<br />mpg: 26.553089\",\"wt: 2.057570<br />mpg: 26.288504\",\"wt: 2.107076<br />mpg: 26.023919\",\"wt: 2.156582<br />mpg: 25.759333\",\"wt: 2.206089<br />mpg: 25.494748\",\"wt: 2.255595<br />mpg: 25.230163\",\"wt: 2.305101<br />mpg: 24.965578\",\"wt: 2.354608<br />mpg: 24.700993\",\"wt: 2.404114<br />mpg: 24.436408\",\"wt: 2.453620<br />mpg: 24.171822\",\"wt: 2.503127<br />mpg: 23.907237\",\"wt: 2.552633<br />mpg: 23.642652\",\"wt: 2.602139<br />mpg: 23.378067\",\"wt: 2.651646<br />mpg: 23.113482\",\"wt: 2.701152<br />mpg: 22.848897\",\"wt: 2.750658<br />mpg: 22.584311\",\"wt: 2.800165<br />mpg: 22.319726\",\"wt: 2.849671<br />mpg: 22.055141\",\"wt: 2.899177<br />mpg: 21.790556\",\"wt: 2.948684<br />mpg: 21.525971\",\"wt: 2.998190<br />mpg: 21.261386\",\"wt: 3.047696<br />mpg: 20.996800\",\"wt: 3.097203<br />mpg: 20.732215\",\"wt: 3.146709<br />mpg: 20.467630\",\"wt: 3.196215<br />mpg: 20.203045\",\"wt: 3.245722<br />mpg: 19.938460\",\"wt: 3.295228<br />mpg: 19.673875\",\"wt: 3.344734<br />mpg: 19.409289\",\"wt: 3.394241<br />mpg: 19.144704\",\"wt: 3.443747<br />mpg: 18.880119\",\"wt: 3.493253<br />mpg: 18.615534\",\"wt: 3.542759<br />mpg: 18.350949\",\"wt: 3.592266<br />mpg: 18.086364\",\"wt: 3.641772<br />mpg: 17.821778\",\"wt: 3.691278<br />mpg: 17.557193\",\"wt: 3.740785<br />mpg: 17.292608\",\"wt: 3.790291<br />mpg: 17.028023\",\"wt: 3.839797<br />mpg: 16.763438\",\"wt: 3.889304<br />mpg: 16.498853\",\"wt: 3.938810<br />mpg: 16.234267\",\"wt: 3.988316<br />mpg: 15.969682\",\"wt: 4.037823<br />mpg: 15.705097\",\"wt: 4.087329<br />mpg: 15.440512\",\"wt: 4.136835<br />mpg: 15.175927\",\"wt: 4.186342<br />mpg: 14.911342\",\"wt: 4.235848<br />mpg: 14.646756\",\"wt: 4.285354<br />mpg: 14.382171\",\"wt: 4.334861<br />mpg: 14.117586\",\"wt: 4.384367<br />mpg: 13.853001\",\"wt: 4.433873<br />mpg: 13.588416\",\"wt: 4.483380<br />mpg: 13.323831\",\"wt: 4.532886<br />mpg: 13.059245\",\"wt: 4.582392<br />mpg: 12.794660\",\"wt: 4.631899<br />mpg: 12.530075\",\"wt: 4.681405<br />mpg: 12.265490\",\"wt: 4.730911<br />mpg: 12.000905\",\"wt: 4.780418<br />mpg: 11.736320\",\"wt: 4.829924<br />mpg: 11.471734\",\"wt: 4.879430<br />mpg: 11.207149\",\"wt: 4.928937<br />mpg: 10.942564\",\"wt: 4.978443<br />mpg: 10.677979\",\"wt: 5.027949<br />mpg: 10.413394\",\"wt: 5.077456<br />mpg: 10.148809\",\"wt: 5.126962<br />mpg:  9.884223\",\"wt: 5.176468<br />mpg:  9.619638\",\"wt: 5.225975<br />mpg:  9.355053\",\"wt: 5.275481<br />mpg:  9.090468\",\"wt: 5.324987<br />mpg:  8.825883\",\"wt: 5.374494<br />mpg:  8.561298\",\"wt: 5.424000<br />mpg:  8.296712\",\"wt: 5.424000<br />mpg:  8.296712\",\"wt: 5.424000<br />mpg:  8.296712\",\"wt: 5.374494<br />mpg:  8.561298\",\"wt: 5.324987<br />mpg:  8.825883\",\"wt: 5.275481<br />mpg:  9.090468\",\"wt: 5.225975<br />mpg:  9.355053\",\"wt: 5.176468<br />mpg:  9.619638\",\"wt: 5.126962<br />mpg:  9.884223\",\"wt: 5.077456<br />mpg: 10.148809\",\"wt: 5.027949<br />mpg: 10.413394\",\"wt: 4.978443<br />mpg: 10.677979\",\"wt: 4.928937<br />mpg: 10.942564\",\"wt: 4.879430<br />mpg: 11.207149\",\"wt: 4.829924<br />mpg: 11.471734\",\"wt: 4.780418<br />mpg: 11.736320\",\"wt: 4.730911<br />mpg: 12.000905\",\"wt: 4.681405<br />mpg: 12.265490\",\"wt: 4.631899<br />mpg: 12.530075\",\"wt: 4.582392<br />mpg: 12.794660\",\"wt: 4.532886<br />mpg: 13.059245\",\"wt: 4.483380<br />mpg: 13.323831\",\"wt: 4.433873<br />mpg: 13.588416\",\"wt: 4.384367<br />mpg: 13.853001\",\"wt: 4.334861<br />mpg: 14.117586\",\"wt: 4.285354<br />mpg: 14.382171\",\"wt: 4.235848<br />mpg: 14.646756\",\"wt: 4.186342<br />mpg: 14.911342\",\"wt: 4.136835<br />mpg: 15.175927\",\"wt: 4.087329<br />mpg: 15.440512\",\"wt: 4.037823<br />mpg: 15.705097\",\"wt: 3.988316<br />mpg: 15.969682\",\"wt: 3.938810<br />mpg: 16.234267\",\"wt: 3.889304<br />mpg: 16.498853\",\"wt: 3.839797<br />mpg: 16.763438\",\"wt: 3.790291<br />mpg: 17.028023\",\"wt: 3.740785<br />mpg: 17.292608\",\"wt: 3.691278<br />mpg: 17.557193\",\"wt: 3.641772<br />mpg: 17.821778\",\"wt: 3.592266<br />mpg: 18.086364\",\"wt: 3.542759<br />mpg: 18.350949\",\"wt: 3.493253<br />mpg: 18.615534\",\"wt: 3.443747<br />mpg: 18.880119\",\"wt: 3.394241<br />mpg: 19.144704\",\"wt: 3.344734<br />mpg: 19.409289\",\"wt: 3.295228<br />mpg: 19.673875\",\"wt: 3.245722<br />mpg: 19.938460\",\"wt: 3.196215<br />mpg: 20.203045\",\"wt: 3.146709<br />mpg: 20.467630\",\"wt: 3.097203<br />mpg: 20.732215\",\"wt: 3.047696<br />mpg: 20.996800\",\"wt: 2.998190<br />mpg: 21.261386\",\"wt: 2.948684<br />mpg: 21.525971\",\"wt: 2.899177<br />mpg: 21.790556\",\"wt: 2.849671<br />mpg: 22.055141\",\"wt: 2.800165<br />mpg: 22.319726\",\"wt: 2.750658<br />mpg: 22.584311\",\"wt: 2.701152<br />mpg: 22.848897\",\"wt: 2.651646<br />mpg: 23.113482\",\"wt: 2.602139<br />mpg: 23.378067\",\"wt: 2.552633<br />mpg: 23.642652\",\"wt: 2.503127<br />mpg: 23.907237\",\"wt: 2.453620<br />mpg: 24.171822\",\"wt: 2.404114<br />mpg: 24.436408\",\"wt: 2.354608<br />mpg: 24.700993\",\"wt: 2.305101<br />mpg: 24.965578\",\"wt: 2.255595<br />mpg: 25.230163\",\"wt: 2.206089<br />mpg: 25.494748\",\"wt: 2.156582<br />mpg: 25.759333\",\"wt: 2.107076<br />mpg: 26.023919\",\"wt: 2.057570<br />mpg: 26.288504\",\"wt: 2.008063<br />mpg: 26.553089\",\"wt: 1.958557<br />mpg: 26.817674\",\"wt: 1.909051<br />mpg: 27.082259\",\"wt: 1.859544<br />mpg: 27.346844\",\"wt: 1.810038<br />mpg: 27.611430\",\"wt: 1.760532<br />mpg: 27.876015\",\"wt: 1.711025<br />mpg: 28.140600\",\"wt: 1.661519<br />mpg: 28.405185\",\"wt: 1.612013<br />mpg: 28.669770\",\"wt: 1.562506<br />mpg: 28.934356\",\"wt: 1.513000<br />mpg: 29.198941\",\"wt: 1.513000<br />mpg: 29.198941\"],\"type\":\"scatter\",\"mode\":\"lines\",\"line\":{\"width\":3.77952755905512,\"color\":\"transparent\",\"dash\":\"solid\"},\"fill\":\"toself\",\"fillcolor\":\"rgba(153,153,153,0.4)\",\"hoveron\":\"points\",\"hoverinfo\":\"x+y\",\"showlegend\":false,\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":23.3059360730594,\"r\":7.30593607305936,\"b\":37.2602739726027,\"l\":37.2602739726027},\"plot_bgcolor\":\"rgba(255,255,255,1)\",\"paper_bgcolor\":\"rgba(255,255,255,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[1.31745,5.61955],\"tickmode\":\"array\",\"ticktext\":[\"2\",\"3\",\"4\",\"5\"],\"tickvals\":[2,3,4,5],\"categoryorder\":\"array\",\"categoryarray\":[\"2\",\"3\",\"4\",\"5\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(235,235,235,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"y\",\"title\":\"Weight (1000lbs)\",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[4.12984123504415,35.317626607855],\"tickmode\":\"array\",\"ticktext\":[\"10\",\"20\",\"30\"],\"tickvals\":[10,20,30],\"categoryorder\":\"array\",\"categoryarray\":[\"10\",\"20\",\"30\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(51,51,51,1)\",\"ticklen\":3.65296803652968,\"tickwidth\":0.66417600664176,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":11.689497716895},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":true,\"gridcolor\":\"rgba(235,235,235,1)\",\"gridwidth\":0.66417600664176,\"zeroline\":false,\"anchor\":\"x\",\"title\":\"Miles per Gallon\",\"titlefont\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":14.6118721461187},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":\"transparent\",\"line\":{\"color\":\"rgba(51,51,51,1)\",\"width\":0.66417600664176,\"linetype\":\"solid\"},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":\"rgba(255,255,255,1)\",\"bordercolor\":\"transparent\",\"borderwidth\":1.88976377952756,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":11.689497716895}},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"modeBarButtonsToAdd\":[{\"name\":\"Collaborate\",\"icon\":{\"width\":1000,\"ascent\":500,\"descent\":-50,\"path\":\"M487 375c7-10 9-23 5-36l-79-259c-3-12-11-23-22-31-11-8-22-12-35-12l-263 0c-15 0-29 5-43 15-13 10-23 23-28 37-5 13-5 25-1 37 0 0 0 3 1 7 1 5 1 8 1 11 0 2 0 4-1 6 0 3-1 5-1 6 1 2 2 4 3 6 1 2 2 4 4 6 2 3 4 5 5 7 5 7 9 16 13 26 4 10 7 19 9 26 0 2 0 5 0 9-1 4-1 6 0 8 0 2 2 5 4 8 3 3 5 5 5 7 4 6 8 15 12 26 4 11 7 19 7 26 1 1 0 4 0 9-1 4-1 7 0 8 1 2 3 5 6 8 4 4 6 6 6 7 4 5 8 13 13 24 4 11 7 20 7 28 1 1 0 4 0 7-1 3-1 6-1 7 0 2 1 4 3 6 1 1 3 4 5 6 2 3 3 5 5 6 1 2 3 5 4 9 2 3 3 7 5 10 1 3 2 6 4 10 2 4 4 7 6 9 2 3 4 5 7 7 3 2 7 3 11 3 3 0 8 0 13-1l0-1c7 2 12 2 14 2l218 0c14 0 25-5 32-16 8-10 10-23 6-37l-79-259c-7-22-13-37-20-43-7-7-19-10-37-10l-248 0c-5 0-9-2-11-5-2-3-2-7 0-12 4-13 18-20 41-20l264 0c5 0 10 2 16 5 5 3 8 6 10 11l85 282c2 5 2 10 2 17 7-3 13-7 17-13z m-304 0c-1-3-1-5 0-7 1-1 3-2 6-2l174 0c2 0 4 1 7 2 2 2 4 4 5 7l6 18c0 3 0 5-1 7-1 1-3 2-6 2l-173 0c-3 0-5-1-8-2-2-2-4-4-4-7z m-24-73c-1-3-1-5 0-7 2-2 3-2 6-2l174 0c2 0 5 0 7 2 3 2 4 4 5 7l6 18c1 2 0 5-1 6-1 2-3 3-5 3l-174 0c-3 0-5-1-7-3-3-1-4-4-5-6z\"},\"click\":\"function(gd) { \\n        // is this being viewed in RStudio?\\n        if (location.search == '?viewer_pane=1') {\\n          alert('To learn about plotly for collaboration, visit:\\\\n https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html');\\n        } else {\\n          window.open('https://cpsievert.github.io/plotly_book/plot-ly-for-collaboration.html', '_blank');\\n        }\\n      }\"}],\"cloud\":false},\"source\":\"A\",\"attrs\":{\"2bac7832611b\":{\"x\":{},\"y\":{},\"type\":\"scatter\"},\"2bacfc06143\":{\"x\":{},\"y\":{}}},\"cur_data\":\"2bac7832611b\",\"visdat\":{\"2bac7832611b\":[\"function (y) \",\"x\"],\"2bacfc06143\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"base_url\":\"https://plot.ly\"},\"evals\":[\"config.modeBarButtonsToAdd.0.click\"],\"jsHooks\":[]}\nHet lijkt erop dat met elke 1000 pond er een afname is in brandstof gebruik met 5.3444716 mijl per gallon\nHet einde\nAls we zo programmeren kunnen we ook:\nTangle: De broncode uit het document halen.\nWeave : Deze code pakken en de resultaten rechtstreeks laten zien.\nDat is wat we gaan doen.\nReproduceerbaar onderzoek\nReproduceerbaar onderzoek is een mogelijk product van dynamische documenten, echter, goed resultaat is niet gegarandeerd!\nGoede uitvoering van reproduceerbaar onderzoek houdt in ieder geval in:\nHet hele project in een directory plaatsen die wordt ondersteund door de ‘version control’.\nCode en data vrijlaten.\nAlles documenteren en de code als documentatie gebruiken!\nFiguren, tabellen en de statistiek zijn het resultaat van scripts en codes die in de tekst staan.\nSchrijf in de codes de paden die worden gebruikt.\nStel ‘seed’ in zodat een volgende persoon dezelfde resultaten krijgt.\nLaat ook informatie zien waarmee de code file wordt uitgevoerd. Je kunt bijvoorbeeld de devtools::session_info() gebruiken.\nOm meer over reproduceerbaarheid en datamanagement te lezen, kun je Vince Buffalo’s Boek erop naslaan[@Buffalo2015].\nMarkdown\nOm RMarkdown helemaal te begrijpen moeten we het eerst hebben over Markdown, wat een systeem is om een simpele, leesbare tekst te maken die eenvoudig kan worden omgezet naar HTML. Markdown is essentieel voor twee dingen:\nEen kale tekst die de syntax vormt\nEen software gereedschap dat in Perl is geschreven.\nZet de kale tekst om in HTML.\n\n\nBelangrijkste doel van Markdown:\nMaakt de syntax van het orginele (pre-HTML) document zo leesbaar als mogelijk.\n\nZou je deze code liever in HTML lezen?\n\n<body>\n  <section>\n    <h1>Paklijst voor bergklimmen<\/h1>\n    <ul>\n      <li>Bergschoenen<\/li>\n      <li>Klimgordel<\/li>\n      <li>Rugzak<\/li>\n      <li>Touw<\/li>\n      <li>Zelfzekering<\/li>\n    <\/ul>\n  <\/section>\n<\/body>\nOf deze code in Markdown?\n\n# Paklijst voor bergklimmen\n\n* Bergschoenen\n* Klimgordel\n* Rugzak  \n* Touw\n* Zelfzekering\nEen beetje een normaal mens vindt de Markdown code zeker makkelijker om te lezen!\nWe zullen meer over de syntax van Markdown praten nadat we RMarkdown hebben ge?ntroduceerd maar laten we ons allereerst beseffen hoeveel makkelijker ons leven is/zal zijn omdat Markdown bestaat! Dank je John Gruber en Aaron Swartz (RIP) voor het ontwikkelen van Markdown in 2004!\nRMarkdown\nRMarkdown is een variant van Markdown dat het makkelijker maakt om met RStudio dynamische documenten, presentaties en rapporten te maken. Het omvat ‘R code chunks’ (ik laat hier even het Engels staan) om met knitr te gebruiken waarmee makkelijker reproduceerbare (web-based) rapporten gemaakt kunnen worden die automatisch aangepast worden wanneer de onderliggende code is veranderd.\nRMarkdown laat jou Markdown combineren met plaatjes, linken, tabellen, LaTeX en de code zelf.\nRStudio zorgt ervoor dat het maken van documenten met RMarkdown makkelijk wordt\nRStudio is (net als R) vrij te gebruiken en draait op elk systeem.\nRMarkdown geeft verschillende typen files waaronder:\nHTML\nPDF\nMarkdown\nMicrosoft Word\nPresentaties:\nOpvallende HTML5 presentaties:\nioslides\nSlidy\nSlidify\n\nPDF presentaties:\nBeamer\n\nHandouts:\nTufte Handouts\n\n\nHTML R Package Vignettes\nEven Entire Websites!\n\nTerwijl er heel veel verschillende documenten kunnen worden geleverd met RMarkdown, leggen we vandaag de nadruk in de eerste plaats op HTML output files omdat die voor mijn onderzoek misschien het meest bruikbaar en flexibel zijn.\nWaarom R Markdown?\nEen aantrekkelijk gereedschap voor reproduceerbare en dynamische rapporten!\nTerwijl het was gemaakt voor R, accepteert het veel programmeertalen. Om het eenvoudig te houden, werken we vandaag alleen met R.\nEen code kan op een aantal manieren worden uitgevoerd:\nInline Code: Een korte code die in de geschreven tekst van het document wordt uitgevoerd.\nCode Chunks: Delen van het document omvatten verschillende zinnen voor programmeer of analyse code. Dat kan een plot of een tabel zijn, maar ook berekeningen van de samenvattende statistiek, pakketten laden, etc.\n\nHet is makkelijk om:\nPlaatjes op te nemen.\nDe Markdown syntax te leren.\nLaTeX vergelijkingen op te nemen.\nInteractieve tabellen op te nemen.\nGebruik de versie via Git.\nDan is het makkelijker om te delen en samen te werken in analyses, projecten en publicaties!\n\nExterne linken toe te voegen - Rmarkdown begrijpt zelfs enige html codes!\nOm mooie documenten te maken.\n\nJe hoeft je geen zorgen te maken over pagina breuken of het plaatsen van de figuren.\nConsolideer jouw code en plaats het in een file:\nPowerpoint, PDFs, html documenten en word files\n\nEenvoudige werkwijze\nIn het kort, om een rapport te maken:\nOpen een .Rmd file.\nMaak een YAML kop (meer hierover zo dadelijk!)\n\nSchrijf de inhoud met RMarkdown syntax.\nNeem mee de R code in code chunks of met een inline code.\nDraai de document output.\nWerkwijze om een rapport te makenOverzicht van de stappen die RMarkdown maakt om een ‘gerenderd’ document te krijgen:\nMaak een .Rmd rapport met ‘R code chunks’ en markdown verhalen (zoals hierboven in stappen beschreven).\nGeef de .Rmd-file aan knitr om de ‘R code chunks’ uit te voeren en een nieuwe .md file te maken.\nKnitr is een pakket binnen R die jou in staat stelt de code binnen RMarkdown documenten uit te voeren zoals HTML, latex, pdf, word en andere document types.\n\nGeef de .md file aan pandoc, die er een definitief document van maakt (b.v. html, Microsoft word, pdf, etc.).\nPandoc is een universeel gereedschap om documenten te converteren en zet het ene document type (in dit geval: .Rmd) om in een ander (in dit geval: HTML)\n\nHoe een Rmd document wordt omgezetHoewel dit mogelijk wat ingewikkeld lijkt, kunnen we op de “Knit” knop drukken boven aan de pagina die er zo uitziet:\nof we kunnen de volgende code runnen:\n\n\nMaak een .Rmd file\nHet wordt tijd! Laten we met RMarkdown gaan werken!\nIn de menu bar, klik je op File -> New File -> RMarkdown\nOf je klikt eenvoudig op het groene plus teken links boven in de hoek van RStudio.\n\n\nHet volgende zal omhoog komen.\nHierbinnen kies je het type output dat je wilt hebben. Opgelet: deze output kan later heel makkelijk worden aangepast!\n\nKlik OK\nYAML koppen\nYAML staat voor “YAML Ain’t Markup Language” en is eigenlijk een soort geklusterde structuur voor de metadata van het document. Het staat tussen twee regels van drie streepjes --- en wordt automatisch omgezet door RStudio. Een eenvoudig voorbeeld:\n\n---\ntitle:  \"Analyse Rapport\"  \nAuthor:  \"Harrie Jonkman\"  \ndate: \"1 Maart 2017\"  \noutput:  html_document\n---\nHet voorbeeld boven zal een HTML document maken. Echter, de volgende opties zijn ook beschikbaar.\nhtml_document\npdf_document\nword_document\nbeamer_presentation (pdf powerpoint)\nioslides_presentation (HTML powerpoint)\nen nog meer …\nVandaag leggen we de nadruk op HTML files. Echter voel je vrij als je hier wat mee wilt spelen door bv. word en pdf documenten te maken. Presentatie documenten kennen een wat andere syntax (bv. om aan te geven wanneer de ene dia eindigt en de andere begint) en dan is er nog wat markdown syntax specifiek voor presentaties maar die gaat voorbij het doel van deze workshop.\nIn deze workshops bouwen we verder voort op de details van YAML koppen.\nMarkdown Basis\nKijk hiernaar RMarkdown Reference Guide\nHaal hier ook informatie vandaan RMarkdown Cheatsheet:\nMarkdown Basis van RStudio’s RMarkdown CheatsheetHandige tips:\nEindig elke regel met twee spaties om een nieuwe paragraaf te beginnen.\nWoorden binnen een code moeten aan beide kanten zo’n kommateken kennen: `\nOm iets tot superscript te maken moet je een ^ aan beide zijden plaatsen. Superscript werd gevormd door Super^script^ te typen.\nVergelijkingen kunnen in een inline code worden geplaatst met $ en als blok gecentreerd binnen het document door $$. Bijvoorbeeld \\(E = mc^2\\) staat tussen de regels terwijl het volgende geblokt wordt opgenomen: \\[E = mc^2\\]\nOpgelet: Om met $ en $$ een superscript ^ te maken, is het nodig om voor elk aLFAnumeriEK dat superscript te gebruiken.\nAnder wiskundig materiaal:\nVierkantswortel: $\\sqrt{b}$ zal \\(\\sqrt{b}\\) maken\nBreuken: $\\frac{1}{2}$ = \\(\\frac{1}{2}\\)\nVergelijkingen met breuken: $f(x)=\\frac{P(x)}{Q(x)}$ = \\(f(x)=\\frac{P(x)}{Q(x)}\\)\n\n\nBinomiale Coefficienten: $\\binom{k}{n}$ = \\(\\binom{k}{n}\\)\nIntegralen: $$\\int_{a}^{b} x^2 dx$$ = \\[\\int_{a}^{b} x^2 dx\\]\n\nShareLaTeX is een prachtige bron voor LaTeX-codes.\n\nNog wat wiskundig materiaal:\nBeschrijving\nCode\nVoorbeelden\nGriekse letters\n$\\alpha$ $\\beta$ $\\gamma$ $\\rho$ $\\sigma$ $\\delta$ $\\epsilon$ $mu$\n\\(\\alpha\\) \\(\\beta\\) \\(\\gamma\\) \\(\\rho\\) \\(\\sigma\\) \\(\\delta\\) \\(\\epsilon\\) \\(\\mu\\)\nBinaire handelingen\n$\\times$ $\\otimes$ $\\oplus$ $\\cup$ $\\cap$\n\\(\\times\\) \\(\\otimes\\) \\(\\oplus\\) \\(\\cup\\) \\(\\cap\\) \\(\\times\\)\nRelationele handelingen\n$< >$ $\\subset$ $\\supset$ $\\subseteq$ $\\supseteq$\n\\(< >\\) \\(\\subset\\) \\(\\supset\\) \\(\\subseteq\\) \\(\\supseteq\\)\nVerder\n$\\int$ $\\oint$ $\\sum$ $\\prod$\n\\(\\int\\) \\(\\oint\\) \\(\\sum\\) \\(\\prod\\)\n\nUitdaging: Probeer de volgende output te maken:\n\nVandaag voel ik mij vet omdat ik RMarkdown leer.\nhoning is heel zoet.\nYAS!!!!!!\nR2 waarden zijn informatief!\n\\(R^{2}\\) beschrijft de variantie verklaard door het model.\nIk kende geen RMarkdown Vandaag heb ik RMarkdown geleerd\nRStudio link\nOutput van het volgende:\n\n# RMarkdown   \n## R   \n### Knitr   \n#### Pandoc  \n##### HTML  \n\\(\\sqrt{b^2 - 4ac}\\)\n\\[\\sqrt{b^2 - 4ac}\\]\n\\(X_{i,j}\\)\n\nVandaag maak ik een dynamisch document!\n\nHet volgende lijstje:\nChocolade Chips Kook Recept\nboter\nsuiker\nEen mengsel van bruine en witte suiker maakt het lekkerder\nmix dat met boter voordat je de eieren eraan toevoegt\n\n\neieren\nvanille\nMix wat droge ingredienten:\nmeel, zout, bak soda\n\nchocolade chips\nFijn feitje! De inhoudsopgave van deze website is gemaakt met koppen met 1-3 pond symbolen! (Daarover dadelijk meer)\nEen Code in het document\nEr zijn twee manieren om een code in een RMarkdown document op te nemen.\nCode in het document: Korte code als een onderdeel van het geschreven document.\nCode Chunks: Delen van het document die verschillende programmeer of analyse codes omvatten. Daarmee kan een figuur of tabel worden gemaakt, statistieken worden berekend, pakketten worden geladen, etc.\nR Code in het document\nEen R code kan in het document wordt gemaakt door een komma hoog achterwaarts (`) en de letter r gevolgd door nog zo’n komma.\nBijvoorbeeld: 211 is 2048.\nStel dat je een p-waarde rapporteert en je wilt niet terug om de statistische test steeds weer uit te voeren. De p-waarde was eerder 0.0045.\nDit is echt handig als de resultaten op papier moeten worden gezet. Bijvoorbeeld, je hebt een aantal statistieken uitgevoerd voor jouw wetenschappelijke vragen is dit een manier waarop R die waarde in a variabele naam bewaart. Bijvoorbeeld: Wijkt het brandstofverbruik van de automaat significant af de auto met handtransmissie significant af binnen de mtcars data set?\n\n\nmpg_auto <- mtcars[mtcars$am == 0,]$mpg # automatic transmission mileage\nmpg_manual <- mtcars[mtcars$am == 1,]$mpg # manual transmission mileage\ntransmission_ttest <- t.test(mpg_auto, mpg_manual)\n\nOm de p-waarde vast te stellen kunnen we transmission_ttest$p.value als R code in het document gebruiken.\nDe p-waarde is dan 0.0013736.\nR Code Chunks\nR code chunks (nogmaals ik gebruik maar de Engelse benaming hier, sorry)kunnen worden gebruikt om de R output in het document te krijgen of om de code als illustratie zichtbaar te maken.\nDe anatomie van een code chunk:\nOm een R code chunk te plaatsen, kun je met de hand typen door ```{r} gevolgd door ``` op een volgende regel. Je kunt ook de Insert a new code chunk knop gebruiken of de ‘shortcut key’. Dat geeft dan de volgende code chunk:\nEen code chunk invoeren\n```{r}\nn <- 10\nseq(n)\n```\nGeef de code chunk een betekenisvolle naam die samenhangt met wat het doet. Hieronder heb ik code chunk 10-random-numbers genoemd:\n\n```{r 10-random-numbers}\nn <- 10\nseq(n)\n```\nDe code chunk input en output zien er dan als volgt uit:\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nKnitr\nKnitr is een R-pakket dat werkt met\nIdentificeren van de code zowel van de chunks als in de tekst zelf\nEvalueren van de hele code en geeft de resultaten terug\nTeruggeven van de geformuleerde resultaten en combineert met de orginele file.\nKnitr draait de code zoals die in de R console zou draaien.\nKnitr werkt vooral met code chunks.\nEen code chunk ziet er als volgt uit:\n\n<div class=\"layout-chunk\" data-layout=\"l-body\">\n\n\n<\/div>\nGoede praktijken met betrekking tot code chunks:\nBenoem/label jouw code chunks!\nIn plaats van de chunk opties te specificeren in iedere chunk, kun je de algemene chunk opties aan het begin van het document vastzetten. Hierover meer in een minuut!\nChunk Labels\nChunk labels krijgen unieke IDs in een document en zijn goed voor:\nOm externe files te genereren zoals plaatjes en ‘cached’ documenten.\nChunk labels zijn vaak output als fouten omhoog komen(vaker voor codes in het document).\nNavigeren door lange .Rmd documenten.\nEen methode om door lange .Rmd files te navigerenAls je de code chunk een naam geef, gebruik dan - of _ tussen woorden voor code chunks labels in plaats van ruimtes. Dat helpt jou en andere gebruikers bij het navigeren in het document.\nChunk labels moeten uniek zijn in het document - anders zal er een fout optreden!\nChunk Opties\nDruk tab als tussen de haakjes code chunk opties omhoog komen.\nEnkele Knitr Chunk Optiesresults = \"asis\" staat voor “as is” en geeft de output van een niet geformateerde versie.\ncollapse is een andere chunk optie die handig kan zijn, zeker als een code chunk veel korte R uitdrukking heeft met wat output.\nEr zijn teveel chunk opties om hier te behandelen. Kijk na deze workshop nog eens wat rond voor deze opties.\nEen mooie website om dat op te doen is Knitr Chunk Options.\n\nUitdaging\nDraai de code chunk hieronder en speel wat met de volgende knitr code chunk opties:\n\n\neval = TRUE/FALSE\necho = TRUE/FALSE\ncollapse = TRUE/FALSE\nresults = \"asis\",\"markup en \"hide\n\n\nSla je resultaten op in markdown.Opgelet: Wees er zeker van dat je jouw chunks een naam geeft!\n\n\n\n1+1\n2*5\nseq(1, 21, by = 3)\nhead(mtcars)\n\nEnkele voorbeelden voortbouwend op de chunk hierboven\nResultaten van results=\"markup\", collapse = TRUE}:\n\n\n[1] 2\n[1] 10\n[1]  1  4  7 10 13 16 19\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nResultaten van results=\"asis\", collapse = TRUE}:\n\n[1] 2 [1] 10 [1] 1 4 7 10 13 16 19 mpg cyl disp hp drat wt qsec vs am gear carb Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1\n\nGlobale opties\nHet kan zijn dat je dezelfde chunk settings wilt handhaven voor het gehele document. Het kan daarom handig zijn om de opties in een keer te typen in plaats van het iedere keer weer voor een chunk te moeten doen. Om dat te doen kun je de globale chunk opties bovenaan het document vaststellen.\n\nknitr::opts_chunk$set(echo = FALSE, \n                      eval = TRUE, \n                      message = FALSE,\n                      warning = FALSE, \n                      fig.path = \"Figures/\",\n                      fig.width = 12, \n                      fig.height = 8)\nAls je bijvoorbeeld met iemand samenwerkt die de code niet wil zien, kun je schrijven eval = TRUE en echo = FALSE gebruiken zodat de code wel gedraaid wordt maar niet getoond. In aanvulling wil je misschien message = FALSE en warning = FALSE gebruiken zodat jouw samenwerkingspartner geen enkele boodschap of waarschuwing van R ziet.\nAls je figuren wilt opslaan en bewaren in een subdirectory binnen het project, gebruik dan fig.path = \"Figures/\". Hier verwijst de \"Figures/\" naar een folder Figures binnen de huidige directory waar de figuur die gemaakt wordt in het document wordt opgeslagen.Opgelet: de figuren worden niet standaard opgeslagen.\nGlobale chunk opties zullen voor de rest van het documenten worden vastgezet. Als je wilt dat een bepaalde chunk afwijkt van de globale opties, maak dat aan het begin van die bepaalde chunk duidelijk.\nFiguren\nKnitr maakt vrij eenvoudig figuren. Als een analyse code binnen een chunk een bepaald figuur moet produceren, dan zal hij dat in het document afdrukken.\nEnkele knitr chunk opties gerelateerd aan figuren:\nfig.width en fig.height\nStandaard: fig.width = 7, fig.height = 7\n\nfig.align: Hoe het figuur uit te lijnen\nOpties omvatten: \"left\", \"right\" en \"center\"\n\nfig.path: Een file pad naar de directory waar knitr de grafische output moet opslaan die er met de chunk wordt gemaakt.\nStandaard: 'figure/'\n\nEr is zelfs een fig.retina(alleen voor HTML output) voor hogere figuur resoluties met retina afdrukken.\n\n\n\nEen enkelvoudig figuur maken:\nMet fig.align = \"center\"\n\n\n\nMet fig.align = \"right\"\n\n\n\nMet fig.align = \"left\"\n\n\n\nMet fig.width = 2, fig.height = 2\n\n\n\nMet fig.width = 10, fig.height = 10\n\n\n\n\n\n\nTabellen\nTabellen kunnen in Markdown voor nogal wat hoofdpijn kosten. We gaan er hier verder niet op in. Als je meer wilt leren over Markdown-tabellen kijk naar documentation on tables op de RMarkdown website.\nEr zijn enkele tabeltypen die handig kunnen zijn. Hier zullen we ons vorig voorbeeld gebruiken van de mtcars data\nIn zijn Knitr in a Knutshell introduceert Dr. Karl Broman: kable, panderen xtable en vooral die eerste twee deden mij plezier:\nkable: Binnen het knitr pakket - niet veel opties maar het ziet er goed uit.\npander: Binnen het pander pakket - heeft veel opties en handigheden. Makkelijk voor het vetmaken van waarden (bv. waarden onder een bepaalde waarde).\nkable en pander tabellen zijn mooi en handig bij het maken van niet-interactieve tabellen:\n\n\nkable(head(mtcars, n = 4)) # kable table with 4 rows\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n4\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n4\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n1\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n1\n\n# Pander table\n# install.packages(\"pander\") # install pander first\nlibrary(pander)\npander(head(mtcars, n = 4))\nTable continues below\n \nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\nMazda RX4\n21\n6\n160\n110\n3.9\n2.62\n16.46\n0\n1\nMazda RX4 Wag\n21\n6\n160\n110\n3.9\n2.875\n17.02\n0\n1\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.32\n18.61\n1\n1\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n \ngear\ncarb\nMazda RX4\n4\n4\nMazda RX4 Wag\n4\n4\nDatsun 710\n4\n1\nHornet 4 Drive\n3\n1\n\nHTML Widgets\nMet de uitgave van de nieuwe RMarkdown v2 is het makkelijker dan ooit tevoren om HTML Widgets te gebruiken. Volg de link om uit te zoeken in welke widgets jij ge?nteresseerd bent!\nOnlangs ontdekte ik bijvoorbeeld het DT pakket waarmee tabellen interactief kunnen worden gemaakt in de HTML output. Daarbij levert Plotly for R echt mooie interactieve grafieken op, welke gebaseerd zijn op Plotly.\nCool, of niet?\n\n\n{\"x\":{\"filter\":\"none\",\"data\":[[\"Mazda RX4\",\"Mazda RX4 Wag\",\"Datsun 710\",\"Hornet 4 Drive\",\"Hornet Sportabout\",\"Valiant\",\"Duster 360\",\"Merc 240D\",\"Merc 230\",\"Merc 280\",\"Merc 280C\",\"Merc 450SE\",\"Merc 450SL\",\"Merc 450SLC\",\"Cadillac Fleetwood\",\"Lincoln Continental\",\"Chrysler Imperial\",\"Fiat 128\",\"Honda Civic\",\"Toyota Corolla\",\"Toyota Corona\",\"Dodge Challenger\",\"AMC Javelin\",\"Camaro Z28\",\"Pontiac Firebird\",\"Fiat X1-9\",\"Porsche 914-2\",\"Lotus Europa\",\"Ford Pantera L\",\"Ferrari Dino\",\"Maserati Bora\",\"Volvo 142E\"],[21,21,22.8,21.4,18.7,18.1,14.3,24.4,22.8,19.2,17.8,16.4,17.3,15.2,10.4,10.4,14.7,32.4,30.4,33.9,21.5,15.5,15.2,13.3,19.2,27.3,26,30.4,15.8,19.7,15,21.4],[6,6,4,6,8,6,8,4,4,6,6,8,8,8,8,8,8,4,4,4,4,8,8,8,8,4,4,4,8,6,8,4],[160,160,108,258,360,225,360,146.7,140.8,167.6,167.6,275.8,275.8,275.8,472,460,440,78.7,75.7,71.1,120.1,318,304,350,400,79,120.3,95.1,351,145,301,121],[110,110,93,110,175,105,245,62,95,123,123,180,180,180,205,215,230,66,52,65,97,150,150,245,175,66,91,113,264,175,335,109],[3.9,3.9,3.85,3.08,3.15,2.76,3.21,3.69,3.92,3.92,3.92,3.07,3.07,3.07,2.93,3,3.23,4.08,4.93,4.22,3.7,2.76,3.15,3.73,3.08,4.08,4.43,3.77,4.22,3.62,3.54,4.11],[2.62,2.875,2.32,3.215,3.44,3.46,3.57,3.19,3.15,3.44,3.44,4.07,3.73,3.78,5.25,5.424,5.345,2.2,1.615,1.835,2.465,3.52,3.435,3.84,3.845,1.935,2.14,1.513,3.17,2.77,3.57,2.78],[16.46,17.02,18.61,19.44,17.02,20.22,15.84,20,22.9,18.3,18.9,17.4,17.6,18,17.98,17.82,17.42,19.47,18.52,19.9,20.01,16.87,17.3,15.41,17.05,18.9,16.7,16.9,14.5,15.5,14.6,18.6],[0,0,1,1,0,1,0,1,1,1,1,0,0,0,0,0,0,1,1,1,1,0,0,0,0,1,0,1,0,0,0,1],[1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,1,1,1,1,1,1,1],[4,4,4,3,3,3,3,4,4,4,4,3,3,3,3,3,3,4,4,4,3,3,3,3,3,4,5,5,5,5,5,4],[4,4,1,1,2,1,4,2,2,4,4,3,3,3,4,4,4,1,2,1,1,2,2,4,2,1,2,2,4,6,8,2]],\"container\":\"<table class=\\\"display\\\">\\n  <thead>\\n    <tr>\\n      <th> <\\/th>\\n      <th>mpg<\\/th>\\n      <th>cyl<\\/th>\\n      <th>disp<\\/th>\\n      <th>hp<\\/th>\\n      <th>drat<\\/th>\\n      <th>wt<\\/th>\\n      <th>qsec<\\/th>\\n      <th>vs<\\/th>\\n      <th>am<\\/th>\\n      <th>gear<\\/th>\\n      <th>carb<\\/th>\\n    <\\/tr>\\n  <\\/thead>\\n<\\/table>\",\"options\":{\"pageLength\":5,\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3,4,5,6,7,8,9,10,11]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false,\"lengthMenu\":[5,10,25,50,100]}},\"evals\":[],\"jsHooks\":[]}\nSpelling controleren\nIn de spelling kunnen natuurlijk altijd fouten zitten en daarom kan het nodig zijn dat we onze spelling in het document willen controleren. Er zijn twee manieren om de spelling te controleren:\nDruk op de “ABC check mark”  links van de vergrootglasknop in RStudio.\nGebruik de aspell() functie van het utils pakket. Je kunt dan echter beter de code chunks overslaan. De aspell() functie kan een filter functie overnemen om bepaalde regels in de files over te slaan en kan worden gebruikt met de knit_filter() die ontworpen is om de code chunks in een file over te slaan.\nKnitr Thema’s\nHet knitr-syntax-thema kan worden aangepast of helemaal naar de hand worden gezet. Als je de standaardthema’s niet wilt, gebruik dan het knit_theme om het te veranderen. Er zijn 80 thema’s opgenomen binnen knitr en we kunnen de namen ervan zien door knit_theme$get().\nWat zijn de eerste 30 knitr thema’s?\n\n\n [1] \"acid\"          \"aiseered\"      \"andes\"         \"anotherdark\"  \n [5] \"autumn\"        \"baycomb\"       \"bclear\"        \"biogoo\"       \n [9] \"bipolar\"       \"blacknblue\"    \"bluegreen\"     \"breeze\"       \n[13] \"bright\"        \"camo\"          \"candy\"         \"clarity\"      \n[17] \"dante\"         \"darkblue\"      \"darkbone\"      \"darkness\"     \n[21] \"darkslategray\" \"darkspectrum\"  \"default\"       \"denim\"        \n[25] \"dusk\"          \"earendel\"      \"easter\"        \"edit-anjuta\"  \n[29] \"edit-eclipse\"  \"edit-emacs\"   \n\nWij kunnen knit_theme$set() gebruiken om het thema vast te zetten. Om het thema op fruit vast te zetten, kunnen we de bijvoorbeeld de volgende code gebruiken:\n\n\n\nHier is de link naar jouw favoriete thema 80 knitr highlight themes.\nAndere programmeer talen\nTerwijl knitr binnen een R omgeving moet draaien, ondersteunt het ook andere programmeertalen waaronder:\nPython\nRuby\nHaskell\nawk/gawk\nsed\nshell scripts\nPerl\nSAS\nTikZ\nGraphviz\nC++\nEn andere talen…\nWe moeten echter het corresponderende software pakket installeren om een taal te gebruiken.\nGebruik de engine functie in Knitr. Deze functie laat de gebruiker de taal van een chunk specificeren.\nengine = \"bash\" zal de boel in bash laden and stelt de gebruiker in staat de scripts zo binnen de code chunk te schrijven .\nPr?sance\nEr zijn verschillende opties die we kunnen controleren binnen ons .Rmd document. Dit onderdeel helpt bij het introduceren en verkennen van sommige van deze opties waarmee je HTML documenten kunt aanpassen.\nCode Folding\nZoals het je misschien is opgevallen heeft elk van de code chunks in dit document een interactieve  knop. Deze wordt gecontroleerd in de YAML kop and is nieuw in RMarkdown v2.\nWanneer de knitr code chunk optie echo = TRUE is gespecificeerd (default = TRUE) zal de R code in het output document verschijnen. Echter, er zijn momenten waarin de gebruiker de code helemaal niet wil laten zien (echo = FALSE).\n-code_folding:\n- code_folding: hide: Kan de R code meenemen maar deze is standaard verborgen.\n- code_folding: show: Laat de R code zien. De lezers kunnen dan op de  knop drukken om de chunk te verstoppen als ze dat willen.\n\noutput: html_document\n    code_folding: show\nInhoudsopgave\nEen inhoudsopgave kan aan het gerenderd document worden toegevoegd door de toc optie in de YAML kop te gebruiken.\nOpties hierbij:\ntoc: of de inhoudsopgave moeten worden meegenomen:\ntoc: true: hier wordt de inhoudsopgave meegenomen\nDefault:toc: false: Hier wordt de inhoudsopgave niet meegenomen\n\ntoc_depth:: Hoeveel niveau’s moeten in de inhoudsopgave worden worden meegenomen?\nDefault: doc_depth: 3 zal koppen tot en met ### meenemen.\n\nnumber_sections: Voegt sectienummers toe aan de koppen. Bijvoorbeeld, dit document heeft number_sections: true\nDefault: number_sections: false\nOpgelet: Met elk # zal er een decimaal punt worden toegevoegd aan alle koppen.\n\ntoc_float:\n2 andere mogelijke parameters binnen toc_float:\ncollapsed: Controleert of de inhoudsopgave alleen aan het begin verschijnt. Het zal met de cursor erover verschijnen.\nDefault: collapsed: TRUE\n\nsmooth_scroll: Controleert of de pagina scrolls werken wanneer op de onderdelen van de inhoudsopgave wordt geklikt.\nDefault: smooth_scroll: true\n\n\n\nBijvoorbeeld:\n\noutput:\n  html_document:\n    toc: true\n    toc_depth: 2\n---\n\nUitdaging: Maak de YAML kop voor een HTML document die het volgende inhoudt:\n\n\nInhoudsopgave\nLaat de inhoudsopgave vloeien\nSectie koppen met twee hashtags (##)\nGenummerde secties\nGeen makkelijke scrolling\n\nThema’s\nRMarkdown heeft verschillende opties die de pr?sance van de HTML documenten controleren. Enkele mogelijkheden waaruit kan worden gekozen, hier met de Engelse termen:\ntheme\nhighlight\nsmart\nDe HTML output thema’s komen van Bootswatch library. Valide HTML themes omvatten de volgende:\ncerulean, cosmo,flatly, journal, readable,spacelab en united.\nBijvoorbeeld, het thema van de pagina is readable.\n\nZet het op nul voor geen thema (in dit geval kun je de css parameter gebruiken om jouw eigen stijl te gebruiken).\nHighlight specificeert de wijze waarop de syntax stijl oplicht. Stijlen die mogelijk zijn omvatten de volgende:\ndefault, espresso, haddock, kate, monochrome, pygments, tango, textmate en zenburn.\nOok hier, plaats nul om syntax oplichting te voorkomen.\nSmart indiceert of de typografisch correcte output wordt weergegeven, zet rechte aanhalingstekens om in gekru, — rechte aanhalingstekens, – om in gekrulde aanhalingstekens en … in ellipsen. Smart is standaard ingesteld.\nBijvoorbeeld:\n\n---\noutput:\n  html_document:\n    theme: slate\n    highlight: tango\n---\nAls je wilt kun je ook jouw eigen stijl-thema produceren en gebruiken. Als je dat zou doen, zou de output sectie van jouw YAML kop er z’on beetje zo uitzien:\n\noutput:\n  html_document:\n    css: styles.css\nAls je nog wat verder wilt gaan en jouw eigen thema wilt schrijven in aanvulling op het oplichten, zou de YAML kop er beetje zo uitzien:\n\n---\noutput:\n  html_document:\n    theme: null\n    highlight: null\n    css: styles.css\n---\nHier is een link naar Pr?sance en Stijl in de HTML output.\nVerbergen\nProbleem: Sommige code chunks nemen veel tijd in beslag en hoeven niet vaak te worden ververst.\nOplossing: Verbergen (Caching)! Als een code chunk niet is aangepast sinds de laatste keer dat het document is gerenderd, zullen de oude resultaten worden gebruik inplaats van de chunk nogmaals te draaien.\nEen hele simpele oplossing: Stop ‘knitting’ voortijdig -> als de rest van het document niet hoeft te worden gedraaid, zet je eenvoudig een knit_exit() vast en de rest van het document wordt genegeerd. Daarvoor in de plaats komen de resultaten en de code chunk uit de vorige tekst terug.\nsloom-laden wordt over gesproken als een object niet in het geheugen wordt geladen totdat het wordt gebruikt. Daarvoor in de plaats wordt er een “belofte” gemaakt, die makkelijk is voor de computer makkelijk is (om daar meer van te leren, tik ?promise).\nAls het document wordt gerenderd, worden verborgen chunks overgeslagen en de output die eerder gecre?erd met de chunks die sloom laden, worden geladen van de cache folder. Echter als er een kleine verandering in de chunk wordt aangebracht (zelfs een witte ruimte telt!) zal knitr deze verandering opmerken en zal ze de chunk draaien als het document wordt gerenderd.\nDe gebruiker kan ook het pad instellen voor waar de verborgen files zijn door cache.path te gebruiken.\nStandaard: cache.path = \"file_name_cache/\"\nBijvoorbeeld: Probeer de github repo voor de vorige les waar de verborgen chunks in de RMarkdown_Lesson_cache directory staan.\n\nEnkele zaken met betrekking tot verbergen:\nR wordt om de paar maanden ververst. R.version.string\nAan externe files van knitr kunnen deze veranderingen voorbij gaan, het kan zijn dat ze ververst moeten worden en dat de resultaten opnieuw gedraaid moeten worden.\nSoms kan een verborgen chunk vertrouwen op objecten van een ander verborgen chunk. Dit kan een serieus probleem zijn - dus wees voorzichtig! We moeten de chunk afhankelijkheden dus dekken!\nChunk afhankelijkheden\nMet de hand\nWe kunnen met de hand specificeren of chunks van elkaar afhankelijk zijn.\ndependson specificeert van welke chunk de huidige chunk afhankelijk is.\nVoorbeelden hiervan:\ndepesndson = 1: Chunk vertrouwt op de eerste Chunk\ndependson = c(6,8): Chunk vertrouwt op de 6de en 8ste chunks\ndependson = -1: Chunk vertrouwt op de vorige chunk.\ndependson = c(-1, -2): Chunk vertrouwt op de twee vorige chunks.\nOpgelet: Als dependson een bepaalde waarde aanneemt, kan het niet afhankelijk zijn van een latere chunk - alleen van vorige chunks. Daarom is het makkelijk om chunks namen te geven.\n\nVoorbeelden:\ndependson = c(\"Chunk-1\", \"Chunk-2\", \"Chunk-3\")\ndependson = c(\"data-generation\", \"data-transforamtion\")\n\n\nAls resultaat, elke keer als de verborgen chunks \"Chunk-1\", \"Chunk-2\" en \"Chunk-3\" opnieuw worden opgebouwd, zal de huidige chunk zijn verborgenheid verliezen en zal het opnieuw worden gedraaid!\nAutomatisch\nVoeg in: autodep chunk optie en de functie dep_auto()\nautodep en dep_auto() staan er voor dat de objecten in de huidige chunk door vorige chunks zijn gemaakt. Dus de huidige chunk hangt af van de vorige chunk.\nVoor een meer conservatieve benadering voeg dep_prev() in.\ndep_prev staat er voor dat een gevouwen chunk afhangt van al zijn vorige chunks. Dus als vorige chunks zijn ververst, zullen ook alle latere chunks worden ververst.\nKnitr indentificeert alleen veranderingen in de opgevouwen chunks, niet in de niet opgevouwen chunks! Gelukkig geeft knitr een waarschuwing wanneer het een afhankelijkheid ziet met een niet opgevouwen chunk.\nCache met de hand laden\nStel dat je aan het eind van een document een z berekent, maar je wilt de z in een eerdere chunk gebruiken. Dit is onmogelijk omdat knitr het document op een liniarie manier samenvoegt en het geen objecten kan gebruiken die in de toekomst worden gemaakt.\nVoeg in: load_cache, die de chunk label zoekt in de ‘cache’ database\n\nload_cache(label, object, notfound = \"NOT AVAILABLE\", \n  path = opts_chunk$get(\"cache.path\"), lazy = TRUE)\nAls jij dan een z in een ‘’inline R expressie’ gebruikt, geeft het NOT AVAILABLE terug en omdat je hebt gespecificeerd notfound = \"NOT AVAILALBE\" zal het naar het einde teruggaan en de waarde z verplaatsen.\nZo handig!\nZijeffecten\nEen zijeffect refereert naar een statusverandering die optreedt buiten een functie die de teruggeven waarde niet representeert.\npar() en options() zijn zijeffecten in de betekenis dat ze niet opgevouwen zijn.\nStel globale opties van de eerste chunk in en vouw deze chunk nooit op.\nWe moeten voorzichtig zijn met de chunk opties om er zeker van te zijn dat de resultaten van de opgevouwen chunks worden ververst.\nWe kunnen ook het sloom-laden afzetten met cache.lazy = FALSE.\nBibliografie\nHet is ook mogelijk om een bibliografie file in de YAML kop mee te nemen. Bibliografie formats die door Pandoc gelezen kunnen worden zijn:\nFormat\nFile extension\nMODS\n.mods\nBibLaTeX\n.bib\nBibTeX\n.bibtex\nRIS\n.ris\nEndNote\n.enl\nEndNote XML\n.xml\nISI\n.wos\nMEDLINE\n.medline\nCopac\n.copac\nJSON citeproc\n.json\nOm een bibliografie in RMarkdown te maken, zijn er twee files nodig:\nEen bibliografie file met informatie over elke referentie.\nEen citaat stijl taal (CSL) om het format de referentie te bepalen.\nEen voorbeeld YAML kop met een bibliografie en een citaat stijl taal (CSL) file is:\n\noutput: html_document\nbibliography: bibliography.bib\ncsl: nature.csl\nBekijk de erg behulpzame webpagina van het R Core team op bibliographies and citations.\nAls je R pakketten wilt citeren, heeft knitr zelfs een functie die write_bib() heet en die .bib overzicht van R pakketten kan leveren. Het wordt zelfs in een file geschreven!\n\n\n\nPlaatsen\nDe bibliografie wordt automatisch aan het einde van het document geplaatst. Daarom moet je jouw .Rmd document met # Referenties eindigen zodat de bibliografie naar de kop voor bibliografie komt.\n\nlaatste woorden...\n\n# Referenties\nStylen van citeren\nCitation Sylte Language (CSL) is een op XML-gebaseerde taal die het format van citaten en bibliografie?n vaststelt. Referentie management programma’s zoals Zotero, Mendeley en Papers gebruiken allemaal CSL.\nZoek jouw favcoriete tijdschrift en CSL in de Zotero Style Repository, waar nu meer dan 8,152 CSLs inzitten. Is er een stijl waar je naar zoekt en die er niet in zit?\n\noutput: html_document\nbibliography: bibliography.bib\ncsl: nature.csl\nIn de github repo voor deze workshop heb ik de nature.csl en the-isme-journal.csl toegevoegd om mee te spelen. Download anders een stijl van de Zotero Style Repository!\nCitaten\nCitaten gaan tussen vierkante haakjes [ ] en worden afgescheiden door punt-komma’s’ ;. Elk citaat moet een sleutel hebben, samen de @ + de citaat identificatie van de database vormen en die optioneel a prefix, a locator en a suffix hebben. Om te controleren wat de citaatsleutel is van een referentie, werp dan een blik op de .bib file. Hier in die file, kun je de sleutel voor elke referentie veranderen. Echter, wees er wel van bewust dat elke ID uniek is!\nHier zijn wat voorbeelden met bijpassende code in het Engels:\nMicrobes control Earth’s biogeochemical cycles [@Falkowski2008].\nCode: Microbes contorl Earth's biogeochemical cycles  [@Falkowski2008].\n\nI love making beautiful plots with ggplot2 [@R-ggplot2]\nCode: I love making beautiful plots with ggplot2 [@R-ggplot2]\n\nDr. Yuhui Xie’s book about Dynamic Documents [@Xie2015] inspired me to host this workshop.\nCode: Dr. Yuhui Xie's book about Dynamic Documents [@Xie2015] inspired me to host this workshop.\n\nA great article in Science regarding biogeography of microbes asks readers to imagine their Alice in Wonderland to shrink down to understand the microbial world [@Green2008].\nCode: A great article in *Science* regarding biogeography of microbes asks readers to imagine they are Alice in Wonderland to and shrink down to understand the microbial world [@Green2008].\n\nHet is cool dat de enige refenties die aan het document worden toegevoegd degene zijn die jijzelf citeert!\nPubliceren via RPubs\nAls je een keer een mooi dynamisch document hebt gemaakt wil je dat mogelijk delen met anderen. Een mogelijkheid om het te delen met de wereld is om het te hosten op RPubs. Met RStudio kan dit heel makkelijk! Doe het volgende:\nMaak een aansprekend .Rmd document.\nKlik op de  knop om jouw gerenderd HTML document te puliceren.\nIn de rechter bovenhoek van het previewscherm klik je op de publiceer  knop en volgt de aanwijzingen.\nOpgelet: Je moet een RPubs profiel hebben aangemaakt.\n\nAls je het profiel hebt let dan op het volgende:\nDe titel van het document.\nEen beschrijving van het document.\nDe URL waar de website wordt gehost.\nOpgelet: Het begin van de URL zal zijn: www.rpubs.com/your_username/name_of_your_choice\n\n\nRPubs vernieuwen\nAls je veranderingen in het document wilt aanbrengen is het makkelijk om de webpagina te vernieuwen. Als je een keer jouw aangepaste documentg hebt gerenderd klik je op de  knop rechtsboven in de hoek van de preview scherm. Het aangepaste document zal dezelfde URL hebben als het orginele document.\nDank\nBedankt dat je aan deze tutorial hebt meegedaan.\nAls je updates van deze les wilt maken, stuur dan me een ‘pull request’. Wat je ook kunt doen is een e-mail sturen naar:\nMarian SchmidtE-mail: marschmi at umich.eduTwitter @micro_marian\nVeel succes met de voortgang van dynamische documenten!\nInformatie over de sessie\n\n\n- Session info -----------------------------------------------------\n setting  value                       \n version  R version 3.5.1 (2018-07-02)\n os       Windows 10 x64              \n system   x86_64, mingw32             \n ui       RTerm                       \n language (EN)                        \n collate  Dutch_Netherlands.1252      \n ctype    Dutch_Netherlands.1252      \n tz       Europe/Berlin               \n date     2019-03-04                  \n\n- Packages ---------------------------------------------------------\n package     * version    date       lib\n assertthat    0.2.0      2017-04-11 [1]\n backports     1.1.3      2018-12-14 [1]\n callr         3.1.1      2018-12-21 [1]\n cli           1.0.1      2018-09-25 [1]\n colorspace    1.4-0      2018-10-06 [1]\n crayon        1.3.4      2017-09-16 [1]\n crosstalk     1.0.0      2016-12-21 [1]\n data.table    1.12.0     2019-01-13 [1]\n desc          1.2.0      2018-05-01 [1]\n devtools      2.0.1      2018-10-26 [1]\n digest        0.6.18     2018-10-10 [1]\n distill       0.6.0.9000 2019-03-04 [1]\n dplyr       * 0.8.0.1    2019-02-15 [1]\n DT          * 0.5        2018-11-05 [1]\n evaluate      0.13       2019-02-12 [1]\n fs            1.2.6      2018-08-23 [1]\n ggplot2     * 3.1.0      2018-10-25 [1]\n glue          1.3.0      2018-07-17 [1]\n gtable        0.2.0      2016-02-26 [1]\n highr         0.7        2018-06-09 [1]\n htmltools     0.3.6      2017-04-28 [1]\n htmlwidgets   1.3        2018-09-30 [1]\n httpuv        1.4.5.1    2018-12-18 [1]\n httr          1.4.0      2018-12-11 [1]\n jsonlite      1.6        2018-12-07 [1]\n knitr       * 1.21       2018-12-10 [1]\n labeling      0.3        2014-08-23 [1]\n later         0.8.0      2019-02-11 [1]\n lazyeval      0.2.1      2017-10-29 [1]\n magrittr      1.5        2014-11-22 [1]\n memoise       1.1.0      2017-04-21 [1]\n mime          0.6        2018-10-05 [1]\n munsell       0.5.0      2018-06-12 [1]\n pander      * 0.6.3      2018-11-06 [1]\n pillar        1.3.1      2018-12-15 [1]\n pkgbuild      1.0.2      2018-10-16 [1]\n pkgconfig     2.0.2      2018-08-16 [1]\n pkgload       1.0.2      2018-10-29 [1]\n plotly      * 4.8.0      2018-07-20 [1]\n plyr          1.8.4      2016-06-08 [1]\n prettyunits   1.0.2      2015-07-13 [1]\n processx      3.2.1      2018-12-05 [1]\n promises      1.0.1      2018-04-13 [1]\n ps            1.3.0      2018-12-21 [1]\n purrr         0.3.0      2019-01-27 [1]\n R6            2.4.0      2019-02-14 [1]\n Rcpp          1.0.0      2018-11-07 [1]\n remotes       2.0.2      2018-10-30 [1]\n rlang         0.3.1      2019-01-08 [1]\n rmarkdown   * 1.11       2018-12-08 [1]\n rprojroot     1.3-2      2018-01-03 [1]\n scales        1.0.0      2018-08-09 [1]\n sessioninfo   1.1.1      2018-11-05 [1]\n shiny         1.2.0      2018-11-02 [1]\n stringi       1.3.1      2019-02-13 [1]\n stringr       1.4.0      2019-02-10 [1]\n testthat      2.0.1      2018-10-13 [1]\n tibble        2.0.1      2019-01-12 [1]\n tidyr         0.8.2      2018-10-28 [1]\n tidyselect    0.2.5      2018-10-11 [1]\n usethis       1.4.0      2018-08-14 [1]\n viridisLite   0.3.0      2018-02-01 [1]\n withr         2.1.2      2018-03-15 [1]\n xfun          0.5        2019-02-20 [1]\n xtable        1.8-3      2018-08-29 [1]\n yaml          2.2.0      2018-07-25 [1]\n source                          \n CRAN (R 3.5.1)                  \n CRAN (R 3.5.2)                  \n CRAN (R 3.5.2)                  \n CRAN (R 3.5.2)                  \n R-Forge (R 3.5.1)               \n CRAN (R 3.5.1)                  \n CRAN (R 3.5.1)                  \n CRAN (R 3.5.2)                  \n CRAN (R 3.5.1)                  \n CRAN (R 3.5.2)                  \n CRAN (R 3.5.2)                  \n Github (rstudio/distill@6aaffa3)\n CRAN (R 3.5.2)                  \n CRAN (R 3.5.2)                  \n CRAN (R 3.5.2)                  \n CRAN (R 3.5.1)                  \n CRAN (R 3.5.2)                  \n CRAN (R 3.5.1)                  \n CRAN (R 3.5.1)                  \n CRAN (R 3.5.1)                  \n CRAN (R 3.5.1)                  \n CRAN (R 3.5.2)                  \n CRAN (R 3.5.2)                  \n CRAN (R 3.5.2)                  \n CRAN (R 3.5.2)                  \n CRAN (R 3.5.2)                  \n CRAN (R 3.5.0)                  \n CRAN (R 3.5.2)                  \n CRAN (R 3.5.1)                  \n CRAN (R 3.5.1)                  \n CRAN (R 3.5.1)                  \n CRAN (R 3.5.1)                  \n CRAN (R 3.5.1)                  \n CRAN (R 3.5.2)                  \n CRAN (R 3.5.2)                  \n CRAN (R 3.5.1)                  \n CRAN (R 3.5.2)                  \n CRAN (R 3.5.1)                  \n CRAN (R 3.5.1)                  \n CRAN (R 3.5.1)                  \n CRAN (R 3.5.1)                  \n CRAN (R 3.5.2)                  \n CRAN (R 3.5.1)                  \n CRAN (R 3.5.2)                  \n CRAN (R 3.5.2)                  \n CRAN (R 3.5.1)                  \n CRAN (R 3.5.2)                  \n CRAN (R 3.5.1)                  \n CRAN (R 3.5.2)                  \n CRAN (R 3.5.2)                  \n CRAN (R 3.5.1)                  \n CRAN (R 3.5.1)                  \n CRAN (R 3.5.1)                  \n CRAN (R 3.5.1)                  \n CRAN (R 3.5.2)                  \n CRAN (R 3.5.2)                  \n CRAN (R 3.5.2)                  \n CRAN (R 3.5.2)                  \n CRAN (R 3.5.2)                  \n CRAN (R 3.5.2)                  \n CRAN (R 3.5.1)                  \n CRAN (R 3.5.1)                  \n CRAN (R 3.5.1)                  \n CRAN (R 3.5.2)                  \n CRAN (R 3.5.2)                  \n CRAN (R 3.5.1)                  \n\n[1] C:/Users/HarrieJonkman/Documents/R/win-library/3.5\n[2] C:/Program Files/R/R-3.5.1/library\n\nReferentie\n\n\n",
    "preview": "posts/2018-11-14-reproducable-research/reproducable-research_files/figure-html5/single-fig-center-1.png",
    "last_modified": "2020-03-13T23:28:57+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-11-14-exploratie/",
    "title": "Data exploratie",
    "description": "Een introductie op data exploratie aan de hand van een boek van Chester Ismay en Albert Y. Kim.",
    "author": [
      {
        "name": "Chester Ismay en Albert Y. Kim, bewerkt door Harrie Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2017-10-15",
    "categories": [],
    "contents": "\nDe tekst vind je hier: http://www.harriejonkman.nl/wp-content/uploads/2018/01/MDtotaal.pdf\n\n\n",
    "preview": {},
    "last_modified": "2020-03-13T23:28:57+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-11-14-psm/",
    "title": "PSM",
    "description": "Om precies het effect van een aanpak of politieke keuze vast te stellen is een ingewikkelde kwestie. Toch is er dat soort onderzoek nodig om de keuze voor programma's te legitimeren. Tegenwoordig is er een heel spectrum van technieken om de impact van programma's vast te stellen. Dit zijn technieken die kunnen worden gebruikt binnen hele verschillende soorten impactstudies. Het is goed daar kennis van te nemen, zeker nu steeds meer mogelijk is omdat er meer data beschikbaar zijn waarop deze evaluaties gebaseerd kunnen worden. Impactstudies worden uitgevoerd om vast te stellen of programma's de effecten opleveren die ze nastreven, om te begrijpen of en waarom deze programma's werken, om vast te stellen in hoeverre veranderingen zijn toe te schrijven aan de inzet van het programma en ook om vast te stellen of de gelden op een goede manier worden besteed. Op dit terrein is er natuurlijk een enorme hoeveelheid literatuur en enkele uitgaven geven ons hiervan een goed en up-to-date overzicht^[Khandker, S.R., Koolwal, G.B. & Samad, H.A. (2010). *Handbook on Impactevaluation. Quantative Methods and Practices*. Washington D.C: The World Bank; Gertler, P.J., Martinez, S., Prenard, P., Rawlings, L.B. & Vermeersch, C.M. (2011). *Impact Evaluation in Practice*. Washington D.C.: The World Bank; Murnane, R.J. & Willet, J.B.(2011). *Methods Matter. Improving Causal Inference in Educational and Social Science Research*. New York: Oxford University]. Experimentele studies kunnen natuurlijk goede impactstudies zijn, met sterke punten en beperkingen. Maar er zijn ook aanvullende methodes die in quasi-experimentele of observationele studies kunnen worden toegepast. Zo zijn er panel datamethodes die gebruikt kunnen worden, regressie discontinu?teit methodes en instrumentele variabelen methodes. Daarnaast zijn er verschillende matchingsmethodes die in impactstudies worden gebruikt. Hier stellen we zo'n matchingsmethode voor die goed gebruikt kan worden in verschillende soorten impactstudies en laten we zien hoe deze uitgevoerd kan worden.",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2017-09-14",
    "categories": [],
    "contents": "\nImpactevaluatie\nOm precies het effect van een aanpak of politieke keuze vast te stellen is een ingewikkelde kwestie. Toch is er dat soort onderzoek nodig om de keuze voor programma’s te legitimeren. Tegenwoordig is er een heel spectrum van technieken om de impact van programma’s vast te stellen. Dit zijn technieken die kunnen worden gebruikt binnen hele verschillende soorten impactstudies. Het is goed daar kennis van te nemen, zeker nu steeds meer mogelijk is omdat er meer data beschikbaar zijn waarop deze evaluaties gebaseerd kunnen worden. Impactstudies worden uitgevoerd om vast te stellen of programma’s de effecten opleveren die ze nastreven, om te begrijpen of en waarom deze programma’s werken, om vast te stellen in hoeverre veranderingen zijn toe te schrijven aan de inzet van het programma en ook om vast te stellen of de gelden op een goede manier worden besteed. Op dit terrein is er natuurlijk een enorme hoeveelheid literatuur en enkele uitgaven geven ons hiervan een goed en up-to-date overzicht2. Experimentele studies kunnen natuurlijk goede impactstudies zijn, met sterke punten en beperkingen. Maar er zijn ook aanvullende methodes die in quasi-experimentele of observationele studies kunnen worden toegepast. Zo zijn er panel datamethodes die gebruikt kunnen worden, regressie discontinu?teit methodes en instrumentele variabelen methodes. Daarnaast zijn er verschillende matchingsmethodes die in impactstudies worden gebruikt. Hier stellen we zo’n matchingsmethode voor die goed gebruikt kan worden in verschillende soorten impactstudies en laten we zien hoe deze uitgevoerd kan worden.\nPropensity Score Matching als optie\nPropensity Score Matching is een statische techniek waarin een individu (of andere eenheid) die behandeld wordt of die ergens aan meedoet, wordt gematcht met een of meer respondenten uit de controlegroep op basis van de propensity score. Deze matchingstechniek versterkt causale argumenten in quasi-experimentele en observationele studies omdat ze selectie-bias reduceert. In dit artikel concentreren we ons op hoe we propensity score matching uitvoeren via een voorbeeld uit het onderwijsveld. Het doel van dit artikel is om informatie te geven zodat het gebruikt kan worden door onderzoekers en mensen die evaluatiestudies uitvoeren3 4.\nStap voor stap\nPropensity score matching is een statistische techniek waarin een geval uit de behandelgroep wordt gematcht met een of meer gevallen uit de controlegroep gebaseerd op ieders propensity score. Elders zijn er veel uitleg en onderbouwingen te vinden voor propensity score (Adelson, 20135; Holland, 1986 6; Rubin, 2005 7; Rudner & Peyton, 20068; Shadish, Cook, & Campbell, 20029; Stone & Tang, 201310 ). In dit artikel concentreren we ons op de vraag hoe we propensity score matching moeten uitvoeren door een voorbeeld uit het onderwijsveld te geven. In dit document willen we vooral een stap-voor-stap voorbeeld geven van de uitvoering van propensity score matching in R. We gebruiken hierbij het MatchIt-pakket met ‘nearest-neighbor 1-to-1 matching’. Terwijl er andere software is dan R voor het uitvoeren van propensity score matching, hebben we voor R gekozen omdat het open-source software is en omdat het breed gebruikt wordt door data-wetenschappers binnen verschillende disciplines. Het doel van dit artikel is om informatie te verschaffen zodat propensity score matching binnen het bereik komt van onderzoek en evaluatie.\nInformatie over de gebruikte dataset\nData van een observationele studie door Falbe (2014) worden hier gebruikt om te illustreren hoe propensity score matching werkt. In die studie gebruikt Falbe algemeen toegankelijke schooldata van verschillende staten om te onderzoeken of een bepaalde interventie een voorspeller was van succes in de resultaten op lezen of rekenen/wiskunde wanneer er voor de schoolgrootte worden gecontrolleerd (tot), percentage studenten van minderheden (min) en het percentage studenten dat een vrije of in prijs gereduceerde lunch ontvangt (dis)11. Voor het voorbeeld hier gebruiken we alleen Falbe’s schooldata van de staat New York. In de New York data set, zijn er 25 stw scholen en 560 niet-stw scholen (stw refereert hier naar een bepaalde interventie). Als matchingsvariabelen koos Falbe schoolgrootte, percentage minderheids studenten en het percentage studenten dat een vrije of een in prijs gereduceerde lunch ontvangt. De reden dat ze voor deze matchings variabelen koos, was dat vorig onderzoek had aangetoond dat deze variabelen samenhingen met academische resultaten. Door op deze variabelen te matchen, kon ze, dat was haar doel, selectie-bias reduceren tussen experimentele (“treated”, bijvoorbeeld stw) en controle groep (“control”, bijvoorbeeld niet-stw-scholen). Let op dat ondanks Falbe in haar studie naar correlaties zocht en niet-experimenteel van opzet was, we hier de termen behandeling (treated) en controle (control) gebruiken omdat deze termen worden gebruikt in het programma en de output van het MatchIt pakket.\nDe stappen in het uitvoeren van Propensity Score Matching in R\nStep 1 Installeer R.\nR is een gratis toegankelijk statistisch pakket dat via URL van het ‘R Core Team’ (2014) kan worden binnengehaald. Zie de referentie12. Let op de specifieke informatie voor het binnenhalen en het openen van de software. R is beschikbaar voor Windows, Mac OS X en Linux systemen. Dit artikel is op de R versie 3.3.2.\nStep 2 Installeer en laad het MatchIt pakket.\nMatchIt is een R-pakket dat R-gebruikers goed in staat stelt om met propensity score matching te werken; specifieke informatie over het MatchIt pakket kan worden gevonden in de artikelen van Ho, Kosoke, King, and Stuart (2007a13, 2007b14, 201115, 201316). Om het MatchIt pakket te gebruiken, moet je dat pakket eerst installeren en laden. Zoals met alle R-pakketten hoef je dat installeren alleen de eerste keer te doen. Bij MatchIt dus ook; echter je moet het pakket wel steeds laden als je de R-software gebruikt. Om een pakket te installeren, open je R en selecteer je het ‘Packages’ -menu. Kies dan: ‘Install Package(s)’. Een pop-up beeld van CRAN Mirror verschijnt. Kies het pakket dat je wilt uit een lijst en klik OK. Een ander beeld (“Packages” genaamd) verschijnt. Ga naar beneden en selecteer MatchIt en klik weer op OK. Het pakket wordt meteen binnengehaald. Als je vervolgens het MatchIt pakket wilt laden, kies dan voor ‘Load Package’ van het dropdown menu onder het ‘Package’-menu in R. Een beeld verschijnt met beschikbare pakketten waar jij jouw pakket uit moet selecteren. Kies uit de lijst voor MatchIt en klik op OK. Het MatchIt pakket wordt nu geladen.\n\n\n\nStep 3 Klaarmaken en laden van de data.\nOm propensity score matching uit te voeren heb je een dataset nodig die bestaat uit gevallen die in rijen staan (bv. individuen) en variabelen die in kolommen staan. Je hebt een groepvariabele nodig en een of meer matchingsvariabelen. De groepvariabele is de variabele die specificeert tot welke groep een geval (een individu of een school) behoort (bv. experimenteel of controle). De matchingsvariabelen zijn de variabelen waar je de groepen op wilt gelijk zetten. In de dataset van Falbe (2014) is bijvoorbeeld de stw-variabele de groepvariabele. Deze variabele geeft aan of een bepaald geval (een school hier) de interventie heeft ontvangen (1) of niet (0). De andere variabelen tot (schoolgrootte), min (percentage studenten met een andere etnische achtergrond in de school) en dis (percentage van de studenten dat een vrije of in prijs gereduceerde lunch ontvangt) zijn de matchingsvariabelen. De dataset die we hiervoor gebruiken kan worden binnengehaald via Randolph (2014a)17. In jouw eigen datasets moet je er wel zeker van zijn dat er geen missende data zijn want dan kan R de analyse niet uitvoeren. Ofschoon er verschillende functies zijn om Excel, SPSS of andere dataformaten in R te importeren, vinden wij het het makkelijkste om de data op te slaan als een .csv-file voordat je de data in R binnehaalt. Als je de file in Excel hebt opgeslagen, heb je de optie om het als een .csv-file op te slaan. Als je de file opslaat, let dan wel op waar deze wordt opgeslagen. Nu moet je de lokatie van jouw file die tussen haakjes staat in de eerste regel van de R-code (zie hieronder) aan jouw lokatie aanpassen. Let op dat in de eerste regel voorwaartste schuine streepjes staan eerder dan terugwaartse om jouw file lokatie te specificeren. Het voorbeeld hieronder is een filelokatie in Windows, waar de data staan in de file die newyork.csv heet in de folder R/PSM genaamd op de C-schijf. De eerste regel leest de data van jouw computer en hernoemt het als mydata. De tweede regel maakt deze data beschikbaar voor deze R sessie. De derde regel van de code hieronder print de variabelenamen en de eerste gevallen in de dataset. We doen dat alleen om de data te controleren en te begrijpen waar elke kolom voorstaat.\n\n\n                               school  tot  min  dis stw\n1           SKANEATELES MIDDLE SCHOOL  380 0.03 0.00   0\n2        MARCUS WHITMAN MIDDLE SCHOOL  276 0.04 0.00   0\n3       BLIND BROOK-RYE MIDDLE SCHOOL  376 0.09 0.00   0\n4            BRONXVILLE MIDDLE SCHOOL  404 0.11 0.00   0\n5            BRIARCLIFF MIDDLE SCHOOL  374 0.12 0.00   0\n6                   RYE MIDDLE SCHOOL  754 0.17 0.00   0\n7           EASTCHESTER MIDDLE SCHOOL  704 0.26 0.00   0\n8             SCARSDALE MIDDLE SCHOOL 1172 0.27 0.00   0\n9  EDGEMONT JUNIOR-SENIOR HIGH SCHOOL  920 0.42 0.00   0\n10        SEVEN BRIDGES MIDDLE SCHOOL  619 0.17 0.01   0\n\nNa de code zie je hierboven de resultaten staan van wat de R-code oplevert. Het laat de eerste tien gevallen in de dataset zien en wat de kolommen inhouden. Het laat zien dat de kolommen van links naar rechts de gevalnummer zijn (een uniek id-nummer voor elke school), school (de naam van elke school), tot (het totale aantal studenten in op die school), min (het percentage van minderheidsstudenten op de school), dis (precentage studenten dat een vrije of in prijs gereduceerde lunch) en stw (of de school de interventie ontvangt met een (1) Scholen die deze wel of (0) niet ontvangen).\nStep 4. Matching uitvoeren en de resultaten evalueren.\nDe volgende stap is nodig om te laten zien hoe de matching wordt uitgevoerd en de resultaten vervolgens kunnen worden geevalueerd. De eerste regel in de code die hieronder te zien is, voert de matching uit en de groepsvariabele is stw en de variabelen waarop wordt gematcht zijn tot, min en dis. Je moet deze variabelen in de code eventueel later vervangen door de variabelenamen die in jouw eigen dataset voorkomen. In de methode hieronder wordt de ‘nearest neighbor-methode’ gebruikt. Het ratio-commanda geeft aan dat er een ??n op ??n matching wordt toegepast en dat betekent dat elk geval van de behandeling wordt gematcht aan een geval uit de controlegroep. Het getal waarin gematcht wordt kan toenemen; meestal wordt hier een getal tussen 1 en 5 gebruikt.\nHieronder zie je de code om de PSM uit te voeren en de resultaten ervan.\n\n\nCall:\nmatchit(formula = stw ~ tot + min + dis, data = mydata, method = \"nearest\", \n    ratio = 1)\n\nSummary of balance for all data:\n         Means Treated Means Control SD Control Mean Diff  eQQ Med\ndistance        0.0943        0.0405     0.0503    0.0537   0.0559\ntot           832.6400      568.8998   333.6746  263.7402 300.0000\nmin             0.1664        0.2767     0.3011   -0.1103   0.0200\ndis             0.1840        0.4079     0.2500   -0.2239   0.2500\n         eQQ Mean   eQQ Max\ndistance   0.0599    0.1875\ntot      310.9600 1124.0000\nmin        0.1276    0.6300\ndis        0.2276    0.4900\n\n\nSummary of balance for matched data:\n         Means Treated Means Control SD Control Mean Diff eQQ Med\ndistance        0.0943        0.0942     0.0513    0.0001  0.0004\ntot           832.6400      830.6400   315.6859    2.0000 99.0000\nmin             0.1664        0.1772     0.1330   -0.0108  0.0200\ndis             0.1840        0.1808     0.1361    0.0032  0.0100\n         eQQ Mean  eQQ Max\ndistance   0.0005   0.0024\ntot      115.8400 247.0000\nmin        0.0260   0.1500\ndis        0.0256   0.0900\n\nPercent Balance Improvement:\n         Mean Diff. eQQ Med eQQ Mean eQQ Max\ndistance    99.8180 99.3674  99.1609 98.7414\ntot         99.2417 67.0000  62.7476 78.0249\nmin         90.2061  0.0000  79.6238 76.1905\ndis         98.5707 96.0000  88.7522 81.6327\n\nSample sizes:\n          Control Treated\nAll           559      25\nMatched        25      25\nUnmatched     534       0\nDiscarded       0       0\n\n\n[1] \"To identify the units, use first mouse button; to stop, use second.\"\n\ninteger(0)\n\n\nEr kunnen ook andere methodes gebruikt worden; een korte beschrijving hiervan vind je in de lijst hieronder. We moedigen MatchIt-gebruikers aan om verschillende methodes te gebruiken en te zien welke methode het beste werkt voor een bepaalde dataset. In dit geval probeerden we alle matchingmethodes die er tegenwoordig gebruikt kunnen worden in MatchIt en kozen voor de ’nearest neighbor-methode’omdat dit leidde tot de laagste gemiddelde verschillen tussen groepen. Sommmige andere methodes zijn\n- Exact Matching - Elke eenheid uit de treated groep heeft precies dezelfde waarden aan die uit de controle groep op elke covariaat. Als er veel covariaten zijn en/of covariaten die een groot aantal waarden aan kunnen nemen, dan kan het zijn dat Exact Matching niet mogelijk is (method = “exact”).\n- Subclassificatie - Deze techniek breekt de data als het ware in enkele subklassen zodat de verdelingen van de covariaten hetzelfde zijn binnen iedere subklasse (method = “subclass”).\n- Nearest Neighbor - Deze techniek matcht een treated-eenheid aan die van de controlegroep in termen van afstand die in een logit-waarde wordt uitgedrukt (method = “nearest”).\n- Optimal Matching - Deze techniek richt zich op het minimaliseren van de gemiddelde absolute afstand over alle gematchte paren (method = “optimal”). Deze methode van matching vraagt een bepaald pakket.\n- Genetic Matching - dit gebruikt een intensief genetisch zoekalgoritme om de treatment aan de controle eenheid te koppelen (method = “genetic”). - Coarsened Exact Matching techniek matcht op een covariaat terwijl de balans op de andere covariaten wordt vastgehouden. Het werkt “goed voor multicategoriale behandelingen, wanneer er met blokken in experimentele designs wordt gewerkt en bij het evalueren van extreme counterfactuals” (Ho, Kosuke, King, & Stuart, 2011, p.1218) (method = “cem”).\nZie documentatie over MatchIt voor verdere detailt over de matchingsprocedure zoals hierboven is uitgelegd. (Ho, Kosuke, King, & Stewart, 2007a19, 2007b20, 201121, 201322).\nDe resultaten van de matching worden opgeslagen in een variabele die heet m.out. De tweede regel geeft een samenvatting van de matching. De derde en vierde regel produceren plots en histogrammen. De resultaten laten zien dat de matching bijzonder goed werkt voor deze dataset. In de samenvatting lezen we dat voor de matching het gemiddel aantal studenten (tot) in de treatment groep van scholen 263.74 studenten minder was dan in de controle (de niet-stw) scholen. De treated-scholen had 11% minder minderheidsstudenten (min) en 22% procent minder studenten in armoede (dis) dan in de controlescholen. Na de matching echter zijn deze verschillen dramatisch teruggebracht zoals we in de samenvatting kunnen lezen. Het gemiddelde verschil in aantallen studenten tussen de treated en de controlescholen is tot 2 teruggebracht; het was 263 voor de matching. Het percentage verschil in minderheidsstudenten tussen de treated en controle scholen is nog slechts 1%; het was 11% voor de matching. Tenslotte, het gemiddelde verschil tussen treated en controle scholen in termen van percentage achterstandsstudenten is tot 3/10 procent teruggebracht; het was 22% voor de matching.\nSamengevat, de treated en controle scholen zijn na de matching vrijwel hetzelfde geworden wat betreft het aantal studenten, het percentage minderheidsstudenten en het percentage studenten dat een vrij en in prijs verlaagde lunch ontvangt. Voor de matching waren de treated scholen gemiddeld groter, hadden ze minder minderheidsstudenten en minder achterstandsstudenten dan de controle scholen. De kolommen rechts in de samenvatting laten de mediaan, het gemiddelde en het maximum quartiel zien tussen de treated en de controledata; kleinere QQ-waarden laten een betere matching zien. Het valt op dat de QQ-waarden na matching kleiner zijn dan voor de matching.\nMet de derde en vierde regel van de code worden jitterplots en histogrammen gemaakt om de kwaliteit van de matching aan te tonen. In een jitterplot representeert elke cirkel een propensityscore van een geval. Bovenaan zie je dat er geen niet-gematchte treatment eenheden zijn. De middelste grafieken laten de sterke match zien tussen de gematchte treatment en controleeenheden. Onderaan zien je de niet gematchte controleeenheden die in de analyses verder niet gebruikt zullen worden.\nDaaronder zie je de histogrammen voor en na de matching. De histogrammen voor de matching, links, verschillen in sterke mate. De histogrammen na de matching, rechts, zijn voor een groot deel hetzelfde. Samengevat kunnen we stellen dat zowel de numerieke als de visuele data laten zien dat de matching succesvol was.\nStep 5. Exporteren van een datafile om vervolgens de analyses te kunnen uitvoeren.\nAls de matching een keer is afgerond, wil je een dataset hebben die bestaat uit enkel gematchte gevallen waarmee je de statistische analyses kunt doen die je voor ogen hebt. Hieronder zie je de code staan waarmee je in de eerste regel een R-dataset maakt met gematchte behandel en controlegevallen (bijvoorbeeld hier zitten de meer dan 500 controlgevallen niet meer in die niet zijn gematcht.) De tweede regel zorgt er voor dat de gematchte data in een .csv-file worden omgezet die verder in R kunnen worden geanalyseerd of eenvoudig kunnen worden omgezet naar een ander statistisch software-pakket; in dit geval is de output dataset opgeslagen als een file die newyork_nearest1 wordt genoemd in een specifieke folder (R/PSM in dit geval) op de C-schijf. (De dataset voor de New York data kan worden gedownload van Randolph (2014b))23.\n\n\n\nWat vervolgens de analyse betreft was Falbe (2014) ge?nteresseerd in de vraag of stw-scholen het beter doen dan niet-stw schools ten aanzien van academische resultaten24. Om die analyse uit te voeren, voegde ze de lees-en rekenscores toe aan de gematchte dataset en voorspelde zo de academische resultaten op basis van de volgende variabelen: of een school een stw-school was of niet, de schoolgrootte, het percentage minderheidstudenten in een school en het percentage studenten dat een vrije of in prijs verlaagde lunche ontvangt. Het blijkt dat matching in dit geval belangrijk was. Zonder matching hadden stw-scholen statistisch gezien betere resultaten dan wanneer ze niet gematcht waren. Fable vond geen statistisch gezien significant verschil tussen stw niet stw-scholen op de academische resultaten. Zonder propensity score matching zou Falbe een hele andere conclusie hebben getrokken ten aanzien van de effectiviteit van de interventie, die dan eigenlijk het resultaat is van selectie-bias. Het is duidelijk dat propensity score matching een bruikbaar gereedschap is om selectie-bias te reduceren en causale conclusies te trekken. We hopen dat door deze stap-voor-stap gids een brede groep onderzoekers en evaluatiemedewerkers propensity score matching aan hun repertoire van data analysetechnieken kan toevoegen en deze techniek in hun werk kan gebruiken.\nReferenties\nAdelson, J. L. (2013). Educational research with real data: Reducing selection bias with propensity score analysis. Practical Assessment Research & Evaluation, 18(15). Retrieved from http://pareonline.net/getvn.asp?v=18&n=15\nFalbe, K. (2014). The relationship between Schools to Watch ? designation and ac study of Colorado, New York, Ohio, and Virginia (Doctoral dissertation). Available from Proquest Dissertations and Theses database. (UMI No. 3581272)\nGertler, P.J., Martinez, S., Prenard, P., Rawlings, L.B. & Vermeersch, C.M. (2011). Impact Evaluation in Practice. Washington D.C.: The World Bank\nHo, D., Kosuke, I., King, G., & Stuart, E. (2007a). MatchIt: Nonparametric preprocessing for parametric causal inference. Political Analysis, 15(3), 199\nHo, D., Kosuke, I., King, G., & Stuart, E. (2007b). Matching as nonparametric preprocessing for reducing model dependence in parametric causal inference. Journal of Statistical Software. Retrieved from http://gking.harvard.edu/matchit/\nHo, D., Kosuke, I. King, G., & Stuart, E. (2011). MatchIt: Nonparametric preprocessing inference [software documentation]. Retrieved from http://gking.harvard.edu/matchit\nHo, D., Kosuke, I., King, G., & Stuart, E. (2013). MatchIt: Nonparametric preprocessing for parametric causal inference [software]. Retrived from http://gking.harvard.edu/matchit\nHolland, P. W. (1986). Statistics and causal inference. Journal of the American Statistical Association, 81(396), 945-960.\nKhandker, S.R., Koolwal, G.B. & Samad, H.A. (2010). Handbook on Impactevaluation. Quantative Methods and Practices. Washington D.C: The World Bank.\nMurnane, R.J. & Willet, J.B.(2011). Methods Matter. Improving Causal Inference in Educational and Social Science Research. New York: Oxford University.\nRandolph, J. J. (2014a). New York educational data set example before matching. Retrieved from http://justusrandolph.net/psm/newyork.csv\nRandolph, J. J. (2014b). New York educational data set example after matching. Retrieved from http://justusrandolph.net/psm/newyork_nearest100.c sv\nR Core Team (2014). R: A language and environment for statistical computing. (3.0.3 ) [Computer software]. Vienna, Austria: Foundation for Statistical Computing. Retrieved from http://www.R-project.org/.\nRubin D. B. (2005). Causal inference using potential outcomes: Design, modeling, decisions. Journal of the American Statistical Association, 100(496), 322-331.\nRudner, L. M., & Peyton, J. (2006). Consider propensity scores to compare treatments. Practical Assessment Research & Evaluation, 11(9). Retrieved from: http://pareonline.net/getvn.asp?v=11&n=9\nShadish, W. R., Cook, T. D., & Campbell, D. T. (2002). Experimental and quasi-experimental designs for generalized causal inference. Boston, MA: Houghton Mifflin.\nStone, C. A. & Tang, Y. (2013). Comparing propensity score methods in balancing covariates and recovering impact in small sample educational program evaluations.\nNote: A previous version of this paper was delivered at the 2014 annual meeting of Mercer University Atlanta Research Conference, Atlanta, GA.\nCitation:\nRandolph, Justus J., Falbe, Kristina, Manuel, Austin Kureethara, & Balloun, Joseph L. (2014). A Step-byStep Guide to Propensity Score Matching in R. Practical Assessment, Research & Evaluation, 19(18). Available online: http://pareonline.net/getvn.asp?v=19&n=18\nCorresponding Author:\nJustus J. Randolph Tift College of Education Mercer University 3001 Mercer University Dr. Atlanta, GA 30341 randolph_jj@mercer.edu\nKhandker, S.R., Koolwal, G.B. & Samad, H.A. (2010). Handbook on Impactevaluation. Quantative Methods and Practices. Washington D.C: The World Bank; Gertler, P.J., Martinez, S., Prenard, P., Rawlings, L.B. & Vermeersch, C.M. (2011). Impact Evaluation in Practice. Washington D.C.: The World Bank; Murnane, R.J. & Willet, J.B.(2011). Methods Matter. Improving Causal Inference in Educational and Social Science Research. New York: Oxford University↩\nKhandker, S.R., Koolwal, G.B. & Samad, H.A. (2010). Handbook on Impactevaluation. Quantative Methods and Practices. Washington D.C: The World Bank; Gertler, P.J., Martinez, S., Prenard, P., Rawlings, L.B. & Vermeersch, C.M. (2011). Impact Evaluation in Practice. Washington D.C.: The World Bank; Murnane, R.J. & Willet, J.B.(2011). Methods Matter. Improving Causal Inference in Educational and Social Science Research. New York: Oxford University↩\nAlles gebaseerd op: Randolph, Justus J., Falbe, Kristina, Manuel, Austin Kureethara, & Balloun, Joseph L. (2014). A Step-byStep Guide to Propensity Score Matching in R. Practical Assessment, Research & Evaluation, 19(18). Available online: http://pareonline.net/getvn.asp?v=19&n=18 ↩\nHet correspondentieadres van hem is: Justus J. Randolph Tift College of Education Mercer University 3001 Mercer University Dr. Atlanta, GA 30341 randolph_jj@mercer.edu↩\nEducational research with real data: Reducing selection bias with propensity score analysis. Practical Assessment Research & Evaluation, 18(15). Retrieved from http://pareonline.net/getvn.asp?v=18&n=15 ↩\nHolland, P. W. (1986). Statistics and causal inference. Journal of the American Statistical Association, 81(396), 945-960. ↩\nRubin D. B. (2005). Causal inference using potential outcomes: Design, modeling, decisions. Journal of the American Statistical Association, 100(496), 322-331. ↩\nRudner, L. M., & Peyton, J. (2006). Consider propensity scores to compare treatments. Practical Assessment Research & Evaluation, 11(9). Retrieved from: http://pareonline.net/getvn.asp?v=11&n=9 ↩\nShadish, W. R., Cook, T. D., & Campbell, D. T. (2002). Experimental and quasi-experimental designs for generalized causal inference. Boston, MA: Houghton Mifflin. ↩\nStone, C. A. & Tang, Y. (2013). Comparing propensity score methods in balancing covariates and recovering impact in small sample educational program evaluations. ↩\nFalbe, K. (2014). The relationship between Schools to Watch ? designation and ac study of Colorado, New York, Ohio, and Virginia (Doctoral dissertation). Available from Proquest Dissertations and Theses database. (UMI No. 3581272)↩\nR Core Team (2014). R: A language and environment for statistical computing. (3.0.3 ) [Computer software]. Vienna, Austria: Foundation for Statistical Computing. Retrieved from http://www.R-project.org/. ↩\nHo, D., Kosuke, I., King, G., & Stuart, E. (2007a). MatchIt: Nonparametric preprocessing for parametric causal inference. Political Analysis, 15(3) , 199 ↩\nHo, D., Kosuke, I., King, G., & Stuart, E. (2007b). Matching as nonparametric preprocessing for reducing model dependence in parametric causal inference. Journal of Statistical Software. Retrieved from http://gking.harvard.edu/matchit/ ↩\nHo, D., Kosuke, I. King, G., & Stuart, E. (2011). MatchIt: Nonparametric preprocessing inference [software documentation]. Retrieved from http://gking.harvard.edu/matchit↩\nHo, D., Kosuke, I., King, G., & Stuart, E. (2013). MatchIt: Nonparametric preprocessing for parametric causal inference [software]. Retrived from http://gking.harvard.edu/matchit ↩\nRandolph, J. J. (2014a). New York educational data set example before matching. Retrieved from http://justusrandolph.net/psm/newyork.csv ↩\nHo, D., Kosuke, I. King, G., & Stuart, E. (2011). MatchIt: Nonparametric preprocessing inference [software documentation]. Retrieved from http://gking.harvard.edu/matchit↩\nHo, D., Kosuke, I., King, G., & Stuart, E. (2007a). MatchIt: Nonparametric preprocessing for parametric causal inference. Political Analysis, 15(3) , 199 ↩\nHo, D., Kosuke, I., King, G., & Stuart, E. (2007b). Matching as nonparametric preprocessing for reducing model dependence in parametric causal inference. Journal of Statistical Software. Retrieved from http://gking.harvard.edu/matchit/ ↩\nHo, D., Kosuke, I. King, G., & Stuart, E. (2011). MatchIt: Nonparametric preprocessing inference [software documentation]. Retrieved from http://gking.harvard.edu/matchit↩\nHo, D., Kosuke, I., King, G., & Stuart, E. (2013). MatchIt: Nonparametric preprocessing for parametric causal inference [software]. Retrived from http://gking.harvard.edu/matchit ↩\nRandolph, J. J. (2014b). New York educational data set example after matching. Retrieved from http://justusrandolph.net/psm/newyork_nearest100.csv↩\nFalbe, K. (2014). The relationship between Schools to Watch ? designation and ac study of Colorado, New York, Ohio, and Virginia (Doctoral dissertation). Available from Proquest Dissertations and Theses database. (UMI No. 3581272). ↩\n",
    "preview": "posts/2018-11-14-psm/psm_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2020-03-13T23:28:57+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-11-14-tufte/",
    "title": "Tufte",
    "description": "De Tufte-stijl is een stijl die Edward Tufte gebruikt in zijn boeken en handouts. Tufte's stijl is bekend vanwege zijn veelvuldig gebruik van opmerkingen aan de zijkant (sidenotes), strakke integratie van zijn grafieken met tekst en zijn duidelijk gezette typografie. Deze stijl is geimplementeerd in repectievelijk LaTeX en HTML/CSS^[Zie Github repositories [tufte-latex](https://github.com/tufte-latex/tufte-latex) en [tufte-css](https://github.com/edwardtufte/tufte-css)], respectively. Beide implementaties zitten nu ook in het [**tufte** pakket](https://github.com/rstudio/tufte). Als je een LaTeX/PDF output wilt, gebruik dan `tufte_handout` format voor handouts en `tufte_book` voor boeken. Voor HTML output, gebruik je `tufte_html`. Deze formatten kunnen worden gespecificeerd in de YAML metadata aan het begin van een R Markdown-document (zie het voorbeeld hieronder), of overgebracht via de `rmarkdown::render()` functie. Zie @R-rmarkdown voor meer informatie over **rmarkdown**.",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2017-09-14",
    "categories": [],
    "contents": "\nHier vind je een document over het werken in de Tufte stijl\nZie Github repositories tufte-latex en tufte-css↩\n",
    "preview": {},
    "last_modified": "2020-03-13T23:28:57+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-11-14-op-weg-naar-infografieken/",
    "title": "Op weg naar infografieken",
    "description": "Hier gaat het om een korte handleiding voor R_gebruikers die omwille van de leesbaarheid en esthetiek hun figuren in het populaire grafische design programma Illustrator willen 'oppoetsen'. Als het op visualisatie aankomt blijven de meeste R-gebruikers binnen dit programma werken. Dat is natuurlijk prima als het gaat om figuren die de analyse moeten ondersteunen en jij degene bent die er alleen naar moet kijken. Dan hoef je ook niets over de context te vermelden, niets verder uit te leggen of ervoor te zorgen dat het er allemaal mooi uitziet. Het doel dan is vooral snel figuren maken zodat je gevoel bij jouw data krijgt. R biedt je ook heel veel mogelijkheden, ook voor goede visualisatie. Echter, als het gaat om het maken van figuren die voor een breder publiek toegankelijk en leesbaar zijn en die zelf een verhaal moeten vertellen, kan het wel eens bruikbaarder en efficiënter zijn om dit R-figuur als PDF op te slaan en aanpassingen door te voeren in een vector georienteerd programma zoals Adobe Illustrator (https://www.adobe.com/nl/) of zijn open-source alternatief Inkscape (https://www.inkscape.org/nl/). Inkscape is vrij toegankelijk maar hier besteden wij enkel aandacht aan het bewerken in Adobe Illustrator.",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2017-07-14",
    "categories": [],
    "contents": "\nHier vind je een goed document\n\n\n",
    "preview": {},
    "last_modified": "2020-03-13T23:28:57+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-11-14-latex/",
    "title": "Latex",
    "description": "Introductie op Latex.",
    "author": [
      {
        "name": "Harrie Jonkman",
        "url": "https://www.harriejonkman.nl"
      }
    ],
    "date": "2017-04-14",
    "categories": [],
    "contents": "\nHier vind je een snelle introductie op de werking van Latex\n\n\n",
    "preview": {},
    "last_modified": "2020-03-13T23:28:57+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-11-14-visualisatie/",
    "title": "Visualisatie",
    "description": "Hoe kun je goed werken aan datavisualisatie met ggplot2 binnen R/RStudio",
    "author": [
      {
        "name": "Zev Ross, bewerkt door Harrie Jonkman",
        "url": "https://Harriejonkman.nl"
      }
    ],
    "date": "2017-02-11",
    "categories": [],
    "contents": "\nInleiding\nggplot is een R-pakket om data te onderzoeken en vooral om mooie en duidelijke figuren te maken. Deze figuren zien er grafisch fantastisch uit en kunnen op verschillende manieren worden aangepast. Er zijn verschillende tutorials en boeken beschikbaar die je uitleggen hoe het werkt. Hier volgen we de uitleg van Zev Ross.\nSnelle eerste blik\nBekijk de dataset eerst goed. In dit geval gaat het om een data uit Chicago over luchtvervuiling over een aantal jaren. Het jaartal zit erin, het aantal doden per dag, de temperatuur, dan twee uitkomstmaten, het tijdstip en de seizoenen. Het zijn deze data die we zichtbaar gaan maken met het pakket ggplot van R.\n\n\nlogical(0)\n\n     city       date death temp dewpoint      pm10        o3 time\n3654 chic 1997-01-01   137 36.0    37.50 13.052268  5.659256 3654\n3655 chic 1997-01-02   123 45.0    47.25 41.948600  5.525417 3655\n3656 chic 1997-01-03   127 40.0    38.00 27.041751  6.288548 3656\n3657 chic 1997-01-04   146 51.5    45.50 25.072573  7.537758 3657\n3658 chic 1997-01-05   102 27.0    11.25 15.343121 20.760798 3658\n3659 chic 1997-01-06   127 17.0     5.75  9.364655 14.940874 3659\n     season\n3654 winter\n3655 winter\n3656 winter\n3657 winter\n3658 winter\n3659 winter\n\nHet default-figuur in ggplot2\nHet standaardfiguur is de volgende waarbij je aangeeft hoe de dataset in elkaar zit, naar welke variabelen je kijkt en wat je wilt zien. Let goed op, je ziet dat ggplot met duidelijke lagen werkt waar steeds passende commando’s bij horen.\n\n\n\nWerken met de titel\nJe kunt er ook een titel aan toevoegen.\n\n\n\nVet en meer ruimte\nDe titel kun je ook vet maken en meer ruimte geven.\n\n\n\nBij een lange titel\nDe titel kun je ook meer ruimte geven via commando (lineheight).\n\n\n\nOp de assen werken\nJe kunt ook tekst aan de x en y as toevoegen o.a. door (labs() of bijvoorbeel xlab())\n\n\n\nVerwijderen van gegevens\nJe kunt ook gegevens op de y-as weghalen door labels (theme(), axis.ticks.y)\n\n\n\nGegevens verder aanpassen\nJe kunt de gegevens op de assen aanpassen, bv. op de x en roteren (axis.text.x)\n\n\n\nEen kleurtje geven\nDe tekst op x en y-as kun je ook nog een kleurtje geven door(theme(), axis.title.x)\n\n\n\nBeperken\nJe kunt de gegevens die je wilt laten zien, ook weer in bereik beperken (ylim).\n\n\n\nGestandardiseerd\nAls de assen hetzelfde moeten zijn, kan ook (coord_equal())\n\n\n\nLabels veranderen\nJe kunt ook de labels aanpassen (label=function(x){}) en de gegevens over de maanden toevoegen.\n\n\n\nLegenda\nDe legenda kan worden aangepast nu.\n\n\n\nTitel legenda\nJe kunt de titel van de legenda ‘uitzetten’ (legend.title)\n\n\n\nAanpassen stijl van de titel\nOok de stijl van de legenda titel kun je aanpassen (legend.title)\n\n\n\nTitel naam veranderen\nJe kunt ook de titelzelf veranderen (name)\n\n\n\nAchtergrondkleur aanpassen\nJe kunt ook de achtergrondkleur van de legenda aanpassen (legend.key)\n\n\n\nOf het symbool\nOf alleen het symbool in de legenda(guides(), guide_legend)\n\n\n\nDe temperatuur ipv punt\nJe kunt ook de temperatuur ipv een punt afdrukken (show_guide)\n\n\n\nAndere mogelijke aanpassingen\nEr zijn nog meer aanpassingen van de legenda mogelijk via (guides(), override.aes)\n\n\n\nAanpassen van de achtergrondkleur\nDe achtergrond is aan te passen (panel.background)\n\n\n\nOok de grid lijnen zijn aan te passen (panel.grid.major)\n\n\n\nBerperkte kleuraanpassingen\nNiet in het panel maar verder wel kleur aanpassen (plot.background)\n\n\n\nWerken met margins\nDe plot margin aanpassen(plot.margin)\nthe default\n\n\n\nWerken met thema’s\nJe kunt ook met thema’s werken, mooi en consistent, bijvoorbeeld hier eentje uit het blad ‘The Economist’.\nUse a new theme (theme_XX())\n\n\n\nElementen\nJe kunt ook de omvang van de elementen aanpassen via text elements (theme_set(), base_size)\n\n\n\nJe kunt ook een thema-stijl zelf ontwikkelen\n\n\nfunction (base_size = 11, base_family = \"\", base_line_size = base_size/22, \n    base_rect_size = base_size/22) \n{\n    half_line <- base_size/2\n    theme(line = element_line(colour = \"black\", size = base_line_size, \n        linetype = 1, lineend = \"butt\"), rect = element_rect(fill = \"white\", \n        colour = \"black\", size = base_rect_size, linetype = 1), \n        text = element_text(family = base_family, face = \"plain\", \n            colour = \"black\", size = base_size, lineheight = 0.9, \n            hjust = 0.5, vjust = 0.5, angle = 0, margin = margin(), \n            debug = FALSE), axis.line = element_blank(), axis.line.x = NULL, \n        axis.line.y = NULL, axis.text = element_text(size = rel(0.8), \n            colour = \"grey30\"), axis.text.x = element_text(margin = margin(t = 0.8 * \n            half_line/2), vjust = 1), axis.text.x.top = element_text(margin = margin(b = 0.8 * \n            half_line/2), vjust = 0), axis.text.y = element_text(margin = margin(r = 0.8 * \n            half_line/2), hjust = 1), axis.text.y.right = element_text(margin = margin(l = 0.8 * \n            half_line/2), hjust = 0), axis.ticks = element_line(colour = \"grey20\"), \n        axis.ticks.length = unit(half_line/2, \"pt\"), axis.title.x = element_text(margin = margin(t = half_line/2), \n            vjust = 1), axis.title.x.top = element_text(margin = margin(b = half_line/2), \n            vjust = 0), axis.title.y = element_text(angle = 90, \n            margin = margin(r = half_line/2), vjust = 1), axis.title.y.right = element_text(angle = -90, \n            margin = margin(l = half_line/2), vjust = 0), legend.background = element_rect(colour = NA), \n        legend.spacing = unit(2 * half_line, \"pt\"), legend.spacing.x = NULL, \n        legend.spacing.y = NULL, legend.margin = margin(half_line, \n            half_line, half_line, half_line), legend.key = element_rect(fill = \"grey95\", \n            colour = \"white\"), legend.key.size = unit(1.2, \"lines\"), \n        legend.key.height = NULL, legend.key.width = NULL, legend.text = element_text(size = rel(0.8)), \n        legend.text.align = NULL, legend.title = element_text(hjust = 0), \n        legend.title.align = NULL, legend.position = \"right\", \n        legend.direction = NULL, legend.justification = \"center\", \n        legend.box = NULL, legend.box.margin = margin(0, 0, 0, \n            0, \"cm\"), legend.box.background = element_blank(), \n        legend.box.spacing = unit(2 * half_line, \"pt\"), panel.background = element_rect(fill = \"grey92\", \n            colour = NA), panel.border = element_blank(), panel.grid = element_line(colour = \"white\"), \n        panel.grid.minor = element_line(size = rel(0.5)), panel.spacing = unit(half_line, \n            \"pt\"), panel.spacing.x = NULL, panel.spacing.y = NULL, \n        panel.ontop = FALSE, strip.background = element_rect(fill = \"grey85\", \n            colour = NA), strip.text = element_text(colour = \"grey10\", \n            size = rel(0.8), margin = margin(0.8 * half_line, \n                0.8 * half_line, 0.8 * half_line, 0.8 * half_line)), \n        strip.text.x = NULL, strip.text.y = element_text(angle = -90), \n        strip.placement = \"inside\", strip.placement.x = NULL, \n        strip.placement.y = NULL, strip.switch.pad.grid = unit(half_line/2, \n            \"pt\"), strip.switch.pad.wrap = unit(half_line/2, \n            \"pt\"), plot.background = element_rect(colour = \"white\"), \n        plot.title = element_text(size = rel(1.2), hjust = 0, \n            vjust = 1, margin = margin(b = half_line)), plot.subtitle = element_text(hjust = 0, \n            vjust = 1, margin = margin(b = half_line)), plot.caption = element_text(size = rel(0.8), \n            hjust = 1, vjust = 1, margin = margin(t = half_line)), \n        plot.tag = element_text(size = rel(1.2), hjust = 0.5, \n            vjust = 0.5), plot.tag.position = \"topleft\", plot.margin = margin(half_line, \n            half_line, half_line, half_line), complete = TRUE)\n}\n<bytecode: 0x000000001dc91dd0>\n<environment: namespace:ggplot2>\n\nfunction (base_size = 12, base_family = \"\") \n{\n  theme(\n    line = element_line(colour = \"black\", size = 0.5, linetype = 1, lineend = \"butt\"), \n    rect = element_rect(fill = \"white\", colour = \"black\", size = 0.5, linetype = 1), \n    text = element_text(family = base_family, face = \"plain\", colour = \"black\", size = base_size, hjust = 0.5, vjust = 0.5, angle = 0, lineheight = 0.9), \n    \n    axis.text = element_text(size = rel(0.8), colour = \"grey50\"), \n    strip.text = element_text(size = rel(0.8)), \n    axis.line = element_blank(), \n    axis.text.x = element_text(vjust = 1), \n    axis.text.y = element_text(hjust = 1), \n    axis.ticks = element_line(colour = \"grey50\"), \n    axis.title.x = element_text(), \n    axis.title.y = element_text(angle = 90), \n    axis.ticks.length = unit(0.15, \"cm\"), \n    axis.ticks.margin = unit(0.1, \"cm\"), \n    \n    legend.background = element_rect(colour = NA), \n    legend.margin = unit(0.2, \"cm\"), \n    legend.key = element_rect(fill = \"grey95\", colour = \"white\"), \n    legend.key.size = unit(1.2, \"lines\"), \n    legend.key.height = NULL, \n    legend.key.width = NULL, \n    legend.text = element_text(size = rel(0.8)), \n    legend.text.align = NULL, \n    legend.title = element_text(size = rel(0.8), face = \"bold\", hjust = 0), \n    legend.title.align = NULL, \n    legend.position = \"right\", \n    legend.direction = NULL, \n    legend.justification = \"center\", \n    legend.box = NULL, \n\n    panel.background = element_rect(fill = \"grey90\", colour = NA), \n    panel.border = element_blank(), \n    panel.grid.major = element_line(colour = \"white\"), \n    panel.grid.minor = element_line(colour = \"grey95\", size = 0.25), \n    panel.margin = unit(0.25, \"lines\"), \n    panel.margin.x = NULL, \n    panel.margin.y = NULL, \n\n    strip.background = element_rect(fill = \"grey80\", colour = NA), \n    strip.text.x = element_text(), \n    strip.text.y = element_text(angle = -90), \n    \n    plot.background = element_rect(colour = \"white\"), \n    plot.title = element_text(size = rel(1.2)), \n    plot.margin = unit(c(1, 1, 0.5, 0.5), \"lines\"), complete = TRUE)\n}\n\nWerken met kleur\nBij categoriale variabelen kun je de kleur zelf aanpassen (scale_color_manual)\n\n\n\nPalet\nJe kunt voor categoriale variabelen ook een ‘’inbouw-palette’ gebruiken (gebaseerd op colorbrewer2.org) (scale_color_brewer):\n\n\n\nKleur keus bij continue variabelen\nBij continue variabelen kun je ook een kleurkeus gebruiken met een schaal (scale_color_gradient(), scale_color_gradient2())\n\n\n\nEenzelfde resultaat\n\n\n\nWerken met annotatie\nDe annotatie-tekst kun je overal in het figuur kwijt top-rechts, top-links etc. (annotation_custom() and textGrob())\n\n\n\nWerken met coordinaten\nDe figuur omgedraaid (coord_flip())\n\n\n\nWerken met verschillende typen plots\nAlternatieven voor de box plot (geom_jitter() and geom_violin()) Box plots kunnen goed zijn, maar ook vervelen. Hier wat alternatieven, eerst - een box plot en dan de rest:\n\n\n\nEen lint creeren\nEerst een lint maken (geom_ribbon())\n\n\n\nEen filter\nDan een filter eroverheen.\n\n\n\nEven wat anders\nHier een correlation plot (geom_tile()) Opgepast, de namen zijn gesorteerd zodat de ordening in de uiteindelijk plot goed is.\n\n\n         death dewpoint    o3 pm10  temp\ndeath        1    -0.47 -0.24 0.00 -0.49\ndewpoint    NA     1.00  0.45 0.33  0.96\no3          NA       NA  1.00 0.21  0.53\npm10        NA       NA    NA 1.00  0.37\ntemp        NA       NA    NA   NA  1.00\n\n       Var1     Var2 value\n1     death    death  1.00\n6     death dewpoint -0.47\n7  dewpoint dewpoint  1.00\n11    death       o3 -0.24\n12 dewpoint       o3  0.45\n13       o3       o3  1.00\n\n\nWerken met vegen (‘smooths’)\nHier de standaard (stat_smooth())\n\n\n\nFormule aanpassen\nJe kunt ook de formule aanpassen(stat_smooth(formula=))\n\n\n\nLiniaire lijn\nJe kunt ook een liniaire lijn trekken (stat_smooth(method=“lm”))\n\n\n\nReferenties\nZev Ross. Beautiful plotting in R: a ggplopt2 cheatsheet Zev Ross\nHarvard University. Introduction to R Graphics wiht ggplot2 Harvard\nDawn Koffman. Introduction to ggplot2 Office of Population Research, Princeton University. Princeton\nRStudio. Data visualisation with ggplot2. Cheatsheet. RStudio\nWickham, H. (2010). ggplot2: Elegant graphics for data analysis(Use R!). Houston: Rice University.\nChang, W. (2013).RGraphics Cookbook. a practical recipes for visualizing data Sebastopol: O’Reilly Media.\n",
    "preview": "posts/2018-11-14-visualisatie/visualisatie_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2020-03-13T23:28:57+01:00",
    "input_file": {}
  }
]
